
@article{ ISI:000458858900001,
Author = {Zhao, Xiaoyan and Hu, Kai and Burns, Scott F. and Hu, Houtian},
Title = {{Classification and sudden departure mechanism of high-speed landslides
   caused by the 2008 Wenchuan earthquake}},
Journal = {{ENVIRONMENTAL EARTH SCIENCES}},
Year = {{2019}},
Volume = {{78}},
Number = {{5}},
Month = {{MAR}},
Abstract = {{The sudden departure mechanism of high-speed landslides has always
   constituted a popular yet difficult research topic. Therefore, although
   a systematic classification of high-speed landslides is necessary,
   particularly since different types of landslides exhibit different
   mechanisms, an associated methodology has yet to be established. This
   paper investigates an easily overlooked phenomenonlandslide-quake, which
   is defined as a local ground vibration triggered by the sudden rupture
   of the locked segment of a high-speed landslide that occurs immediately
   prior to the departure of the landslide. This new concept is employed as
   a foundation for a new classification. The characteristics and formation
   conditions of landslide-quake are discussed, based upon which the
   high-speed landslides that occurred following the 2008 Wenchuan
   earthquake are classified into three different types. Subsequently, the
   sudden departure mechanism of each type of landslide is analyzed,
   thereby providing a new method for research on the sudden departure
   mechanism of high-speed landslides.}},
DOI = {{10.1007/s12665-019-8083-9}},
Article-Number = {{125}},
ISSN = {{1866-6280}},
EISSN = {{1866-6299}},
Unique-ID = {{ISI:000458858900001}},
}

@article{ ISI:000458710800033,
Author = {Cheng, Yu-Ting and Lung, Shih-Chun Candice and Hwang, Jing-Shiang},
Title = {{New approach to identifying proper thresholds for a heat warning system
   using health risk increments}},
Journal = {{ENVIRONMENTAL RESEARCH}},
Year = {{2019}},
Volume = {{170}},
Pages = {{282-292}},
Month = {{MAR}},
Abstract = {{Background: A critical adaptation strategy for reducing heat-related
   health risk under climate change is to establish a heat warning system
   with a proper threshold that requires evaluation of heat-health
   relationships using empirical data.
   Objectives: This work presents a new approach to selecting proper
   health-based thresholds for a heat warning system which are different
   from thresholds of heat-health relationship.
   Methods: The proposed approach examined heat-health relationships
   through analyzing 15 years of health records with a modified generalized
   additive model (GAM), compared risk ratio increments (RRIs) of threshold
   candidates against a reference, assessed frequency of days above these
   candidates, and presented results graphically for easy communication.
   The candidate with the maximum RRI and proper occurring frequency is
   potentially the best threshold. Three heat indicators, including
   wet-bulb globe temperature (WBGT), temperature (T), and apparent
   temperature (AT), as well as three health outcomes, including all-cause
   mortality, heat related hospital admissions, and heat-related emergency
   visits were evaluated.
   Results: Risk ratios for all three health outcomes showed a consistent
   rising trend with increasing threshold candidates for all three heat
   indicators among different age and gender groups. WBGT had the most
   obvious increasing trend of RRIs with the three health outcomes. The
   maximum RRI was observed in heat-related emergency visits (242\%),
   followed by heat-related hospital admissions (73\%), and all-cause
   mortality (9\%). The RRIs assessed for the three health outcomes pointed
   to the same thresholds, 33.0 degrees C, 34.0 degrees C, and 37.5 degrees
   C for WBGT, T, and AT, respectively. The number of days above these
   thresholds and for warning to be issued ranged between 0 and 7 days
   during 2000-2014.
   Discussion: This study demonstrated a new approach to determining
   heat-warning thresholds with different heat indicators and health
   outcomes. The proposed approach provides a straightforward, feasible,
   and flexible scientific tool that assists the authorities around the
   world in selecting a proper threshold for a heat warning system.}},
DOI = {{10.1016/j.envres.2018.12.059}},
ISSN = {{0013-9351}},
EISSN = {{1096-0953}},
Unique-ID = {{ISI:000458710800033}},
}

@article{ ISI:000456639900021,
Author = {Luo, Huiying and Astitha, Marina and Hogrefe, Christian and Mathur,
   Rohit and Rao, S. Trivikrama},
Title = {{A new method for assessing the efficacy of emission control strategies}},
Journal = {{ATMOSPHERIC ENVIRONMENT}},
Year = {{2019}},
Volume = {{199}},
Pages = {{233-243}},
Month = {{FEB 15}},
Abstract = {{Regional-scale air quality models and observations at routine air
   quality monitoring sites are used to determine attainment/non-attainment
   of the ozone air quality standard in the United States. In current
   regulatory applications, a regional-scale air quality model is applied
   for a base year and a future year with reduced emissions using the same
   meteorological conditions as those in the base year. Because of the
   stochastic nature of the atmosphere, the same meteorological conditions
   would not prevail in the future year. Therefore, we use multi-decadal
   observations to develop a new method for estimating the confidence
   bounds for the future ozone design value (based on the 4th highest value
   in the daily maximum 8-hr ozone concentration time series, DM8HR) for
   each emission loading scenario along with the probability of the design
   value exceeding a given ozone threshold concentration at all monitoring
   sites in the contiguous United States. To this end, we spectrally
   decompose the observed DM8HR ozone time series covering the period from
   1981 to 2014 using the Kolmogorov-Zurbenko (KZ) filter and examine the
   variability in the relative strengths of the short-term variations
   (induced by synoptic-scale weather fluctuations; referred to as synoptic
   component, SY) and the long-term component (dictated by changes in
   emissions, seasonality and other slow-changing processes such as climate
   change; referred to as baseline component, BL). Results indicate that
   combining the projected change in the ozone baseline level with the
   adjusted synoptic forcing in historical ozone observations enables us to
   provide a probabilistic assessment of the efficacy of a selected
   emissions control strategy in complying with the ozone standard in
   future years. In addition, attainment demonstration is illustrated with
   a real-world application of the proposed methodology by using air
   quality model simulations, thereby helping build confidence in the use
   of regional-scale air quality models for supporting regulatory policies.}},
DOI = {{10.1016/j.atmosenv.2018.11.010}},
ISSN = {{1352-2310}},
EISSN = {{1873-2844}},
ORCID-Numbers = {{Hogrefe, Christian/0000-0003-3280-3513}},
Unique-ID = {{ISI:000456639900021}},
}

@article{ ISI:000447915400072,
Author = {Arismendi, Ivan and Groom, Jeremiah D.},
Title = {{A novel approach for examining downstream thermal responses of streams
   to contemporary forestry}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2019}},
Volume = {{651}},
Number = {{1}},
Pages = {{736-748}},
Month = {{FEB 15}},
Abstract = {{Temperature is a fundamental driver of aquatic environments. Changes in
   thermal regimes due to timber harvest may be detrimental for cold-water
   instream biota. Although it is understood that stream temperature may
   increase immediately below timber harvest operations, the understanding
   of how thermal responses propagate downstream is less clear. Here, we
   examine the effects of timber harvest on stream temperature pre-(2-3
   years) and post-harvest (5 years) at 16 sites (average annual streamflow
   rates <0.283 m(3) s(-1)) located in the Coast Range, Oregon, USA. At
   each site, an array of temperature sensors were deployed on the extremes
   of three consecutive reaches: an upstream unharvested reference reach, a
   treatment reach, and a downstream unharvested reach. We used several
   metrics to describe and evaluate changes over time and space focusing on
   the responses of downstream reaches. Primarily, we evaluated the
   differences over time in daily maximum temperature between the two
   sensors located at the downstream unharvested reach. Furthermore, using
   a statistical ordination technique, we examined the spatial and temporal
   variability of an array of sensors for daily maximum temperature.
   Moreover, we assessed distributional shifts (statistical moments) of
   hourly temperature differences between the two sensors at the downstream
   unharvested reach over time. Lastly, we used a combination of
   statistical moments and the ordination technique to provide an index
   that describes the behavior of site-specific thermal disturbance over
   time. We found that stream reaches responded differently to upstream
   timber harvest operations between pre and post-harvest summer seasons.
   In addition, we showed distinct patterns of longitudinal variability of
   temperature across sites and summer seasons with increases, decreases or
   mixed responses including no change downstream. Overall, the net change
   of daily maximum temperature at the downstream reach revealed that the
   highest difference occurred during the first and second year
   post-harvest and, in some cases, a distinctive shift in stream warming
   and cooling occurred between the day and the night. Observed temperature
   patterns in downstream reaches were most similar to the pre-harvest
   conditions at the fifth year post-harvest. Collectively, we offer a
   novel approach for assessing stream temperature regime change using
   multiple metrics that can improve our understanding of thermal effects
   downstream of timber harvest, taking in consideration idiosyncratic
   responses across sites and time. (C) 2018 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.scitotenv.2018.09.208}},
ISSN = {{0048-9697}},
EISSN = {{1879-1026}},
ORCID-Numbers = {{Groom, Jeremiah/0000-0002-0110-1036}},
Unique-ID = {{ISI:000447915400072}},
}

@article{ ISI:000459212800012,
Author = {Wang, Liang and Hua, Zulin and Wang, Yulin},
Title = {{Estimation of annual reference condition of Chlorophyll-a based on the
   segmental linear regression and power-law relationship in Taihu Lake}},
Journal = {{WATER SCIENCE AND TECHNOLOGY-WATER SUPPLY}},
Year = {{2019}},
Volume = {{19}},
Number = {{1}},
Pages = {{107-114}},
Month = {{FEB}},
Abstract = {{The reference conditions of Chlorophyll-a in lakes should be established
   in order to improve the water quality in these water bodies. A new
   method using segmental linear regression was developed to estimate the
   reference condition of Chlorophyll-a. The method can overcome the
   shortcomings of other methods, such as the quantile-selection-based
   method, which contains a certain level of subjectivity. The new method
   was used to estimate the annual reference condition of Chlorophyll-a in
   Taihu Lake. The log-log segmental regression results indicate that the
   distribution of specific volume (reciprocal of concentration) of
   Chlorophyll-a in Taihu Lake had a power-law tail. Both segmental
   regression and bootstrap approaches show two credible change points in
   the power-law relationship. This study's analysis shows that the value
   of 4.4 mg.m(-3) was an appropriate annual reference value of
   Chlorophyll-a in Taihu Lake. Thus, the method would be useful in
   determining the numerical reference conditions of Chlorophyll-a for
   other shallow lakes.}},
DOI = {{10.2166/ws.2018.060}},
ISSN = {{1606-9749}},
Unique-ID = {{ISI:000459212800012}},
}

@article{ ISI:000458223000002,
Author = {Orozco-Fuentes, S. and Griffiths, G. and Holmes, M. J. and Ettelaie, R.
   and Smith, J. and Baggaley, A. W. and Parker, N. G.},
Title = {{Early warning signals in plant disease outbreaks}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2019}},
Volume = {{393}},
Pages = {{12-19}},
Month = {{FEB 1}},
Abstract = {{Infectious disease outbreaks in plants threaten ecosystems, agricultural
   crops and food trade. Currently, several fungal diseases are affecting
   forests worldwide, posing a major risk to tree species, habitats and
   consequently ecosystem decay. Prediction and control of disease spread
   are difficult, mainly due to the complexity of the interaction between
   individual components involved. In this work, we introduce a
   lattice-based epidemic model coupled with a stochastic process that
   mimics, in a very simplified way, the interaction between the hosts and
   pathogen. We studied the disease spread by measuring the propagation
   velocity of the pathogen on the susceptible hosts. Our quantitative
   results indicate the occurrence of a critical transition between two
   stable phases: local confinement and an extended epiphytotic outbreak
   that depends on the density of the susceptible individuals. Quantitative
   predictions of epiphytotics are performed using the framework
   early-warning indicators for impending regime shifts, widely applied on
   dynamical systems. These signals forecast successfully the outcome of
   the critical shift between the two stable phases before the system
   enters the epiphytotic regime. Our study demonstrates that early-warning
   indicators could be useful for the prediction of forest disease
   epidemics through mathematical and computational models suited to more
   specific pathogen-host-environmental interactions. Our results may also
   be useful to identify a suitable planting density to slow down disease
   spread and in the future, design highly resilient forests.}},
DOI = {{10.1016/j.ecolmodel.2018.11.003}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
Unique-ID = {{ISI:000458223000002}},
}

@article{ ISI:000458978100001,
Author = {Kang, Ran and Zhang, Yiming and Huang, Qingqing and Meng, Junhua and
   Ding, Ruofan and Chang, Yunjian and Xiong, Lili and Guo, Zhiyun},
Title = {{EnhancerDB: a resource of transcriptional regulation in the context of
   enhancers}},
Journal = {{DATABASE-THE JOURNAL OF BIOLOGICAL DATABASES AND CURATION}},
Year = {{2019}},
Month = {{JAN 24}},
Abstract = {{Enhancers can act as cis-regulatory elements to control transcriptional
   regulation by recruiting DNA-binding transcription factors (TFs) in a
   tissue-specific manner. Recent studies show that enhancers regulate not
   only protein-coding genes but also microRNAs (miRNAs), and mutations
   within the TF binding sites (TFBSs) located on enhancers will cause a
   variety of diseases such as cancer. However, a comprehensive resource to
   integrate these regulation elements for revealing transcriptional
   regulations in the context of enhancers is not currently available.
   Here, we introduce EnhancerDB, a web-accessible database to provide a
   resource to browse and search regulatory relationships identified in
   this study, including 131 054 581 TF-enhancer, 17 059 enhancer-miRNAs,
   318 993 enhancer-genes, 4 639 558 TF-miRNAs, 1 059 695 TF-genes, 11 439
   394 enhancer-single-nucleotide polymorphisms (SNPs) and 23 334 genes
   associated with expression quantitative trait loci (eQTL) SNP and
   expression profile of TF/gene/miRNA across multiple human tissues/cell
   lines. We also developed a tool that further allows users to define
   tissue-specific enhancers by setting the threshold score of tissue
   specificity of enhancers. In addition, links to external resources are
   also available at EnhancerDB.}},
DOI = {{10.1093/database/bay141}},
Article-Number = {{bay141}},
ISSN = {{1758-0463}},
Unique-ID = {{ISI:000458978100001}},
}

@article{ ISI:000455578500002,
Author = {Dunn, Katherine A. and Kenney, Toby and Gu, Hong and Bielawski, Joseph
   P.},
Title = {{Improved inference of site-specific positive selection under a
   generalized parametric codon model when there are multinucleotide
   mutations and multiple nonsynonymous rates}},
Journal = {{BMC EVOLUTIONARY BIOLOGY}},
Year = {{2019}},
Volume = {{19}},
Month = {{JAN 14}},
Abstract = {{BackgroundAn excess of nonsynonymous substitutions, over neutrality, is
   considered evidence of positive Darwinian selection. Inference for
   proteins often relies on estimation of the nonsynonymous to synonymous
   ratio (=d(N)/d(S)) within a codon model. However, to ease computational
   difficulties, is typically estimated assuming an idealized substitution
   process where (i) all nonsynonymous substitutions have the same rate
   (regardless of impact on organism fitness) and (ii) instantaneous double
   and triple (DT) nucleotide mutations have zero probability (despite
   evidence that they can occur). It follows that estimates of represent an
   imperfect summary of the intensity of selection, and that tests based on
   the >1 threshold could be negatively impacted.ResultsWe developed a
   general-purpose parametric (GPP) modelling framework for codons. This
   novel approach allows specification of all possible instantaneous codon
   substitutions, including multiple nonsynonymous rates (MNRs) and
   instantaneous DT nucleotide changes. Existing codon models are specified
   as special cases of the GPP model. We use GPP models to implement
   likelihood ratio tests for >1 that accommodate MNRs and DT mutations.
   Through both simulation and real data analysis, we find that failure to
   model MNRs and DT mutations reduces power in some cases and inflates
   false positives in others. False positives under traditional M2a and M8
   models were very sensitive to DT changes. This was exacerbated by the
   choice of frequency parameterization (GY vs. MG), with rates sometimes
   >90\% under MG. By including MNRs and DT mutations, accuracy and power
   was greatly improved under the GPP framework. However, we also find that
   over-parameterized models can perform less well, and this can contribute
   to degraded performance of LRTs.ConclusionsWe suggest GPP models should
   be used alongside traditional codon models. Further, all codon models
   should be deployed within an experimental design that includes (i)
   assessing robustness to model assumptions, and (ii) investigation of
   non-standard behaviour of MLEs. As the goal of every analysis is to
   avoid false conclusions, more work is needed on model selection methods
   that consider both the increase in fit engendered by a model parameter
   and the degree to which that parameter is affected by un-modelled
   evolutionary processes.}},
DOI = {{10.1186/s12862-018-1326-7}},
Article-Number = {{22}},
ISSN = {{1471-2148}},
Unique-ID = {{ISI:000455578500002}},
}

@article{ ISI:000454957300025,
Author = {Fernandes, Milena B. and van Gils, Jos and Erftemeijer, Paul L. A. and
   Daly, Rob and Gonzalez, Dennis and Rouse, Karen},
Title = {{A novel approach to determining dynamic nitrogen thresholds for seagrass
   conservation}},
Journal = {{JOURNAL OF APPLIED ECOLOGY}},
Year = {{2019}},
Volume = {{56}},
Number = {{1}},
Pages = {{253-261}},
Month = {{JAN}},
Abstract = {{Seagrass decline is often related to eutrophication, with sudden and
   drastic losses attributed to tipping points in nutrient loads. The
   identification of these threshold loads is an important step in the
   sustainable management of nutrient discharges. In this study, a novel
   methodological approach is presented to simulate the spatial and
   temporal dynamics of coastal nitrogen loads and its relationship with
   seagrass loss. The analysis allows the identification of nitrogen
   thresholds associated with the loss of Posidonia and Amphibolis spp. in
   Adelaide, South Australia. The identified thresholds varied between 0.8
   and 1.1 t N km(-2) over 6 months and explained up to 95\% of all
   seagrass losses in the region. Posidonia spp. was predominantly lost
   nearshore, indicating a threshold for this species in the higher end of
   the values reported here. In contrast, Amphibolis spp. were selectively
   lost further offshore, suggesting a lower threshold for the onset of
   fragmentation and decline. Nitrogen pressure was concentrated over the
   wetter months of the year between late autumn and early spring. Pulsed
   pressure peaking in August likely affected thresholds by reducing
   resilience in the system. Synthesis and applications. This study
   derives, for the first time, nitrogen load thresholds for the loss of
   the seagrasses Posidonia and Amphibolis spp., highlighting the low
   tolerance of these keystone temperate species to nitrogen pressure. The
   methodology developed as part of the work provides a tool to inform
   planning and improved management of nutrient discharges from land to
   facilitate seagrass conservation.}},
DOI = {{10.1111/1365-2664.13252}},
ISSN = {{0021-8901}},
EISSN = {{1365-2664}},
ORCID-Numbers = {{Erftemeijer, Paul/0000-0002-2904-7422}},
Unique-ID = {{ISI:000454957300025}},
}

@article{ ISI:000450376400007,
Author = {Xiao, Yang and Wang, De and Fang, Jia},
Title = {{Exploring the disparities in park access through mobile phone data:
   Evidence from Shanghai, China}},
Journal = {{LANDSCAPE AND URBAN PLANNING}},
Year = {{2019}},
Volume = {{181}},
Pages = {{80-91}},
Month = {{JAN}},
Abstract = {{The benefits that urban green resources bring to humanity have received
   increasing attention, with the evidence from recent studies into public
   service provision specifically regarding access to green space being
   rather mixed. Despite a growing literature base, there is no consensus
   among scholars on how to measure green space access properly. The
   traditional GIS-based approach is criticized for inappropriately
   specifying geographic units and threshold distances, and for ignoring
   people's self-movement. This research proposes a novel approach,
   emphasizing the actual park users' activities, both spatially and
   temporally. We took advantage of the large dataset available from mobile
   phones to analyze billions of anonymized data samples in order to
   characterize the behavioral patterns of millions of people who accessed
   green space in an experimental procedure. We chose Shanghai as the case
   study because residential segregation had occurred expected to be
   accompanied by issues of environmental justice. The results found that
   social equity could be achieved, even where territorial inequity was
   manifest in a high-population-density context that is undergoing rapid
   urban growth and transition. Shanghai's vulnerable groups are not found
   to be unequally treated at present, but there are warning signs that
   market mechanisms may worsen the uneven development. Therefore, the
   local municipalities are required to rethink how to provide green
   infrastructure to different social groups, responding to the inequality
   and uneven development that capital can bring.}},
DOI = {{10.1016/j.landurbplan.2018.09.013}},
ISSN = {{0169-2046}},
EISSN = {{1872-6062}},
Unique-ID = {{ISI:000450376400007}},
}

@article{ ISI:000454288900001,
Author = {Jiao, Haiming and Wang, Xinchuang and Wu, Jinru and Gao, Kaixuan},
Title = {{Improvement of forest canopy height estimation model by combining
   Geoscience Laser Altimetry System full waveform and multispectral remote
   sensing data over sloping terrain}},
Journal = {{JOURNAL OF APPLIED REMOTE SENSING}},
Year = {{2018}},
Volume = {{12}},
Number = {{4}},
Month = {{DEC 22}},
Abstract = {{Light detection and ranging (LiDAR) technology can provide accurate data
   for the vertical structure of a forest. Such data are fundamental for
   the study of aboveground biomass in the global carbon cycle. Most
   studies to date have used Geoscience Laser Altimetry System (GLAS)
   waveforms and terrain parameters as raw data in creating forest canopy
   height estimation models. Whether other parameters can be introduced to
   improve the accuracy of these models remain to be studied. In this
   study, we introduce parameters for forest texture to augment the
   topographic index model. Texture parameters were combined with field
   observations of canopy height to increase the accuracy of canopy height
   estimates over sloping terrain in the Lushuihe Forest District, a
   typical forest in the Changbai Mountains, Jilin Province, China. The
   study area consists mainly of broadleaf forest, coniferous forest, and
   mixed forest. Fifty-five sample plots covering different terrain reliefs
   and forest types were selected. The study areas were divided into high
   and low relief areas (i.e., hills and plains) using four different
   terrain relief thresholds (5, 10, 15, and 20 m) to select the best
   partition value. The results showed that the most accurate estimate of
   canopy height was given by the improved model using a 20-m threshold.
   Compared with the baseline topographic index model, R-2 for
   leave-one-out cross-validation between the estimates given by the
   improved model and observed data increased from 0.777 to 0.952 and RMSE
   decreased from 2.90 to 1.35 m. The increased accuracy of canopy height
   estimates is due to the correction of GLAS waveform parameters by
   introducing texture parameters to incorporate changes in canopy
   structure because of forest aging and different forest types. (C) 2018
   Society of Photo Optical Instrumentation Engineers (SPIE)}},
DOI = {{10.1117/I.JRS.12.045019}},
Article-Number = {{045019}},
ISSN = {{1931-3195}},
Unique-ID = {{ISI:000454288900001}},
}

@article{ ISI:000454083700001,
Author = {Xu, Bo and Li, Kun and Zheng, Wei and Liu, Xiaoxia and Zhang, Yijia and
   Zhao, Zhehuan and He, Zengyou},
Title = {{Protein complexes identification based on go attributed network
   embedding}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2018}},
Volume = {{19}},
Month = {{DEC 20}},
Abstract = {{BackgroundIdentifying protein complexes from protein-protein interaction
   (PPI) network is one of the most important tasks in proteomics. Existing
   computational methods try to incorporate a variety of biological
   evidences to enhance the quality of predicted complexes. However, it is
   still a challenge to integrate different types of biological information
   into the complexes discovery process under a unified framework.
   Recently, attributed network embedding methods have be proved to be
   remarkably effective in generating vector representations for nodes in
   the network. In the transformed vector space, both the topological
   proximity and node attributed affinity between different nodes are
   preserved. Therefore, such attributed network embedding methods provide
   us a unified framework to integrate various biological evidences into
   the protein complexes identification process.ResultsIn this article, we
   propose a new method called GANE to predict protein complexes based on
   Gene Ontology (GO) attributed network embedding. Firstly, it learns the
   vector representation for each protein from a GO attributed PPI network.
   Based on the pair-wise vector representation similarity, a weighted
   adjacency matrix is constructed. Secondly, it uses the clique mining
   method to generate candidate cores. Consequently, seed cores are
   obtained by ranking candidate cores based on their densities on the
   weighted adjacency matrix and removing redundant cores. For each seed
   core, its attachments are the proteins with correlation score that is
   larger than a given threshold. The combination of a seed core and its
   attachment proteins is reported as a predicted protein complex by the
   GANE algorithm. For performance evaluation, we compared GANE with six
   protein complex identification methods on five yeast PPI networks.
   Experimental results showes that GANE performs better than the competing
   algorithms in terms of different evaluation metrics.ConclusionsGANE
   provides a framework that integrate many valuable and different
   biological information into the task of protein complex identification.
   The protein vector representation learned from our attributed PPI
   network can also be used in other tasks, such as PPI prediction and
   disease gene prediction.}},
DOI = {{10.1186/s12859-018-2555-x}},
Article-Number = {{535}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000454083700001}},
}

@article{ ISI:000450379200015,
Author = {Zhou, Yuyu and Li, Xuecao and Asrar, Ghassem R. and Smith, Steven J. and
   Imhoff, Marc},
Title = {{A global record of annual urban dynamics (1992-2013) from nighttime
   lights}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{219}},
Pages = {{206-220}},
Month = {{DEC 15}},
Abstract = {{The nighttime light (NTL) observations from Defense Meteorological
   Satellite Program/Operational Linescane System (DMSP/OLS) offer great
   potentials to study urban dynamics from regional to global scales, for
   more than two decades. In this paper, we presented a new approach to
   develop spatially and temporally consistent global urban maps from 1992
   to 2013, using the DMSP/OLS NTL observations. First, potential urban
   clusters were delineated using the NTL data and a segmentation method.
   Then, a quantile-based approach was used to remove rural and suburban
   areas sequentially in the potential urban clusters. Finally, the derived
   series of urban extents in the entire study period (1992-2013) were
   improved for temporal consistency. We found the percentage of global
   urban areas relative to the world's land surface area increased from
   0.23\% in 1992 to 0.53\% in 2013. Asia is the continent with the most
   significant urban growth, worldwide. The time series of global urban
   maps were evaluated for the spatial agreement and temporal consistency
   using a variety of widely used independent land-cover products. This
   evaluation indicates that the proposed approach is robust and performs
   well in deriving global urban dynamics across different spatial scales,
   i.e., cluster, province (or state), country, and region. Moreover, this
   quantile-based approach is advantageous, compared with other methods
   used in previous studies, because it does not require additional data
   for enhancement or calibration. The new time series of urban maps from
   this study offer a new dataset for studying global urbanization during
   the past decades and unique information to explore potential future
   trajectories of urban development, which appears to be distinct for
   different nations/regions, globally. Such information is pre-requisite
   for achieving the sustainable development goals, and associated targets,
   during ensuing decades.}},
DOI = {{10.1016/j.rse.2018.10.015}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{zhou, yuyu/0000-0003-1765-6789
   li, xuecao/0000-0002-6942-0746}},
Unique-ID = {{ISI:000450379200015}},
}

@article{ ISI:000450067800010,
Author = {Bjornland, Thea and Bye, Anja and Ryeng, Einar and Wisloff, Ulrik and
   Langaas, Mette},
Title = {{Powerful extreme phenotype sampling designs and score tests for genetic
   association studies}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2018}},
Volume = {{37}},
Number = {{28}},
Pages = {{4234-4251}},
Month = {{DEC 10}},
Abstract = {{We consider cross-sectional genetic association studies (common and rare
   variants) where non-genetic information is available or feasible to
   obtain for N individuals, but where it is infeasible to genotype all N
   individuals. We consider continuously measurable Gaussian traits
   (phenotypes). Genotyping n < N extreme phenotype individuals can yield
   better power to detect phenotype-genotype associations, as compared to
   randomly selecting n individuals. We define a person as having an
   extreme phenotype if the observed phenotype is above a specified
   threshold or below a specified threshold. We consider a model where
   these thresholds can be tailored to each individual. The classical
   extreme sampling design is to set equal thresholds for all individuals.
   We introduce a design (z-extreme sampling) where personalized thresholds
   are defined based on the residuals of a regression model including only
   non-genetic (fully available) information. We derive score tests for the
   situation where only n extremes are analyzed (complete case analysis)
   and for the situation where the non-genetic information on N - n
   non-extremes is included in the analysis (all case analysis). For the
   classical design, all case analysis is generally more powerful than
   complete case analysis. For the z-extreme sample, we show that all case
   and complete case tests are equally powerful. Simulations and data
   analysis also show that z-extreme sampling is at least as powerful as
   the classical extreme sampling design and the classical design is shown
   to be at times less powerful than random sampling. The method of
   dichotomizing extreme phenotypes is also discussed.}},
DOI = {{10.1002/sim.7914}},
ISSN = {{0277-6715}},
EISSN = {{1097-0258}},
Unique-ID = {{ISI:000450067800010}},
}

@article{ ISI:000450383000012,
Author = {Chen, Ying and Wild, Oliver and Wang, Yu and Ran, Liang and Teich,
   Monique and Groess, Johannes and Wang, Lina and Spindler, Gerald and
   Herrmann, Hartmut and van Pinxteren, Dominik and McFiggans, Gordon and
   Wiedensohler, Alfred},
Title = {{The influence of impactor size cut-off shift caused by hygroscopic
   growth on particulate matter loading and composition measurements}},
Journal = {{ATMOSPHERIC ENVIRONMENT}},
Year = {{2018}},
Volume = {{195}},
Pages = {{141-148}},
Month = {{DEC}},
Abstract = {{The mass loading and composition of atmospheric particles are important
   in determining their climate and health effects, and are typically
   measured by filter sampling. However, particle sampling under ambient
   conditions can lead to a shift in the size cut-off threshold induced by
   hygroscopic growth, and the influence of this on measurement of particle
   loading and composition has not been adequately quantified. Here, we
   propose a method to assess this influence based on kappa-Kohler theory.
   A global perspective is presented based on previously reported annual
   climatological values of hygroscopic properties, meteorological
   parameters and particle volume size distributions. Measurements at
   background sites in Europe may be more greatly influenced by the cut-off
   shift than those from other continents, with a median influence of
   10-20\% on the total mass of sampled particles. However, the influence
   is generally much smaller (< 7\%) at urban sites, and is negligible for
   dust and particles in the Arctic. Sea-salt particles experience the
   largest influence (median value similar to 50\%), resulting from their
   large size, high hygroscopicity and the high relative humidity (RH) in
   marine air-masses. We estimate a difference of similar to 30\% in this
   influence of sea-salt particle sampling between relatively dry (RH =
   60\%) and humid (RH = 90\%) conditions. Given the variation in the
   cut-off shift in different locations and at different times, a
   consistent consideration of this influence using the approach we
   introduce here is critical for observational studies of the long-term
   and spatial distribution of particle loading and composition, and
   crucial for robust validation of aerosol modules in modelling studies.}},
DOI = {{10.1016/j.atmosenv.2018.09.049}},
ISSN = {{1352-2310}},
EISSN = {{1873-2844}},
ORCID-Numbers = {{CHEN, YING/0000-0002-0319-4950}},
Unique-ID = {{ISI:000450383000012}},
}

@article{ ISI:000449449800009,
Author = {Piiroinen, Rami and Fassnacht, Fabian Ewald and Heiskanen, Janne and
   Maeda, Eduardo and Mack, Benjamin and Pellikka, Petri},
Title = {{Invasive tree species detection in the Eastern Arc Mountains
   biodiversity hotspot using one class classification}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{218}},
Pages = {{119-131}},
Month = {{DEC 1}},
Abstract = {{Eucalyptus spp. and Acacia mearnsii are common exotic tree species in
   eastern Africa that have shown (strong) invasive behavior in some
   regions. Acacia mearnsii is considered a highly invasive species that is
   replacing native species and Eucalyptus spp. are known to consume high
   amounts of groundwater with suspected effects on native flora. Mapping
   the occurrence of these species in the Taita Hills, Kenya (part of the
   Eastern Arc Mountains Biodiversity Hotspot) is important as there is
   lack of knowledge on their occurrence and ecological impact in the area.
   Mapping methods that require a lot of fieldwork are impractical in areas
   like the Taita Hills, where the terrain is rugged and the infrastructure
   is poor. Our aim was hence to map the occurrence of these tree species
   in a 100 km(2) area using airborne imaging spectroscopy and laser
   scanning. We used a one class biased support vector machine (BSVM)
   classifier as it needs labeled training data only for the positive
   classes (A. mearnsii and Eucalyptus spp.), which potentially reduces the
   amount of required fieldwork. We also introduce a new approach for
   parameterizing and setting the threshold level simultaneously for the
   BSVM classifier. The second aim was to link the occurrence of these
   species to selected environmental variables. The results showed that the
   BSVM classifier is suitable for mapping Acacia mearnsii and Eucalyptus
   spp., holding the potential to improve the efficiency of field data
   collection. The introduced parametrization/threshold selection method
   performed better than other commonly used approaches. The crown level
   Fl-score was 0.76 for Eucalyptus spp. and 0.78 for A. mearnsii. We show
   that Eucalyptus spp. and A. mearnsii trees cover 0.8\% and 1.6\% of the
   study area, respectively. Both species are particularly located on
   steeper slopes and higher altitudes. Both species have significant
   occurrences in areas close to the biggest remaining native forest patch
   (Ngangao) in the study area. Nonetheless, follow-up studies are needed
   to evaluate their impact on the native flora and fauna, as well as their
   impact on the water resources. The maps created in this study in
   combination with such follow-up studies could serve as base data to
   generate guidelines that authorities can use to take action in handling
   the problems these species are causing.}},
DOI = {{10.1016/j.rse.2018.09.018}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{Heiskanen, Janne/0000-0002-3899-8860}},
Unique-ID = {{ISI:000449449800009}},
}

@article{ ISI:000449071000032,
Author = {Bogers, R. P. and van Gils, A. and Clahsen, S. C. S. and Vercruijsse, W.
   and van Kamp, I. and Baliatsas, C. and Rosmalen, J. G. M. and Bolte, J.
   F. B.},
Title = {{Individual variation in temporal relationships between exposure to
   radiofrequency electromagnetic fields and non-specific physical
   symptoms: A new approach in studying `electrosensitivity'}},
Journal = {{ENVIRONMENT INTERNATIONAL}},
Year = {{2018}},
Volume = {{121}},
Number = {{1}},
Pages = {{297-307}},
Month = {{DEC}},
Abstract = {{Background: Everyday exposure to radiofrequency electromagnetic fields
   (RF-EMF) emitted from wireless devices such as mobile phones and base
   stations, radio and television transmitters is ubiquitous. Some people
   attribute non-specific physical symptoms (NSPS) such as headache and
   fatigue to exposure to RF-EMF. Most previous laboratory studies or
   studies that analyzed populations at a group level did not find evidence
   of an association between RF-EMF exposure and NSPS.
   Objectives: We explored the association between exposure to RF-EMF in
   daily life and the occurrence of NSPS in individual self-declared
   electrohypersensitive persons using body worn exposimeters and
   electronic diaries.
   Methods: We selected seven individuals who attributed their NSPS to
   RF-EMF exposure. The level of and variability in personal RF-EMF
   exposure and NSPS were determined during a three-week period. Data were
   analyzed using time series analysis in which exposure as measured and
   recorded in the diary was correlated with NSPS.
   Results: We found statistically significant correlations between
   perceived and actual exposure to wireless internet (WiFi - rate of
   change and number of peaks above threshold) and base stations for mobile
   telecommunications (GSM + UMTS downlink, rate of change) and NSPS scores
   in four of the seven participants. In two persons a higher EMF exposure
   was associated with higher symptom scores, and in two other persons it
   was associated with lower scores. Remarkably, we found no significant
   correlations between NSPS and time weighted average power density, the
   most commonly used exposure metric.
   Conclusions: RF-EMF exposure was associated either positively or
   negatively with NSPS in some but not all of the selected self-declared
   electrohypersensitive persons.}},
DOI = {{10.1016/j.envint.2018.08.064}},
ISSN = {{0160-4120}},
EISSN = {{1873-6750}},
ORCID-Numbers = {{Vercruijsse, Wendy/0000-0001-7939-4855
   Rosmalen, Judith/0000-0002-6393-0032}},
Unique-ID = {{ISI:000449071000032}},
}

@article{ ISI:000456691200007,
Author = {Maliet, Odile and Gascuel, Fanny and Lambert, Amaury},
Title = {{Ranked Tree Shapes, Nonrandom Extinctions, and the Loss of Phylogenetic
   Diversity}},
Journal = {{SYSTEMATIC BIOLOGY}},
Year = {{2018}},
Volume = {{67}},
Number = {{6}},
Pages = {{1025-1040}},
Month = {{NOV}},
Abstract = {{Phylogenetic diversity (PD) is a measure of the evolutionary legacy of a
   group of species, which can be used to define conservation priorities.
   It has been shown that an important loss of species diversity can
   sometimes lead to a much less important loss of PD, depending on the
   topology of the species tree and on the distribution of its branch
   lengths. However, the rate of decrease of PD strongly depends on the
   relative depths of the nodes in the tree and on the order in which
   species become extinct. We introduce a new, sampling-consistent,
   three-parameter model generating random trees with covarying topology,
   clades relative depths, and clades relative extinction risks. This model
   can be seen as an extension to Aldous' one parameter splitting model
   (beta, which controls for tree balance) with two additional parameters:
   a new parameter alpha quantifying the relation between age and richness
   of subclades, and a parameter eta quantifying the relation between
   relative abundance and richness of subclades, taken herein as a proxy
   for overall extinction risk. We show on simulated phylogenies that loss
   of PD depends on the combined effect of all three parameters, beta,
   alpha, and eta. In particular, PD may decrease as fast as species
   diversity when high extinction risks are clustered within small, old
   clades, corresponding to a parameter range that we term the ``danger
   zone{''} (beta <-1 or alpha< 0; eta > 1). Besides, when high extinction
   risks are clustered within large clades, the loss of PD can be higher in
   trees that are more balanced (beta > 0), in contrast to the predictions
   of earlier studies based on simpler models. We propose a Monte-Carlo
   algorithm, tested on simulated data, to infer all three parameters.
   Applying it to a real data set comprising 120 bird clades (class Aves)
   with known range sizes, we show that parameter estimates precisely fall
   close to the danger zone: the combination of their ranking tree shape
   and nonrandom extinctions risks makes them prone to a sudden collapse of
   PD.}},
DOI = {{10.1093/sysbio/syy030}},
ISSN = {{1063-5157}},
EISSN = {{1076-836X}},
Unique-ID = {{ISI:000456691200007}},
}

@article{ ISI:000452692500038,
Author = {Tarazona, Yonatan and Mantas, Vasco M. and Pereira, A. J. S. C.},
Title = {{Improving tropical deforestation detection through using photosynthetic
   vegetation time series - (PVts-beta)}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2018}},
Volume = {{94}},
Number = {{1}},
Pages = {{367-379}},
Month = {{NOV}},
Abstract = {{This paper proposes a new approach of change detection that reduces
   seasonality in time series by using Photosynthetic Vegetation Time
   Series (PVTS) from satellite images. With this approach, each pixel
   value represents at the subpixel level a fraction of the photosynthetic
   forest's activity. Our hypothesis is based on an assumption that
   photosynthetic vegetation fractions will remain constant until a
   disturbing agent (natural or anthropic) occurs. Using Landsat data, we
   compared our approach with the Carnegie Landsat Systems Analysis-Lite
   (CLASlite) approach and with the national reports of the Ministry of the
   Environment of Peru (MINAM). After reducing seasonal variations in
   Landsat data, we detected deforestation events with a new detection
   method. Our approach (which was called PVts-beta) of detection is a
   simple method that does not model the seasonality and it only requires
   as inputs: i) the average and standard deviation of the time series of a
   pixel and ii) a threshold magnitude (beta) that was calibrated to detect
   deforestation events in tropical forests. For the PVts-beta approach,
   the results of calibration show that deforestation was optimally
   detected for beta = (5,6), higher or lower than this range, the biases
   favor to false detections and favor the omission of deforestation too.
   On the other hand, the overall accuracy for the PVts-beta approach was
   91.1\%, with an omission and commission of 8.3\% and 0.5\% respectively,
   while for CLASlite the overall accuracy was 79.2\%, with an omission and
   commission of 20.8\% and 0.0\% respectively. The differences in the
   overall accuracy between the PVts-beta and CLASlite approach were
   significant, being atmospheric noise a main problem which CLASlite
   usually does not work optimally. The strength of our PVts-beta approach
   is the early detection at the subpixel level of deforestation events
   that, added to our new method of change detection explain the little
   omission obtained in the results. Therefore, the PVts-beta approach-that
   we propose here- provides the opportunity to monitoring deforestation
   events in tropical forests at sub-annual scales using Landsat data, and
   it can be used for near-real-time change detection monitoring without a
   doubt.}},
DOI = {{10.1016/j.ecolind.2018.07.012}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
ResearcherID-Numbers = {{Pereira, Alcides/M-5900-2013}},
ORCID-Numbers = {{Pereira, Alcides/0000-0002-7392-2255}},
Unique-ID = {{ISI:000452692500038}},
}

@article{ ISI:000451835900033,
Author = {Koch, Hillary and Starenki, Dmytro and Cooper, Sara J. and Myers,
   Richard M. and Li, Qunhua},
Title = {{powerTCR: A model-based approach to comparative analysis of the clone
   size distribution of the T cell receptor repertoire}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2018}},
Volume = {{14}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Sequencing of the T cell receptor repertoire is a powerful tool for
   deeper study of immune response, but the unique structure of this type
   of data makes its meaningful quantification challenging. We introduce a
   new method, the Gamma-GPD spliced threshold model, to address this
   difficulty. This biologically interpretable model captures the
   distribution of the TCR repertoire, demonstrates stability across
   varying sequencing depths, and permits comparative analysis across any
   number of sampled individuals. We apply our method to several datasets
   and obtain insights regarding the differentiating features in the T cell
   receptor repertoire among sampled individuals across conditions. We have
   implemented our method in the open-source R package powerTCR.}},
DOI = {{10.1371/journal.pcbi.1006571}},
Article-Number = {{e1006571}},
ISSN = {{1553-7358}},
Unique-ID = {{ISI:000451835900033}},
}

@article{ ISI:000449887100012,
Author = {Ward, Delphi F. L. and Wotherspoon, Simon and Melbourne-Thomas, Jessica
   and Haapkylae, Jessica and Johnson, Craig R.},
Title = {{Detecting ecological regime shifts from transect data}},
Journal = {{ECOLOGICAL MONOGRAPHS}},
Year = {{2018}},
Volume = {{88}},
Number = {{4}},
Pages = {{694-715}},
Month = {{NOV}},
Abstract = {{Timely detection of ecological regime shifts is a key problem for
   ecosystem managers, because changed ecosystem dynamics and function will
   usually necessitate a change in management strategies. However,
   currently available methods for detecting regime shifts depend on having
   multiple long time series data from both before and after the regime
   shift. This data requirement is prohibitive for many ecosystems. Here,
   we present a new approach for detecting regime shifts from
   one-dimensional spatial (transect) data from just a single time step
   either side of the transition. Characteristic length scale (CLS)
   estimation is a method of attractor reconstruction combined with
   nonlinear prediction that enables identification of the emergent scale
   at which deterministic behavior of the system is best observed.
   Importantly, previous studies show that a fundamental change in
   ecosystem dynamics, from one domain of attraction to another, is
   reflected in a change in the CLS, i.e., the approach enables
   distinguishing regime shifts from variability in dynamics around a
   single attractor. Until now the method required highly resolved
   two-dimensional spatial data, but here we adapted the approach so that
   the CLS can be estimated from one-dimensional transect data. We
   demonstrate its successful application to both model and real ecosystem
   data. In our model test cases, we detected change in the CLS in cases
   where the shape (topology) of the interaction network had changed,
   leading to a shift in community composition. In an examination of
   benthic transect data from four Indonesian coral reefs, changes in the
   CLS for two of the reefs indicate a regime shift. This new development
   in estimating CLSs makes it possible to detect regime shifts in systems
   where data are limited, removing ambiguity in the interpretation of
   community change.}},
DOI = {{10.1002/ecm.1312}},
ISSN = {{0012-9615}},
EISSN = {{1557-7015}},
ORCID-Numbers = {{Wotherspoon, Simon/0000-0002-6947-4445
   Johnson, Craig/0000-0002-9511-905X
   Ward, Delphi/0000-0002-1802-2617}},
Unique-ID = {{ISI:000449887100012}},
}

@article{ ISI:000447752100006,
Author = {Abdullah, Ahmad S. and Ozok, Yasa Eksioglu and Rahebi, Javad},
Title = {{A novel method for retinal optic disc detection using bat meta-heuristic
   algorithm}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2018}},
Volume = {{56}},
Number = {{11}},
Pages = {{2015-2024}},
Month = {{NOV}},
Abstract = {{Normally, the optic disc detection of retinal images is useful during
   the treatment of glaucoma and diabetic retinopathy. In this paper, the
   novel preprocessing of a retinal image with a bat algorithm (BA)
   optimization is proposed to detect the optic disc of the retinal image.
   As the optic disk is a bright area and the vessels that emerge from it
   are dark, these facts lead to the selected segments being regions with a
   great diversity of intensity, which does not usually happen in
   pathological regions. First, in the preprocessing stage, the image is
   fully converted into a gray image using a gray scale conversion, and
   then morphological operations are implemented in order to remove dark
   elements such as blood vessels, from the images. In the next stage, a
   bat algorithm (BA) is used to find the optimum threshold value for the
   optic disc location. In order to improve the accuracy and to obtain the
   best result for the segmented optic disc, the ellipse fitting approach
   was used in the last stage to enhance and smooth the segmented optic
   disc boundary region. The ellipse fitting is carried out using the least
   square distance approach. The efficiency of the proposed method was
   tested on six publicly available datasets, MESSIDOR, DRIVE, DIARETDB1,
   DIARETDB0, STARE, and DRIONS-DB. The optic disc segmentation average
   overlaps and accuracy was in the range of 78.5-88.2\% and 96.6-99.91\%
   in these six databases. The optic disk of the retinal images was
   segmented in less than 2.1s per image. The use of the proposed method
   improved the optic disc segmentation results for healthy and
   pathological retinal images in a low computation time.}},
DOI = {{10.1007/s11517-018-1840-1}},
ISSN = {{0140-0118}},
EISSN = {{1741-0444}},
ORCID-Numbers = {{S. ABDULLAH, AHMED/0000-0002-8578-1760}},
Unique-ID = {{ISI:000447752100006}},
}

@article{ ISI:000447476600005,
Author = {Lolika, Paride O. and Modnak, Chairat and Mushayabasa, Steady},
Title = {{On the dynamics of brucellosis infection in bison population with
   vertical transmission and culling}},
Journal = {{MATHEMATICAL BIOSCIENCES}},
Year = {{2018}},
Volume = {{305}},
Pages = {{42-54}},
Month = {{NOV}},
Abstract = {{We introduce a new mathematical modeling framework that seek to improve
   our quantitative understanding of the influence of chronic brucellosis
   and culling control on brucellosis dynamics in periodic and non-periodic
   environments. We conduct both epidemic and endemic analysis, with a
   focus on the threshold dynamics characterized by the basic reproduction
   numbers. In addition, we also perform an optimal control study to
   explore optimal culling strategy in periodic and non-periodic
   environment.}},
DOI = {{10.1016/j.mbs.2018.08.009}},
ISSN = {{0025-5564}},
EISSN = {{1879-3134}},
ORCID-Numbers = {{Mushayabasa, Steady/0000-0002-7423-2934}},
Unique-ID = {{ISI:000447476600005}},
}

@article{ ISI:000444927200006,
Author = {Li, Tianyu and Meng, Qingmin},
Title = {{A mixture emissivity analysis method for urban land surface temperature
   retrieval from Landsat 8 data}},
Journal = {{LANDSCAPE AND URBAN PLANNING}},
Year = {{2018}},
Volume = {{179}},
Pages = {{63-71}},
Month = {{NOV}},
Abstract = {{Land surface temperature (LST) retrieval from satellite imagery is one
   of the most practical ways to consistently monitor urban thermal
   environment. Given the heterogeneous nature of urban landscape, an
   implicit assumption should be considered in remotely sensed LST
   determinations that a mixed urban land cover aggregation is the
   combination of its constituent components. Currently, the common LST
   retrieval method which utilize emissivity measures estimated by NDVI
   threshold method (NDVITHM), including mono window (MW), single channel
   (SC), and split window algorithms (SW), does not take into account
   heterogeneity of pixels. While in this study, a new approach, the
   mixture analysis of emissivity (MAoE), is proposed to calculate
   temperature by estimating pixel emissivity from mixed land cover
   classes. We conduct a comparison of six approaches by the combinations
   of three LST retrieval algorithms with NDVITHM and MAoE respectively.
   The differences among strategies are characterized and analyzed by
   comparing LST estimates from Landsat 8 thermal images. The LST gradients
   derived from transect analysis are found consistently similar for
   combinations of two LST algorithms (MW and SC) and the two emissivity
   estimation algorithms (MAoE and NDVITHM). LSTs derived from SW
   algorithms using band 10 have the highest mean values, while the SC
   algorithms have moderate mean values and the MW algorithms have the
   lowest values. Standard deviations of estimated LST from MAoE are
   smaller compared with NDVITHM methods, SC retrieval algorithm with MAoE
   has the smallest standard deviation, and NDVITHM temperature estimation
   could be more impacted by different land use land cover types.}},
DOI = {{10.1016/j.landurbplan.2018.07.010}},
ISSN = {{0169-2046}},
EISSN = {{1872-6062}},
Unique-ID = {{ISI:000444927200006}},
}

@article{ ISI:000444423900001,
Author = {He, Ye and Lin, Huazhen and Tu, Dongsheng},
Title = {{A single-index threshold Cox proportional hazard model for identifying a
   treatment-sensitive subset based on multiple biomarkers}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2018}},
Volume = {{37}},
Number = {{23}},
Pages = {{3267-3279}},
Month = {{OCT 15}},
Abstract = {{In this paper, we introduce a single-index threshold Cox proportional
   hazard model to select and combine biomarkers to identify patients who
   may be sensitive to a specific treatment. A penalized smoothed partial
   likelihood is proposed to estimate the parameters in the model. A
   simple, efficient, and unified algorithm is presented to maximize this
   likelihood function. The estimators based on this likelihood function
   are shown to be consistent and asymptotically normal. Under mild
   conditions, the proposed estimators also achieve the oracle property.
   The proposed approach is evaluated through simulation analyses and
   application to the analysis of data from two clinical trials, one
   involving patients with locally advanced or metastatic pancreatic cancer
   and one involving patients with resectable lung cancer.}},
DOI = {{10.1002/sim.7837}},
ISSN = {{0277-6715}},
EISSN = {{1097-0258}},
ORCID-Numbers = {{Tu, Dongsheng/0000-0003-4842-2184}},
Unique-ID = {{ISI:000444423900001}},
}

@article{ ISI:000447053100003,
Author = {Nowack, Peer and Braesicke, Peter and Haigh, Joanna and Abraham, Nathan
   Luke and Pyle, John and Voulgarakis, Apostolos},
Title = {{Using machine learning to build temperature-based ozone
   parameterizations for climate sensitivity simulations}},
Journal = {{ENVIRONMENTAL RESEARCH LETTERS}},
Year = {{2018}},
Volume = {{13}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{A number of studies have demonstrated the importance of ozone in climate
   change simulations, for example concerning global warming projections
   and atmospheric dynamics. However, fully interactive atmospheric
   chemistry schemes needed for calculating changes in ozone are
   computationally expensive. Climate modelers therefore often use
   climatological ozone fields, which are typically neither consistent with
   the actual climate state simulated by each model nor with the specific
   climate change scenario. This limitation applies in particular to
   standard modeling experiments such as preindustrial control or abrupt
   4xCO(2) climate sensitivity simulations. Here we suggest a novel method
   using a simple linear machine learning regression algorithm to predict
   ozone distributions for preindustrial and abrupt 4xCO(2) simulations.
   Using the atmospheric temperature field as the only input, the
   regression reliably predicts three-dimensional ozone distributions at
   monthly to daily time intervals. In particular, the representation of
   stratospheric ozone variability is much improved compared with a fixed
   climatology, which is important for interactions with dynamical
   phenomena such as the polar vortices and the Quasi-Biennial Oscillation.
   Our method requires training data covering only a fraction of the usual
   length of simulations and thus promises to be an important stepping
   stone towards a range of new computationally efficient methods to
   consider ozone changes in long climate simulations. We highlight key
   development steps to further improve and extend the scope of machine
   learning-based ozone parameterizations.}},
DOI = {{10.1088/1748-9326/aae2be}},
Article-Number = {{104016}},
ISSN = {{1748-9326}},
ResearcherID-Numbers = {{Braesicke, Peter/D-8330-2016
   }},
ORCID-Numbers = {{Braesicke, Peter/0000-0003-1423-0619
   Voulgarakis, Apostolos/0000-0002-6656-4437
   Nowack, Peer/0000-0003-4588-7832
   Haigh, Joanna/0000-0001-5504-4754}},
Unique-ID = {{ISI:000447053100003}},
}

@article{ ISI:000446282300001,
Author = {Horacek, Jaroslav and Koucky, Vaclav and Hladik, Milan},
Title = {{Novel approach to computerized breath detection in lung function
   diagnostics}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2018}},
Volume = {{101}},
Pages = {{1-6}},
Month = {{OCT 1}},
Abstract = {{Background: Breath detection, i.e. its precise delineation in time is a
   crucial step in lung function data analysis as obtaining any clinically
   relevant index is based on the proper localization of breath ends.
   Current threshold or smoothing algorithms suffer from severe inaccuracy
   in cases of suboptimal data quality. Especially in infants, the precise
   analysis is of utmost importance. The key objective of our work is to
   design an algorithm for accurate breath detection in severely distorted
   data.
   Methods: Flow and gas concentration data from multiple breath washout
   test were the input information. Based on universal physiological
   characteristics of the respiratory tract we designed an algorithm for
   breath detection. Its accuracy was tested on severely distorted data
   from 19 patients with different types of breathing disorders. Its
   performance was compared to the performance of currently used algorithms
   and to the breath counts estimated by human experts.
   Results: The novel algorithm outperformed the threshold algorithms with
   respect to their accuracy and had similar performance to human experts.
   It proved to be a highly robust and efficient approach in severely
   distorted data. This was demonstrated on patients with different
   pulmonary disorders.
   Conclusion: Our newly proposed algorithm is highly robust and universal.
   It works accurately even on severely distorted data, where the other
   tested algorithms failed. It does not require any pre-set thresholds or
   other patient-specific inputs. Consequently, it may be used with a broad
   spectrum of patients. It has the potential to replace current approaches
   to the breath detection in pulmonary function diagnostics.}},
DOI = {{10.1016/j.compbiomed.2018.07.017}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{Hladik, Milan/J-9663-2012
   Horacek, Jaroslav/K-3143-2017}},
ORCID-Numbers = {{Hladik, Milan/0000-0002-7340-8491
   Horacek, Jaroslav/0000-0002-0672-4339}},
Unique-ID = {{ISI:000446282300001}},
}

@article{ ISI:000446328200005,
Author = {Potts, Jonathan R. and Borger, Luca and Scantlebury, D. Michael and
   Bennett, Nigel C. and Alagaili, Abdulaziz and Wilson, Rory P.},
Title = {{Finding turning-points in ultra-high-resolution animal movement data}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2018}},
Volume = {{9}},
Number = {{10}},
Pages = {{2091-2101}},
Month = {{OCT}},
Abstract = {{1. Recent advances in biologging have resulted in animal location data
   at unprecedentedly high temporal resolutions, sometimes many times per
   second. However, many current methods for analysing animal movement
   (e.g. step selection analysis or state-space modelling) were developed
   with lower-resolution data in mind. To make such methods usable with
   high-resolution data, we require techniques to identify features within
   the trajectory where movement deviates from a straight line.
   2. We propose that the intricacies of movement paths, and particularly
   turns, reflect decisions made by animals so that turn points are
   particularly relevant to behavioural ecologists. As such, we introduce a
   fast, accurate algorithm for inferring turning-points in high-resolution
   data. For analysing big data, speed and scalability are vitally
   important. We test our algorithm on simulated data, where varying
   amounts of noise were added to paths of straight-line segments
   interspersed with turns. We also demonstrate our algorithm on data of
   free-ranging oryx Oryx leucoryx. We compare our algorithm to existing
   statistical techniques for break-point inference.
   3. The algorithm scales linearly and can analyse several
   hundred-thousand data points in a few seconds on a mid-range desktop
   computer. It identified turnpoints in simulated data with complete
   accuracy when the noise in the headings had a standard deviation of +/-
   8 degrees, well within the tolerance of many modern biologgers. It has
   comparable accuracy to the existing algorithms tested, and is up to
   three orders of magnitude faster.
   4. Our algorithm, freely available in R and Python, serves as an initial
   step in processing ultra high-resolution animal movement data, resulting
   in a rarefied path that can be used as an input into many existing
   step-and-turn methods of analysis. The resulting path consists of points
   where the animal makes a clear turn, and thereby provides valuable data
   on decisions underlying movement patterns. As such, it provides an
   important breakthrough required as a starting point for analysing
   subsecond resolution data.}},
DOI = {{10.1111/2041-210X.13056}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ResearcherID-Numbers = {{Borger, Luca/C-6003-2008
   }},
ORCID-Numbers = {{Borger, Luca/0000-0001-8763-5997
   Alagaili, Abdulaziz/0000-0002-9733-4220
   Potts, Jonathan/0000-0002-8564-2904}},
Unique-ID = {{ISI:000446328200005}},
}

@article{ ISI:000445933400002,
Author = {Stephens, Scott A. and Bell, Robert G. and Lawrence, Judy},
Title = {{Developing signals to trigger adaptation to sea-level rise}},
Journal = {{ENVIRONMENTAL RESEARCH LETTERS}},
Year = {{2018}},
Volume = {{13}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Dynamic adaptive policy pathways (DAPP) is emerging as a
   `fit-for-purpose' method for climate-change adaptation planning to
   address widening future uncertainty and long planning timeframes. A key
   component of DAPP is to monitor indicators of change such as flooding
   and storm events, which can trigger timely adaptive actions (change
   pathway/behavior) ahead of thresholds. Signals and triggers are needed
   to support DAPP-the signal provides early warning of the emergence of
   the trigger (decision-point), and the trigger initiates the process to
   change pathway before a harmful adaptation-threshold is reached. We
   demonstrate a new approach to designing signals and triggers using the
   case of increased flooding as sea level continues to rise. The flooding
   frequency is framed in terms of probable timing of several events
   reaching a specific height threshold within a set monitoring period.
   This framing is well suited to adaptive planning for different hazards,
   because it allows the period over which threshold exceedances are
   monitored to be specified, and thus allows action before
   adaptation-thresholds are reached, while accounting for the potential
   range of timing and providing a probability of premature warning, or of
   triggering adaptation too late. For our New Zealand sea level case
   study, we expect early signals to be observed in 10 year monitoring
   periods beginning 2021. Some urgency is therefore required to begin the
   assessment, planning and community engagement required to develop
   adaptive plans and associated signals and triggers for monitoring.
   Worldwide, greater urgency is required at tide-dominated sites than
   those adapted to large storm-surges. Triggers can be designed with
   confidence that a change in behavior pathway (e.g. relocating
   communities) will be triggered before an adaptation-threshold occurs.
   However, it is difficult to avoid the potential for premature
   adaptation. Therefore, political, social, economic, or cultural signals
   are also needed to complement the signals and triggers based on
   coastal-hazard considerations alone.}},
DOI = {{10.1088/1748-9326/aadf96}},
Article-Number = {{104004}},
ISSN = {{1748-9326}},
ORCID-Numbers = {{Bell, Robert/0000-0002-8490-8942
   Stephens, Scott/0000-0002-6573-8757}},
Unique-ID = {{ISI:000445933400002}},
}

@article{ ISI:000445244500010,
Author = {Cardos, Juan Luis H. and Martinez, Isabel and Aragon, Gregorio and
   Ellis, Christopher J.},
Title = {{Role of past and present landscape structure in determining epiphyte
   richness in fragmented Mediterranean forests}},
Journal = {{LANDSCAPE ECOLOGY}},
Year = {{2018}},
Volume = {{33}},
Number = {{10}},
Pages = {{1757-1768}},
Month = {{OCT}},
Abstract = {{Context The anthropocene is characterised by global landscape
   modification, and the structure of remnant habitats can explain
   different patterns of species richness. The most pervasive processes of
   degradation include habitat loss and fragmentation. However, a recovery
   of modified landscape is occurring in some areas.
   Objectives The main goal is to know how lichen and bryophyte epiphytic
   richness growing on Mediterranean forests is influenced not only by
   fragments characteristics but also by the structure of the landscape. We
   introduce a temporal dimension in order to evaluate if the historical
   landscape structure is relevant for current epiphytic communities.
   Methods 40 well-preserved forest fragments were selected in a landscape
   with a large habitat loss over decades, but with a recovery of forest
   surface in the last 55years. The most relevant fragment and
   landscape-scale attributes were considered. Some of the variables were
   measured in three different years to incorporate a temporal framework.
   Results The results showed that variables at fragment scale had a higher
   influence, whereas variables at the landscape scale were irrelevant.
   Among all the historical variables analyzed, only the shift in forest
   fragment size had influence on species richness.
   Conclusions Mediterranean forests had suffered fragmentation along
   centuries. Their epiphytic communities also suffer the hard conditions
   of Mediterranean climate. Our results indicate that Mediterranean
   epiphytic communities may be in a threshold since it they will never be
   similar to those communities existing previous fragmentation process
   even a recovery habitat occur or, they may require more time to response
   to this habitat recovery.}},
DOI = {{10.1007/s10980-018-0700-6}},
ISSN = {{0921-2973}},
EISSN = {{1572-9761}},
ORCID-Numbers = {{Martinez, Isabel/0000-0002-5924-1648}},
Unique-ID = {{ISI:000445244500010}},
}

@article{ ISI:000440390800081,
Author = {Wang, Jiabiao and Zhao, Jianshi and Lei, Xiaohui and Wang, Hao},
Title = {{New approach for point pollution source identification in rivers based
   on the backward probability method}},
Journal = {{ENVIRONMENTAL POLLUTION}},
Year = {{2018}},
Volume = {{241}},
Pages = {{759-774}},
Month = {{OCT}},
Abstract = {{Pollution risk from the discharge of industrial waste or accidental
   spills during transportation poses a considerable threat to the security
   of rivers. The ability to quickly identify the pollution source is
   extremely important to enable emergency disposal of pollutants. This
   study proposes a new approach for point source identification of sudden
   water pollution in rivers, which aims to determine where (source
   location), when (release time) and how much pollutant (released mass)
   was introduced into the river. Based on the backward probability method
   (BPM) and the linear regression model (LR), the proposed LR-BPM converts
   the ill-posed problem of source identification into an optimization
   model, which is solved using a Differential Evolution Algorithm (DEA).
   The decoupled parameters of released mass are not dependent on prior
   information, which improves the identification efficiency. A
   hypothetical case study with a different number of pollution sources was
   conducted to test the proposed approach, and the largest relative errors
   for identified location, release time, and released mass in all tests
   were not greater than 10\%. Uncertainty in the LR BPM is mainly due to a
   problem with model equifinality, but averaging the results of repeated
   tests greatly reduces errors. Furthermore, increasing the gauging
   sections further improves identification results. A real-world case
   study examines the applicability of the LR BPM in practice, where it is
   demonstrated to be more accurate and time-saving than two existing
   approaches, Bayesian MCMC and basic DEA. (C) 2018 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.envpol.2018.05.093}},
ISSN = {{0269-7491}},
EISSN = {{1873-6424}},
Unique-ID = {{ISI:000440390800081}},
}

@article{ ISI:000453743200002,
Author = {Wang, Xuefeng and Chen, Hao and Zhang, Nancy R.},
Title = {{DNA copy number profiling using single-cell sequencing}},
Journal = {{BRIEFINGS IN BIOINFORMATICS}},
Year = {{2018}},
Volume = {{19}},
Number = {{5}},
Pages = {{731-736}},
Month = {{SEP}},
Abstract = {{Currently, there is a lack of software for detecting copy number
   variations and constructing copy number profile for the whole genome
   from single-cell DNA sequencing data, which are often of low coverage
   and high technical noises. Here we introduce a new toolkit, SCNV, which
   features an efficient bin-free segmentation approach and provides the
   highest resolution possible for breakpoint detection and the subsequent
   copy number calling. SCNV can auto-tune parameters based on a set of
   normal cells from the same batch to adjust for the technical noise level
   of the data, facilitating its application to data gathered from
   different platforms and different studies.}},
DOI = {{10.1093/bib/bbx004}},
ISSN = {{1467-5463}},
EISSN = {{1477-4054}},
Unique-ID = {{ISI:000453743200002}},
}

@article{ ISI:000448088100045,
Author = {Thoms, Martin and Delong, Michael},
Title = {{Ecosystem Responses to Water Resource Developments in a Large Dryland
   River}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2018}},
Volume = {{54}},
Number = {{9}},
Pages = {{6643-6655}},
Month = {{SEP}},
Abstract = {{Large floodplain rivers in dryland regions are becoming increasingly
   modified through water resource developments. Identifying ecosystem
   responses in these systems is challenging because of their natural
   variability, limited data, and the myriad of ways they are modified.
   This study used organic samples from snail, mussel, and fish specimens
   obtained from museum collections for the determination of carbon and
   nitrogen stable isotope ratios, for the Barwon-Darling River, Australia,
   between 1869 and 2005, a period of extensive water development. Three
   hypotheses were posed for this study: (1) The trophic status and food
   web character of the Barwon-Darling River will change in response to
   water resource developments; (2) responses in the trophic status and
   food web character will differ between different hydrogeomorphic zones
   identified along the river; and (3) food chain lengths will increase in
   response to water resource developments. Substantive changes in trophic
   status and components of the food web were detected between before and
   after water developments. Three lines of evidence support the conclusion
   of anthropogenically driven changes in the food web of this dryland
   floodplain-river system. Stable isotope ratios of fish, mussels, and
   snails differed between both hydrogeomorphic zones of the river and the
   before and after disturbance periods. Layman metrics, representing
   community niche space, differed between before and after disturbance
   periods. Also, both mean trophic position and food chain length differed
   between predisturbance and postdisturbance for the two functional
   process zones. The substantial shift in basal source contribution over
   time is a potential indicator of a state change and loss of resilience
   in this system.
   Plain Language Summary Resilience is the ability to absorb shocks and
   retain a normal functioning. This study indicates a loss of resilience
   in the aquatic ecosystem of the Barwon-Darling River as a result of
   water resource developments. A novel approach of using museum specimens
   of molluscs and fish enabled the trophic status and food web character
   over a 130-year period to be investigated, for the Barwon-Darling River,
   Australia, a system heavily impacted by water resource development.
   Marked differences in the trophic status and components of the food web
   were detected between before and after water resource development
   periods as indicated by the stable isotope ratios of the molluscs and
   fish. However, differences varied between two river zones. Food chain
   lengths increased but were greater in the upper river zone. Basal source
   contributions to the food web also changed substantially over time in
   association with water resource development. A flip to pelagic basal
   source dominance suggests the loss of resilience of this river system
   and potential regime shift.}},
DOI = {{10.1029/2018WR022956}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ORCID-Numbers = {{Delong, Michael/0000-0001-9897-4652
   Thoms, Martin/0000-0002-8074-0476}},
Unique-ID = {{ISI:000448088100045}},
}

@article{ ISI:000447126900030,
Author = {Necaibia, Salah and Kelaiaia, Mounia Samira and Labar, Hocine and
   Necaibia, Ammar},
Title = {{Efficient Design and simulation of solar power system with MPPT-based
   soft switching SEPIC converter at different load levels}},
Journal = {{ENVIRONMENTAL PROGRESS \& SUSTAINABLE ENERGY}},
Year = {{2018}},
Volume = {{37}},
Number = {{5}},
Pages = {{1792-1799}},
Month = {{SEP-OCT}},
Abstract = {{This article proposes a novel method to improve the efficiency of
   Perturb and Observe (P\&O) maximum power point tracking (MPPT)
   algorithm. As the PV module has low conversion efficiency, the output
   power of PV module depends on both operation environments and load
   change. The proposed method can improve the tracking velocity and reduce
   the steady-state oscillation of the conventional P\&O method even under
   sudden load change. The adaptive duty cycle is proposed and executed in
   the microcontroller (PIC16F877A). The two MPPT techniques, conventional
   P\&O method and variable step-size method, are simulated in the ISIS
   PROTUES environment. The simulation results show the high tracking
   accuracy and fast speed convergence of the proposed method. (c) 2017
   American Institute of Chemical Engineers Environ Prog, 37: 1792-1799,
   2018}},
DOI = {{10.1002/ep.12828}},
ISSN = {{1944-7442}},
EISSN = {{1944-7450}},
Unique-ID = {{ISI:000447126900030}},
}

@article{ ISI:000444317200019,
Author = {Jain, Chirag and Koren, Sergey and Dilthey, Alexander and Phillippy,
   Adam M. and Aluru, Srinivas},
Title = {{A fast adaptive algorithm for computing whole-genome homology maps}},
Journal = {{BIOINFORMATICS}},
Year = {{2018}},
Volume = {{34}},
Number = {{17}},
Pages = {{748-756}},
Month = {{SEP 1}},
Note = {{17th European Conference on Computational Biology (ECCB), Athens,
   GREECE, SEP 08-12, 2018}},
Abstract = {{Motivation: Whole-genome alignment is an important problem in genomics
   for comparing different species, mapping draft assemblies to reference
   genomes and identifying repeats. However, for large plant and animal
   genomes, this task remains compute and memory intensive. In addition,
   current practical methods lack any guarantee on the characteristics of
   output alignments, thus making them hard to tune for different
   application requirements.
   Results: We introduce an approximate algorithm for computing local
   alignment boundaries between long DNA sequences. Given a minimum
   alignment length and an identity threshold, our algorithm computes the
   desired alignment boundaries and identity estimates using kmer-based
   statistics, and maintains sufficient probabilistic guarantees on the
   output sensitivity. Further, to prioritize higher scoring alignment
   intervals, we develop a plane-sweep based filtering technique which is
   theoretically optimal and practically efficient. Implementation of these
   ideas resulted in a fast and accurate assembly-to-genome and
   genome-to-genome mapper. As a result, we were able to map an
   error-corrected whole-genome NA12878 human assembly to the hg38 human
   reference genome in about 1 min total execution time and <4 GB memory
   using eight CPU threads, achieving significant improvement in
   memory-usage over competing methods. Recall accuracy of computed
   alignment boundaries was consistently found to be > 97\% on multiple
   datasets. Finally, we performed a sensitive self-alignment of the human
   genome to compute all duplications of length >= 1 Kbp and >= 90\%
   identity. The reported output achieves good recall and covers twice the
   number of bases than the current UCSC browser's segmental duplication
   annotation.}},
DOI = {{10.1093/bioinformatics/bty597}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Phillippy, Adam/U-5731-2018}},
ORCID-Numbers = {{Phillippy, Adam/0000-0003-2983-8934}},
Unique-ID = {{ISI:000444317200019}},
}

@article{ ISI:000442503900002,
Author = {Podschwit, Harry and Guttorp, Peter and Larkin, Narasimhan and Steel, E.
   Ashley},
Title = {{Estimating wildfire growth from noisy and incomplete incident data using
   a state space model}},
Journal = {{ENVIRONMENTAL AND ECOLOGICAL STATISTICS}},
Year = {{2018}},
Volume = {{25}},
Number = {{3}},
Pages = {{325-340}},
Month = {{SEP}},
Abstract = {{Wildfire behaviors are complex and are of interest to fire managers and
   scientists for a variety of reasons. Many of these important behaviors
   are directly measured from the cumulative burn area time series of
   individual wildfires; however, estimating cumulative burn area time
   series is challenging due to the magnitude of measurement errors and
   missing entries. To resolve this, we introduce two state space models
   for reconstructing wildfire burn area using repeated observations from
   multiple data sources that include different levels of measurement error
   and temporal gaps. The constant growth parameter model uses a few
   parameters and assumes a burn area time series that follows a logistic
   growth curve. The non-constant growth parameter model uses a
   time-varying logistic growth curve to produce detailed estimates of the
   burn area time series that permit sudden pauses and pulses of growth. We
   apply both reconstruction models to burn area data from 13 large
   wildfire incidents to compare the quality of the burn area time series
   reconstructions and computational requirements. The constant growth
   parameter model reconstructs burn area time series with minimal
   computational requirements, but inadequately fits observed data in most
   cases. The non-constant growth parameter model better describes burn
   area time series, but can also be highly computationally demanding.
   Sensitivity analyses suggest that in a typical application, the
   reconstructed cumulative burn area time series is fairly robust to minor
   changes in the prior distributions.}},
DOI = {{10.1007/s10651-018-0407-5}},
ISSN = {{1352-8505}},
EISSN = {{1573-3009}},
Unique-ID = {{ISI:000442503900002}},
}

@article{ ISI:000437295900003,
Author = {Saha-Chaudhuri, P. and Heagerty, P. J.},
Title = {{Dynamic thresholds and a summary ROC curve: Assessing prognostic
   accuracy of longitudinal markers}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2018}},
Volume = {{37}},
Number = {{18}},
Pages = {{2700-2714}},
Month = {{AUG 15}},
Abstract = {{Cancer patients, chronic kidney disease patients, and subjects infected
   with HIV are routinely monitored over time using biomarkers that
   represent key health status indicators. Furthermore, biomarkers are
   frequently used to guide initiation of new treatments or to inform
   changes in intervention strategies. Since key medical decisions can be
   made on the basis of a longitudinal biomarker, it is important to
   evaluate the potential accuracy associated with longitudinal monitoring.
   To characterize the overall accuracy of a time-dependent marker, we
   introduce a summary ROC curve that displays the overall sensitivity
   associated with a time-dependent threshold that controls time-varying
   specificity. The proposed statistical methods are similar to concepts
   considered in disease screening, yet our methods are novel in choosing a
   potentially time-dependent threshold to define a positive test, and our
   methods allow time-specific control of the false-positive rate. The
   proposed summary ROC curve is a natural averaging of time-dependent
   incident/dynamic ROC curves and therefore provides a single summary of
   net error rates that can be achieved in the longitudinal setting.}},
DOI = {{10.1002/sim.7675}},
ISSN = {{0277-6715}},
EISSN = {{1097-0258}},
ORCID-Numbers = {{Saha Chaudhuri, Paramita/0000-0003-1987-320X}},
Unique-ID = {{ISI:000437295900003}},
}

@article{ ISI:000445731100004,
Author = {Zhao, Ying and Song, Kaishan},
Title = {{Relationships Between DOC and CDOM Based on the Total Carbon-Specific
   Fluorescence Intensities for River Waters Across China}},
Journal = {{JOURNAL OF GEOPHYSICAL RESEARCH-BIOGEOSCIENCES}},
Year = {{2018}},
Volume = {{123}},
Number = {{8}},
Pages = {{2353-2361}},
Month = {{AUG}},
Abstract = {{Weak correlations between chromophoric dissolved organic matter (CDOM)
   absorption coefficient a(440) and dissolved organic carbon (DOC) were
   observed due to weak absorption for river waters especially in the
   Qinghai-Tibetan Plateau, and thus, it is impossible to estimate DOC
   concentrations by CDOM absorption across China. Therefore, it is
   necessary to estimate DOC concentrations through the fluorescent
   fraction of CDOM (i.e., FDOM). FDOM was proposed as a new method for
   estimation of large-scale DOC concentrations in river waters across
   China. A total of 301 water samples from eight river basins across China
   were selected to assess FDOM by excitation-emission matrix (EEM)
   fluorescence. Five fluorescent regions were obtained by EEM coupled with
   fluorescence regional integration (FRI) (EEM-FRI) method. However, weak
   correlations between the fluorescence intensities FR(1-5), F-SUM, and
   DOC concentrations were observed for all water samples, respectively.
   Therefore, the total carbon-specific fluorescence intensities F-SUM/DOC
   were used to establish relationships between DOC and FDOM. All 301 water
   samples excluding five outliers with F-SUM/DOC higher than 8,000
   nm/(mg/L)) were divided into four groups based on the threshold values
   for FSUM/DOC, and strong positive correlations between FSUM and DOC were
   observed in each group (R-2 = 0.843, F-SUM/DOC < 1,000 nm/(mg/L); R-2 =
   0.928, 1,000 < F-SUM/DOC < 2,000 nm/(mg/L); R-2 = 0.964, 2,000 <
   F-SUM/DOC < 3,000 nm/(mg/L); and R-2 = 0.953, 3,000 < F-SUM/DOC < 8,000
   nm/(mg/L)), respectively. This result indicated that the riverine DOC
   concentrations on large-scale across China can be estimated directly by
   FDOM properties.}},
DOI = {{10.1029/2017JG004374}},
ISSN = {{2169-8953}},
EISSN = {{2169-8961}},
Unique-ID = {{ISI:000445731100004}},
}

@article{ ISI:000445451800038,
Author = {Molina, Oscar M. and Zeidouni, Mehdi},
Title = {{Analytical Model to Detect Fault Permeability Alteration Induced by
   Fault Reactivation in Compartmentalized Reservoirs}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2018}},
Volume = {{54}},
Number = {{8}},
Pages = {{5841-5855}},
Month = {{AUG}},
Abstract = {{Underground fluid injection induces changes in in situ stress condition
   of the target formation and local faults that can potentially lead to
   fault reactivation, which may result in the leakage of injected and/or
   native fluids into neighboring formations. In this paper, we introduce
   an analytical method to detect fault reactivation caused by fluid
   injection into deep faulted aquifers considering across-fault leakage.
   The fundamental assumption made in our model is that fault permeability
   will be altered upon fault slip. Therefore, we model fault reactivation
   as a sudden change in fault permeability at the onset of fault slip. The
   fault is modeled as a linear interface between two permeable formations
   with equal rock and fluid properties. The governing equations are
   coupled through the fault interface and are solved using the
   Laplace-Fourier integral transform technique. Based on the analytical
   solution, we find the characteristic bottomhole pressure and pressure
   derivative responses that enable detecting fault reactivation using
   diagnostic plots. We observe that pressure derivative undergoes a rapid
   change at the onset of fault slip followed by a late-time trend to
   attain a new equilibrium governed by the altered fault permeability.
   Furthermore, we discuss the evolution of the across-fault leakage rate
   upon and after fault slippage. Results from this study are presented in
   the form diagnostic plots and type curves that may be used for reservoir
   and fault characterization purposes.
   Plain Language Summary The purpose of this study is to derive an
   analytical model that uses pressure data from an injector well to
   determine whether a fault has been reactivated due to fluid injection,
   even if no seismicity has been felt at all. It is well documented that
   nonconductive/slightly conductive faults suddenly allow fluids to
   migrate upon fault slippage; hence, we propose a mathematical model in
   which fault permeability suddenly changes at the onset of fault
   reactivation. We consider that fluid can only migrate across the fault.
   We solve the mathematical problem using an integral transformation
   technique. The general solution allows us to infer how pressure at the
   well would respond to a sudden change in fault permeability; we examine
   this behavior with pressure derivative too. We replicate this procedure
   for various values of altered fault permeability, and onset times and
   results are compiled in the form of diagnostic plots (to diagnose
   whether fault reactivation had occurred) and type curves (to estimate
   when fault reactivation occurred and what was the resulting altered
   fault permeability).}},
DOI = {{10.1029/2018WR022872}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
Unique-ID = {{ISI:000445451800038}},
}

@article{ ISI:000443940300006,
Author = {Klesse, S. and Babst, F. and Lienert, S. and Spahni, R. and Joos, F. and
   Bouriaud, O. and Carrer, M. and Di Filippo, A. and Poulter, B. and
   Trotsiuk, V. and Wilson, R. and Frank, D. C.},
Title = {{A Combined Tree Ring and Vegetation Model Assessment of European Forest
   Growth Sensitivity to Interannual Climate Variability}},
Journal = {{GLOBAL BIOGEOCHEMICAL CYCLES}},
Year = {{2018}},
Volume = {{32}},
Number = {{8}},
Pages = {{1226-1240}},
Month = {{AUG}},
Abstract = {{The response of forest growth to climate variability varies along
   environmental gradients. A growth increase and decrease with warming is
   usually observed in cold-humid and warm-dry regions, respectively.
   However, it remains poorly known where the sign of these temperature
   effects switches. Here we introduce a newly developed European tree ring
   network that has been specifically collected to reconstruct forest
   aboveground biomass increment (ABI). We quantify, how the long-term
   (1910-2009) interannual variability of ABI depends on local mean
   May-August temperature and test, if a dynamic global vegetation model
   ensemble reflects the resulting patterns. We find that sites at 8
   degrees C mean May-August temperature increase ABI on average by 5.7 +/-
   1.3\%, whereas sites at 20 degrees C decrease ABI by 3.0 +/-
   1.8\%m(-2)year(-1) Delta degrees C-1. A threshold temperature between
   beneficial and detrimental effects of warming and the associated
   increase in water demand on tree growth emerged at 15.9 +/- 1.4 degrees
   C mean May-August temperature. Because interannual variability increases
   proportionally with mean growth rate-that is, the coefficient of
   variation stays constant-we were able to validate these findings with a
   much larger tree ring data set that had been established following
   classic dendrochronological sampling schemes. While the observed climate
   sensitivity pattern is well reflected in the dynamic global vegetation
   model ensemble, there is a large spread of threshold temperatures
   between the individual models. Also, individual models disagree strongly
   on the magnitude of climate impact at the coldest and warmest locations,
   suggesting where model improvement is most needed to more accurately
   predict forest growth and effectively guide silvicultural practices.}},
DOI = {{10.1029/2017GB005856}},
ISSN = {{0886-6236}},
EISSN = {{1944-9224}},
ResearcherID-Numbers = {{Babst, Flurin/C-5651-2017
   Joos, Fortunat/B-4118-2018
   }},
ORCID-Numbers = {{Babst, Flurin/0000-0003-4106-7087
   Joos, Fortunat/0000-0002-9483-6030
   Wilson, Rob/0000-0003-4486-8904}},
Unique-ID = {{ISI:000443940300006}},
}

@article{ ISI:000443168200226,
Author = {Mason, Kylie and Lindberg, Kirstin and Read, Deborah and Borman, Barry},
Title = {{The Importance of Using Public Health Impact Criteria to Develop
   Environmental Health Indicators: The Example of the Indoor Environment
   in New Zealand}},
Journal = {{INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH}},
Year = {{2018}},
Volume = {{15}},
Number = {{8}},
Month = {{AUG}},
Abstract = {{Developing environmental health indicators is challenging and applying a
   conceptual framework and indicator selection criteria may not be
   sufficient to prioritise potential indicators to monitor. This study
   developed a new approach for prioritising potential environmental health
   indicators, using the example of the indoor environment for New Zealand.
   A three-stage process of scoping, selection, and design was implemented.
   A set of potential indicators (including 4 exposure indicators and 20
   health indicators) were initially identified and evaluated against
   indicator selection criteria. The health indicators were then further
   prioritised according to their public health impact and assessed by the
   five following sub-criteria: number of people affected (based on
   environmental burden of disease statistics); severity of health impact;
   whether vulnerable populations were affected and/or large inequalities
   were apparent; whether the indicator related to multiple environmental
   exposures; and policy relevance. Eight core indicators were ultimately
   selected, as follows: living in crowded households, second-hand smoke
   exposure, maternal smoking at two weeks post-natal, asthma prevalence,
   asthma hospitalisations, lower respiratory tract infection
   hospitalisations, meningococcal disease notifications, and sudden
   unexpected death in infancy (SUDI). Additionally, indicators on living
   in damp and mouldy housing and children's injuries in the home, were
   identified as potential indicators, along with attributable burden
   indicators. Using public health impact criteria and an environmental
   burden of disease approach was valuable in prioritising and selecting
   the most important health impacts to monitor, using robust evidence and
   objective criteria.}},
DOI = {{10.3390/ijerph15081786}},
Article-Number = {{1786}},
ISSN = {{1660-4601}},
Unique-ID = {{ISI:000443168200226}},
}

@article{ ISI:000439505600023,
Author = {Beardmore, Robert E. and Cook, Emily and Nilsson, Susanna and Smith,
   Adam R. and Tillmann, Anna and Esquivel, Brooke D. and Haynes, Ken and
   Gow, Neil A. R. and Brown, Alistair J. P. and White, Theodore C. and
   Gudelj, Ivana},
Title = {{Drug-mediated metabolic tipping between antibiotic resistant states in a
   mixed-species community}},
Journal = {{NATURE ECOLOGY \& EVOLUTION}},
Year = {{2018}},
Volume = {{2}},
Number = {{8}},
Pages = {{1312-1320}},
Month = {{AUG}},
Abstract = {{Microbes rarely exist in isolation, rather, they form intricate
   multi-species communities that colonize our bodies and inserted medical
   devices. However, the efficacy of antimicrobials is measured in clinical
   laboratories exclusively using microbial monocultures. Here, to
   determine how multi-species interactions mediate selection for
   resistance during antibiotic treatment, particularly following drug
   withdrawal, we study a laboratory community consisting of two microbial
   pathogens. Single-species dose responses are a poor predictor of
   community dynamics during treatment so, to better understand those
   dynamics, we introduce the concept of a dose-response mosaic, a
   multi-dimensional map that indicates how species' abundance is affected
   by changes in abiotic conditions. We study the dose-response mosaic of a
   two-species community with a `Gene x Gene x Environment x Environment'
   ecological interaction whereby Candida glabrata, which is resistant to
   the antifungal drug fluconazole, competes for survival with Candida
   albicans, which is susceptible to fluconazole. The mosaic comprises
   several zones that delineate abiotic conditions where each species
   dominates. Zones are separated by loci of bifurcations and tipping
   points that identify what environmental changes can trigger the loss of
   either species. Observations of the laboratory communities corroborated
   theory, showing that changes in both antibiotic concentration and
   nutrient availability can push populations beyond tipping points, thus
   creating irreversible shifts in community composition from
   drug-sensitive to drug-resistant species. This has an important
   consequence: resistant species can increase in frequency even if an
   antibiotic is withdrawn because, unwittingly, a tipping point was passed
   during treatment.}},
DOI = {{10.1038/s41559-018-0582-7}},
ISSN = {{2397-334X}},
ORCID-Numbers = {{Brown, Alistair JP/0000-0003-1406-4251
   Gow, Neil/0000-0002-2776-5850}},
Unique-ID = {{ISI:000439505600023}},
}

@article{ ISI:000438687900001,
Author = {Ghorbani, Saeed and Barari, Morteza and Hoseini, Mojtaba},
Title = {{Presenting a new method to improve the detection of micro-seismic events}},
Journal = {{ENVIRONMENTAL MONITORING AND ASSESSMENT}},
Year = {{2018}},
Volume = {{190}},
Number = {{8}},
Month = {{AUG}},
Abstract = {{Seismic events such as earthquakes are one of the most important issues
   in the field of geology. Meanwhile, less attention has been paid to
   micro-seismic events, despite the high number of earthquakes.
   Earthquakes, regardless of their size, affect human life; therefore,
   their detection and management is considered an important issue. For
   this purpose, experts developed seismic arrays as a system of linked
   seismometers. These systems equipped with sensors and seismographs are
   able to receive a range of waves from the earth, which are then sent to
   the central seismic station for analysis. So far, many tools and methods
   have been devised to analyze seismic data. However, the dominant method
   in most seismic mechanisms is trigger function, based on STA/LTA
   (short-time-average through long-time-average trigger). These mechanisms
   have considerable threshold in terms of earthquake range, so many
   micro-events are ignored as noise. Generally, in this field of geology,
   computer science techniques have been used to detect and classify these
   events. Statistical methods such as kurtosis, variance, and skewness can
   be applied to understand the changes in the signal curves of geophones
   in a seismic event, thereby helping in the initial detection of fuzzy
   features. According to the last 3 years' reports of global data mining
   agencies such as Rexer, KDnugget, and Gartner, Rapid Miner is one of the
   most popular tools for data mining in recent years. Furthermore, these
   institutions considered artificial neural networks, especially
   multilayer perceptron (MLP) and base radial function (RBF), to be among
   the most successful algorithms for detection and classification of
   stream data. In this research, the recorded data from several seismic
   experiments has been classified by a hybrid model. Hence, the present
   study was aimed to enhance the authenticity of data based on the
   application of effective variables. This was undertaken through use of a
   fuzzy method and an integrated neural network algorithm, involving MLP
   perceptron and radial network of RBF in the form of a collective
   learning system, in order to identify seismic events on a small scale.
   Based on the results, in comparison to basic methods, the proposed
   method significantly improved using the actual error and
   root-mean-square error (RMSE) criteria.}},
DOI = {{10.1007/s10661-018-6837-6}},
Article-Number = {{464}},
ISSN = {{0167-6369}},
EISSN = {{1573-2959}},
Unique-ID = {{ISI:000438687900001}},
}

@article{ ISI:000438293400009,
Author = {Tuncer, Necibe and Marctheva, Maia and LaBarre, Brian and Payoute,
   Sabrina},
Title = {{Structural and Practical Identifiability Analysis of Zika
   Epidemiological Models}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{2018}},
Volume = {{80}},
Number = {{8}},
Pages = {{2209-2241}},
Month = {{AUG}},
Abstract = {{The Zika virus (ZIKV) epidemic has caused an ongoing threat to global
   health security and spurred new investigations of the virus. Use of
   epidemiological models for arbovirus diseases can be a powerful tool to
   assist in prevention and control of the emerging disease. In this
   article, we introduce six models of ZIKV, beginning with a general
   vector-borne model and gradually including different transmission routes
   of ZIKV. These epidemiological models use various combinations of
   disease transmission (vector and direct) and infectious classes
   (asymptomatic and pregnant), with addition to loss of immunity being
   included. The disease-induced death rate is omitted from the models. We
   test the structural and practical identifiability of the models to find
   whether unknown model parameters can uniquely be determined. The models
   were fit to obtain time-series data of cumulative incidences and
   pregnant infections from the Florida Department of Health Daily Zika
   Update Reports. The average relative estimation errors (AREs) were
   computed from the Monte Carlo simulations to further analyze the
   identifiability of the models. We show that direct transmission rates
   are not practically identifiable; however, fixed recovery rates improve
   identifiability overall. We found ARE is low for each model (only
   slightly higher for those that account for a pregnant class) and help to
   confirm a reproduction number greater than one at the start of the
   Florida epidemic. Basic reproduction number, , is an epidemiologically
   important threshold value which gives the number of secondary cases
   generated by one infected individual in a totally susceptible population
   in duration of infectiousness. Elasticity of the reproduction numbers
   suggests that the mosquito-to-human ratio, mosquito life span and biting
   rate have the greatest potential for reducing the reproduction number of
   Zika, and therefore, corresponding control measures need to be focused
   on.}},
DOI = {{10.1007/s11538-018-0453-z}},
ISSN = {{0092-8240}},
EISSN = {{1522-9602}},
Unique-ID = {{ISI:000438293400009}},
}

@article{ ISI:000434751900003,
Author = {Wu, Xia and Wang, Peijuan and Huo, Zhiguo and Wu, Dingrong and Yang,
   Jianying},
Title = {{Crop Drought Identification Index for winter wheat based on
   evapotranspiration in the Huang-Huai-Hai Plain, China}},
Journal = {{AGRICULTURE ECOSYSTEMS \& ENVIRONMENT}},
Year = {{2018}},
Volume = {{263}},
Pages = {{18-30}},
Month = {{AUG 1}},
Abstract = {{Frequent occurrences of drought events can lead to winter wheat drought
   disasters. To prevent drought damage and reduce potential losses, it is
   important to establish an index to provide support for winter wheat
   drought monitoring, prevention, and mitigation, and further to
   understand the precise spatiotemporal characteristics of winter wheat
   droughts. In this study, meteorological factors, remote sensing
   products, disaster records, and phenophases of winter wheat in the
   Huang-Huai-Hai Plain were integrated to establish a Crop Drought
   Identification Index for winter wheat drought disasters. The CDII was
   expressed as the ratio of actual evapotranspiration (ETa) and crop
   evapotranspiration under standard conditions (ELc), in which ETa was
   simulated through the Two Source Energy Balance (TSEB) model and ETc was
   calculated based on the Penman-Monteith method using daily
   meteorological data and MODIS remotely sensed products. The CDII for
   winter wheat at different developmental stages was formed by
   establishing the drought sample sequences and determining the thresholds
   based on a Lilliefors goodness-of-fit test and the upper threshold of a
   95\% confidence interval. Validation showed that the identification
   results by CDII corresponded with 86.2\% drought records. The spatial
   distributions of drought characteristics for winter wheat in the
   Huang-Huai-Hai Plain were mapped. The thresholds of the CDII at the
   before wintering stage, returning green-jointing stage, and heading
   stage were higher than that at the other two developmental stages.
   Drought frequency was higher across the whole Huang Huai-Hai Plain at
   the before wintering stage and returning green-jointing stage of winter
   wheat. The regions with higher drought frequency were concentrated in
   the northern part of the Huang-Huai-Hai Plain at the heading stage and
   in the mid-western Shandong Province at the milky ripening physiological
   maturity stage. This study took the drought distribution in the
   2006-2007 growing season and heading stages from 2000 to 2013 as
   examples, the results indicated that the CDII could identify the actual
   drought of winter wheat reasonably. The findings indicate the CDII is
   useful for monitoring and assessing winter wheat drought disasters at a
   regional scale. It can also provide a new method for crop drought
   analysis.}},
DOI = {{10.1016/j.agee.2018.05.001}},
ISSN = {{0167-8809}},
EISSN = {{1873-2305}},
Unique-ID = {{ISI:000434751900003}},
}

@article{ ISI:000439889500002,
Author = {Zheng, Bo and Yu, Jianshe},
Title = {{Characterization of Wolbachia enhancing domain in mosquitoes with
   imperfect maternal transmission}},
Journal = {{JOURNAL OF BIOLOGICAL DYNAMICS}},
Year = {{2018}},
Volume = {{12}},
Number = {{1}},
Pages = {{596-610}},
Month = {{JUL 19}},
Abstract = {{A novel method to reduce the burden of dengue is to seed wild mosquitoes
   with Wolbachia-infected mosquitoes in dengue-endemic areas. Concerns in
   current mathematical models are to locate the Wolbachia introduction
   threshold. Our recent findings manifest that the threshold is highly
   dependent on the initial population size once Wolbachia infection alters
   the logistic control death rate of infected females. However, counting
   mosquitoes is beyond the realms of possibility. A plausible method is to
   monitor the infection frequency. We propose the concept of Wolbachia
   enhancing domain in which the infection frequency keeps increasing. A
   detailed description of the domain is presented. Our results suggest
   that both the initial population size and the infection frequency should
   be taken into account for optimal release strategies. Both Wolbachia
   fixation and extinction permit the oscillation of the infection
   frequency.}},
DOI = {{10.1080/17513758.2018.1499969}},
ISSN = {{1751-3758}},
EISSN = {{1751-3766}},
Unique-ID = {{ISI:000439889500002}},
}

@article{ ISI:000438248700015,
Author = {Fabris, Fabio and Doherty, Aoife and Palmer, Daniel and de Magalhaes,
   Joao Pedro and Freitas, Alex A.},
Title = {{A new approach for interpreting Random Forest models and its application
   to the biology of ageing}},
Journal = {{BIOINFORMATICS}},
Year = {{2018}},
Volume = {{34}},
Number = {{14}},
Pages = {{2449-2456}},
Month = {{JUL 15}},
Abstract = {{Motivation: This work uses the Random Forest (RF) classification
   algorithm to predict if a gene is over-expressed, under-expressed or has
   no change in expression with age in the brain. RFs have high predictive
   power, and RF models can be interpreted using a feature (variable)
   importance measure. However, current feature importance measures
   evaluate a feature as a whole (all feature values). We show that, for a
   popular type of biological data (Gene Ontology-based), usually only one
   value of a feature is particularly important for classification and the
   interpretation of the RF model. Hence, we propose a new algorithm for
   identifying the most important and most informative feature values in an
   RF model.
   Results: The new feature importance measure identified highly relevant
   Gene Ontology terms for the aforementioned gene classification task,
   producing a feature ranking that is much more informative to biologists
   than an alternative, state-of-the-art feature importance measure.}},
DOI = {{10.1093/bioinformatics/bty087}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{de Magalhaes, Joao Pedro/B-4741-2010}},
ORCID-Numbers = {{de Magalhaes, Joao Pedro/0000-0002-6363-2465}},
Unique-ID = {{ISI:000438248700015}},
}

@article{ ISI:000432467700035,
Author = {Kalkhajeh, Yusef Kianpoor and Sorensen, Helle and Huang, Biao and Guan,
   Dong-Xing and Luo, Jun and Hu, Wenyou and Holm, Peter E. and Hansen,
   Hans Christian Bruun},
Title = {{DGT technique to assess P mobilization from greenhouse vegetable soils
   in China: A novel approach}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2018}},
Volume = {{630}},
Pages = {{331-339}},
Month = {{JUL 15}},
Abstract = {{Intensive phosphorus (P) inputs to plastic-covered greenhouse vegetable
   production (PGVP) in China has led to excessive soil P accumulation
   increasing the potential for leaching to surface waters. This study
   examined the mobility and hence the potential risk of P losses through
   correlations between soil solution P (PRA) and soil extractable P as
   determined by conventional soil P test methods (STPs) including degree
   of P saturations (DPSs), and diffusive gradient in thin-films (DGT P)
   technique. A total of 75 topsoil samples were chosen from five
   representative Chinese PGVPs covering a wide range of physiochemical
   soil properties and cultivation history. Total P and Olsen P contents
   varied from 260 to 4900, and 5 to 740 mg kg(-1), respectively, while
   P-Sol concentrations were between 0.01 and 10.8 mg L-1 reflecting the
   large differences in vegetation history, fertilization schemes, and soil
   types. Overall, DGT P provided the best correlation with P-SoL (r(2) =
   0.97) demonstrating that DGT P is a versatile measure of P mobility
   regardless of soil type. Among the DPSs tested, oxalate extractable Al
   (DPSOx-Al) had the best con-elation with P-Sol ( (r(2) = 0.87). In the
   STP versus P-Sol relationships, STP break-points above which P
   mobilization increases steeply were 513 mu g L-1 and 190 mg kg(-1) for
   DGT P or Olsen P, respectively, corresponding to P-Sol concentration of
   0.88 mg L-1. However, for P-Sol concentration of 0.1 mg L-1 that
   initiates eutrophication, the corresponding DGT P and Olsen P values
   were 27 mu g L-1 and 22 mg kg(-1), respectively. Over 80\% of the
   investigated soils had DGT P and Olsen P above these values, and thus
   are at risk of P mobilization threatening receiving waters by
   eutrophication. This paper demonstrates that the DGT extracted P is a
   powerful measure for soluble P and hence for assessment of P mobility
   from a broad range of soil types. (C) 2018 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.scitotenv.2018.02.228}},
ISSN = {{0048-9697}},
EISSN = {{1879-1026}},
ResearcherID-Numbers = {{Guan, Dong-Xing/M-2872-2016
   }},
ORCID-Numbers = {{Guan, Dong-Xing/0000-0002-9797-0681
   Hu, Wenyou/0000-0002-2823-2888}},
Unique-ID = {{ISI:000432467700035}},
}

@article{ ISI:000438492800015,
Author = {Chen, Chong and Linse, Katrin and Uematsu, Katsuyuki and Sigwart, Julia
   D.},
Title = {{Cryptic niche switching in a chemosymbiotic gastropod}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2018}},
Volume = {{285}},
Number = {{1882}},
Month = {{JUL 11}},
Abstract = {{Life stages of some animals, including amphibians and insects, are so
   different that they have historically been seen as different species.
   `Metamorphosis' broadly encompasses major changes in organism bodies
   and, importantly, concomitant shifts in trophic strategies. Many marine
   animals have a biphasic lifestyle, with small pelagic larvae undergoing
   one or more metamorphic transformations before settling into a
   permanent, adult morphology on the benthos. Post-settlement, the
   hydrothermal vent gastropod Gigantopelta chessoia experiences a further,
   cryptic metamorphosis at body sizes around 5-7 mm. The terminal adult
   stage is entirely dependent on chemoautotrophic symbionts; smaller
   individuals do not house symbionts and presumably depend on grazing.
   Using high-resolution X-ray microtomography to reconstruct the internal
   organs in a growth series, we show that this sudden transition in small
   but sexually mature individuals dramatically reconfigures the organs,
   but is in no way apparent from external morphology. We introduce the
   term `cryptometamorphosis' to identify this novel phenomenon of a major
   body change and trophic shift, not related to sexual maturity,
   transforming only the internal anatomy. Understanding energy flow in
   ecosystems depends on the feeding ecology of species; the present study
   highlights the possibility for adult animals to make profound shifts in
   biology that influence energy dynamics.}},
DOI = {{10.1098/rspb.2018.1099}},
Article-Number = {{20181099}},
ISSN = {{0962-8452}},
EISSN = {{1471-2954}},
ORCID-Numbers = {{Sigwart, Julia/0000-0002-3005-6246}},
Unique-ID = {{ISI:000438492800015}},
}

@article{ ISI:000440957500011,
Author = {Huang, Qiongyu and Fleming, Christen H. and Robb, Benjamin and
   Lothspeich, Audrey and Songer, Melissa},
Title = {{How different are species distribution model predictions?-Application of
   a new measure of dissimilarity and level of significance to giant panda
   Ailuropoda melanoleuca}},
Journal = {{ECOLOGICAL INFORMATICS}},
Year = {{2018}},
Volume = {{46}},
Pages = {{114-124}},
Month = {{JUL}},
Abstract = {{Species distribution models (SDMs) are widely used for predicting
   species spatial distributions. Different model setup and data input
   however can lead to variable model predictions. Existing studies on
   quantifying SDM dissimilarity primarily rely on partitioning the
   variability in SDM-produced community level metrics such as species
   richness and turnover rate which are threshold-dependent and are
   generated with binary range maps of multiple species. Most existing
   measurements of spatial dissimilarity constitute geometric comparisons,
   which is limited compared to a more information-theoretic application of
   statistical dissimilarity measures using SDM predictions as direct input
   without renormalization. We introduce a novel method to quantify the
   degree of dissimilarity and its level of significance between unscaled
   SDM predictions of a single species. We apply the method to giant panda
   Ailuropoda melanoleuca data as well as pairs of simulated species
   distributions. We utilize a pixel-based Bhattacharyya distance to
   quantify the degree of dissimilarity among predictions of giant panda
   habitat of different combinations of model types, Global Climate Models
   (GCMs) and Representative Concentration Pathways (RCPs). Comparisons are
   also made between pairs of simulated species with different degrees of
   dissimilarity in spatial distribution. To evaluate the level of
   significance, the observed dissimilarity measure is compared against a
   null distribution that captures the level of dissimilarity caused by
   small and random variations. Specific pairs of climate scenarios
   (HadGEM2-ES with HadGEM2-AO and HadGEM2-AO with MIROC5) consistently
   produce statistically similar predictions of giant panda habitat; the
   highest level of RCP tends to result in more similar predictions,
   suggesting a convergence of model predictions. The simulated scenarios
   also show that the proposed method is able to effectively differentiate
   a range of artificial species with varying degree of dissimilarity in
   their resource selection preference. Our method can also reflect the
   dissimilarity that cannot be quantified by traditional metrics that rely
   on geometric comparisons. The proposed method supplements existing
   studies by utilizing a novel application of statistical comparisons to
   measure dissimilarity between user-defined pairs of models. It provides
   a robust way to construct the null distribution of dissimilarity that
   contrasts the degree of the observed dissimilarity with the intrinsic
   model variability. Our study provides useful insight to facilitate
   building more computationally efficient and robust ensemble SDMs, and it
   lends a practical tool to help understand the processes that contribute
   to prediction variability among SDMs.}},
DOI = {{10.1016/j.ecoinf.2018.06.004}},
ISSN = {{1574-9541}},
EISSN = {{1878-0512}},
Unique-ID = {{ISI:000440957500011}},
}

@article{ ISI:000439940200010,
Author = {Jonko, Alexandra and Urban, Nathan M. and Nadiga, Balu},
Title = {{Towards Bayesian hierarchical inference of equilibrium climate
   sensitivity from a combination of CMIP5 climate models and observational
   data}},
Journal = {{CLIMATIC CHANGE}},
Year = {{2018}},
Volume = {{149}},
Number = {{2}},
Pages = {{247-260}},
Month = {{JUL}},
Abstract = {{Despite decades of research, large multi-model uncertainty remains about
   the Earth's equilibrium climate sensitivity to carbon dioxide forcing as
   inferred from state-of-the-art Earth system models (ESMs). Statistical
   treatments of multi-model uncertainties are often limited to simple ESM
   averaging approaches. Sometimes models are weighted by how well they
   reproduce historical climate observations. Here, we propose a novel
   approach to multi-model combination and uncertainty quantification.
   Rather than averaging a discrete set of models, our approach samples
   from a continuous distribution over a reduced space of simple model
   parameters. We fit the free parameters of a reduced-order climate model
   to the output of each member of the multi-model ensemble. The
   reduced-order parameter estimates are then combined using a hierarchical
   Bayesian statistical model. The result is a multi-model distribution of
   reduced-model parameters, including climate sensitivity. In effect, the
   multi-model uncertainty problem within an ensemble of ESMs is converted
   to a parametric uncertainty problem within a reduced model. The
   multi-model distribution can then be updated with observational data,
   combining two independent lines of evidence. We apply this approach to
   24 model simulations of global surface temperature and net
   top-of-atmosphere radiation response to abrupt quadrupling of carbon
   dioxide, and four historical temperature data sets. Our reduced order
   model is a 2-layer energy balance model. We present probability
   distributions of climate sensitivity based on (1) the multi-model
   ensemble alone and (2) the multi-model ensemble and observations.}},
DOI = {{10.1007/s10584-018-2232-0}},
ISSN = {{0165-0009}},
EISSN = {{1573-1480}},
Unique-ID = {{ISI:000439940200010}},
}

@article{ ISI:000439717700007,
Author = {Bastide, Paul and Ane, Cecile and Robin, Stephane and Mariadassou,
   Mahendra},
Title = {{Inference of Adaptive Shifts for Multivariate Correlated Traits}},
Journal = {{SYSTEMATIC BIOLOGY}},
Year = {{2018}},
Volume = {{67}},
Number = {{4}},
Pages = {{662-680}},
Month = {{JUL}},
Abstract = {{To study the evolution of several quantitative traits, the classical
   phylogenetic comparative framework consists of amultivariate random
   process running along the branches of a phylogenetic tree. The
   Ornstein-Uhlenbeck (OU) process is sometimes preferred to the simple
   Brownian motion (BM) as it models stabilizing selection toward an
   optimum. The optimum for each trait is likely to be changing over the
   long periods of time spanned by large modern phylogenies. Our goal is to
   automatically detect the position of these shifts on a phylogenetic
   tree, while accounting for correlations between traits, which might
   exist because of structural or evolutionary constraints. We show that,
   in the presence of shifts, phylogenetic Principal Component Analysis
   fails to decorrelate traits efficiently, so that any method aiming at
   finding shifts needs to deal with correlation simultaneously. We
   introduce here a simplification of the full multivariate OU model, named
   scalar OU, which allows for noncausal correlations and is still
   computationally tractable. We extend the equivalence between the OU and
   a BMon a rescaled tree to our multivariate framework. We describe an
   Expectation-Maximization (EM) algorithm that allows for a maximum
   likelihood estimation of the shift positions, associated with a new
   model selection criterion, accounting for the identifiability issues for
   the shift localization on the tree. The method, freely available as an
   R-package (PhylogeneticEM) is fast, and can deal withmissing values. We
   demonstrate its efficiency and accuracy compared to another
   state-of-the-art method (l1ou) on a wide range of simulated scenarios
   and use this new framework to reanalyze recently gathered data sets
   onNewWorldMonkeys and Anolis lizards.}},
DOI = {{10.1093/sysbio/syy005}},
ISSN = {{1063-5157}},
EISSN = {{1076-836X}},
Unique-ID = {{ISI:000439717700007}},
}

@article{ ISI:000438247800039,
Author = {Nouri, Nima and Kleinstein, Steven H.},
Title = {{A spectral clustering-based method for identifying clones from
   high-throughput B cell repertoire sequencing data}},
Journal = {{BIOINFORMATICS}},
Year = {{2018}},
Volume = {{34}},
Number = {{13}},
Pages = {{341-349}},
Month = {{JUL 1}},
Note = {{26th Annual Conference on Intelligent Systems for Molecular Biology
   (ISMB), Chicago, IL, JUL 06-10, 2018}},
Abstract = {{Motivation: B cells derive their antigen-specificity through the
   expression of Immunoglobulin (Ig) receptors on their surface. These
   receptors are initially generated stochastically by somatic
   rearrangement of the DNA and further diversified following
   antigen-activation by a process of somatic hypermutation, which
   introduces mainly point substitutions into the receptor DNA at a high
   rate. Recent advances in next-generation sequencing have enabled
   large-scale profiling of the B cell Ig repertoire from blood and tissue
   samples. A key computational challenge in the analysis of these data is
   partitioning the sequences to identify descendants of a common B cell
   (i.e. a clone). Current methods group sequences using a fixed distance
   threshold, or a likelihood calculation that is
   computationally-intensive. Here, we propose a new method based on
   spectral clustering with an adaptive threshold to determine the local
   sequence neighborhood. Validation using simulated and experimental
   datasets demonstrates that this method has high sensitivity and
   specificity compared to a fixed threshold that is optimized for these
   measures. In addition, this method works on datasets where choosing an
   optimal fixed threshold is difficult and is more computationally
   efficient in all cases. The ability to quickly and accurately identify
   members of a clone from repertoire sequencing data will greatly improve
   downstream analyses. Clonally-related sequences cannot be treated
   independently in statistical models, and clonal partitions are used as
   the basis for the calculation of diversity metrics, lineage
   reconstruction and selection analysis. Thus, the spectral
   clustering-based method here represents an important contribution to
   repertoire analysis.}},
DOI = {{10.1093/bioinformatics/bty235}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000438247800039}},
}

@article{ ISI:000437096400012,
Author = {Wei, Hua and Zhang, Xiao-Hua and Clift, Cassandra and Yamaguchi, Naohiro
   and Morad, Martin},
Title = {{CRISPR/Cas9 Gene editing of RyR2 in human stem cell-derived
   cardiomyocytes provides a novel approach in investigating dysfunctional
   Ca2+ signaling}},
Journal = {{CELL CALCIUM}},
Year = {{2018}},
Volume = {{73}},
Pages = {{104-111}},
Month = {{JUL}},
Abstract = {{Type-2 ryanodine receptors (RyR2s) play a pivotal role in cardiac
   excitation-contraction coupling by releasing Ca2+ from sarcoplasmic
   reticulum (SR) via a Ca2+ -induced Ca2+ release (CICR) mechanism. Two
   strategies have been used to study the structure-function
   characteristics of RyR2 and its disease associated mutations: (1)
   heterologous cell expression of the recombinant mutant RyR2s, and (2)
   knock-in mouse models harboring RyR2 point mutations. Here, we establish
   an alternative approach where Ca2+ signaling aberrancy caused by the
   RyR2 mutation is studied in human cardiomyocytes with robust CICR
   mechanism. Specifically, we introduce point mutations in wild-type RYR2
   of human induced pluripotent stem cells (hiPSCs) by CRISPR/Cas9 gene
   editing, and then differentiate them into cardiomyocytes. To verify the
   reliability of this approach, we introduced the same disease-associated
   RyR2 mutation, F2483I, which was studied by us in hiPSC-derived
   cardiomyocytes (hiPSC-CMs) from a patient biopsy. The gene-edited F2483I
   hiPSC-CMs exhibited longer and wandering Ca2+ sparks, elevated diastolic
   Ca2+ leaks, and smaller SR Ca2+ stores, like those of patient-derived
   cells. Our CRISPR/Cas9 gene editing approach validated the feasibility
   of creating myocytes expressing the various RyR2 mutants, making
   comparative mechanistic analysis and pharmacotherapeutic approaches for
   RyR2 pathologies possible.}},
DOI = {{10.1016/j.ceca.2018.04.009}},
ISSN = {{0143-4160}},
EISSN = {{1532-1991}},
Unique-ID = {{ISI:000437096400012}},
}

@article{ ISI:000436855200007,
Author = {Zan, Yanjun and Carlborg, Orjan},
Title = {{A multilocus association analysis method integrating phenotype and
   expression data reveals multiple novel associations to flowering time
   variation in wild-collected Arabidopsis thaliana}},
Journal = {{MOLECULAR ECOLOGY RESOURCES}},
Year = {{2018}},
Volume = {{18}},
Number = {{4}},
Pages = {{798-808}},
Month = {{JUL}},
Abstract = {{The adaptation to a new habitat often results in a confounding between
   genomewide genotype and beneficial alleles. When the confounding is
   strong, or the allelic effects is weak, it is a major statistical
   challenge to detect the adaptive polymorphisms. We describe a novel
   approach to dissect polygenic traits in natural populations. First,
   candidate adaptive loci are identified by screening for loci directly
   associated with the adaptive trait or the expression of genes known to
   affect it. Then, a multilocus genetic architecture is inferred using a
   backward elimination association analysis across all candidate loci with
   an adaptive false discovery rate-based threshold. Effects of population
   stratification are controlled by accounting for genomic kinship in both
   steps of the analysis and also by simultaneously testing all candidate
   loci in the multilocus model. We illustrate the method by exploring the
   polygenic basis of an important adaptive trait, flowering time in
   Arabidopsis thaliana, using public data from the 1,001 genomes project.
   We revealed associations between 33 (29) loci and flowering time at 10
   (16)degrees C in this collection of natural accessions, where standard
   genomewide association analysis methods detected five (3) loci. The 33
   (29) loci explained approximately 55.1 (48.7)\% of the total phenotypic
   variance of the respective traits. Our work illustrates how the genetic
   basis of highly polygenic adaptive traits in natural populations can be
   explored in much greater detail using new multilocus mapping approaches
   taking advantage of prior biological information, genome and
   transcriptome data.}},
DOI = {{10.1111/1755-0998.12757}},
ISSN = {{1755-098X}},
EISSN = {{1755-0998}},
Unique-ID = {{ISI:000436855200007}},
}

@article{ ISI:000435970600011,
Author = {Ebrahimzadeh, Elias and Manuchehri, Mohammad Sajad and Amoozegar, Sana
   and Araabi, Babak Nadjar and Soltanian-Zadeh, Hamid},
Title = {{A time local subset feature selection for prediction of sudden cardiac
   death from ECG signal}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2018}},
Volume = {{56}},
Number = {{7}},
Pages = {{1253-1270}},
Month = {{JUL}},
Abstract = {{Prediction of sudden cardiac death continues to gain universal attention
   as a promising approach to saving millions of lives threatened by sudden
   cardiac death (SCD). This study attempts to promote the literature from
   mere feature extraction analysis to developing strategies for
   manipulating the extracted features to target improvement of
   classification accuracy. To this end, a novel approach to local feature
   subset selection is applied using meticulous methodologies developed in
   previous studies of this team for extracting features from non-linear,
   time-frequency, and classical processes. We are therefore enabled to
   select features that differ from one another in each 1-min interval
   before the incident. Using the proposed algorithm, SCD can be predicted
   12 min before the onset; thus, more propitious results are achieved.
   Additionally, through defining a utility function and employing
   statistical analysis, the alarm threshold has effectively been
   determined as 83\%. Having selected the best combination of features,
   the two classes are classified using the multilayer perceptron (MLP)
   classifier. The most effective features would subsequently be discussed
   considering their prevalence in the rank-based selection. The results
   indicate the significant capacity of the proposed method for predicting
   SCD as well as selecting the appropriate processing method at any time
   before the incident.}},
DOI = {{10.1007/s11517-017-1764-1}},
ISSN = {{0140-0118}},
EISSN = {{1741-0444}},
ORCID-Numbers = {{Ebrahimzadeh, Elias/0000-0001-8682-936X}},
Unique-ID = {{ISI:000435970600011}},
}

@article{ ISI:000434754600085,
Author = {Ye, Jianfeng and Xu, Zuxin and Chen, Hao and Wang, Liang and Benoit,
   Gaboury},
Title = {{Reduction of clog matter in constructed wetlands by metabolism of
   Eisenia foetida: Process and modeling}},
Journal = {{ENVIRONMENTAL POLLUTION}},
Year = {{2018}},
Volume = {{238}},
Pages = {{803-811}},
Month = {{JUL}},
Abstract = {{Introducing of earthworms to constructed wetlands (CWs) has been
   considered as a new approach to solve the clogging problems in the
   long-established systems. Despite its potential advantage, the
   correlational researches are still in the stage of preliminary
   observation and speculation. This paper presents a comprehensive and
   in-depth research about the positive effects of earthworms (Eisenia
   foetida) on clog matter (CM) reduction through different pathways,
   including in vivo metabolism and uptake, conversion, transport, and
   promotion of microorganism quantities. The results showed that the
   metabolism and uptake by Eisenia foetida could effectively reduce the CM
   content at an average removal rate of 0.155 mg g(-1), d(-1), which was
   obviously higher than the rate of CM decomposition by microorganisms
   alone. Through the metabolism of earthworms, the amounts of proteins and
   polysaccharides in CM were decreased, while the amounts of humin and
   nucleic acids were increased. Simultaneously, the viscosity of CM was
   reduced by 0.0082 mPa s g(-1) d(-1), and the quantity of microorganisms
   was increased by 0.0109 mg g(-1) d(-1), which finally made the treated
   CM can be easily washed away and decomposed. Furthermore, earthworms
   could reduce the CM content in the clogging layer by transporting the
   metabolic products out. A regression model was further performed for
   describing the interaction between earthworm and CM. The simulated value
   of porosity fitted well with the measured one, suggesting that the
   earthworms can increase the substrate porosity at a rate of 0.33 mL
   g(-1) d(-1). This study quantitively depicted the mechanisms of
   earthworms on the decrement of CM content in CWs, which is of great
   benefit for the engineering management of constructed wetlands in the
   future. We also proposed that the density of introduced earthworms
   should exceed a certain threshold for effectively increasing the
   substrate porosity and solving the clogging problems. (C) 2018 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.envpol.2018.03.062}},
ISSN = {{0269-7491}},
EISSN = {{1873-6424}},
Unique-ID = {{ISI:000434754600085}},
}

@article{ ISI:000433616600010,
Author = {Moser, Carlee B. and White, Laura F.},
Title = {{Estimating age-specific reproductive numbersA comparison of methods}},
Journal = {{STATISTICAL METHODS IN MEDICAL RESEARCH}},
Year = {{2018}},
Volume = {{27}},
Number = {{7}},
Pages = {{2050-2059}},
Month = {{JUL}},
Abstract = {{Large outbreaks, such as those caused by influenza, put a strain on
   resources necessary for their control. In particular, children have been
   shown to play a key role in influenza transmission during recent
   outbreaks, and targeted interventions, such as school closures, could
   positively impact the course of emerging epidemics. As an outbreak is
   unfolding, it is important to be able to estimate reproductive numbers
   that incorporate this heterogeneity and to use surveillance data that is
   routinely collected to more effectively target interventions and obtain
   an accurate understanding of transmission dynamics. There are a growing
   number of methods that estimate age-group specific reproductive numbers
   with limited data that build on methods assuming a homogenously mixing
   population. In this article, we introduce a new approach that is
   flexible and improves on many aspects of existing methods. We apply this
   method to influenza data from two outbreaks, the 2009 H1N1 outbreaks in
   South Africa and Japan, to estimate age-group specific reproductive
   numbers and compare it to three other methods that also use existing
   data from social mixing surveys to quantify contact rates among
   different age groups. In this exercise, all estimates of the
   reproductive numbers for children exceeded the critical threshold of one
   and in most cases exceeded those of adults. We introduce a flexible new
   method to estimate reproductive numbers that describe heterogeneity in
   the population.}},
DOI = {{10.1177/0962280216673676}},
ISSN = {{0962-2802}},
EISSN = {{1477-0334}},
ORCID-Numbers = {{White, Laura/0000-0002-0588-8235}},
Unique-ID = {{ISI:000433616600010}},
}

@article{ ISI:000432462000043,
Author = {Menesguen, Alain and Desmit, Xavier and Duliere, Valerie and Lacroix,
   Genevieve and Thouvenin, Benedicte and Thieu, Vincent and Dussauze,
   Morgan},
Title = {{How to avoid eutrophication in coastal seas? A new approach to derive
   river-specific combined nitrate and phosphate maximum concentrations}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2018}},
Volume = {{628-629}},
Pages = {{400-414}},
Month = {{JUL 1}},
Abstract = {{Since 1950, increase in nitrogen (N) and phosphorus (P) river loadings
   in the North-East Atlantic (NEA) continental seas has induced a deep
   change in the marine coastal ecosystems, leading to eutrophication
   symptoms in some areas. In order to recover a Good Ecological Status
   (GES) in the NEA, as required by European Water Framework Directive
   (WFD) and Marine Strategy Framework Directive (MSFD), reductions in N-
   and P-river loadings are necessary but they need to be minimal due to
   their economic impact on the farming industry. In the frame of the
   ``EMoSEM{''} European project, we used two marine 3D ecological models
   (ECO-MARS3D, MIRO\&CO) covering the Bay of Biscay, the English Channel
   and the southern North Sea to estimate the contributions of various
   sources (riverine, oceanic and atmospheric) to the winter nitrate and
   phosphate marine concentrations. The various distributed descriptors
   provided by the simulations allowed also to find a log-linear
   relationship between the 90th percentile of satellite-derived
   chlorophyll concentrations and the ``fully bioavailable{''} nutrients,
   i.e. simulated nutrient concentrations weighted by light and
   stoichiometric limitation factors. Any GES threshold on the 90th
   percentile of marine chlorophyll concentration can then be translated in
   maximum admissible `fully bioavailable' DIN and DIP concentrations, from
   which an iterative linear optimization method can compute river-specific
   minimal abatements of N and P loadings. The method has been applied to
   four major river groups, assuming either a conservative (8 mu g Chl L-1)
   or a more socially acceptable (15 mu g Chl L-1) GES chlorophyll
   concentration threshold. In the conservative case, maximum admissible
   winter concentrations for nutrients correspond to marine background
   values, whereas in the lenient case, they are close to values
   recommended by the WFD/MSFD. Both models suggest that to reach
   chlorophyll GES, strong reductions of DIN and DIP are required in the
   Eastern French and Belgian-Dutch river groups. (C) 2018 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.scitotenv.2018.02.025}},
ISSN = {{0048-9697}},
EISSN = {{1879-1026}},
Unique-ID = {{ISI:000432462000043}},
}

@article{ ISI:000431748200007,
Author = {Peng, Jian and Hu, Yi'na and Liu, Yanxu and Ma, Jing and Zhao, Shiquan},
Title = {{A new approach for urban-rural fringe identification: Integrating
   impervious surface area and spatial continuous wavelet transform}},
Journal = {{LANDSCAPE AND URBAN PLANNING}},
Year = {{2018}},
Volume = {{175}},
Pages = {{72-79}},
Month = {{JUL}},
Abstract = {{Urban-rural fringe is the frontier of urban expansion and the most
   dynamic area in urban region. Identifying the urban-rural fringe
   accurately is of great significance, as it helps to measure the extent
   of urbanization and its environmental effects from the urban-rural
   contrast perspective. However, traditional studies with spatial discrete
   data and threshold methods make it difficult to identify its boundary
   objectively and accurately. To solve this problem, a new approach was
   proposed in this study, integrating the spatial continuous data of
   impervious surface area (ISA) and the method of spatial continuous
   wavelet transform (SCWT). Taking Beijing City, one of the largest city
   in China, as a case, this study identified the boundary of urban-rural
   fringe, which extended mainly in the southwestern and northeastern
   direction, with the area increased by 18.75\% between 2009 and 2014. The
   accuracy of the identification result was verified by spatial
   differentiation characteristics of nighttime light and normalized
   difference vegetation index. Meanwhile, when compared with the
   subjective threshold method, it showed that SCWT could identify the
   fringe more accurately. And the ISA based identification result was more
   consistent with the actual spatial patterns of urban development in
   Beijing City than land use degree index based one, which showed the
   remotely sensed spatial continuous data performed more effectively well
   on identifying the urban-rural fringe. The proposed approach can
   identify the urban-rural fringe directly from remote sensing images,
   which is of great significance to quickly urbanization monitoring and
   thus sustainable urban planning and management.}},
DOI = {{10.1016/j.landurbplan.2018.03.008}},
ISSN = {{0169-2046}},
EISSN = {{1872-6062}},
Unique-ID = {{ISI:000431748200007}},
}

@article{ ISI:000433650700023,
Author = {Yoo, Jung-Moon and Choo, Gyo-Hwang and Lee, Kwon-Ho and Wu, Dong L. and
   Yang, Jung-Hyun and Park, Jun-Dong and Choi, Yong-Sang and Shin,
   Dong-Bin and Jeong, Jin-Hee and Yoo, Jung-Min},
Title = {{Improved detection of low stratus and fog at dawn from dual
   geostationary (COMS and FY-2D) satellites}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{211}},
Pages = {{292-306}},
Month = {{JUN 15}},
Abstract = {{A novel method was proposed to detect low stratus and fog (LSF) at dawn
   during the summertime using near simultaneous observations from dual
   geostationary-orbit satellites (GEOs): the Korean Communication, Ocean
   and Meteorological Satellite (COMS; 128.2 degrees E) and the Chinese
   Feng-Yun-2D (FY-2D; 86.5 degrees E). The orbital positions of the GEOs
   provided a large difference (similar to 46.5 degrees) in the viewing
   zenith angle (VZA) in the study region (122-132 degrees E, 32.5-42.5
   degrees N) and high contrast observations at dawn. Numerical simulations
   were carried out to derive the optical properties of the LSF and the
   radiative differences between the GEOs due to both the VZA and spectral
   response function (SRF). The conventional threshold method, which used
   the visible reflectance at 0.67 mu m (120.67) and the brightness
   temperature difference between 3.7 mu m and 11 mu m (BTD3.7-11), had
   limitations in detecting LSF at dawn due to weak visible light and fast
   night-to-day transition. We utilized the observed stereo differences
   (Delta R-0.67, Delta BTD3.7-11) between the two GEOs to improve LSF
   detection. The dual-satellite observations were verified with
   ground-based data from 45 stations in South Korea co-located with each
   GEO pixel. The Delta R-0.67 threshold value showed better accuracy (78\%
   vs. 67\%) than the conventional R-0.67 threshold method. In addition,
   the Delta BTD3.7-11 threshold was better (55\% vs. 38\%) than the Delta
   BTD3.7-11 threshold. The dual-satellite method allowed more reliable LSF
   detection using the combination of Delta R-0.67 and R-0.67, particularly
   for LSF without cumuliform or high clouds. Our method is applicable to
   multiple geostationary satellites for continuous LSF monitoring.}},
DOI = {{10.1016/j.rse.2018.04.019}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Wu, Dong/D-5375-2012
   }},
ORCID-Numbers = {{Lee, Kwon-Ho/0000-0002-0844-5245}},
Unique-ID = {{ISI:000433650700023}},
}

@article{ ISI:000433650700032,
Author = {Devred, E. and Martin, J. and Sathyendranath, S. and Stuart, V. and
   Horne, E. and Platt, T. and Forget, M. -H. and Smith, P.},
Title = {{Development of a conceptual warning system for toxic levels of
   Alexandrium fundyense in the Bay of Fundy based on remote sensing data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{211}},
Pages = {{413-424}},
Month = {{JUN 15}},
Abstract = {{Harmful algal blooms (HABs) present a potential danger for human health
   and commercial activities, especially in coastal regions. Observing
   systems increasingly rely on remote sensors to monitor and possibly
   predict the locations and intensity of such blooms. Here we present a
   novel approach for detecting HABs of Alexandrium fundyense in the Bay of
   Fundy, Canada. A. fundyense is considered toxic for individuals who
   consume shellfish when cell abundances adjacent to shellfish harvesting
   areas are as low as 200 cells.L-1, making it difficult to use direct
   remote sensing techniques to assess the threat in the early stages of
   the development of the bloom. Using in situ A. fundyense cell abundance
   measurements, together with satellite observations of sea-surface
   temperature and the occurrence of diatom-dominated phytoplankton
   populations, a warning system was developed based on three levels of
   alerts: green (low abundance of A. fundyense), orange (possible threat
   of A. fundyense) and red (high probability of A. fundyense
   concentrations that would result in shellfish toxicity above safe levels
   for human consumption). Combined information on diatom phenology and
   variations in sea-surface temperature are key to the timing of A.
   fundyense blooms: our data reveal that the termination of the diatom
   spring bloom, associated with the warming of the water, can trigger an
   increase in A. fundyense cell abundance. The objective criteria for a
   HAB warning system was developed and tested in the Bay of Fundy using
   two different datasets: one to develop the algorithm (data collected
   between 1998 and 2007) and one to assess its performance (data collected
   in 2011). The warning system is based on the cautionary principle that a
   false negative (warning not issued when it should have been) is far more
   serious than a false positive (warning issued when it should not have
   been). The overall success of the algorithm when tested on the
   validation dataset is about 70\% using a threshold of 150 A. fundyense
   cells.L-1, with a low occurrence of false negative red alerts (< 8\%).
   The satellite data -based warning can be used to optimize an in situ
   monitoring system, which can be designed to be more intensive when the
   warning status is orange or red. This study demonstrates that combined
   satellite information on phytoplankton phonology and sea-surface
   temperature can help predict low abundances of toxic A. fundyense cells.
   It also highlights the importance of an integrated approach combining
   satellite and in situ observations to monitor HABs.}},
DOI = {{10.1016/j.rse.2018.04.022}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
Unique-ID = {{ISI:000433650700032}},
}

@article{ ISI:000435115500009,
Author = {Jo, Yousang and Kim, Sanghyeon and Lee, Doheon},
Title = {{Identification of common coexpression modules based on quantitative
   network comparison}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2018}},
Volume = {{19}},
Number = {{8}},
Month = {{JUN 13}},
Note = {{11th International Workshop on Data and Text Mining in Bimedical
   Informatics (DTMBIO), Singapore, SINGAPORE, NOV 10, 2017}},
Abstract = {{Background: Finding common molecular interactions from different samples
   is essential work to understanding diseases and other biological
   processes. Coexpression networks and their modules directly reflect
   sample-specific interactions among genes. Therefore, identification of
   common coexpression network or modules may reveal the molecular
   mechanism of complex disease or the relationship between biological
   processes. However, there has been no quantitative network comparison
   method for coexpression networks and we examined previous methods for
   other networks that cannot be applied to coexpression network.
   Therefore, we aimed to propose quantitative comparison methods for
   coexpression networks and to find common biological mechanisms between
   Huntington's disease and brain aging by the new method.
   Results: We proposed two similarity measures for quantitative comparison
   of coexpression networks. Then, we performed experiments using known
   coexpression networks. We showed the validity of two measures and
   evaluated threshold values for similar coexpression network pairs from
   experiments. Using these similarity measures and thresholds, we
   quantitatively measured the similarity between disease-specific and
   aging-related coexpression modules and found similar Huntington's
   disease-aging coexpression module pairs.
   Conclusions: We identified similar Huntington's disease-aging
   coexpression module pairs and found that these modules are related to
   brain development, cell death, and immune response. It suggests that
   up-regulated cell signalling related cell death and immune/inflammation
   response may be the common molecular mechanisms in the pathophysiology
   of HD and normal brain aging in the frontal cortex.}},
DOI = {{10.1186/s12859-018-2193-3}},
Article-Number = {{213}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000435115500009}},
}

@article{ ISI:000434975000001,
Author = {Hadigol, Mohammad and Khiabanian, Hossein},
Title = {{MERIT reveals the impact of genomic context on sequencing error rate in
   ultra-deep applications}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2018}},
Volume = {{19}},
Month = {{JUN 8}},
Abstract = {{Background: Rapid progress in high-throughput sequencing (HTS) and the
   development of novel library preparation methods have improved the
   sensitivity of detecting mutations in heterogeneous samples,
   specifically in high-depth (> 500x) clinical applications. However, HTS
   methods are bounded by their technical and theoretical limitations and
   sequencing errors cannot be completely eliminated. Comprehensive
   quantification of the background noise can highlight both the efficiency
   and the limitations of any HTS methodology, and help differentiate true
   mutations at low abundance from artifacts.
   Results: We introduce MERIT (Mutation Error Rate Inference Toolkit),
   designed for in-depth quantification of erroneous substitutions and
   small insertions and deletions. MERIT incorporates an all-inclusive
   variant caller and considers genomic context, including the nucleotides
   immediately at 5' and 3', thereby establishing error rates for 96
   possible substitutions as well as four single-base and 16 double-base
   indels. We applied MERIT to ultra-deep sequencing data (1,300,000x)
   obtained from the amplification of multiple clinically relevant loci,
   and showed a significant relationship between error rates and genomic
   contexts. In addition to observing significant difference between
   transversion and transition rates, we identified variations of more than
   100-fold within each error type at high sequencing depths. For instance,
   T>G transversions in trinucleotide GTCs occurred 133.5 +/- 65.9 more
   often than those in ATAs. Similarly, C>T transitions in GCGs were
   observed at 73.8 +/- 10.5 higher rate than those in TCTs. We also
   devised an in silico approach to determine the optimal sequencing depth,
   where errors occur at rates similar to those of expected true mutations.
   Our analyses showed that increasing sequencing depth might improve
   sensitivity for detecting some mutations based on their genomic context.
   For example, T>G rate of error in GTCs did not change when sequenced
   beyond 10,000x; in contrast, T>G rate in TTAs consistently improved even
   at above 500,000x.
   Conclusions: Our results demonstrate significant variation in nucleotide
   misincorporation rates, and suggest that genomic context should be
   considered for comprehensive profiling of specimen-specific and
   sequencing artifacts in high-depth assays. This data provide strong
   evidence against assigning a single allele frequency threshold to call
   mutations, for it can result in substantial false positive as well as
   false negative variants, with important clinical consequences.}},
DOI = {{10.1186/s12859-018-2223-1}},
Article-Number = {{219}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Khiabanian, Hossein/0000-0003-1446-4394}},
Unique-ID = {{ISI:000434975000001}},
}

@article{ ISI:000434892900034,
Author = {Ikehara, Ryohei and Suetake, Mizuki and Komiya, Tatsuki and Furuki,
   Genki and Ochiai, Asumi and Yamasaki, Shinya and Bower, William R. and
   Law, Gareth T. W. and Ohnuki, Toshihiko and Grambow, Bernd and Ewing,
   Rodney C. and Utsunomiya, Satoshi},
Title = {{Novel Method of Quantifying Radioactive Cesium-Rich Microparticles
   (CsMPs) in the Environment from the Fukushima Daiichi Nuclear Power
   Plant}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2018}},
Volume = {{52}},
Number = {{11}},
Pages = {{6390-6398}},
Month = {{JUN 5}},
Abstract = {{Highly radioactive cesium-rich microparticles (CsMPs) were released from
   the Fukushima Daiichi nuclear power plant (FDNPP) to the surrounding
   environment at an early stage of the nuclear disaster in March of 2011;
   however, the quantity of released CsMPs remains undetermined. Here, we
   report a novel method to quantify the number of CsMPs in surface soils
   at or around Fukushima and the fraction of radioactivity they
   contribute, which we call ``quantification of CsMPs{''} (QCP) and is
   based on autoradiography. Here, photostimulated luminescence (PSL) is
   linearly correlated to the radioactivity of various microparticles, with
   a regression coefficient of 0.0523 becquerel/PSL/h (Bq/PSL/h). In soil
   collected from Nagadoro, Fukushima, Japan, CsMPs were detected in soil
   sieved with a 114 mu m mesh. There was no overlap between the
   radioactivities of CsMPs and clay particles adsorbing Cs. Based on the
   distribution of radioactivity of CsMPs, the threshold radioactivity of
   CsMPs in the size fraction of <114 mu m was determined to be 0.06 Bq.
   Based on this method, the number and radioactivity fraction of CsMPs in
   four surface soils collected from the vicinity of the FDNPP were
   determined to be 48-318 particles per gram and 8.53-31.8\%,
   respectively. The QCP method is applicable to soils with a total
   radioactivity as high as , similar to 10(6) Bq/kg. This novel method is
   critically important and can be used to quantitatively understand the
   distribution and migration of the highly radioactive CsMPs in
   near-surface environments surrounding Fukushima.}},
DOI = {{10.1021/acs.est.7b06693}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Utsunomiya, Satoshi/S-3934-2017
   }},
ORCID-Numbers = {{Utsunomiya, Satoshi/0000-0002-6197-9705
   Law, Gareth/0000-0002-2320-6330}},
Unique-ID = {{ISI:000434892900034}},
}

@article{ ISI:000437111000004,
Author = {Lee, Alexandra J. and Chang, Ivan and Burel, Julie G. and Arlehamn,
   Cecilia S. Lindestam and Mandava, Aishwarya and Weiskopf, Daniela and
   Peters, Bjoern and Sette, Alessandro and Scheuermann, Richard H. and
   Qian, Yu},
Title = {{DAFi: A directed recursive data filtering and clustering approach for
   improving and interpreting data clustering identification of cell
   populations from polychromatic flow cytometry data}},
Journal = {{CYTOMETRY PART A}},
Year = {{2018}},
Volume = {{93A}},
Number = {{6}},
Pages = {{597-610}},
Month = {{JUN}},
Abstract = {{Computational methods for identification of cell populations from
   polychromatic flow cytometry data are changing the paradigm of cytometry
   bioinformatics. Data clustering is the most common computational
   approach to unsupervised identification of cell populations from
   multidimensional cytometry data. However, interpretation of the
   identified data clusters is labor-intensive. Certain types of
   user-defined cell populations are also difficult to identify by fully
   automated data clustering analysis. Both are roadblocks before a
   cytometry lab can adopt the data clustering approach for cell population
   identification in routine use. We found that combining recursive data
   filtering and clustering with constraints converted from the user manual
   gating strategy can effectively address these two issues. We named this
   new approach DAFi: Directed Automated Filtering and Identification of
   cell populations. Design of DAFi preserves the data-driven
   characteristics of unsupervised clustering for identifying novel cell
   subsets, but also makes the results interpretable to experimental
   scientists through mapping and merging the multidimensional data
   clusters into the user-defined two-dimensional gating hierarchy. The
   recursive data filtering process in DAFi helped identify small data
   clusters which are otherwise difficult to resolve by a single run of the
   data clustering method due to the statistical interference of the
   irrelevant major clusters. Our experiment results showed that the
   proportions of the cell populations identified by DAFi, while being
   consistent with those by expert centralized manual gating, have smaller
   technical variances across samples than those from individual manual
   gating analysis and the nonrecursive data clustering analysis. Compared
   with manual gating segregation, DAFi-identified cell populations avoided
   the abrupt cut-offs on the boundaries. DAFi has been implemented to be
   used with multiple data clustering methods including K-means, FLOCK,
   FlowSOM, and the ClusterR package. For cell population identification,
   DAFi supports multiple options including clustering, bisecting,
   slope-based gating, and reversed filtering to meet various autogating
   needs from different scientific use cases. (c) 2018 International
   Society for Advancement of Cytometry}},
DOI = {{10.1002/cyto.a.23371}},
ISSN = {{1552-4922}},
EISSN = {{1552-4930}},
ORCID-Numbers = {{Lindestam Arlehamn, Cecilia/0000-0001-7302-8002
   Weiskopf, Daniela/0000-0003-2968-7371}},
Unique-ID = {{ISI:000437111000004}},
}

@article{ ISI:000436621100017,
Author = {An, Shuai and Zhao, Yong-Fang and Lu, Xiao-Ying and Wang, Zhi-Gong},
Title = {{Quantitative evaluation of extrinsic factors influencing electrical
   excitability in neuronal networks: Voltage Threshold Measurement Method
   (VTMM)}},
Journal = {{NEURAL REGENERATION RESEARCH}},
Year = {{2018}},
Volume = {{13}},
Number = {{6}},
Pages = {{1026-1035}},
Month = {{JUN}},
Abstract = {{The electrical excitability of neural networks is influenced by
   different environmental factors. Effective and simple methods arc
   required to objectively and quantitatively evaluate the influence of
   such factors, including variations in temperature and pharmaceutical
   dosage. The aim of this paper was to introduce `the voltage threshold
   measurement method', which is a new method using microelectrode arrays
   that can quantitatively evaluate the influence of different factors on
   the electrical excitability of neural networks. We sought to verify the
   feasibility and efficacy of the method by studying the effects of
   acetylcholine, ethanol, and temperature on hippocampal neuronal networks
   and hippocampal brain slices. First, we determined the voltage of the
   stimulation pulse signal that elicited action potentials in the two
   types of neural networks under normal conditions. Second, we obtained
   the voltage thresholds for the two types of neural networks under
   different concentrations of acetylcholine, ethanol, and different
   temperatures. Finally, we obtained the relationship between voltage
   threshold and the three influential factors. Our results indicated that
   the normal voltage thresholds of the hippocampal neuronal network and
   hippocampal slice preparation were 56 and 31 mV, respectively. The
   voltage thresholds of the two types of neural networks were inversely
   proportional to acetylcholine concentration, and had an exponential
   dependency on ethanol concentration. The curves of the voltage threshold
   and the temperature of the medium for the two types of neural networks
   were U-shaped. The hippocampal neuronal network and hippocampal slice
   preparations lost their excitability when the temperature of the medium
   decreased below 34 and 33 degrees C or increased above 42 and 43 degrees
   C, respectively. These results demonstrate that the voltage threshold
   measurement method is effective and simple for examining the
   performance/excitability of neuronal networks.}},
DOI = {{10.4103/1673-5374.233446}},
ISSN = {{1673-5374}},
EISSN = {{1876-7958}},
Unique-ID = {{ISI:000436621100017}},
}

@article{ ISI:000435053200021,
Author = {Attila, Jenni and Kauppila, Pirkko and Kallio, Kari Y. and Alasalmi,
   Hanna and Keto, Vesa and Bruun, Eeva and Koponen, Sampsa},
Title = {{Applicability of Earth Observation chlorophyll-a data in assessment of
   water status via MERIS - With implications for the use of OLCI sensors}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{212}},
Pages = {{273-287}},
Month = {{JUN}},
Abstract = {{Earth Observation (EO) offers spatially and temporally unique data for
   generating information required under various environmental regulations
   for assessing the status of surface waters. These requirements, which
   are laid down in, for example, European Union directives and the Clean
   Water Act in the United States, share two core elements with respect to
   status assessments: 1) the status assessment is done using discrete
   classes, typically for water bodies, sub-areas or critical sites
   representative for certain area of interest, and 2) phytoplankton
   chlorophyll a (chl-a) is one of the main variables considered. We
   analysed the benefits of using chl-a concentrations derived from EO data
   for the status assessments specified in the EU Water Framework Directive
   (WFD). Our study focused especially on EO observations' ability to
   capture extreme and transient events (such as instances of cyanobacteria
   blooms) more frequently than the monitoring-station data conventionally
   employs. The accuracy of EO-based chl-a assessment was studied for, in
   all, 129 Finnish water bodies in the area of the Baltic Sea, in Northern
   Europe. Natural conditions in this coastal area particularly its
   multitude of small bays, numerous estuaries, and mosaic of islands
   impose exceptionally strict requirements for an EO instrument's spatial
   resolution. The analysis revealed that an instrument with a 300 m
   resolution, such as the MEdium Resolution Imaging Spectrometer (MERIS)
   or Ocean and Land Colour Instrument (OLCI), can be used to estimate the
   water quality in 62\% of these water bodies. Processing of MERIS data
   into chl-a concentrations by means of a FUB inversion model demonstrated
   good accuracy relative to monitoring stations' measurements for the
   open-water season in 2003-2011. This extensive dataset showed a 23\%
   difference in modal values between EO- and station sampling -based chl-a
   concentrations. The bias in EO chl-a estimates was found to increase
   with low Secchi disk depth, elevated turbidity, and the presence of
   intensive phytoplankton blooms. The monitoring-station and EO data
   showed similar distributions of chl-a observations for a given day and
   location, a finding that supports the comprehensive use of EO-derived
   chl-a concentrations in assessment. For determination of a water body's
   status, the EO data required but also allowed for statistical analysis
   that differs from what has typically been utilised with sparse
   measurements from monitoring-station data. The geometric mean or the
   mode of the EO observations was found to represent the main bulk of the
   chl-a concentrations well statistically. In contrast, the arithmetic
   mean of EO observations yields chl-a concentrations that are roughly
   1.1-1.6 mu g/I higher and hence can lead to over-estimation in the
   associated status assessment. This paper also presents a new approach
   applicable for evaluating the validity of EO-based algorithms for any
   coastal water area requiring assessment. With this quality-grade (QG)
   method, the EO chl-a estimation accuracy is rated in terms of three
   grades, with water bodies taken as the evaluation units. For this, the
   method utilises statistical differences between EO and station-sampling
   chl-a concentrations and applies background information on optical
   properties obtained from measurements at routine-monitoring stations.
   The QG method showed the EO-based chl-a accuracy to suffice for
   assessing the status of 65\% of the coastal water bodies examined. At
   concentrations representing the threshold for the target of ``good
   status{''} under the WFD, the EO approach produced 0.
   6 mu g/I higher chl-a values than the stations' sampling did. The MERIS
   results point to clear benefits of using OLCI-based status assessment
   throughout the Sentinel-3 era.}},
DOI = {{10.1016/j.rse.2018.02.043}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
Unique-ID = {{ISI:000435053200021}},
}

@article{ ISI:000434132100003,
Author = {Caballero-Megido, C. and Hillier, J. and Wyncoll, D. and Bosher, L. and
   Gouldby, B.},
Title = {{Technical note: comparison of methods for threshold selection for
   extreme sea levels}},
Journal = {{JOURNAL OF FLOOD RISK MANAGEMENT}},
Year = {{2018}},
Volume = {{11}},
Number = {{2}},
Pages = {{127-140}},
Month = {{JUN}},
Abstract = {{Extreme value analysis is an important tool for studying coastal flood
   risk, but requires the estimation of a threshold to define an extreme',
   which is traditionally undertaken visually. Such subjective judgement is
   not accurately reproducible, so recently a number of quantitative
   approaches have been proposed. This paper therefore reviews existing
   methods, illustrated with coastal tide-gauge data and the Generalized
   Pareto Distribution, and proposes a new automated method that mimics the
   enduringly popular visual inspection method. In total, five different
   types of statistical threshold selection and their variants are
   evaluated by comparison to manually derived thresholds, demonstrating
   that the new method is a useful, complementary tool.}},
DOI = {{10.1111/jfr3.12296}},
ISSN = {{1753-318X}},
ResearcherID-Numbers = {{Hillier, John/D-1484-2009}},
ORCID-Numbers = {{Hillier, John/0000-0002-0221-8383}},
Unique-ID = {{ISI:000434132100003}},
}

@article{ ISI:000432784400020,
Author = {Woldegiorgis, Befekadu Taddesse and Wu, Fu-Chun and Van Griensven, Ann
   and Bauwens, Willy},
Title = {{A heuristic probabilistic approach to estimating size-dependent mobility
   of nonuniform sediment}},
Journal = {{STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT}},
Year = {{2018}},
Volume = {{32}},
Number = {{6}},
Pages = {{1771-1782}},
Month = {{JUN}},
Abstract = {{This paper presents a heuristic probabilistic approach to estimating the
   size-dependent mobilities of nonuniform sediment based on the pre- and
   post-entrainment particle size distributions (PSDs), assuming that the
   PSDs are lognormally distributed. The approach fits a lognormal
   probability density function to the pre-entrainment PSD of bed sediment
   and uses the threshold particle size of incipient motion and the concept
   of sediment mixture to estimate the PSDs of the entrained sediment and
   post-entrainment bed sediment. The new approach is simple in physical
   sense and significantly reduces the complexity and computation time and
   resource required by detailed sediment mobility models. It is calibrated
   and validated with laboratory and field data by comparing to the
   size-dependent mobilities predicted with the existing empirical
   lognormal cumulative distribution function approach. The novel features
   of the current approach are: (1) separating the entrained and
   non-entrained sediments by a threshold particle size, which is a
   modified critical particle size of incipient motion by accounting for
   the mixed-size effects, and (2) using the mixture-based pre- and
   post-entrainment PSDs to provide a continuous estimate of the
   size-dependent sediment mobility.}},
DOI = {{10.1007/s00477-017-1471-3}},
ISSN = {{1436-3240}},
EISSN = {{1436-3259}},
ORCID-Numbers = {{Woldegiorgis, Befekadu Taddesse/0000-0002-0576-4312}},
Unique-ID = {{ISI:000432784400020}},
}

@article{ ISI:000430389300004,
Author = {Lambert, Ben and MacLean, Adam L. and Fletcher, Alexander G. and Combes,
   Alexander N. and Little, Melissa H. and Byrne, Helen M.},
Title = {{Bayesian inference of agent-based models: a tool for studying kidney
   branching morphogenesis}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2018}},
Volume = {{76}},
Number = {{7}},
Pages = {{1673-1697}},
Month = {{JUN}},
Abstract = {{The adult mammalian kidney has a complex, highly-branched collecting
   duct epithelium that arises as a ureteric bud sidebranch from an
   epithelial tube known as the nephric duct. Subsequent branching of the
   ureteric bud to form the collecting duct tree is regulated by
   subcellular interactions between the epithelium and a population of
   mesenchymal cells that surround the tips of outgrowing branches. The
   mesenchymal cells produce glial cell-line derived neurotrophic factor
   (GDNF), that binds with RET receptors on the surface of the epithelial
   cells to stimulate several subcellular pathways in the epithelium. Such
   interactions are known to be a prerequisite for normal branching
   development, although competing theories exist for their role in
   morphogenesis. Here we introduce the first agent-based model of ex vivo
   kidney uretic branching. Through comparison with experimental data, we
   show that growth factor-regulated growth mechanisms can explain early
   epithelial cell branching, but only if epithelial cell division depends
   in a switch-like way on the local growth factor concentration; cell
   division occurring only if the driving growth factor level exceeds a
   threshold. We also show how a recently-developed method, ``Approximate
   Approximate Bayesian Computation{''}, can be used to infer key model
   parameters, and reveal the dependency between the parameters controlling
   a growth factor-dependent growth switch. These results are consistent
   with a requirement for signals controlling proliferation and chemotaxis,
   both of which are previously identified roles for GDNF.}},
DOI = {{10.1007/s00285-018-1208-z}},
ISSN = {{0303-6812}},
EISSN = {{1432-1416}},
ResearcherID-Numbers = {{Combes, Alexander/B-7228-2011
   Little, Melissa/A-6170-2010}},
ORCID-Numbers = {{Combes, Alexander/0000-0001-6008-8786
   Little, Melissa/0000-0003-0380-2263}},
Unique-ID = {{ISI:000430389300004}},
}

@article{ ISI:000428194000103,
Author = {Hollert, Henner and Crawford, Sarah E. and Brack, Werner and Brinkmann,
   Markus and Fischer, Elske and Hartmann, Kai and Keiter, Steffen and
   Ottermanns, Richard and Ouellet, Jacob D. and Rinke, Karsten and Roesch,
   Manfred and Ross-Nickoll, Martina and Schaeffer, Andreas and Schueth,
   Christoph and Schulze, Tobias and Schwarz, Anja and Seiler,
   Thomas-Benjamin and Wessels, Martin and Hinderer, Matthias and Schwalb,
   Antje},
Title = {{Looking back - Looking forward: A novel multi-time slice
   weight-of-evidence approach for defining reference conditions to assess
   the impact of human activities on lake systems}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2018}},
Volume = {{626}},
Pages = {{1036-1046}},
Month = {{JUN 1}},
Abstract = {{Lake ecosystems are sensitive recorders of environmental changes that
   provide continuous archives at annual to decadal resolution over
   thousands of years. The systematic investigation of land use changes and
   emission of pollutants archived in Holocene lake sediments as well as
   the reconstruction of contamination, background conditions, and
   sensitivity of lake systems offer an ideal opportunity to study
   environmental dynamics and consequences of anthropogenic impact that
   increasingly pose risks to human well-being. This paper discusses the
   use of sediment and other lines of evidence in providing a record of
   historical and current contamination in lake ecosystems. We present a
   novel approach to investigate impacts from human activities using
   chemical analytical, bioanalytical, ecological, paleolimnological,
   paleoecotoxicological, archeological as well as modeling techniques.
   This multi-time slice weight-of-evidence (WOE) approach will generate
   knowledge on conditions prior to anthropogenic influence and provide
   knowledge to (i) create a better understanding of the effects of
   anthropogenic disturbances on biodiversity, (ii) assess water quality by
   using quantitative data on historical pollution and persistence of
   pollutants archived over thousands of years in sediments, and (iii)
   define environmental threshold values using modeling methods. This
   techniquemay be applied in order to gain insights into reference
   conditions of surface and ground waters in catchments with a long
   history of land use and human impact, which is still a major need that
   is currently not yet addressed within the context of the European Water
   Framework Directive. (C) 2018 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.scitotenv.2018.01.113}},
ISSN = {{0048-9697}},
EISSN = {{1879-1026}},
ResearcherID-Numbers = {{Schulze, Tobias/B-1553-2010
   }},
ORCID-Numbers = {{Schulze, Tobias/0000-0002-9744-8914
   Keiter, Steffen/0000-0002-2356-6686
   Seiler, Thomas-Benjamin/0000-0001-8127-510X
   Hartmann, Kai/0000-0003-2540-6798
   Crawford, Sarah/0000-0003-4341-701X}},
Unique-ID = {{ISI:000428194000103}},
}

@article{ ISI:000431354300001,
Author = {Felton, Jr., Melvin A. and Yu, Alfred B. and Boothe, David L. and Oie,
   Kelvin S. and Franaszczuk, Piotr J.},
Title = {{Resonance Analysis as a Tool for Characterizing Functional Division of
   Layer 5 Pyramidal Neurons}},
Journal = {{FRONTIERS IN COMPUTATIONAL NEUROSCIENCE}},
Year = {{2018}},
Volume = {{12}},
Month = {{MAY 3}},
Abstract = {{Evidence suggests that layer 5 pyramidal neurons can be divided into
   functional zones with unique afferent connectivity and membrane
   characteristics that allow for post-synaptic integration of feedforward
   and feedback inputs. To assess the existence of these zones and their
   interaction, we characterized the resonance properties of a
   biophysically-realistic compartmental model of a neocortical layer 5
   pyramidal neuron. Consistent with recently published theoretical and
   empirical findings, our model was configured to have a ``hot zone{''} in
   distal apical dendrite and apical tuft where both high-and low-threshold
   Ca2+ ionic conductances had densities 1-2 orders of magnitude higher
   than anywhere else in the apical dendrite. We simulated injection of
   broad spectrum sinusoidal currents with linearly increasing frequency to
   calculate the input impedance of individual compartments, the transfer
   impedance between the soma and key compartments within the dendritic
   tree, and a dimensionless term we introduce called resonance quality. We
   show that input resonance analysis distinguished at least four distinct
   zones within the model based on properties of their frequency
   preferences: basal dendrite which displayed little resonance;
   soma/proximal apical dendrite which displayed resonance at 5-23 Hz,
   strongest at 5-10 Hz and hyperpolarized/resting membrane potentials;
   distal apical dendrite which displayed resonance at 8-19 Hz, strongest
   at 10 Hz and depolarized membrane potentials; and apical tuft which
   displayed a weak resonance largely between 8 and 10 Hz across a wide
   range of membrane potentials. Transfer resonance analysis revealed that
   changes in subthreshold electrical coupling were found to modulate the
   transfer resonant frequency of signals transmitted from distal apical
   dendrite and apical tuft to the soma, which would impact the frequencies
   that individual neurons are expected to respond to and reinforce.
   Furthermore, eliminating the hot zone was found to reduce amplification
   of resonance within the model, which contributes to reduced excitability
   when perisomatic and distal apical regions receive coincident
   stimulating current injections. These results indicate that the
   interactions between different functional zones should be considered in
   a more complete understanding of neuronal integration. Resonance
   analysis may therefore be a useful tool for assessing the integration of
   inputs across the entire neuronal membrane.}},
DOI = {{10.3389/fncom.2018.00029}},
Article-Number = {{29}},
ISSN = {{1662-5188}},
Unique-ID = {{ISI:000431354300001}},
}

@article{ ISI:000431983200001,
Author = {Hu, Tianchen and Bayly, Philip V.},
Title = {{Finite element models of flagella with sliding radial spokes and
   interdoublet links exhibit propagating waves under steady dynein loading}},
Journal = {{CYTOSKELETON}},
Year = {{2018}},
Volume = {{75}},
Number = {{5}},
Pages = {{185-200}},
Month = {{MAY}},
Abstract = {{It remains unclear how flagella generate propulsive, oscillatory
   waveforms. While it is well known that dynein motors, in combination
   with passive cytoskeletal elements, drive the bending of the axoneme by
   applying shearing forces and bending moments to microtubule doublets,
   the origin of rhythmicity is still mysterious. Most conceptual models of
   flagellar oscillation involve dynein regulation or switching, so that
   dynein activity first on one side of the axoneme, then the other, drives
   bending. In contrast, a ``viscoelastic flutter{''} mechanism has
   recently been proposed, based on a dynamic structural instability.
   Simple mathematical models of coupled elastic beams in viscous fluid,
   subjected to steady, axially distributed, dynein forces of sufficient
   magnitude, can exhibit oscillatory motion without any switching or
   dynamic regulation. Here we introduce more realistic finite element (FE)
   models of 6-doublet and 9-doublet flagella, with radial spokes and
   interdoublet links that slide along the central pair or corresponding
   doublet. These models demonstrate the viscoelastic flutter mechanism.
   Above a critical force threshold, these models exhibit an abrupt onset
   of propulsive, wavelike oscillations typical of flutter instability.
   Changes in the magnitude and spatial distribution of steady dynein
   force, or to viscous resistance, lead to behavior qualitatively
   consistent with experimental observations. This study demonstrates the
   ability of FE models to simulate nonlinear interactions between axonemal
   components during flagellar beating, and supports the plausibility of
   viscoelastic flutter as a mechanism of flagellar oscillation.}},
DOI = {{10.1002/cm.21432}},
ISSN = {{1949-3584}},
EISSN = {{1949-3592}},
ORCID-Numbers = {{Bayly, Philip/0000-0003-4303-0704}},
Unique-ID = {{ISI:000431983200001}},
}

@article{ ISI:000431785600002,
Author = {Wang, Steve C. and Zhong, Ling},
Title = {{Estimating the number of pulses in a mass extinction}},
Journal = {{PALEOBIOLOGY}},
Year = {{2018}},
Volume = {{44}},
Number = {{2}},
Pages = {{199-218}},
Month = {{MAY}},
Abstract = {{The Signor-Lipps effect states that even a sudden mass extinction will
   invariably appear gradual in the fossil record, due to incomplete fossil
   preservation. Most previous work on the Signor-Lipps effect has focused
   on testing whether taxa in a mass extinction went extinct simultaneously
   or gradually. However, many authors have proposed scenarios in which
   taxa went extinct in distinct pulses. Little methodology has been
   developed for quantifying characteristics of such pulsed extinction
   events. Here we introduce a method for estimating the number of pulses
   in a mass extinction, based on the positions of fossil occurrences in a
   stratigraphic section. Rather than using a hypothesis test and assuming
   simultaneous extinction as the default, we reframe the question by
   asking what number of pulses best explains the observed fossil record.
   Using a two-step algorithm, we are able to estimate not just the number
   of extinction pulses but also a confidence level or posterior
   probability for each possible number of pulses. In the first step, we
   find the maximum likelihood estimate for each possible number of pulses.
   In the second step, we calculate the Akaike information criterion and
   Bayesian information criterion weights for each possible number of
   pulses, and then apply a k-nearest neighbor classifier to these weights.
   This method gives us a vector of confidence levels for the number of
   extinction pulses-for instance, we might be 80\% confident that there
   was a single extinction pulse, 15\% confident that there were two
   pulses, and 5\% confident that there were three pulses. Equivalently, we
   can state that we are 95\% confident that the number of extinction
   pulses is one or two. Using simulation studies, we show that the method
   performs well in a variety of situations, although it has difficulty in
   the case of decreasing fossil recovery potential, and it is most
   effective for small numbers of pulses unless the sample size is large.
   We demonstrate the method using a data set of Late Cretaceous ammonites.}},
DOI = {{10.1017/pab.2016.30}},
ISSN = {{0094-8373}},
EISSN = {{1938-5331}},
ORCID-Numbers = {{Wang, Steve/0000-0002-8953-285X}},
Unique-ID = {{ISI:000431785600002}},
}

@article{ ISI:000430897300047,
Author = {Engram, Melanie and Arp, Christopher D. and Jones, Benjamin M. and
   Ajadi, Olaniyi A. and Meyer, Franz J.},
Title = {{Analyzing floating and bedfast lake ice regimes across Arctic Alaska
   using 25 years of space-borne SAR imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{209}},
Pages = {{660-676}},
Month = {{MAY}},
Abstract = {{Late-winter lake ice regimes are controlled by water depth relative to
   maximum ice thickness (MIT). When MIT exceeds maximum water depth, lakes
   freeze to the bottom with bedfast ice (BI) and when MIT is less than
   maximum water depth lakes have floating ice (El). Both airborne radar
   and space-borne synthetic aperture radar (SAR) imagery (Ku-, X-, C-, and
   L-band) have been used previously to determine whether lakes have a BI
   or El regime in a given year, across a number of years, or across large
   regions. In this study, we use a combination of ERS-1/2, RADARSAT-2,
   Envisat, and Sentinel-1 SAR imagery for seven lake-rich regions in
   Arctic Alaska to analyze lake ice regime extents and dynamics over a
   25-year period (1992-2016). Our interactive threshold classification
   method determines a unique statistic-based intensity threshold for each
   SAR scene, allowing for the comparison of classification results from
   C-band SAR data acquired with different polarizations and incidence
   angles. Additionally, our novel method accommodates declining signal
   strength in aging extended-mission satellite SAR instruments. Comparison
   of SAR ice regime classifications with extensive field measurements from
   six years yielded a 93\% accuracy. Significant declines in BI regimes
   were only observed in the Fish Creek area with 3\% of lakes exhibiting
   transitional ice regimes lakes that switch from BI to FI during this
   25-year period. This analysis suggests that the potential conversion
   from BI to FI regimes is primarily a function of lake depth
   distributions in addition to regional differences in climate
   variability. Remote sensing of lake ice regimes with C-band SAR is a
   useful tool to monitor the associated thermal impacts on permafrost,
   since lake ice regimes can be used as a proxy for of sub-lake permafrost
   thaw, considered by the Global Climate Observing System as an Essential
   Climate Variable (ECV). Continued winter warming and variable snow
   conditions in the Arctic are expected and our long-term analysis
   provides a valuable baseline for predicting where potential future lake
   ice regimes shifts will be most pronounced.}},
DOI = {{10.1016/j.rse.2018.02.022}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
Unique-ID = {{ISI:000430897300047}},
}

@article{ ISI:000436197700011,
Author = {Nakahashi, Wataru and Horiuchi, Shiro and Ihara, Yasuo},
Title = {{Estimating hominid life history: the critical interbirth interval}},
Journal = {{POPULATION ECOLOGY}},
Year = {{2018}},
Volume = {{60}},
Number = {{1-2, SI}},
Pages = {{127-142}},
Month = {{APR}},
Abstract = {{Unlike any great apes, humans have expanded into a wide variety of
   habitats during the course of evolution, beginning with the transition
   by australopithecines from forest to savanna habitation. Novel
   environments are likely to have imposed hominids a demographic challenge
   due to such factors as higher predation risk and scarcer food resources.
   In fact, recent studies have found a paucity of older relative to
   younger adults in hominid fossil remains, indicating considerably high
   adult mortality in australopithecines, early Homo, and Neanderthals. It
   is not clear to date why only human ancestors among all hominoid species
   could survive in these harsh environments. In this paper, we explore the
   possibility that hominids had shorter interbirth intervals to enhance
   fertility than the extant apes. To infer interbirth intervals in fossil
   hominids, we introduce the notion of the critical interbirth interval,
   or the threshold length of birth spacing above which a population is
   expected to go to extinction. We develop a new method to obtain the
   critical interbirth intervals of hominids based on the observed ratios
   of older adults to all adults in fossil samples. Our analysis suggests
   that the critical interbirth intervals of australopithecines, early
   Homo, and Neanderthals are significantly shorter than the observed
   interbirth intervals of extant great apes. We also discuss possible
   factors that may have caused the evolutionary divergence of hominid life
   history traits from those of great apes.}},
DOI = {{10.1007/s10144-018-0610-0}},
ISSN = {{1438-3896}},
EISSN = {{1438-390X}},
ORCID-Numbers = {{Nakahashi, Wataru/0000-0003-4081-6365}},
Unique-ID = {{ISI:000436197700011}},
}

@article{ ISI:000434186400008,
Author = {Vasco, D. W.},
Title = {{An Extended Trajectory Mechanics Approach for Calculating the Path of a
   Pressure Transient: Derivation and Illustration}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2018}},
Volume = {{54}},
Number = {{4}},
Pages = {{2642-2660}},
Month = {{APR}},
Abstract = {{Following an approach used in quantum dynamics, an exponential
   representation of the hydraulic head transforms the diffusion equation
   governing pressure propagation into an equivalent set of ordinary
   differential equations. Using a reservoir simulator to determine one set
   of dependent variables leaves a reduced set of equations for the path of
   a pressure transient. Unlike the current approach for computing the path
   of a transient, based on a high-frequency asymptotic solution, the
   trajectories resulting from this new formulation are valid for arbitrary
   spatial variations in aquifer properties. For a medium containing
   interfaces and layers with sharp boundaries, the trajectory mechanics
   approach produces paths that are compatible with travel time fields
   produced by a numerical simulator, while the asymptotic solution
   produces paths that bend too strongly into high permeability regions.
   The breakdown of the conventional asymptotic solution, due to the
   presence of sharp boundaries, has implications for model parameter
   sensitivity calculations and the solution of the inverse problem. For
   example, near an abrupt boundary, trajectories based on the asymptotic
   approach deviate significantly from regions of high sensitivity observed
   in numerical computations. In contrast, paths based on the new
   trajectory mechanics approach coincide with regions of maximum
   sensitivity to permeability changes.
   Plain Language Summary I present a new approach for computing the path
   of a pressure transient in a highly heterogeneous porous medium. The
   approach utilizes methods developed in quantum dynamics. The
   trajectories are useful for visualizing pressure propagation in
   complicated physical models and may form the basis for an efficient
   tomographic imaging algorithm.}},
DOI = {{10.1002/2017WR021360}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ResearcherID-Numbers = {{Vasco, Donald/G-3696-2015}},
ORCID-Numbers = {{Vasco, Donald/0000-0003-1210-8628}},
Unique-ID = {{ISI:000434186400008}},
}

@article{ ISI:000434186400016,
Author = {Perri, Saverio and Entekhabi, Dara and Molini, Annalisa},
Title = {{Plant Osmoregulation as an Emergent Water-Saving Adaptation}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2018}},
Volume = {{54}},
Number = {{4}},
Pages = {{2781-2798}},
Month = {{APR}},
Abstract = {{Soil salinity affects plant transpiration and growth through two main
   pathways: the osmotic effect of salt in the soil (osmotic stress;
   analogous to water stress) and the toxic effect of salt within the plant
   (ionic stress; salt specific). However, the drastic and sudden reduction
   of transpiration exhibited by most species in response to an increase of
   salinity in the root zone is mainly associated with the osmotic phase,
   while ionic stress appears at a later time, causing the premature
   senescence of leaves and the reduction of the plant photosynthetic area.
   To better investigate the effects of salinity on plant-water relations,
   we introduce a parsimonious soil-plant-atmosphere continuum (SPAC) model
   accounting for both salt exclusion at the root level and
   osmoregulation-i.e.,the adjustment of internal water potential in
   response to salt stress. The model is used to interpret a paradox
   observed in salt-tolerant species where transpiration is maximum at an
   intermediate value of salinity (C-Tr,C-max), and is lower in more fresh
   (C < C-Tr,C-max) and more saline (C > C-Tr,C-max) conditions. Such
   nonmonotonic transpiration-salt concentration (T-r-C) patterns can be
   largely explained by plant osmoregulation, while the peak of
   transpiration at C-Tr,C- max tends to disappear over longer time scales,
   when ionic stress appears and morphological adaptations become
   predominant. Osmoregulation emerges here as a water-saving behavior
   similar to the strategies that xerophytes use to cope with aridity. The
   maximum of transpiration at C-Tr,C- max is thus the result of a
   trade-off between the enhancement of salt-tolerance and optimal carbon
   assimilation.
   Plain Language Summary Soil salinization represents a major threat for
   the food security and sustainable development of drylands, with salt
   affected soils-presently covering more than 9 billion ha
   worldwide-expected to further increase due to climate change, land use
   modifications and erroneous irrigation/groundwater abstraction
   practices. Despite this fact, the effects of salinity on the rate at
   which plants transpire and grow in salt affected soils are rarely
   considered in ecological and ecohydrological models, and the different
   processes leading to salt tolerance are yet poorly understood. Here, we
   introduce a simple model of how salt tolerant species adapt to elevated
   salt concentrations in the soil, and of how such adaptations
   substantially lead to plant osmoregulation, as an emergent water-saving
   behavior similar to the strategies that aridity-tolerant species
   (xerophytes) use to cope with extreme water scarcity. The bottom line is
   that salt-tolerant plants experience salt-stress as an alternative form
   of water-limitation, and developed both short-and long-term adaptations
   accordingly. Our findings are instrumental to a better comprehension of
   the interplay between soil salinization, salt tolerance and efficient
   water use that is, in turn, the key to understand the potential of
   salt-tolerant crops and contrast soil salinization.}},
DOI = {{10.1002/2017WR022319}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ORCID-Numbers = {{perri, saverio/0000-0001-6382-1381
   Molini, Annalisa/0000-0003-3815-3929}},
Unique-ID = {{ISI:000434186400016}},
}

@article{ ISI:000431396900001,
Author = {Zurlinden, Todd J. and Reisfeld, Brad},
Title = {{A Novel Method for the Development of Environmental Public Health
   Indicators and Benchmark Dose Estimation Using a Health-Based End Point
   for Chlorpyrifos}},
Journal = {{ENVIRONMENTAL HEALTH PERSPECTIVES}},
Year = {{2018}},
Volume = {{126}},
Number = {{4}},
Month = {{APR}},
Abstract = {{BACKGROUND: Organophosphorus (OP) compounds are the most widely used
   group of insecticides in the world. Risk assessments for these chemicals
   have focused primarily on 10\% inhibition of acetylcholinesterase in the
   brain as the critical metric of effect. Aside from cholinergic effects
   resulting from acute exposure, many studies suggest a linkage between
   cognitive deficits and long-term OP exposure.
   OBJECTIVE: In this proof-of-concept study, we focused on one of the most
   widely used OP insecticides in the world, chlorpyrifos (CPF), and
   utilized an existing physiologically based phammcokinetic (PBPK) model
   and a novel phammcodynamic (PD) dose response model to develop a point
   of departure benchmark dose estimate for cognitive deficits following
   long-term, low-dose exposure to this chemical in rodents.
   METHODS: Utilizing a validated PBPK/PD model for CPF, we generated a
   database of predicted biomarkers of exposure and internal dose metrics
   in both rat and human. Using simulated peak brain CPF concentrations, we
   developed a dose response model to predict CPF-induced spatial memory
   deficits and correlated these changes to relevant biomarkers of exposure
   to derive a benchmark dose specific to `neurobehavioral changes. We
   extended these cognitive deficit predictions to humans and simulated
   corresponding exposures using a model parameterized for humans.
   RESULTS: Results from this study indicate that the human-equivalent
   benchmark dose (BMD) based on a 15\% cognitive deficit as an end point
   is lower than that using the present threshold for 10\% brain AChE
   inhibition. This predicted human-equivalent subchronic BMD threshold
   compares to occupational exposure levels determined from biomarkers of
   exposure and corresponds to similar exposure conditions where deficits
   in cognition are observed.
   CONCLUSIONS: Quantitative PD models based on neurobehavioral testing in
   animals offer an important addition to the methodologies used for
   establishing useful environmental public health indicators and BMDs, and
   predictions from such models could help inform the human health risk
   assessment for chlorpyrifos.}},
DOI = {{10.1289/EHP1743}},
Article-Number = {{047009}},
ISSN = {{0091-6765}},
EISSN = {{1552-9924}},
ORCID-Numbers = {{Reisfeld, Brad/0000-0003-1310-1495}},
Unique-ID = {{ISI:000431396900001}},
}

@article{ ISI:000430761100022,
Author = {Hazbavi, Zeinab and Baartman, Jantiene E. M. and Nunes, Joao P. and
   Keesstra, Saskia D. and Sadeghi, Seyed Hamidreza},
Title = {{Changeability of reliability, resilience and vulnerability indicators
   with respect to drought patterns}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2018}},
Volume = {{87}},
Pages = {{196-208}},
Month = {{APR}},
Abstract = {{Climate-related extremes such as droughts have led to significant
   impacts on some watersheds. To assess watershed health and develop
   effective management plans, information about the function and structure
   of the watersheds in the context of their climatic response, especially
   to take into account rainfall anomalies and climate change adaptation,
   is needed. Integration of climatic variables with reliability,
   resilience and vulnerability (RRV) indicators, is a novel approach for
   generating this information. This study investigated the behavior of RRV
   indicators with respect to rainfall variability and drought patterns for
   three watersheds governed by different climates. Reliability was defined
   as the probability of a watershed to be in the range of satisfactory
   Standardized Precipitation Index (SPI) values. Resilience was indicated
   as the speed of recovery from an unsatisfactory condition. Vulnerability
   was defined as a function of the exposure of a watershed to climate
   change and variation using the SPI. The study areas were the Foyle
   Watershed in Northern Ireland (temperate oceanic, Cfb), the Xarrama
   Watershed in Portugal (Mediterranean hot summer, Csa) and the Shazand
   Watershed in Iran (moderate to cold semi-arid (Bsk). Based on the SPI
   pattern of each watershed, the SPI of -0.1 for the Foyle and Xarrama
   watersheds and +0.1 for the Shazand Watershed was selected as the
   drought threshold. The drought based RRV index was subsequently
   calculated from long-term (1981-2012) RRV indicators, resulting in means
   of 0.52 +/- 0.25, 0.53 +/- 0.21 and 0.30 +/- 0.18 for the three
   watersheds, respectively. These means reflect the status of the
   watersheds in terms of climatic conditions, which was moderate dry
   (0.41-0.60) for the Foyle and Xarrama watersheds and dry (0.21-0.40) for
   the Shazand Watershed. The temporal trend of the drought based RRV index
   was found to be non-significantly increasing (P-value > 0.52) for the
   Foyle and Xarrama watersheds and non-significantly decreasing for the
   Shazand Watershed (P-value > 0.48). The vulnerability indicator and
   drought based RRV index were significantly (p-value = 0.00) affected by
   the climatological gradient. The results of the conceptual framework
   linked to statistical trends can provide researchers, policy makers, and
   land managers a more comprehensive base to assess variability of
   watershed health and design drought management plans.}},
DOI = {{10.1016/j.ecolind.2017.12.054}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
ORCID-Numbers = {{Keesstra, Saskia/0000-0003-4129-9080
   Hazbavi, Zeinab/0000-0001-6960-2876}},
Unique-ID = {{ISI:000430761100022}},
}

@article{ ISI:000429949900007,
Author = {Meng, Xin and Xiong, Li-yang and Yang, Xian-wu and Yang, Bi-sheng and
   Tang, Guo-an},
Title = {{A terrain openness index for the extraction of karst Fenglin and
   Fengcong landform units from DEMs}},
Journal = {{JOURNAL OF MOUNTAIN SCIENCE}},
Year = {{2018}},
Volume = {{15}},
Number = {{4}},
Pages = {{752-764}},
Month = {{APR}},
Abstract = {{The Fenglin and Fengcong landform units are considered to be an
   important representation for defining the degree of development of Karst
   landforms. However, these terrain features have been proven difficult to
   delineate and extract automatically because of their complex morphology.
   In this paper, a new method for identifying the Fenglin and Fengcong
   landform units is proposed. This method consists of two steps: (1)
   terrain openness calculation and (2) toe line extraction. The proposed
   method is applied and validated in the Karst case area of Guilin by
   using ASTER GDEM with one arc-second resolution. The openness of both
   the positive and negative terrain and a threshold were used to extract
   toe lines for segmenting depressions and pinnacles in Fenglin and
   Fengcong landforms. A comparison between the extracted Fenglin and
   Fengcong landform units and their real units from high resolution images
   was carried out to evaluate the capability of the proposed method.
   Results show the proposed method can effectively extract the Fenglin and
   Fengcong landform units, and has an overall accuracy of 93.28\%. The
   proposed method is simple and easy to implement and is expected to play
   an important role in the automatic extraction of similar landform units
   in the Karst area.}},
DOI = {{10.1007/s11629-017-4742-z}},
ISSN = {{1672-6316}},
EISSN = {{1993-0321}},
ResearcherID-Numbers = {{XIONG, Li-Yang/U-3882-2017
   }},
ORCID-Numbers = {{XIONG, Li-Yang/0000-0001-7930-3319
   Yang, Bisheng/0000-0001-7736-0803}},
Unique-ID = {{ISI:000429949900007}},
}

@article{ ISI:000427591500015,
Author = {Sebastian Gimenez, Carlos and Locatelli, Paola and Montini Ballarin,
   Florencia and Orlowski, Alejandro and Dewey, Ricardo A. and Pena,
   Milagros and Abel Abraham, Gustavo and Alejandro Aiello, Ernesto and del
   Rosario Bauza, Maria and Cuniberti, Luis and Daniela Olea, Fernanda and
   Crottogini, Alberto},
Title = {{Aligned ovine diaphragmatic myoblasts overexpressing human connexin-43
   seeded on poly (L-lactic acid) scaffolds for potential use in cardiac
   regeneration}},
Journal = {{CYTOTECHNOLOGY}},
Year = {{2018}},
Volume = {{70}},
Number = {{2}},
Pages = {{651-664}},
Month = {{APR}},
Abstract = {{Diaphragmatic myoblasts (DMs) are precursors of type-1 muscle cells
   displaying high exhaustion threshold on account that they contract and
   relax 20 times/min over a lifespan, making them potentially useful in
   cardiac regeneration strategies. Besides, it has been shown that
   biomaterials for stem cell delivery improve cell retention and viability
   in the target organ. In the present study, we aimed at developing a
   novel approach based on the use of poly (L-lactic acid) (PLLA) scaffolds
   seeded with DMs overexpressing connexin-43 (cx43), a gap junction
   protein that promotes inter-cell connectivity. DMs isolated from ovine
   diaphragm biopsies were characterized by immunohistochemistry and
   ability to differentiate into myotubes (MTs) and transduced with a
   lentiviral vector encoding cx43. After confirming cx43 expression
   (RT-qPCR and Western blot) and its effect on inter-cell connectivity
   (fluorescence recovery after photobleaching), DMs were grown on
   fiber-aligned or random PLLA scaffolds. DMs were successfully isolated
   and characterized. Cx43 mRNA and protein were overexpressed and favored
   inter-cell connectivity. Alignment of the scaffold fibers not only
   aligned but also elongated the cells, increasing the contact surface
   between them. This novel approach is feasible and combines the
   advantages of bioresorbable scaffolds as delivery method and a cell type
   that on account of its features may be suitable for cardiac
   regeneration. Future studies on animal models of myocardial infarction
   are needed to establish its usefulness on scar reduction and cardiac
   function.}},
DOI = {{10.1007/s10616-017-0166-4}},
ISSN = {{0920-9069}},
EISSN = {{1573-0778}},
ORCID-Numbers = {{Dewey, Ricardo/0000-0003-1677-2745
   Orlowski, Alejandro/0000-0001-9462-5463}},
Unique-ID = {{ISI:000427591500015}},
}

@article{ ISI:000426924000009,
Author = {Ouaret, Rachid and Ionescu, Anda and Petrehus, Viorel and Candau, Yves
   and Ramalho, Olivier},
Title = {{Spectral band decomposition combined with nonlinear models: application
   to indoor formaldehyde concentration forecasting}},
Journal = {{STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT}},
Year = {{2018}},
Volume = {{32}},
Number = {{4}},
Pages = {{985-997}},
Month = {{APR}},
Abstract = {{This paper proposes a new approach for forecasting continuous indoor air
   quality time series and in particular the concentration of a common air
   pollutant in offices like formaldehyde. Forecasting is achieved through
   the combination of the spectral band decomposition using fast Fourier
   transform and nonlinear time series modeling. Two nonlinear models have
   been tested: a threshold autoregressive (TAR) model and a Chaos
   dynamics-based modeling. This study shows the benefit of the Fourier
   decomposition coupled with nonlinear modeling of each extracted
   component, compared to forecasting applied directly on the raw data.
   Both TAR and Chaos dynamics models are able to reproduce nonlinearities,
   with slightly better performance in the case of the second model. These
   hybrid models provide good performance on forecast time horizon up to 12
   h ahead.}},
DOI = {{10.1007/s00477-017-1510-0}},
ISSN = {{1436-3240}},
EISSN = {{1436-3259}},
ORCID-Numbers = {{OUARET, Rachid/0000-0002-0502-2108}},
Unique-ID = {{ISI:000426924000009}},
}

@article{ ISI:000426597100008,
Author = {Libbrecht, Maxwell W. and Bilmes, Jeffrey A. and Noble, William Stafford},
Title = {{Choosing non-redundant representative subsets of protein sequence data
   sets using submodular optimization}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS}},
Year = {{2018}},
Volume = {{86}},
Number = {{4}},
Pages = {{454-466}},
Month = {{APR}},
Abstract = {{Selecting a non-redundant representative subset of sequences is a common
   step in many bioinformatics workflows, such as the creation of
   non-redundant training sets for sequence and structural models or
   selection of operational taxonomic units from metagenomics data.
   Previous methods for this task, such as CD-HIT, PISCES, and UCLUST,
   apply a heuristic threshold-based algorithm that has no theoretical
   guarantees. We propose a new approach based on submodular optimization.
   Submodular optimization, a discrete analogue to continuous convex
   optimization, has been used with great success for other representative
   set selection problems. We demonstrate that the submodular optimization
   approach results in representative protein sequence subsets with greater
   structural diversity than sets chosen by existing methods, using as a
   gold standard the SCOPe library of protein domain structures. In this
   setting, submodular optimization consistently yields protein sequence
   subsets that include more SCOPe domain families than sets of the same
   size selected by competing approaches. We also show how the optimization
   framework allows us to design a mixture objective function that performs
   well for both large and small representative sets. The framework we
   describe is the best possible in polynomial time (under some
   assumptions), and it is flexible and intuitive because it applies a
   suite of generic methods to optimize one of a variety of objective
   functions.}},
DOI = {{10.1002/prot.25461}},
ISSN = {{0887-3585}},
EISSN = {{1097-0134}},
ORCID-Numbers = {{Libbrecht, Maxwell/0000-0003-2502-0262}},
Unique-ID = {{ISI:000426597100008}},
}

@article{ ISI:000428007200002,
Author = {Hirt, Christian},
Title = {{Artefact detection in global digital elevation models (DEMs): The
   Maximum Slope Approach and its application for complete screening of the
   SRTM v4.1 and MERIT DEMs}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{207}},
Pages = {{27-41}},
Month = {{MAR 15}},
Abstract = {{Despite post-processing efforts by space agencies and research
   institutions, contemporary global digital elevation models (DEMs) may
   contain artefacts, i.e., erroneous features that do not exist in the
   actual terrain, such as spikes, holes and line errors. The goal of the
   present paper is to illuminate the artefact issue of current global DEM
   data sets that might be an obstacle for any geoscience study using
   terrain information. We introduce the Maximum Slope Approach (MSA) as a
   technique that uses terrain slopes as indicator to detect and localize
   spurious artefacts. The MSA relies on the strong sensitivity of terrain
   slopes for sudden steps in the DEM that is a direct feature of larger
   artefacts. In a numerical case study, the MSA is applied for globally
   complete screening of two SRTM-based 3 arc-second DEMs, the SRTM v4.1
   and the MERIT-DEM. Based on 0.1 degrees x 0.1 degrees sub-divisions and
   a 5 m/m slope threshold, 1341 artefacts were detected in SRTM v4.1 vs.
   108 in MERIT. Most artefacts spatially correlate with SRTM voids (and
   thus with the void-filling) and not with the SRTM-measured elevations.
   The strong contrast in artefact frequency (factor similar to 12) is
   attributed to the SRTM v4.1 hole filling. Our study shows that over
   parts of the Himalaya Mountains the SRTM v4.1 data set is contaminated
   by step artefacts where the use of this DEM cannot be recommended. Some
   caution should be exercised, e.g., over parts of the Andes and Rocky
   Mountains. The same holds true for derived global products that depend
   on SRTM v4.1, such as gravity maps. Primarily over the major mountain
   ranges, the MERIT model contains artefacts, too, but in smaller numbers.
   As a conclusion, globally complete artefact screening is recommended
   prior to the public release of any DEM data set. However, such a quality
   check should also be considered by users before using DEM data.
   MSA-based artefact screening is not only limited to DEMs, but can be
   applied as quality assurance measure to other gridded data sets such as
   digital bathymetric models or gridded physical quantities such as
   gravity or magnetics.}},
DOI = {{10.1016/j.rse.2017.12.037}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ORCID-Numbers = {{Christian, Hirt/0000-0002-3176-7939}},
Unique-ID = {{ISI:000428007200002}},
}

@article{ ISI:000428335900001,
Author = {Stovall, William R. and Taylor, Helen R. and Black, Michael and Grosser,
   Stefanie and Rutherford, Kim and Gemmell, Neil J.},
Title = {{Genetic sex assignment in wild populations using
   genotyping-by-sequencing data: A statistical threshold approach}},
Journal = {{MOLECULAR ECOLOGY RESOURCES}},
Year = {{2018}},
Volume = {{18}},
Number = {{2}},
Pages = {{179-190}},
Month = {{MAR}},
Abstract = {{Establishing the sex of individuals in wild systems can be challenging
   and often requires genetic testing. Genotyping-by-sequencing (GBS) and
   other reduced-representation DNA sequencing (RRS) protocols (e.g.,
   RADseq, ddRAD) have enabled the analysis of genetic data on an
   unprecedented scale. Here, we present a novel approach for the discovery
   and statistical validation of sex-specific loci in GBS data sets. We
   used GBS to genotype 166 New Zealand fur seals (NZFS, Arctocephalus
   forsteri) of known sex. We retained monomorphic loci as potential
   sex-specific markers in the locus discovery phase. We then used (i) a
   sex-specific locus threshold (SSLT) to identify significantly
   male-specific loci within our data set; and (ii) a significant
   sex-assignment threshold (SSAT) to confidently assign sex in silico the
   presence or absence of significantly male-specific loci to individuals
   in our data set treated as unknowns (98.9\% accuracy for females; 95.8\%
   for males, estimated via cross-validation). Furthermore, we assigned sex
   to 86 individuals of true unknown sex using our SSAT and assessed the
   effect of SSLT adjustments on these assignments. From 90 verified
   sex-specific loci, we developed a panel of three sex-specific PCR
   primers that we used to ascertain sex independently of our GBS data,
   which we show amplify reliably in at least two other pinniped species.
   Using monomorphic loci normally discarded from large SNP data sets is an
   effective way to identify robust sex-linked markers for nonmodel
   species. Our novel pipeline can be used to identify and statistically
   validate monomorphic and polymorphic sex-specific markers across a range
   of species and RRS data sets.}},
DOI = {{10.1111/1755-0998.12767}},
ISSN = {{1755-098X}},
EISSN = {{1755-0998}},
ResearcherID-Numbers = {{Rutherford, Kim/Y-7995-2018
   }},
ORCID-Numbers = {{Rutherford, Kim/0000-0001-6277-726X
   Black, Michael/0000-0003-1174-6054}},
Unique-ID = {{ISI:000428335900001}},
}

@article{ ISI:000427262800009,
Author = {Ensor, J. and Deeks, J. J. and Martin, E. C. and Riley, R. D.},
Title = {{Meta-analysis of test accuracy studies using imputation for partial
   reporting of multiple thresholds}},
Journal = {{RESEARCH SYNTHESIS METHODS}},
Year = {{2018}},
Volume = {{9}},
Number = {{1}},
Pages = {{100-115}},
Month = {{MAR}},
Abstract = {{Introduction: For tests reporting continuous results, primary studies
   usually provide test performance at multiple but often different
   thresholds. This creates missing data when performing a meta-analysis at
   each threshold. A standard meta-analysis (no imputation {[}NI]) ignores
   such missing data. A single imputation (SI) approach was recently
   proposed to recover missing threshold results. Here, we propose a new
   method that performs multiple imputation of the missing threshold
   results using discrete combinations (MIDC).
   Methods: The new MIDC method imputes missing threshold results by
   randomly selecting from the set of all possible discrete combinations
   which lie between the results for 2 known bounding thresholds. Imputed
   and observed results are then synthesised at each threshold. This is
   repeated multiple times, and the multiple pooled results at each
   threshold are combined using Rubin's rules to give final estimates. We
   compared the NI, SI, and MIDC approaches via simulation.
   Results: Both imputation methods outperform the NI method in
   simulations. There was generally little difference in the SI and MIDC
   methods, but the latter was noticeably better in terms of estimating the
   between-study variances and generally gave better coverage, due to
   slightly larger standard errors of pooled estimates. Given selective
   reporting of thresholds, the imputation methods also reduced bias in the
   summary receiver operating characteristic curve. Simulations demonstrate
   the imputation methods rely on an equal threshold spacing assumption. A
   real example is presented.
   Conclusions: The SI and, in particular, MIDC methods can be used to
   examine the impact of missing threshold results in meta-analysis of test
   accuracy studies.}},
DOI = {{10.1002/jrsm.1276}},
ISSN = {{1759-2879}},
EISSN = {{1759-2887}},
ORCID-Numbers = {{Ensor, Joie/0000-0001-7481-0282}},
Unique-ID = {{ISI:000427262800009}},
}

@article{ ISI:000424881800002,
Author = {Yamane, Lauren and Botsford, Louis W. and Kilduff, D. Patrick},
Title = {{Tracking restoration of population diversity via the portfolio effect}},
Journal = {{JOURNAL OF APPLIED ECOLOGY}},
Year = {{2018}},
Volume = {{55}},
Number = {{2}},
Pages = {{472-481}},
Month = {{MAR}},
Abstract = {{1. Declines in diversity among populations managed together have
   diminished aggregate stability through a decreased portfolio effect.
   Although the portfolio effect has been quantified in a variety of ways,
   management recommendations for the recovery of lost diversity rarely
   specify the stability benefits possible through such improvements.
   2. We introduce a metric, the Diversity Deficit (DD), that relates past
   losses and potential gains in aggregate stability to the changes in
   population diversity (i.e. covariability among population time series).
   We illustrate the use of this metric in retrospective analyses of the
   aggregate Sacramento River Fall-run Chinook salmon stock (Oncorhynchus
   tshawytscha), and project potential future improvements in stability
   through population diversity.
   3. In the retrospective analysis, we removed individual time series from
   the stability calculations to determine their effects on times and
   locations of past losses in diversity and stability. We found an early
   threefold loss in stock stability resulting from the presence of a
   single tributary, the Sacramento River mainstem. Other shifts in
   stability resulted from an increase in variability of a single
   population, and from the synchronizing effects of low ocean survival
   that led to the 2008-2009 fishery closure. Only one, smaller increase in
   the DD (i.e. in lost stability) was due to portfolio-wide increases in
   covariabilities among tributary abundances.
   4. In a prospective analysis using the DD applied to California salmon,
   we found that increasing biodiversity to the point of population
   independence and to its early high value would have reduced the
   probability of triggering a fishery closure.
   5. Synthesis and applications. Analyses with the Diversity Deficit (DD)
   metric illustrate a way to identify the times and locations of losses in
   population diversity, and to quantify how much restoration of population
   diversity could increase stability, and thus benefit resource services.
   In our research, the benefit was a reduction in the probability of
   falling below a critical management threshold leading to fishery
   closure, but other tangible benefits (e.g. reduction in probability of
   extinction) would also be possible.}},
DOI = {{10.1111/1365-2664.12978}},
ISSN = {{0021-8901}},
EISSN = {{1365-2664}},
Unique-ID = {{ISI:000424881800002}},
}

@article{ ISI:000427389100001,
Author = {Gueron, Geraldine and Anselmino, Nicolas and Chiarella, Paula and Ortiz,
   Emiliano G. and Vickers, Sofia Lage and Paez, Alejandra V. and Giudice,
   Jimena and Contin, Mario D. and Leonardi, Daiana and Jaworski, Felipe
   and Manzano, Veronica and Strazza, Ariel and Montagna, Daniela R. and
   Labanca, Estefania and Cotignola, Javier and D'Accorso, Norma and
   Woloszynska-Read, Anna and Navone, Nora and Meiss, Roberto P. and
   Ruggiero, Raul and Vazquez, Elba},
Title = {{Game-changing restraint of Ros-damaged phenylalanine, upon tumor
   metastasis}},
Journal = {{CELL DEATH \& DISEASE}},
Year = {{2018}},
Volume = {{9}},
Month = {{FEB 2}},
Abstract = {{An abrupt increase in metastatic growth as a consequence of the removal
   of primary tumors suggests that the concomitant resistance (CR)
   phenomenon might occur in human cancer. CR occurs in murine tumors and
   ROS-damaged phenylalanine, meta-tyrosine (m-Tyr), was proposed as the
   serum anti-tumor factor primarily responsible for CR. Herein, we
   demonstrate for the first time that CR happens in different experimental
   human solid tumors (prostate, lung anaplastic, and nasopharyngeal
   carcinoma). Moreover, m-Tyr was detected in the serum of mice bearing
   prostate cancer (PCa) xenografts. Primary tumor growth was inhibited in
   animals injected with m-Tyr. Further, the CR phenomenon was reversed
   when secondary implants were injected into mice with phenylalanine
   (Phe), a protective amino acid highly present in primary tumors. PCa
   cells exposed to m-Tyr in vitro showed reduced cell viability,
   downregulated NF kappa B/STAT3/Notch axis, and induced autophagy;
   effects reversed by Phe. Strikingly, m-Tyr administration also impaired
   both, spontaneous metastasis derived from murine mammary carcinomas
   (4T1, C7HI, and LMM3) and PCa experimental metastases. Altogether, our
   findings propose m-Tyr delivery as a novel approach to boost the
   therapeutic efficacy of the current treatment for metastasis preventing
   the escape from tumor dormancy.}},
DOI = {{10.1038/s41419-017-0147-8}},
Article-Number = {{140}},
ISSN = {{2041-4889}},
ORCID-Numbers = {{Giudice, Jimena/0000-0002-3330-7784
   Manzano, Veronica Elena/0000-0002-3763-3181
   Vazquez, Elba/0000-0002-0460-0236}},
Unique-ID = {{ISI:000427389100001}},
}

@article{ ISI:000427427300046,
Author = {Thompson, Robin N. and Gilligan, Christopher A. and Cunniffe, Nik J.},
Title = {{Control fast or control smart: When should invading pathogens be
   controlled?}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2018}},
Volume = {{14}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{The intuitive response to an invading pathogen is to start disease
   management as rapidly as possible, since this would be expected to
   minimise the future impacts of disease. However, since more spread data
   become available as an outbreak unfolds, processes underpinning pathogen
   transmission can almost always be characterised more precisely later in
   epidemics. This allows the future progression of any outbreak to be
   forecast more accurately, and so enables control interventions to be
   targeted more precisely. There is also the chance that the outbreak
   might die out without any intervention whatsoever, making prophylactic
   control unnecessary. Optimal decision-making involves continuously
   balancing these potential benefits of waiting against the possible costs
   of further spread. We introduce a generic, extensible data-driven
   algorithm based on parameter estimation and outbreak simulation for
   making decisions in real-time concerning when and how to control an
   invading pathogen. The Control Smart Algorithm (CSA) resolves the
   trade-off between the competing advantages of controlling as soon as
   possible and controlling later when more information has become
   available. We show-using a generic mathematical model representing the
   transmission of a pathogen of agricultural animals or plants through a
   population of farms or fields-how the CSA allows the timing and level of
   deployment of vaccination or chemical control to be optimised. In
   particular, the algorithm outperforms simpler strategies such as
   intervening when the outbreak size reaches a pre-specified threshold, or
   controlling when the outbreak has persisted for a threshold length of
   time. This remains the case even if the simpler methods are fully
   optimised in advance. Our work highlights the potential benefits of
   giving careful consideration to the question of when to start disease
   management during emerging outbreaks, and provides a concrete framework
   to allow policy-makers to make this decision.}},
DOI = {{10.1371/journal.pcbi.1006014}},
Article-Number = {{e1006014}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
ORCID-Numbers = {{Thompson, Robin/0000-0001-8545-5212}},
Unique-ID = {{ISI:000427427300046}},
}

@article{ ISI:000425218400006,
Author = {Benard, P. and Zarebanadkouki, M. and Hedwig, C. and Holz, M. and Ahmed,
   M. A. and Carminati, A.},
Title = {{Pore-Scale Distribution of Mucilage Affecting Water Repellency in the
   Rhizosphere}},
Journal = {{VADOSE ZONE JOURNAL}},
Year = {{2018}},
Volume = {{17}},
Number = {{1}},
Month = {{FEB}},
Abstract = {{The physical properties of the rhizosphere are strongly influenced by
   root-exuded mucilage, and there is increasing evidence that mucilage
   affects the wettability of soils on drying. We introduce a conceptual
   model of mucilage deposition during soil drying and its impact on soil
   wettability. We hypothesized that as soil dries, water menisci recede
   and draw mucilage toward the contact region between particles. At low
   mucilage contents (milligrams per gram of soil), mucilage deposits have
   the shape of thin filaments that are bypassed by infiltrating water. At
   higher contents, mucilage deposits occupy a large fraction of the pore
   space and make the rhizosphere hydrophobic. This hypothesis was
   confirmed by microscope images and contact angle measurements. We
   measured the initial contact angle of quartz sand (0.5-0.63- and
   0.125-0.2-mm diameter), silt (36-63-mm diameter), and glass beads
   (0.1-0.2-mm diameter) mixed with varying amounts of chia (Salvia
   hispanica L.) seed mucilage (dry content range 0.2-19 mg g(-1)) using
   the sessile drop method. We observed a threshold-like occurrence of
   water repellency. At low mucilage contents, the water drop infiltrated
   within 300 ms. Above a critical mucilage content, the soil
   particle-mucilage mixture turned water repellent. The critical mucilage
   content decreased with increasing soil particle size. Above this
   critical content, mucilage deposits have the shape of hollow cylinders
   that occupy a large fraction of the pore space. Below the critical
   mucilage content, mucilage deposits have the shape of thin filaments.
   This study shows how the microscopic heterogeneity of mucilage
   distribution impacts the macroscopic wettability of mucilage-embedded
   soil particles.}},
DOI = {{10.2136/vzj2017.01.0013}},
Article-Number = {{UNSP 170013}},
ISSN = {{1539-1663}},
ResearcherID-Numbers = {{Zarebanadkouki, Mohsen/L-8854-2015
   Carminati, Andrea/V-2336-2018}},
ORCID-Numbers = {{Zarebanadkouki, Mohsen/0000-0001-6342-5792
   }},
Unique-ID = {{ISI:000425218400006}},
}

@article{ ISI:000425109900007,
Author = {Schliep, Erin M. and Gelfand, Alan E. and Holland, David M.},
Title = {{Alternating Gaussian process modulated renewal processes for modeling
   threshold exceedances and durations}},
Journal = {{STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT}},
Year = {{2018}},
Volume = {{32}},
Number = {{2}},
Pages = {{401-417}},
Month = {{FEB}},
Abstract = {{It is often of interest to model the incidence and duration of threshold
   exceedance events for an environmental variable over a set of monitoring
   locations. Such data arrive over continuous time and can be considered
   as observations of a two-state process yielding, sequentially, a length
   of time in the below threshold state followed by a length of time in the
   above threshold state, then returning to the below threshold state, etc.
   We have a two-state continuous time Markov process, often referred to as
   an alternating renewal process. The process is observed over a truncated
   time window and, within this window, duration in each state is modeled
   using a distinct cumulative intensity specification. Initially, we model
   each intensity over the window using a parametric regression
   specification. We extend the regression specification adding temporal
   random effects to enrich the model using a realization of a log Gaussian
   process over time. With only one type of renewal, this specification is
   referred to as a Gaussian process modulated renewal process. Here, we
   introduce Gaussian process modulation to the intensity for each state.
   Model fitting is done within a Bayesian framework. We clarify that
   fitting with a customary log Gaussian process specification over a
   lengthy time window is computationally infeasible. The nearest neighbor
   Gaussian process, which supplies sparse covariance structure, is adopted
   to enable tractable computation. We propose methods for both generating
   data under our models and for conducting model comparison. The model is
   applied to hourly ozone data for four monitoring sites at different
   locations across the United States for the ozone season of 2014. For
   each site, we obtain estimated profiles of up-crossing and down-crossing
   intensity functions through time. In addition, we obtain inference
   regarding the number of exceedances, the distribution of the duration of
   exceedance events, and the proportion of time in the above and below
   threshold state for any time interval.}},
DOI = {{10.1007/s00477-017-1417-9}},
ISSN = {{1436-3240}},
EISSN = {{1436-3259}},
Unique-ID = {{ISI:000425109900007}},
}

@article{ ISI:000425027700014,
Author = {Hamilton, Olivia N. P. and Kincaid, Sophie E. and Constantine, Rochelle
   and Kozmian-Ledward, Lily and Walker, Cameron G. and Fewster, Rachel M.},
Title = {{Accounting for uncertainty in duplicate identification and group size
   judgements in mark-recapture distance sampling}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2018}},
Volume = {{9}},
Number = {{2}},
Pages = {{354-362}},
Month = {{FEB}},
Abstract = {{1. Mark-recapture distance sampling (MRDS) surveys with two independent
   observers are widely used to estimate wildlife population abundance. The
   analysis relies on accurate identification of duplicate sightings common
   to both observers, and correct judgements of group size, both of which
   are hard to achieve for species that exhibit complex grouping patterns.
   2. In this paper, we examine the impact of these sources of uncertainty
   on bias and precision of abundance estimates, using a case study of 22
   aerial surveys of common dolphins (Delphinus delphis) in the Hauraki
   Gulf, New Zealand. We develop a novel probabilistic method to identify
   duplicate observations, and account for various sources of uncertainty
   using a simulation-intensive approach.
   3. For our case study, identifying duplicates using reasonable but
   arbitrary thresholds of time and angle discrepancies created a range of
   abundance estimates differing by up to 20\%, whereas our novel
   threshold-free probabilistic analysis returned an estimate roughly
   central to this range. Uncertainty in group size made a smaller impact
   of up to 5\% on abundance estimates. All analysis choices returned
   similar values for the coefficient of variation, from 20 to 23\%.
   4. Generating robust estimates of abundance, and accounting for all
   associated sources of uncertainty, is critical for informing
   conservation management. Our novel approach provides a way to eliminate
   arbitrary decisions associated with MRDS, and account for a wider range
   of uncertainties. Our method allows for the reliable application of MRDS
   to a wider range of terrestrial and marine species, and will be a useful
   tool for producing robust abundance estimates for species that exhibit
   complex grouping patterns.}},
DOI = {{10.1111/2041-210X.12895}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ORCID-Numbers = {{Fewster, Rachel/0000-0002-4384-978X
   Kozmian-Ledward, Lily/0000-0001-6522-3502}},
Unique-ID = {{ISI:000425027700014}},
}

@article{ ISI:000419339800010,
Author = {Coutts, Shaun R. and Helmstedt, Kate J. and Bennett, Joseph R.},
Title = {{Invasion lags: The stories we tell ourselves and our inability to infer
   process from pattern}},
Journal = {{DIVERSITY AND DISTRIBUTIONS}},
Year = {{2018}},
Volume = {{24}},
Number = {{2}},
Pages = {{244-251}},
Month = {{FEB}},
Abstract = {{Aim: Many alien species experience a lag phase between arriving in a
   region and becoming invasive, which can provide a valuable window of
   opportunity for management. Our ability to predict which species are
   experiencing lags has major implications for management decisions that
   are worth billions of dollars and that may determine the survival of
   some native species. To date, timing and causes of lag and release have
   been identified post hoc, based on historical narratives.
   Location: Global.
   Methods: We use a simple but realistic simulation of population spread
   over a fragmented landscape. To break the invasion lag, we introduce a
   sudden, discrete change in dispersal.
   Results: We show that the ability to predict invasion lags is minimal
   even under controlled circumstances. We also show a non-negligible risk
   of falsely attributing lag breaks to mechanisms based on invasion
   trajectories and coincidences in timing.
   Main conclusions: We suggest that post hoc narratives may lead us to
   erroneously believe we can predict lags and that a precautionary
   approach is the only sound management practice for most alien species.}},
DOI = {{10.1111/ddi.12669}},
ISSN = {{1366-9516}},
EISSN = {{1472-4642}},
ORCID-Numbers = {{Coutts, Shaun/0000-0003-0282-4559}},
Unique-ID = {{ISI:000419339800010}},
}

@article{ ISI:000446321000024,
Author = {Biggs, Reinette and Peterson, Garry D. and Rocha, Juan C.},
Title = {{The Regime Shifts Database: a framework for analyzing regime shifts in
   social-ecological systems}},
Journal = {{ECOLOGY AND SOCIETY}},
Year = {{2018}},
Volume = {{23}},
Number = {{3}},
Abstract = {{Regime shifts, i.e., large, persistent, and usually unexpected changes
   in ecosystems and social-ecological systems, can have major impacts on
   ecosystem services, and consequently, on human well-being. However, the
   vulnerability of different regions to various regime shifts is largely
   unknown because evidence for the existence of regime shifts in different
   ecosystems and parts of the world is scattered and highly uneven.
   Furthermore, research tends to focus on individual regime shifts rather
   than comparisons across regime shifts, limiting the potential for
   identifying common drivers that could reduce the risk of multiple regime
   shifts simultaneously. Here, we introduce the Regime Shifts Database, an
   open-access database that systematically synthesizes information on
   social-ecological regime shifts across a wide range of systems using a
   consistent, comparative framework, providing a wide-ranging information
   resource for environmental planning, assessment, research, and teaching
   initiatives. The database currently contains 28 generic types of regime
   shifts and > 300 specific case studies. Each entry provides a
   literature-based synthesis of the key drivers and feedbacks underlying
   the regime shift, as well as impacts on ecosystem services and human
   well-being, and possible management options. Across the 28 regime
   shifts, climate change and agriculture-related activities are the most
   prominent among a wide range of drivers. Biodiversity, fisheries, and
   aquatic ecosystems are particularly widely affected, as are key aspects
   of human well-being, including livelihoods, food and nutrition, and an
   array of cultural ecosystem services. We hope that the database will
   stimulate further research and teaching on regime shifts that can inform
   policy and practice and ultimately enhance our collective ability to
   manage and govern large, abrupt, systemic changes in the Anthropocene.}},
DOI = {{10.5751/ES-10264-230309}},
Article-Number = {{9}},
ISSN = {{1708-3087}},
ResearcherID-Numbers = {{Peterson, Garry/C-1309-2008}},
ORCID-Numbers = {{Peterson, Garry/0000-0003-0173-0112}},
Unique-ID = {{ISI:000446321000024}},
}

@inproceedings{ ISI:000443968600019,
Author = {DeRamus, Marci L. and Kraft, Timothy W.},
Editor = {{Ash, JD and Anderson, RE and LaVail, MM and Rickman, CB and Hollyfield, JG and Grimm, C}},
Title = {{Optimizing ERG Measures of Scotopic and Photopic Critical Flicker
   Frequency}},
Booktitle = {{RETINAL DEGENERATIVE DISEASES: MECHANISMS AND EXPERIMENTAL THERAPY}},
Series = {{Advances in Experimental Medicine and Biology}},
Year = {{2018}},
Volume = {{1074}},
Pages = {{145-150}},
Note = {{17th International Symposium on Retinal Degeneration (RD), Kyoto, JAPAN,
   SEP 19-24, 2016}},
Organization = {{Fdn Fighting Blindness; BrightFocus Fdn; Pro Retina; Fritz Tobler Fdn;
   Harrington Discovery Inst}},
Abstract = {{A visual response to flickering light requires complex retinal
   computation, and thus ERG measures are an excellent test of retinal
   circuit fidelity. Critical flicker frequency (CFF) is the frequency at
   which the retinal response is no longer modulated. Traditionally, CFF is
   obtained with a series of steady flicker stimuli with increasing
   frequencies. However, this method is slow and susceptible to
   experimental drift and/or adaptational effects. The current study
   compares the steady flicker method to CFF measurements obtained using a
   frequency sweep protocol. We introduce a light source programmed to
   produce a linear sweep of frequencies in a single trial. Using the
   traditional steady flicker method and a criterion response of 3 mu V, we
   obtained a scotopic CFF of 18.4 +/- 0.66 Hz and a photopic CFF of 44.4
   +/- 1.67 Hz. Our sweep flicker method, used on the same animals,
   produces a waveform best analyzed by Fourier transform; wherein a 6.18
   log mu V-2 threshold was found to yield CFF values equal to those of the
   steady flicker method. Thus, the two flicker ERG techniques give
   comparable results, under both dark-and light-adapted conditions, and
   the flicker sweep method is faster to administer and analyze and may be
   less susceptible to blinking, breathing, and eye movement artifacts.}},
DOI = {{10.1007/978-3-319-75402-4\_18}},
ISSN = {{0065-2598}},
EISSN = {{2214-8019}},
ISBN = {{978-3-319-75402-4; 978-3-319-75401-7}},
Unique-ID = {{ISI:000443968600019}},
}

@article{ ISI:000429306500009,
Author = {Bersimisi, Sotiris and Sachlasi, Athanasios and Chakraborti, Subha},
Title = {{A sequential test for assessing observed agreement between raters}},
Journal = {{BIOMETRICAL JOURNAL}},
Year = {{2018}},
Volume = {{60}},
Number = {{1}},
Pages = {{128-145}},
Month = {{JAN}},
Abstract = {{Assessing the agreement between two or more raters is an important topic
   in medical practice. Existing techniques, which deal with categorical
   data, are based on contingency tables. This is often an obstacle in
   practice as we have to wait for a long time to collect the appropriate
   sample size of subjects to construct the contingency table. In this
   paper, we introduce a nonparametric sequential test for assessing
   agreement, which can be applied as data accrues, does not require a
   contingency table, facilitating a rapid assessment of the agreement. The
   proposed test is based on the cumulative sum of the number of
   disagreements between the two raters and a suitable statistic
   representing the waiting time until the cumulative sum exceeds a
   predefined threshold. We treat the cases of testing two raters'
   agreement with respect to one or more characteristics and using two or
   more classification categories, the case where the two raters extremely
   disagree, and finally the case of testing more than two raters'
   agreement. The numerical investigation shows that the proposed test has
   excellent performance. Compared to the existing methods, the proposed
   method appears to require significantly smaller sample size with
   equivalent power. Moreover, the proposed method is easily generalizable
   and brings the problem of assessing the agreement between two or more
   raters and one or more characteristics under a unified framework, thus
   providing an easy to use tool to medical practitioners.}},
DOI = {{10.1002/bimj.201600239}},
ISSN = {{0323-3847}},
EISSN = {{1521-4036}},
ORCID-Numbers = {{Sachlas, Athanasios/0000-0003-0546-3162}},
Unique-ID = {{ISI:000429306500009}},
}

@article{ ISI:000425787000004,
Author = {Zhu, Hongchun and Huang, Wei and Liu, Haiying},
Title = {{Loess terrain segmentation from digital elevation models based on the
   region growth method}},
Journal = {{PHYSICAL GEOGRAPHY}},
Year = {{2018}},
Volume = {{39}},
Number = {{1}},
Pages = {{51-66}},
Abstract = {{Regional automatic segmentation - automatic terrain segmentation
   according to terrain features - is significant for modern geographical
   analysis. We propose a new approach of terrain region segmentation based
   on the region growth method. This method features actual runoff nodes as
   seed points. The corresponding growth threshold is defined based on
   statistical analysis of quantitative indexes of topographic features.
   Terrain segmentation of some regions is completed using the growth
   threshold. The corresponding edge boundaries of different terrain
   regions are extracted by image processing. Thus, the automatic
   segmentation of the terrain region is realized by the edge boundary. The
   application of the method to a typical Chinese loess landform area and
   automatic segmentation of three types of terrain regions - gully
   interfluve land, gully slope land, and gully groove land - are achieved
   by analyzing characteristics of the curvature structure of surface
   profiles. Segmentation results, compared with results of visual
   interpretation from a high-precision digital orthophoto map, show an
   average accuracy of segmentation of 93.51\%. Topographic factor features
   of segmentation results are statistically analyzed. This study presents
   an effective and practical approach for segmenting terrain regions. This
   approach may be incorporated into the theory and method system of
   digital terrain analysis.}},
DOI = {{10.1080/02723646.2017.1342215}},
ISSN = {{0272-3646}},
EISSN = {{1930-0557}},
Unique-ID = {{ISI:000425787000004}},
}

@article{ ISI:000423537000001,
Author = {Pavelescu, Irina and Vilarrasa-Blasi, Josep and Planas-Riverola, Ainoa
   and Gonzalez-Garcia, Mary-Paz and Cano-Delgado, Ana I. and Ibanes, Marta},
Title = {{A Sizer model for cell differentiation in Arabidopsis thaliana root
   growth}},
Journal = {{MOLECULAR SYSTEMS BIOLOGY}},
Year = {{2018}},
Volume = {{14}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Plant roots grow due to cell division in the meristem and subsequent
   cell elongation and differentiation, a tightly coordinated process that
   ensures growth and adaptation to the changing environment. How the newly
   formed cells decide to stop elongating becoming fully differentiated is
   not yet understood. To address this question, we established a novel
   approach that combines the quantitative phenotypic variability of
   wild-type Arabidopsis roots with computational data from mathematical
   models. Our analyses reveal that primary root growth is consistent with
   a Sizer mechanism, in which cells sense their length and stop elongating
   when reaching a threshold value. The local expression of brassinosteroid
   receptors only in the meristem is sufficient to set this value. Analysis
   of roots insensitive to BR signaling and of roots with gibberellin
   biosynthesis inhibited suggests distinct roles of these hormones on cell
   expansion termination. Overall, our study underscores the value of using
   computational modeling together with quantitative data to understand
   root growth.}},
DOI = {{10.15252/msb.20177687}},
Article-Number = {{e7687}},
ISSN = {{1744-4292}},
ORCID-Numbers = {{Cano- Delgado, Ana I./0000-0002-8071-6724}},
Unique-ID = {{ISI:000423537000001}},
}

@article{ ISI:000422616500026,
Author = {Zhang, Yadan and Ji, Zhong and Tan, Xia and Zhou, Bingqing},
Title = {{Noise Reduction of the Electrocardiography Signal and Thoracic Impedance
   Differential Signal Based on Adaptive EEMD and Wavelet Thresholding}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2018}},
Volume = {{8}},
Number = {{1}},
Pages = {{140-144}},
Month = {{JAN}},
Abstract = {{The feature points of electrocardiography (ECG) signals and thoracic
   impedance differential (TID) signals that are synchronously acquired in
   clinics and de-noised through classical approaches are often not
   preserved completely or are metamorphosed due to noise disturbance. In
   an attempt to resolve this issue, a novel approach combining the
   adaptive ensemble empirical mode decomposition (AEEMD) and wavelet
   threshold de-noising (WTD) has been proposed for the noise removal from
   ECG and TID signals. Compared with the results in other two related
   methods, the operating efficiency of this proposed method became higher
   and the effect of the noise removal was the best by the evaluation of
   signal noise rate (SNR) and correlation coefficient (R), root mean
   square error (RMSE) and the running time ratio between every two methods
   (TR). Eventually, the significance of the proposed method in the
   combined analysis of ECG and TID signals in clinics has been affirmed
   through the accurate identification of the feature points of these
   de-noised signals.}},
DOI = {{10.1166/jmihi.2018.2247}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000422616500026}},
}

@article{ ISI:000419874400012,
Author = {Li, Xiaochun and Xu, Huiping and Shen, Changyu and Grannis, Shaun},
Title = {{Automated linkage of patient records from disparate sources}},
Journal = {{STATISTICAL METHODS IN MEDICAL RESEARCH}},
Year = {{2018}},
Volume = {{27}},
Number = {{1}},
Pages = {{172-184}},
Month = {{JAN}},
Abstract = {{We introduce an automated method of record linkage that has two key
   features, automated selection of match field interactions to include in
   the model for estimation and automated threshold determination for
   classifying record pairs to matches or non-matches. We applied our
   method to two real-world examples. The first example demonstrated
   results consistent with our earlier work: When data quality is adequate
   and the match field discriminating power is high, matching algorithms
   exhibit similar performance. The second example demonstrated that our
   method yields a lower false positive rate and higher positive predictive
   value than the Fellegi-Sunter model in the face of low data quality.
   When compared to the Fellegi-Sunter model, simulation studies suggest
   that our method exhibits better overall performance as indicated by
   higher area under the curve, and less biased estimates for both the
   match prevalence rate and the m- and u-probabilities over a range of
   data scenarios, especially when the match prevalence is extreme.
   Computationally, our method is as efficient as the Fellegi-Sunter model.
   We recommend this method in situations that an unsupervised linking
   algorithm is needed.}},
DOI = {{10.1177/0962280215626180}},
ISSN = {{0962-2802}},
EISSN = {{1477-0334}},
Unique-ID = {{ISI:000419874400012}},
}

@article{ ISI:000419477800005,
Author = {Gilmore, Kayla L. and Doubleday, Zoe A. and Gillanders, Bronwyn M.},
Title = {{Testing hypoxia: physiological effects of long-term exposure in two
   freshwater fishes}},
Journal = {{OECOLOGIA}},
Year = {{2018}},
Volume = {{186}},
Number = {{1}},
Pages = {{37-47}},
Month = {{JAN}},
Abstract = {{Hypoxic or oxygen-free zones are linked to large-scale mortalities of
   fauna in aquatic environments. Studies investigating the hypoxia
   tolerance of fish are limited and focused on marine species and
   short-term exposure. However, there has been minimal effort to
   understand the implications of long-term exposure on fish and their
   ability to acclimate. To test the effects of long-term exposure (months)
   of fish to hypoxia we devised a novel method to control the level of
   available oxygen. Juvenile golden perch (Macquaria ambigua ambigua), and
   silver perch (Bidyanus bidyanus), two key native species found within
   the Murray Darling Basin, Australia, were exposed to different
   temperatures (20, 24 and 28 degrees C) combined with normoxic (6-8
   mgO(2) L-1 or 12-14 kPa) and hypoxic (3-4 mgO(2) L-1 or 7-9 kPa)
   conditions. After 10 months, fish were placed in individual respirometry
   chambers to measure standard and maximum metabolic rate (SMR and MMR),
   absolute aerobic scope (AAS) and hypoxia tolerance. Golden perch had a
   much higher tolerance to hypoxia exposure than silver perch, as most
   silver perch died after only 1 month exposure. Golden perch acclimated
   to hypoxia had reduced MMR at 20 and 28 degrees C, but there was no
   change to SMR. Long-term exposure to hypoxia improved the tolerance of
   golden perch to hypoxia, compared to individuals held under normoxic
   conditions suggesting that golden perch can acclimate to levels around 3
   mgO(2) L-1 (kPa similar to 7) and lower. The contrasting tolerance of
   two sympatric fish species to hypoxia highlights our lack of
   understanding of how hypoxia effects fish after long-term exposure.}},
DOI = {{10.1007/s00442-017-3992-3}},
ISSN = {{0029-8549}},
EISSN = {{1432-1939}},
ResearcherID-Numbers = {{Doubleday, Zoe/N-9955-2013}},
ORCID-Numbers = {{Doubleday, Zoe/0000-0003-0045-6377}},
Unique-ID = {{ISI:000419477800005}},
}

@article{ ISI:000419102100011,
Author = {Saravia, Leonardo A. and Momo, Fernando R.},
Title = {{Biodiversity collapse and early warning indicators in a spatial phase
   transition between neutral and niche communities}},
Journal = {{OIKOS}},
Year = {{2018}},
Volume = {{127}},
Number = {{1}},
Pages = {{111-124}},
Month = {{JAN}},
Abstract = {{The dynamics of ecological communities have been described by neutral
   and niche theories that are now increasingly integrated into unified
   models. It is known that a critical transition exists between these two
   states, but the spatial aspect of this transition has not been studied.
   Our aim is to study the spatial aspect of the transition and propose
   early warning signals to detect it. We used a stochastic, spatially
   explicit model that spans a continuum from neutral to niche communities,
   and is driven by the intensity of hierarchical competition. The
   transition is indicated by the emergence of a large patch formed by one
   species that connects the whole area. The properties of this patch can
   be used as early warning indicators of a critical transition. If
   competition intensity increases beyond the critical point, our model
   shows a sudden decrease of the Shannon diversity index and a gentle
   decline in species richness. The critical point occurs at a very low
   value of competitive intensity, with the rate of migration from the
   metacommunity greatly influencing the position of this critical point.
   As an example, we apply our new method of early warning indicators to
   the Barro Colorado Tropical forest, which, as expected, appears to be
   far from a critical transition. Low values of competitive intensity were
   also reported by previous studies for different high-diversity real
   communities, suggesting that these communities are located before the
   critical point. A small increase of competitive interactions could push
   them across the transition, however, to a state in which diversity is
   much lower. Thus this new early warnings indicator could be used to
   monitor high diversity ecosystems that are still undisturbed.}},
DOI = {{10.1111/oik.04256}},
ISSN = {{0030-1299}},
EISSN = {{1600-0706}},
ORCID-Numbers = {{Saravia, Leonardo/0000-0002-7911-4398}},
Unique-ID = {{ISI:000419102100011}},
}

@article{ ISI:000418464400014,
Author = {Chen, Yaoliang and Lu, Dengsheng and Luo, Lifeng and Pokhrel, Yadu and
   Deb, Kalyanmoy and Huang, Jingfeng and Ran, Youhua},
Title = {{Detecting irrigation extent, frequency, and timing in a heterogeneous
   arid agricultural region using MODIS time series, Landsat imagery, and
   ancillary data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2018}},
Volume = {{204}},
Pages = {{197-211}},
Month = {{JAN}},
Abstract = {{Mapping irrigated area, frequency, timing, and amount is important for
   sustainable management of water resources in semi-arid and arid regions.
   Various studies exist on the mapping of irrigation using remote sensing
   and census statistics, but they mainly focus on the mapping of
   irrigation extent without taking frequency and timing into account. In
   this study, we proposed a new approach to extract irrigation attributes
   including irrigation extent, frequency and timing using multi-source
   data moderate resolution imaging spectroradiometer (MODIS), Landsat, and
   ancillary data. A time-series dataset with 30-m spatial resolution was
   generated by fusing 480-m time-series MODIS and Landsat imagery. We used
   the greenness index (the ratio of NIR and green spectral bands) to
   detect irrigation events during the first half of the growing season.
   Rainfall events were assumed as water supplement events along with
   irrigation events. The number of water supplement stages were then
   recorded cumulatively when a water supplement event was detected using a
   threshold-based model. To estimate the possible dates of each water
   supplement stage, Gaussian process regression and linear regression
   models were applied. The new framework was applied to the Hexi Corridor
   in northwestern China, an intensively irrigated region with a semi-arid
   climate. Results show that the overall accuracy of water supplement
   stage using the proposed method is 87\%. Validation of the number of
   water supplement stages and possible dates of water supply by GRP model
   with a ``strict{''} (or ``loose{''}) assessment method shows an overall
   accuracy of 55\% (94\%) and 59\% (89\%), respectively. The good accuracy
   of the additional independent validations for different years and sites
   demonstrates the robustness of the proposed method, suggesting the
   general applicability to other regions. Overall, this research
   demonstrates that the proposed method is promising in detecting
   irrigation attributes such as frequency and timing which have not been
   explored in previous research.}},
DOI = {{10.1016/j.rse.2017.10.030}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{rslab, hiwater/O-7037-2015
   }},
ORCID-Numbers = {{Huang, Jingfeng/0000-0003-4627-6021
   Chen, Yaoliang/0000-0002-9763-7803}},
Unique-ID = {{ISI:000418464400014}},
}

@article{ ISI:000411897700101,
Author = {Lee, Sang-Ryong and Lee, Jechan and Cho, Seong-Heon and Kim, Jieun and
   Oh, Jeong-Ik and Tsang, Daniel C. W. and Jeong, Kwang-Hwa and Kwon,
   Eilhann E.},
Title = {{Quantification of volatile fatty acids from cattle manure via
   non-catalytic esterification for odour indication}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2018}},
Volume = {{610}},
Pages = {{992-996}},
Month = {{JAN 1}},
Abstract = {{This report proposes a new approach to evaluate the odour nuisance of
   cattle manure samples from three different cattle breeds (i.e., native
   cattle, beef cattle, and milk cow) by means of quantification and
   speciation of volatile fatty acids (VFAs). To this end, non-catalytic
   esterification thermally induced in the presence of a porous material
   (silica) was undertaken, and the optimal operational parameters such as
   the derivatizing temperature (330 degrees C) for the maximum yield (>=
   99 +/- 0.4\%) of volatile fatty acid methyl esters (VFAMEs) were
   established. Among the VFA species in cattle manure based on
   quantification of VFAs, the major species were acetic, butyric and
   valeric acid. Considering the odour threshold of each VFA, our
   experimental results suggested that the major contributors to odour
   nuisance were C4-5 VFA species (i.e., butyric and valeric acid).
   Hydrothermal treatment was performed at 150 degrees C for 0-40 min to
   correlate the formation of VFAs with different types of cattle feed
   formulations. Our experimental data demonstrated that the formation of
   total VFAs is linearly proportional to the hydrothermal treatment
   duration and the total content of VFAs in native cattle, beef cattle,
   and milk cow manure samples reached up to similar to 1000, similar to
   3200, and similar to 2800 ppm, respectively. Thus, this study
   demonstrated that the degree of VFA formation is highly dependent on
   cattle feed formulations, which rely significantly on the protein
   content. Furthermore, the hydrothermal treatment provides a favourable
   condition for generating more VFAs. In this context, producing cattle
   manure into refused derived fuel (RDF) via a hydrothermal treatment is
   not a viable option to control odour. (C) 2017 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.scitotenv.2017.08.168}},
ISSN = {{0048-9697}},
EISSN = {{1879-1026}},
ResearcherID-Numbers = {{Tsang, Daniel/E-5442-2012
   }},
ORCID-Numbers = {{Tsang, Daniel/0000-0002-6850-733X
   Lee, Jechan/0000-0002-9759-361X}},
Unique-ID = {{ISI:000411897700101}},
}

@article{ ISI:000418219000014,
Author = {Mueller, Johann and Drewes, Joerg E. and Huebner, Uwe},
Title = {{Sequential biofiltration - A novel approach for enhanced biological
   removal of trace organic chemicals from wastewater treatment plant
   effluent}},
Journal = {{WATER RESEARCH}},
Year = {{2017}},
Volume = {{127}},
Pages = {{127-138}},
Month = {{DEC 15}},
Abstract = {{Recent studies revealed the benefits of oligotrophic and oxic conditions
   for the biological removal of many trace organic chemicals (TOrCs)
   during soil-aquifer treatment. These findings indicate an unused tuning
   potential that might also be applicable in engineered biofiltration
   systems with drastically reduced hydraulic retention times for an
   enhanced mitigation of TOrCs during wastewater treatment. This study
   introduces the novel approach of sequential biofiltration (SBF) for the
   advanced treatment of secondary effluent using two granular media
   filters operated in series with an intermediate aeration step aiming for
   oxic and oligotrophic conditions in the second filter stage. Results
   from the experiments conducted at pilot-scale confirm a reduced
   substrate availability and predominantly oxic conditions in the second
   filter stage of the SBF setup. An increased removal of several TOrCs was
   observed in an SBF system as compared to a conventional single-stage
   biofiltration unit operated at the same overall empty bed contact time
   (EBCT). Short-term tests with varying EBCTs in the first filter stage
   revealed a high degree of system robustness of TOrC mitigation when
   confronted with sudden hydraulic load variations. Higher removal of
   several TOrCs at increased EBCTs in the second filter stage indicates
   that EBCT might play a crucial role for the degradation of certain
   compounds. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.watres.2017.10.009}},
ISSN = {{0043-1354}},
ORCID-Numbers = {{Huebner, Uwe/0000-0003-2547-6366
   Drewes, Jorg E./0000-0003-2520-7596}},
Unique-ID = {{ISI:000418219000014}},
}

@article{ ISI:000417843000026,
Author = {Ebler, Jana and Schoenhuth, Alexander and Marschall, Tobias},
Title = {{Genotyping inversions and tandem duplications}},
Journal = {{BIOINFORMATICS}},
Year = {{2017}},
Volume = {{33}},
Number = {{24}},
Pages = {{4015-4023}},
Month = {{DEC 15}},
Abstract = {{Motivation: Next Generation Sequencing (NGS) has enabled studying
   structural genomic variants (SVs) such as duplications and inversions in
   large cohorts. SVs have been shown to play important roles in multiple
   diseases, including cancer. As costs for NGS continue to decline and
   variant data-bases become ever more complete, the relevance of
   genotyping also SVs from NGS data increases steadily, which is in stark
   contrast to the lack of tools to do so.
   Results: We introduce a novel statistical approach, called DIGTYPER
   (Duplication and Inversion GenoTYPER), which computes genotype
   likelihoods for a given inversion or duplication and reports the maximum
   likelihood genotype. In contrast to purely coverage-based approaches,
   DIGTYPER uses breakpoint-spanning read pairs as well as split alignments
   for genotyping, enabling typing also of small events. We tested our
   approach on simulated and on real data and compared the genotype
   predictions to those made by DELLY, which discovers SVs and computes
   genotypes, and SVTyper, a genotyping program used to genotype variants
   detected by LUMPY. DIGTYPER compares favorable especially for
   duplications (of all lengths) and for shorter inversions (up to 300 bp).
   In contrast to DELLY, our approach can genotype SVs from data bases
   without having to rediscover them.}},
DOI = {{10.1093/bioinformatics/btx020}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000417843000026}},
}

@article{ ISI:000413377200020,
Author = {van den Heiligenberg, Harm A. R. M. and Heimeriks, Gaston J. and
   Hekkert, Marko P. and van Oort, Frank G.},
Title = {{A habitat for sustainability experiments: Success factors for
   innovations in their local and regional contexts}},
Journal = {{JOURNAL OF CLEANER PRODUCTION}},
Year = {{2017}},
Volume = {{169}},
Pages = {{204-215}},
Month = {{DEC 15}},
Abstract = {{The sustainability challenge requires various forms of experimentation
   with inventions, which may lead to an upscaling process in which the
   invention and its applications will spread to other users and regions in
   the world. However, many experiments fail. In this paper, we explore the
   success factors for sustahiability experiments in their contribution to
   a longer-term regime change. These factors are related to the experiment
   itself as well as to the habitat in which the experiment takes place. A
   habitat is regarded as a configuration of contextual factors, which are
   mainly locally or regionally embedded. We introduce complementary
   insights from transition management literature and regional innovation
   systems literature to hypothesise that various types of experiments have
   distinctive favourite habitats, each with their specific success
   factors. Our exploratory survey among 56 sustainability experiments
   throughout Europe in the area of food, mobility and energy innovation
   suggests that user involvement is the most important success factor.
   Other important factors are the cooperation in local and regional
   networks, the policy instruments from the local and regional government,
   the dissemination of learning experiences, and the existence of a local
   or regional vision of the future. We conclude that entrepreneurs, users,
   local and regional governments as well as other regional partners should
   collaborate actively to make sustain ability experiments more
   successful. (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jclepro.2017.06.177}},
ISSN = {{0959-6526}},
EISSN = {{1879-1786}},
Unique-ID = {{ISI:000413377200020}},
}

@article{ ISI:000415776100006,
Author = {Lu, Jing and Wang, Xiao Hua and Babanin, Alexander V. and Aijaz, Saima
   and Sun, Younjong and Teng, Yong and Jung, Kyung-Tae and Qiao, Fangli},
Title = {{Modeling of suspended sediment concentrations under combined
   wave-current flow over rippled bed}},
Journal = {{ESTUARINE COASTAL AND SHELF SCIENCE}},
Year = {{2017}},
Volume = {{199}},
Pages = {{59-73}},
Month = {{DEC 5}},
Abstract = {{Ripples appear and disappear dynamically on coastal bed. The bottom
   stress can significantly be enhanced when ripples appear, and then the
   sediment transport will be influenced by the ripple enhanced stress.
   However, ripples' impact on suspended sediments is seldom discussed. In
   this study, a bedform (ripples) module based on combined wave and
   current flow is coupled with a bottom boundary layer (BBL) model. This
   BBL model outputs our improved bottom shear stress (BSS) to both the
   sediment model (UNSW-sed) and the hydrodynamic model (POM). Model
   results in Jervis Bay of Australia show that the simulated suspended
   sediment concentration (SSC) of an abrupt rising is significantly
   improved by considering ripples rather than setting a uniform roughness
   (K-b) without ripples. However, the SSC is still underestimated by using
   previous schemes. Differently from the previous estimation of
   ripple-enhanced shear velocity U-{*}cwe, noted as (U-{*}cwe\_NL, we
   introduce an U-{*}cwe improved by calculating through ripple-enhanced
   ripple-enhanced Kb, which is noted as U-{*}cwe\_Kb. Simulation shows
   that U-{*}cwe\_Kb produces significantly increased SSC under high wave
   conditions, resulting in reasonable agreements with the measurements.
   The wave friction factor f(w) is shown to play a crucial role in causing
   the difference between U-{*}cwe\_Kb and U-{*}cwe\_NL. (C) 2017 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.ecss.2017.09.020}},
ISSN = {{0272-7714}},
EISSN = {{1096-0015}},
ORCID-Numbers = {{Babanin, Alexander/0000-0002-8595-8204}},
Unique-ID = {{ISI:000415776100006}},
}

@article{ ISI:000423089400021,
Author = {Lecca, Paola and Bagagiolo, Fabio and Scarpa, Marina},
Title = {{Hybrid deterministic/stochastic simulation of complex biochemical
   systems}},
Journal = {{MOLECULAR BIOSYSTEMS}},
Year = {{2017}},
Volume = {{13}},
Number = {{12}},
Pages = {{2672-2686}},
Month = {{DEC}},
Abstract = {{In a biological cell, cellular functions and the genetic regulatory
   apparatus are implemented and controlled by complex networks of chemical
   reactions involving genes, proteins, and enzymes. Accurate computational
   models are indispensable means for understanding the mechanisms behind
   the evolution of a complex system, not always explored with wet lab
   experiments. To serve their purpose, computational models, however,
   should be able to describe and simulate the complexity of a biological
   system in many of its aspects. Moreover, it should be implemented by
   efficient algorithms requiring the shortest possible execution time, to
   avoid enlarging excessively the time elapsing between data analysis and
   any subsequent experiment. Besides the features of their topological
   structure, the complexity of biological networks also refers to their
   dynamics, that is often non-linear and stiff. The stiffness is due to
   the presence of molecular species whose abundance fluctuates by many
   orders of magnitude. A fully stochastic simulation of a stiff system is
   computationally time-expensive. On the other hand, continuous models are
   less costly, but they fail to capture the stochastic behaviour of small
   populations of molecular species. We introduce a new efficient hybrid
   stochastic-deterministic computational model and the software tool
   MoBioS (MOlecular Biology Simulator) implementing it. The mathematical
   model of MoBioS uses continuous differential equations to describe the
   deterministic reactions and a Gillespielike algorithm to describe the
   stochastic ones. Unlike the majority of current hybrid methods, the
   MoBioS algorithm divides the reactions' set into fast reactions,
   moderate reactions, and slow reactions and implements a hysteresis
   switching between the stochastic model and the deterministic model. Fast
   reactions are approximated as continuous-deterministic processes and
   modelled by deterministic rate equations. Moderate reactions are those
   whose reaction waiting time is greater than the fast reaction waiting
   time but smaller than the slow reaction waiting time. A moderate
   reaction is approximated as a stochastic (deterministic) process if it
   was classified as a stochastic (deterministic) process at the time at
   which it crosses the threshold of low (high) waiting time. A Gillespie
   First Reaction Method is implemented to select and execute the slow
   reactions. The performances of MoBios were tested on a typical example
   of hybrid dynamics: that is the DNA transcription regulation. The
   simulated dynamic profile of the reagents' abundance and the estimate of
   the error introduced by the fully deterministic approach were used to
   evaluate the consistency of the computational model and that of the
   software tool.}},
DOI = {{10.1039/c7mb00426e}},
ISSN = {{1742-206X}},
EISSN = {{1742-2051}},
ORCID-Numbers = {{Scarpa, Marina/0000-0001-8365-2706}},
Unique-ID = {{ISI:000423089400021}},
}

@article{ ISI:000419408600009,
Author = {Bearup, Daniel and Blasius, Bernd},
Title = {{Ecotone formation induced by the effects of tidal flooding: A conceptual
   model of the mud flat-coastal wetland ecosystem}},
Journal = {{ECOLOGICAL COMPLEXITY}},
Year = {{2017}},
Volume = {{32}},
Number = {{B, SI}},
Pages = {{217-227}},
Month = {{DEC}},
Abstract = {{The boundary between mud flat and coastal wetland ecosystems is highly
   productive and a haven of considerable biodiversity. It is also embedded
   in a highly dynamic environment and can be easily destabilised by
   environmental changes, invasive species, and human activity. Thus,
   understanding the processes which govern the formation of this ecotone
   is important both for conservation and economic reasons. In this study
   we introduce a simple conceptual model for this joint ecosystem, which
   demonstrates that the interaction between tidal flooding and habitat
   elevation is able to produce an ecotone with similar characteristics to
   that observed in empirical studies. In particular, the transition from
   mud flat to vegetated state is locally abrupt, occurring at a critical
   threshold elevation, but, on broader spatial scales can occur over a
   range of elevations determined by the variability in high tide water
   levels. Additionally, the model shows the potential for regime shifts,
   resulting from periods of unusual weather or the invasion of a fast
   growing, or flood resistant, species. (C) 2016 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.ecocom.2016.11.005}},
ISSN = {{1476-945X}},
EISSN = {{1476-9840}},
ORCID-Numbers = {{Bearup, Daniel/0000-0001-8524-7659}},
Unique-ID = {{ISI:000419408600009}},
}

@article{ ISI:000418247700009,
Author = {Fardghassemi, Yasmin and Tauffenberger, Arnaud and Gosselin, Sarah and
   Parker, J. Alex},
Title = {{Rescue of ATXN3 neuronal toxicity in Caenorhabditis elegans by chemical
   modification of endoplasmic reticulum stress}},
Journal = {{DISEASE MODELS \& MECHANISMS}},
Year = {{2017}},
Volume = {{10}},
Number = {{12}},
Pages = {{1465-1480}},
Month = {{DEC 1}},
Abstract = {{Polyglutamine expansion diseases are a group of hereditary
   neurodegenerative disorders that develop when a CAG repeat in the
   causative genes is unstably expanded above a certain threshold. The
   expansion of trinucleotide CAG repeats causes hereditary adult-onset
   neurodegenerative disorders, such as Huntington's disease,
   dentatorubral-pallidoluysian atrophy, spinobulbar muscular atrophy and
   multiple forms of spinocerebellar ataxia (SCA). The most common
   dominantly inherited SCA is the type 3 (SCA3), also known as
   Machado-Joseph disease (MJD), which is an autosomal dominant,
   progressive neurological disorder. The gene causatively associated with
   MJD is ATXN3. Recent studies have shown that this gene modulates
   endoplasmic reticulum (ER) stress. We generated transgenic
   Caenorhabditis elegans strains expressing human ATXN3 genes in
   motoneurons, and animals expressing mutant ATXN3-CAG89 alleles showed
   decreased lifespan, impaired movement, and rates of neurodegeneration
   greater than wild-type ATXN3-CAG10 controls. We tested three
   neuroprotective compounds (Methylene Blue, guanabenz and salubrinal)
   believed to modulate ER stress and observed that these molecules rescued
   ATXN3-CAG89 phenotypes. Furthermore, these compounds required specific
   branches of the ER unfolded protein response (UPRER), reduced global ER
   and oxidative stress, and polyglutamine aggregation. We introduce new C.
   elegans models for MJD based on the expression of full-length ATXN3 in a
   limited number of neurons. Using these models, we discovered that
   chemical modulation of the UPRER reduced neurodegeneration and warrants
   investigation in mammalian models of MJD.}},
DOI = {{10.1242/dmm.029736}},
ISSN = {{1754-8403}},
EISSN = {{1754-8411}},
ORCID-Numbers = {{Parker, Alex/0000-0002-3333-2445
   Tauffenberger, Arnaud/0000-0003-0207-9659}},
Unique-ID = {{ISI:000418247700009}},
}

@article{ ISI:000416900000006,
Author = {Luo, Xiaobo and Peng, Yidong and Gao, Yanghua},
Title = {{An Improved Optimal Segmentation Threshold Algorithm and Its Application
   in the Built-up Quick Mapping}},
Journal = {{JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING}},
Year = {{2017}},
Volume = {{45}},
Number = {{6}},
Pages = {{953-964}},
Month = {{DEC}},
Abstract = {{For purpose of improving the accuracy of the built-up quick mapping,
   this paper proposed an improved optimal segmentation threshold
   algorithm, namely the improved double-window flexible pace search
   (IDFPS) approach, by redesigning the valuation criteria and the sampling
   method based on the double-window flexible pace search (DFPS) approach.
   Moreover, the Normalized Difference Built-up Index (NDBI), the
   Index-based Built-up Index (IBI), the Enhanced Built-up and Bareness
   Index (EBBI) and the Urban Index (UI) inversed from Landsat 5 TM images
   were used for quick mapping by the IDFPS approach and the DFPS approach
   in different geographical areas. Results from the experiments
   exemplified by Chongqing (a mountain city) and Chengdu (a plain city)
   showed that the IDFPS approach was comprehensively superior to the DFPS
   approach. The IDFPS approach had more than 4.30\% higher overall
   accuracy and 0.12 higher Kappa coefficients than the DFPS approach when
   both were implemented simultaneously at both the above-mentioned study
   areas. Besides, a new discovery in this paper was found that the UI had
   a better performance with higher overall accuracy and Kappa coefficient,
   lower omission error and commission error than the NDBI, IBI and EBBI
   because of the strong relationship between the UI and the density of
   built-up land. This new method has an important reference value for
   built-up quick mapping and some other applied researches.}},
DOI = {{10.1007/s12524-016-0656-4}},
ISSN = {{0255-660X}},
EISSN = {{0974-3006}},
Unique-ID = {{ISI:000416900000006}},
}

@article{ ISI:000416496700025,
Author = {Spaulding, Nicole E. and Sneed, Sharon B. and Handley, Michael J. and
   Bohleber, Pascal and Kurbatov, Andrei V. and Pearce, Nicholas J. and
   Erhard, Tobias and Mayewski, Paul A.},
Title = {{A New Multielement Method for LA-ICP-MS Data Acquisition from Glacier
   Ice Cores}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2017}},
Volume = {{51}},
Number = {{22}},
Pages = {{13282-13287}},
Month = {{NOV 21}},
Abstract = {{To answer pressing new research questions about the rate and timing of
   abrupt climate transitions, a robust system for ultrahigh-resolution
   sampling of glacier ice is needed. Here, we present a multielement
   method of LA-ICP-MS analysis wherein an array of chemical elements is
   simultaneously measured from the same ablation area. Although
   multielement techniques are commonplace for high-concentration
   materials, prior to the development of this method, all LA-ICP-MS
   analyses of glacier ice involved a single element per ablation pass or
   spot. This new method, developed using the LA-ICP-MS system at the W. M.
   Keck Laser Ice Facility at the University of Maine Climate Change
   Institute, has already been used to shed light on our flawed
   understanding of natural levels of Pb in Earth's atmosphere.}},
DOI = {{10.1021/acs.est.7b03950}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Erhardt, Tobias/C-4589-2019}},
Unique-ID = {{ISI:000416496700025}},
}

@article{ ISI:000417116700010,
Author = {Muras, Valentin and Claussen, Bjoern and Nasiri, Hamid and Fritz,
   Guenter and Steuber, Julia},
Title = {{A miniaturized assay for kinetic characterization of the
   Na+-translocating NADH:ubiquinone oxidoreductase from Vibrio cholerae}},
Journal = {{ANALYTICAL BIOCHEMISTRY}},
Year = {{2017}},
Volume = {{537}},
Pages = {{56-59}},
Month = {{NOV 15}},
Abstract = {{We demonstrate the miniaturization of an enzymatic assay for the
   determination of NADH oxidation and quinone reduction by the
   Na+-translocating NADH quinone oxidoreductase (NQR) in the 96-well plate
   format. The assay is based on the spectrophotometric detection of NADH
   consumption and quinol formation. We validated the new method with known
   inhibitors of the NQR and optimized conditions for high-throughput
   screening as demonstrated by excellent Z-factors well above the accepted
   threshold (>= 0.5). Overall, the method allows the screening and
   identification of potential inhibitors of the NQR, and rapid
   characterization of NQR variants obtained by site-specific mutagenesis.
   (C) 2017 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.ab.2017.08.025}},
ISSN = {{0003-2697}},
EISSN = {{1096-0309}},
ORCID-Numbers = {{Fritz, Guenter/0000-0002-4571-8812}},
Unique-ID = {{ISI:000417116700010}},
}

@article{ ISI:000415921900018,
Author = {Xue, Alexander T. and Hickerson, Michael J.},
Title = {{multi-dice: r package for comparative population genomic inference under
   hierarchical co-demographic models of independent single-population size
   changes}},
Journal = {{MOLECULAR ECOLOGY RESOURCES}},
Year = {{2017}},
Volume = {{17}},
Number = {{6}},
Pages = {{e212-e224}},
Month = {{NOV}},
Abstract = {{Population genetic data from multiple taxa can address comparative
   phylogeographic questions about community-scale response to
   environmental shifts, and a useful strategy to this end is to employ
   hierarchical co-demographic models that directly test multi-taxa
   hypotheses within a single, unified analysis. This approach has been
   applied to classical phylogeographic data sets such as mitochondrial
   barcodes as well as reduced-genome polymorphism data sets that can yield
   10,000s of SNPs, produced by emergent technologies such as RAD-seq and
   GBS. A strategy for the latter had been accomplished by adapting the
   site frequency spectrum to a novel summarization of population genomic
   data across multiple taxa called the aggregate site frequency spectrum
   (aSFS), which potentially can be deployed under various inferential
   frameworks including approximate Bayesian computation, random forest and
   composite likelihood optimization. Here, we introduce the r package
   multi-dice, a wrapper program that exploits existing simulation software
   for flexible execution of hierarchical model-based inference using the
   aSFS, which is derived from reduced genome data, as well as
   mitochondrial data. We validate several novel software features such as
   applying alternative inferential frameworks, enforcing a minimal
   threshold of time surrounding co-demographic pulses and specifying
   flexible hyperprior distributions. In sum, multi-dice provides
   comparative analysis within the familiar R environment while allowing a
   high degree of user customization, and will thus serve as a tool for
   comparative phylogeography and population genomics.}},
DOI = {{10.1111/1755-0998.12686}},
ISSN = {{1755-098X}},
EISSN = {{1755-0998}},
Unique-ID = {{ISI:000415921900018}},
}

@article{ ISI:000415071200020,
Author = {Kopriva, Ivica and Ju, Wei and Zhang, Bin and Shi, Fei and Xiang, Dehui
   and Yu, Kai and Wang, Ximing and Bagci, Ulas and Chen, Xinjian},
Title = {{Single-Channel Sparse Non-Negative Blind Source Separation Method for
   Automatic 3-D Delineation of Lung Tumor in PET Images}},
Journal = {{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}},
Year = {{2017}},
Volume = {{21}},
Number = {{6}},
Pages = {{1656-1666}},
Month = {{NOV}},
Abstract = {{In this paper, we propose a novel method for single-channel blind
   separation of nonoverlapped sources and, to the best of our knowledge,
   apply it for the first time to automatic segmentation of lung tumors in
   positron emission tomography (PET) images. Our approach first converts a
   3-D PET image into a pseudo-multichannel image. Afterward,
   regularization free sparseness constrained non-negative matrix
   factorization is used to separate tumor from other tissues. By using
   complexity based criterion, we select tumor component as the one with
   minimal complexity. We have compared the proposed method with threshold
   based on 40\% and 50\% maximum standardized uptake value (SUV), graph
   cuts (GC), random walks (RW), and affinity propagation (AP) algorithms
   on 18 nonsmall cell lung cancer datasets with respect to ground truth
   (GT) provided by two radiologists. Dice similarity coefficient averaged
   with respect to two GTs is: 0.78 +/- 0.12 by the proposed algorithm,
   0.78 +/- 0.1 by GC, 0.77 +/- 0.13 by AP, 0.77 +/- 0.07 by RW, and 0.75
   +/- 0.13 by 50\% maximum SUV threshold. Since the proposed method
   achieved performance comparable with interactive methods, considering
   the unique challenges of lung tumor segmentation from PET images, our
   findings support possibility of using our fully automated method in
   routine clinics. The source codes will be available at
   www.mipav.net/English/research/research.html.}},
DOI = {{10.1109/JBHI.2016.2624798}},
ISSN = {{2168-2194}},
ORCID-Numbers = {{Kopriva, Ivica/0000-0002-8610-8877}},
Unique-ID = {{ISI:000415071200020}},
}

@article{ ISI:000414207900004,
Author = {Feng, Wen-Jing and Cai, Li-Ming and Liu, Kaihui},
Title = {{Dynamics of a dengue epidemic model with class-age structure}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMATHEMATICS}},
Year = {{2017}},
Volume = {{10}},
Number = {{8}},
Month = {{NOV}},
Abstract = {{We introduce the class-age-dependent rates of the infected and
   vaccinated class in the compartmental model of dengue transmission. An
   age-structured host-vector interaction model incorporating vaccination
   effects is formulated and analyzed for the spread of dengue. Moreover,
   the basic reproduction number is derived, which serves as a threshold
   value determining the stability of the equilibrium points. By
   constructing suitable Lyapunov functional, the global asymptotic
   stability of the equilibria, of the model is established in terms of the
   basic reproduction number. In particular, the disease-free equilibrium
   of the model is globally asymptotically stable if the basic reproduction
   number is less than one, while the disease persists and the unique
   endemic equilibrium is globally asymptotically stable if the basic
   reproduction number is greater than one. The analysis of our model
   indicates that our model is realistic to give a. hint to control the
   transmission of dengue. Furthermore, it follows from the formulation of
   the infection-free equilibrium of susceptible humans S-h(0) and the
   basic reproduction number R-0 that both of them are decreasing with
   respect to the vaccination parameter psi(h), which indicates that
   appropriate vaccinating program may contribute to prevent the
   transmission of Dengue disease.}},
DOI = {{10.1142/S1793524517501091}},
Article-Number = {{1750109}},
ISSN = {{1793-5245}},
EISSN = {{1793-7159}},
Unique-ID = {{ISI:000414207900004}},
}

@article{ ISI:000413104600017,
Author = {Ouyang, H. T.},
Title = {{Characteristics of recursive and non-recursive adaptive network-based
   fuzzy inference system models for the forecast of typhoon inundation
   levels}},
Journal = {{INTERNATIONAL JOURNAL OF ENVIRONMENTAL SCIENCE AND TECHNOLOGY}},
Year = {{2017}},
Volume = {{14}},
Number = {{11}},
Pages = {{2495-2506}},
Month = {{NOV}},
Abstract = {{The accurate forecasting of typhoon inundation levels is vital for
   damage mitigation actions during such an event. The objective of this
   paper is to investigate the characteristics of adaptive network-based
   fuzzy inference system models for the forecasting of typhoon inundation
   levels. A novel approach of recursively using the model to achieve
   higher prediction lead times is proposed. The approach is advantageous
   in conducting water level forecasts for various prediction lead times
   using a single model, whereas common non-recursive models are only
   applicable for the designed prediction leads. In this study, a total of
   6 models, with various configurations and types of recursions, are
   constructed based on the cross-correlations between rainfall and water
   level records. The performance of each model is evaluated and compared
   using three indices: coefficient of efficiency, relative time shift, and
   threshold statistics. The best recursive and non-recursive models are
   selected and compared with traditional approaches based on
   autoregressive models with exogenous input. The results show that
   although the recursive models display somewhat lesser but comparable
   forecasting capacities compared to the non-recursive models, the former
   models have achieved forecasts single handedly for all the prediction
   leads using single models only. On the other hand, although the
   non-recursive models exhibit better forecasting capacities, this is at
   the cost of using multiple models, with each designed for a specific
   prediction lead time. In comparison with other traditional approaches,
   both the recursive and non-recursive types of models demonstrate
   superior performance on all the aspects inspected.}},
DOI = {{10.1007/s13762-017-1336-9}},
ISSN = {{1735-1472}},
EISSN = {{1735-2630}},
Unique-ID = {{ISI:000413104600017}},
}

@article{ ISI:000408401800013,
Author = {Cao, Pengxing and Klonis, Nectarios and Zaloumis, Sophie and Khoury,
   David S. and Cromer, Deborah and Davenport, Miles P. and Tilley, Leann
   and Simpson, Julie A. and McCaw, James M.},
Title = {{A mechanistic model quantifies artemisinin-induced parasite growth
   retardation in blood-stage Plasmodium falciparum infection}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2017}},
Volume = {{430}},
Pages = {{117-127}},
Month = {{OCT 7}},
Abstract = {{Falciparum malaria is a major parasitic disease causing widespread
   morbidity and mortality globally. Artemisinin derivatives the most
   effective and widely-used antimalarials that have helped reduce the
   burden of malaria by 60\% in some areas over the past decade have
   recently been found to induce growth retardation of blood-stage
   Plasmodium falciparum when applied at clinically relevant
   concentrations. To date, no model has been designed to quantify the
   growth retardation effect and to predict the influence of this property
   on in vivo parasite killing. Here we introduce a mechanistic model of
   parasite growth from the ring to trophozoite stage of the parasite's
   life cycle, and by modelling the level of staining with an RNA-binding
   dye, we demonstrate that the model is able to reproduce fluorescence
   distribution data from in vitro experiments using the laboratory 3D7
   strain. We quantify the dependence of growth retardation on drug
   concentration and identify the concentration threshold above which
   growth retardation is evident. We estimate that the parasite life cycle
   is prolonged by up to 10 hours. We illustrate that even such a
   relatively short delay in growth may significantly influence in vivo
   parasite dynamics, demonstrating the importance of considering growth
   retardation in the design of optimal artemisinin-based dosing regimens.
   (C) 2017 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2017.07.017}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
ORCID-Numbers = {{Zaloumis, Sophie/0000-0002-8253-8896
   Davenport, Miles/0000-0002-4751-1831
   McCaw, James/0000-0002-2452-3098}},
Unique-ID = {{ISI:000408401800013}},
}

@article{ ISI:000418736000029,
Author = {Zhang, Yi and Nishizawa, Osamu and Park, Hyuck and Kiyama, Tamotsu and
   Lei, Xinglin and Xue, Ziqiu},
Title = {{The Pathway-Flow Relative Permeability of CO2: Measurement by Lowered
   Pressure Drops}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2017}},
Volume = {{53}},
Number = {{10}},
Pages = {{8626-8638}},
Month = {{OCT}},
Abstract = {{We introduce a simple method to measure the relative permeability of
   supercritical CO2 in low-permeability rocks. The method is built on the
   assumption of the stability of formed CO2 percolation pathway under
   lowered pressure drops. Initially, a continuous CO2 flow pathway is
   created under a relatively high-pressure drop. Then, several subsequent
   steps of lowered pressure drops are performed while monitoring the
   associated flow rates. When the pressure drop is lower than a threshold
   value, the created flow pathway is assumed to be adequately stable and
   does not vary significantly during successive flows, with the average
   saturation and flow rate achieving a quasi steady state. The relative
   permeability of CO2 is then calculated from the relationship between the
   pressure drop and flow rate at several lowered pressure drops according
   to the extended form of Darcy's law. We demonstrate this method using
   both numerical modeling and an experimental test using X-ray CT imaging.
   The results indicate the validity of the assumption for the stability of
   flow pathway under lowered pressure drops. A linear relationship between
   the lowered pressure drops and the corresponding CO2 flow rate is found.
   Furthermore, the measurement results suggest that the relative
   permeability of CO2 can still be high in low-permeability rocks if the
   CO2 saturation is higher than the threshold value required to build a
   flow pathway. The proposed method is important for measuring the
   pathway-flow relative permeability of nonwetting fluids in
   low-permeability rocks.}},
DOI = {{10.1002/2017WR020580}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ResearcherID-Numbers = {{Lei, Xinglin/K-7177-2016
   }},
ORCID-Numbers = {{Lei, Xinglin/0000-0003-2088-6323
   Zhang, Yi/0000-0002-5981-6042}},
Unique-ID = {{ISI:000418736000029}},
}

@article{ ISI:000413233000012,
Author = {Piyoosh, Atul Kant and Ghosh, Sanjay Kumar},
Title = {{Effect of autocorrelation on temporal trends in rainfall in a valley
   region at the foothills of Indian Himalayas}},
Journal = {{STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT}},
Year = {{2017}},
Volume = {{31}},
Number = {{8}},
Pages = {{2075-2096}},
Month = {{OCT}},
Abstract = {{This study examines the effect of autocorrelation on step and monotonic
   trends in seasonal and annual rainfall. Initially, for step change,
   modified-Pettitt test is applied in two ways. First, using the corrected
   and unbiased trend-free-pre-whitening (TFPWcu) approach. Second, using a
   new approach in which time series is modelled by intervention analysis
   for modified Pettitt test. Subsequently, for monotonic trends,
   Mann-Kendall (MK) and six approaches of modified Mann-Kendall (MMK) test
   are applied to NCDC data for period 1901-2012 and its sub-periods.
   Approaches of MMK include pre-whitening (PW), trend-free-pre-whitening
   (TFPW), TFPWcu, two Variance Correction Approaches (VCAs) based on
   empirical formula (VCA:CF1) and Monte-Carlo-Simulations (VCA:CF2) and
   long term persistence (MK-LTP). A single change point is identified in
   1970 for annual and monsoon rainfall from original and
   modified-Pettitt's test using TFPWcu, while time series modelling
   approach has not exhibited any change point. Process shift in rainfall
   series is also studied using CUSUM and multiple change points are
   identified using Segment-Neighbourhood method. Outcomes of MMK show that
   TFPWcu is able to efficiently limit the effect of autocorrelation and
   may be preferred over PW and TFPW. The VCA:CF2 is not dependent on whole
   autocorrelation structure and corrects variance of all data series using
   lag-1 autocorrelation and may be preferred over VCA:CF1. MK-LTP
   considers long term persistence and it has exhibited presence of weaker
   trends than exhibited by other approaches. VCA:CF2 and MK-LTP are used
   to study trends of rainfall in Dehradun.}},
DOI = {{10.1007/s00477-016-1347-y}},
ISSN = {{1436-3240}},
EISSN = {{1436-3259}},
ORCID-Numbers = {{Piyoosh, Atul/0000-0003-0053-9885}},
Unique-ID = {{ISI:000413233000012}},
}

@article{ ISI:000411751100001,
Author = {Chakraborty, Shouvik and Chatterjee, Sankhadeep and Dey, Nilanjan and
   Ashour, Amira S. and Ashour, Ahmed S. and Shi, Fuqian and Mali, Kalyani},
Title = {{Modified cuckoo search algorithm in microscopic image segmentation of
   hippocampus}},
Journal = {{MICROSCOPY RESEARCH AND TECHNIQUE}},
Year = {{2017}},
Volume = {{80}},
Number = {{10}},
Pages = {{1051-1072}},
Month = {{OCT}},
Abstract = {{Microscopic image analysis is one of the challenging tasks due to the
   presence of weak correlation and different segments of interest that may
   lead to ambiguity. It is also valuable in foremost meadows of technology
   and medicine. Identification and counting of cells play a vital role in
   features extraction to diagnose particular diseases precisely. Different
   segments should be identified accurately in order to identify and to
   count cells in a microscope image. Consequently, in the current work, a
   novel method for cell segmentation and identification has been proposed
   that incorporated marking cells. Thus, a novel method based on cuckoo
   search after pre-processing step is employed. The method is developed
   and evaluated on light microscope images of rats' hippocampus which used
   as a sample for the brain cells. The proposed method can be applied on
   the color images directly. The proposed approach incorporates the
   McCulloch's method for levy flight production in cuckoo search (CS)
   algorithm. Several objective functions, namely Otsu's method, Kapur
   entropy and Tsallis entropy are used for segmentation. In the cuckoo
   search process, the Otsu's between class variance, Kapur's entropy and
   Tsallis entropy are employed as the objective functions to be optimized.
   Experimental results are validated by different metrics, namely the peak
   signal to noise ratio (PSNR), mean square error, feature similarity
   index and CPU running time for all the test cases. The experimental
   results established that the Kapur's entropy segmentation method based
   on the modified CS required the least computational time compared to
   Otsu's between-class variance segmentation method and the Tsallis
   entropy segmentation method. Nevertheless, Tsallis entropy method with
   optimized multi-threshold levels achieved superior performance compared
   to the other two segmentation methods in terms of the PSNR.}},
DOI = {{10.1002/jemt.22900}},
ISSN = {{1059-910X}},
EISSN = {{1097-0029}},
ORCID-Numbers = {{Ashour, Amira/0000-0003-3217-6185
   Chatterjee, Sankhadeep/0000-0002-3930-4699
   Chakraborty, Shouvik/0000-0002-3427-7492}},
Unique-ID = {{ISI:000411751100001}},
}

@article{ ISI:000411751100008,
Author = {Shvedchenko, Dmitry O. and Suvorova, Elena I.},
Title = {{Combination of thresholding and fitting methods for measuring
   nanoparticle sizes and size distributions in (S)TEM}},
Journal = {{MICROSCOPY RESEARCH AND TECHNIQUE}},
Year = {{2017}},
Volume = {{80}},
Number = {{10}},
Pages = {{1113-1122}},
Month = {{OCT}},
Abstract = {{The practical need for a simple and reliable tool for routine size
   analysis of nanoparticles with diameters down to a few nm embedded in a
   polymer matrix motivated the development of a new approach. The idea
   underlying the method proposed in this work is to combine intensity
   thresholding and contrast fitting procedures in the same software for
   particle recognition and measurements of sizes and size distributions of
   nanoparticles in transmission and scanning transmission electron
   microscopy images. Particle recognition in images is performed in an
   interactive process of manual setting the numerical threshold level
   after image preprocessing. We show that fitting the calculated gray
   level distribution to the real images is able to provide a maximum
   accuracy in measurements of the particle diameters in contrast to
   thresholding approaches. The fitting procedure is applied in the
   vicinity of nanoparticle images with the mass-thickness, diffraction,
   and chemical contrast. The grayscale function associated to the
   nanoparticle thickness is described using polynomial g(t)
   =g(0)+g(1)t+g(2)t(2)+g(3)t(3)... with degree 2 and undetermined
   coefficients. The program for particle detection and size
   measurementAnalyzer of Nanoparticles (AnNa)has been written and is
   described here. It was successfully tested on systems containing Ag
   nanoparticles grown and stabilized in aqueous solutions of different
   polymers for biomedical use and is available from the authors.}},
DOI = {{10.1002/jemt.22908}},
ISSN = {{1059-910X}},
EISSN = {{1097-0029}},
ORCID-Numbers = {{Suvorova, Elena/0000-0001-8697-564X}},
Unique-ID = {{ISI:000411751100008}},
}

@article{ ISI:000411549800063,
Author = {Sachs, Susanne and Geipel, Gerhard and Bok, Frank and Oertel, Jana and
   Fahmy, Karim},
Title = {{Calorimetrically Determined U(VI) Toxicity in Brassica napus Correlates
   with Oxidoreductase Activity and U(VI) Speciation}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2017}},
Volume = {{51}},
Number = {{18}},
Pages = {{10843-10849}},
Month = {{SEP 19}},
Abstract = {{Radioecological studies depend on the quantitative toxicity assessment
   of environmental radionuclides. At low dose exposure, the life span of
   affected organisms is barely shortened, enabling the transfer of
   radionuclides through an almost intact food chain. Lethality-based
   toxicity estimates are not adequate in this regime because they require
   higher concentrations. However, increased radionuclide concentration
   alters its speciation, rendering the extrapolation to the low dose
   exposure chemically inconsistent. Here, we demonstrate that
   microcalorimetry provides a sensitive real-time monitor of toxicity of
   uranium (in the U(VI) oxidation state) in a plant cell model of Brassica
   napus. We introduce the calorimetric descriptor ``metabolic capacity{''}
   and show that it correlates with enzymatically determined cell
   viability. It is independent of physiological models and robust against
   the naturally occurring fluctuations in the metabolic response to U(VI)
   of plant cell cultures. In combination with time-resolved laser-induced
   fluorescence spectroscopy and thermodynamic modeling, we show that the
   plant cell metabolism is affected predominantly by hydroxo-species of
   U(VI) with an IC50 threshold of similar to 90 mu M. The data emphasize
   the yet little-exploited potential of microcalorimetry for the
   speciation-sensitive ecotoxicology of radionuclides.}},
DOI = {{10.1021/acs.est.7b02564}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Fahmy, Karim/B-2708-2017
   Oertel, Jana/T-1048-2017}},
ORCID-Numbers = {{Fahmy, Karim/0000-0002-8752-5824
   }},
Unique-ID = {{ISI:000411549800063}},
}

@article{ ISI:000409541400015,
Author = {Le, Trang T. and Simmons, W. Kyle and Misaki, Masaya and Bodurka, Jerzy
   and White, Bill C. and Savitz, Jonathan and McKinney, Brett A.},
Title = {{Differential privacy-based evaporative cooling feature selection and
   classification with relief-F and random forests}},
Journal = {{BIOINFORMATICS}},
Year = {{2017}},
Volume = {{33}},
Number = {{18}},
Pages = {{2906-2913}},
Month = {{SEP 15}},
Abstract = {{Motivation: Classification of individuals into disease or clinical
   categories from high-dimensional biological data with low prediction
   error is an important challenge of statistical learning in
   bioinformatics. Feature selection can improve classification accuracy
   but must be incorporated carefully into cross-validation to avoid
   overfitting. Recently, feature selection Methods based on differential
   privacy, such as differentially private random forests and reusable
   holdout sets, have been proposed. However, for domains such as
   bioinformatics, where the number of features is much larger than the
   number of observations p >> n, these differential privacy methods are
   susceptible to overfitting.
   Methods: We introduce private Evaporative Cooling, a stochastic
   privacy-preserving machine learning algorithm that uses Relief-F for
   feature selection and random forest for privacy preserving
   classification that also prevents overfitting. We relate the
   privacy-preserving threshold mechanism to a thermodynamic
   Maxwell-Boltzmann distribution, where the temperature represents the
   privacy threshold. We use the thermal statistical physics concept of
   Evaporative Cooling of atomic gases to perform backward stepwise
   privacy-preserving feature selection.
   Results: On simulated data with main effects and statistical
   interactions, we compare accuracies on holdout and validation sets for
   three privacy-preserving methods: the reusable holdout, reusable holdout
   with random forest, and private Evaporative Cooling, which uses Relief-F
   feature selection and random forest classification. In simulations where
   interactions exist between attributes, private Evaporative Cooling
   provides higher classification accuracy without overfitting based on an
   independent validation set. In simulations without interactions,
   thresholdout with random forest and private Evaporative Cooling give
   comparable accuracies. We also apply these privacy methods to human
   brain resting-state fMRI data from a study of major depressive disorder.
   Availability and implementation: Code available at
   http://insilico.utulsa.edu/software/privateEC.
   Contact: brett-mckinney@utulsa.edu
   Supplementary information: Supplementary data are available at
   Bioinformatics online.}},
DOI = {{10.1093/bioinformatics/btx298}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Savitz, Jonathan/C-3088-2009
   }},
ORCID-Numbers = {{Savitz, Jonathan/0000-0001-8143-182X
   Le, Trang/0000-0003-3737-6565}},
Unique-ID = {{ISI:000409541400015}},
}

@article{ ISI:000409135300002,
Author = {Ushakov, M. V. and Nedosekina, T. V.},
Title = {{Method for the evaluation of thermal requirements for development based
   on phenological observations}},
Journal = {{RUSSIAN JOURNAL OF ECOLOGY}},
Year = {{2017}},
Volume = {{48}},
Number = {{5}},
Pages = {{409-416}},
Month = {{SEP}},
Abstract = {{A new method for the evaluation of threshold temperature of development
   involving phenological observations and based on hyperbolic imaging of
   dependence between the day of development and temperature values close
   to the threshold was proposed. The rare species Clematis integrifolia
   was used as an example to evaluate thermal requirements for six stages
   of its development. The method proposed in this study was also compared
   with some common methods for the estimation of threshold temperature
   basing on phenological data.}},
DOI = {{10.1134/S1067413617050137}},
ISSN = {{1067-4136}},
EISSN = {{1608-3334}},
ResearcherID-Numbers = {{Ushakov, Michail/O-9751-2018}},
ORCID-Numbers = {{Ushakov, Michail/0000-0001-6615-7041}},
Unique-ID = {{ISI:000409135300002}},
}

@article{ ISI:000408567600002,
Author = {Li, Xungui and Wei, Ning and Wei, Xia},
Title = {{Complexity Analysis of Precipitation-Runoff Series Based on a New
   Parameter-Optimization Method of Entropy}},
Journal = {{JOURNAL OF HYDROLOGIC ENGINEERING}},
Year = {{2017}},
Volume = {{22}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{Different parameters of dimension m and tolerance threshold r in
   approximate entropy (ApEn) and sample entropy (SampEn) can cause
   inconsistency of complexity comparison in hydrologic time series. A new
   approach to determine the optimal common parameters m and r is presented
   to solve this inconsistency. Time series of runoff and precipitation
   during 1956-2010 in the Jinghe watershed of the Chinese Loess Plateau
   were analyzed. The optimal common parameters were determined to be m = 2
   and r = 0.11 standard deviation of time series. The runoff and
   precipitation show significant decreasing trends, which are opposite of
   the significant increasing trends of ApEn and SampEn. The runoff and
   precipitation have close relationships with their corresponding
   complexity. The decreasing trend of precipitation has important
   influences on the increase of runoff complexity. The ApEn is better than
   the SampEn for describing the general trend of the time series, while
   the SampEn has a stronger capacity to identify the turning points in the
   time series. This new approach has wide applicability. (C) 2017 American
   Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)HE.1943-5584.0001554}},
Article-Number = {{04017029}},
ISSN = {{1084-0699}},
EISSN = {{1943-5584}},
ORCID-Numbers = {{Li, Xungui/0000-0003-3963-0686}},
Unique-ID = {{ISI:000408567600002}},
}

@article{ ISI:000409208900001,
Author = {Li, Weikai and Wang, Zhengxia and Zhang, Limei and Qiao, Lishan and
   Shen, Dinggang},
Title = {{Remodeling Pearson's Correlation for Functional Brain Network Estimation
   and Autism Spectrum Disorder Identification}},
Journal = {{FRONTIERS IN NEUROINFORMATICS}},
Year = {{2017}},
Volume = {{11}},
Month = {{AUG 31}},
Abstract = {{Functional brain network (FBN) has been becoming an increasingly
   important way to model the statistical dependence among neural time
   courses of brain, and provides effective imaging biomarkers for
   diagnosis of some neurological or psychological disorders. Currently,
   Pearson's Correlation (PC) is the simplest and most widely-used method
   in constructing FBNs. Despite its advantages in statistical meaning and
   calculated performance, the PC tends to result in a FBN with dense
   connections. Therefore, in practice, the PC-based FBN needs to be
   sparsified by removing weak (potential noisy) connections. However, such
   a scheme depends on a hard-threshold without enough flexibility.
   Different from this traditional strategy, in this paper, we propose a
   new approach for estimating FBNs by remodeling PC as an optimization
   problem, which provides a way to incorporate biological/physical priors
   into the FBNs. In particular, we introduce an L1-norm regularizer into
   the optimization model for obtaining a sparse solution. Compared with
   the hard-threshold scheme, the proposed framework gives an
   elegantmathematical formulation for sparsifying PC-based networks. More
   importantly, it provides a platform to encode other biological/ physical
   priors into the PC-based FBNs. To further illustrate the flexibility of
   the proposed method, we extend the model to a weighted counterpart for
   learning both sparse and scale-free networks, and then conduct
   experiments to identify autismspectrumdisorders (ASD) fromnormal
   controls (NC) based on the constructed FBNs. Consequently, we achieved
   an 81.52\% classification accuracy which outperforms the baseline and
   state-of-the-art methods.}},
DOI = {{10.3389/fninf.2017.00055}},
Article-Number = {{55}},
ISSN = {{1662-5196}},
Unique-ID = {{ISI:000409208900001}},
}

@article{ ISI:000409151600031,
Author = {Rodrigues, Joao and Belo, David and Gamboa, Hugo},
Title = {{Noise detection on ECG based on agglomerative clustering of
   morphological features}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2017}},
Volume = {{87}},
Pages = {{322-334}},
Month = {{AUG 1}},
Abstract = {{Biosignals are usually contaminated with artifacts from limb movements,
   muscular contraction or electrical interference. Many algorithms of the
   literature, such as threshold methods and adaptive filters, focus, on
   detecting these noisy patterns. This study introduces a novel method for
   noise and artifact detection in electrocardiogram based on time series
   clustering.
   The algorithm starts with the extraction of features that best
   characterize the shape and behaviour of the signal over time and groups
   its samples in separated clusters by means of an agglomerative
   clustering approach. The method has been tested in numerous datasets to
   reveal that it is independent on specific records and globally, the
   algorithm was able to successfully detect noisy patterns and artifacts
   with a sensitivity of 88\%, a specificity of 92\% and an accuracy of
   91\%, demonstrating a good performance in pattern detection based on
   morphological clustering.
   This algorithm can be applied to the detection and sectioning of
   multiple types of noise for more accurate denoising and adapted for
   signal classification.}},
DOI = {{10.1016/j.compbiomed.2017.06.009}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{Gamboa, Hugo/M-8799-2013}},
ORCID-Numbers = {{Gamboa, Hugo/0000-0002-4022-7424}},
Unique-ID = {{ISI:000409151600031}},
}

@article{ ISI:000405457000004,
Author = {Manfredo, Michael J. and Bruskotter, Jeremy T. and Teel, Tara L. and
   Fulton, David and Schwartz, Shalom H. and Arlinghaus, Robert and Oishi,
   Shigehiro and Uskul, Ayse K. and Redford, Kent and Kitayama, Shinobu and
   Sullivan, Leeann},
Title = {{Why social values cannot be changed for the sake of conservation}},
Journal = {{CONSERVATION BIOLOGY}},
Year = {{2017}},
Volume = {{31}},
Number = {{4}},
Pages = {{772-780}},
Month = {{AUG}},
Abstract = {{The hope for creating widespread change in social values has endured
   among conservation professionals since early calls by Aldo Leopold for a
   ``land ethic.{''} However, there has been little serious attention in
   conservation to the fields of investigation that address values, how
   they are formed, and how they change. We introduce a social-ecological
   systems conceptual approach in which values are seen not only as
   motivational goals people hold but also as ideas that are deeply
   embedded in society's material culture, collective behaviors,
   traditions, and institutions. Values define and bind groups,
   organizations, and societies; serve an adaptive role; and are typically
   stable across generations. When abrupt value changes occur, they are in
   response to substantial alterations in the social-ecological context.
   Such changes build on prior value structures and do not result in
   complete replacement. Given this understanding of values, we conclude
   that deliberate efforts to orchestrate value shifts for conservation are
   unlikely to be effective. Instead, there is an urgent need for research
   on values with a multilevel and dynamic view that can inform innovative
   conservation strategies for working within existing value structures.
   New directions facilitated by a systems approach will enhance
   understanding of the role values play in shaping conservation challenges
   and improve management of the human component of conservation.}},
DOI = {{10.1111/cobi.12855}},
ISSN = {{0888-8892}},
EISSN = {{1523-1739}},
ORCID-Numbers = {{Arlinghaus, Robert/0000-0003-2861-527X}},
Unique-ID = {{ISI:000405457000004}},
}

@article{ ISI:000408234300006,
Author = {Kujanova, Katerina and Matouskova, Milada},
Title = {{Identification of hydromorphological reference sites using the new
   REFCON method, with an application to rivers in the Czech Republic}},
Journal = {{ECOHYDROLOGY \& HYDROBIOLOGY}},
Year = {{2017}},
Volume = {{17}},
Number = {{3}},
Pages = {{235-245}},
Month = {{JUL}},
Abstract = {{This paper presents a new approach to establishing reference sites and
   determining hydromorphological characteristics for establishing
   type-specific hydromorphological reference conditions for rivers in the
   Czech Republic - the REFCON method - as well as its application. This
   method is based on hydromorphological river types. Using available maps
   and field surveys, it determines potential reference sites for
   establishing reference conditions, and subsequently, on the basis of an
   assessment of anthropogenic impacts using set criteria, it identifies
   stream reference sites.
   Reference sites and river types are validated using field survey data.
   This method also identifies characteristics of channel pattern, flow,
   riverbed structures, sediment, and variability of cross-sectional
   profiles used for establishing type-specific hydromorphological
   reference conditions. Reference conditions can be expressed using
   threshold values of individual characteristics (e.g., channel slope,
   entrenchment ratio, specific stream power); they can also be described
   qualitatively, that is, verbally (e.g., channel pattern, valley type).
   Nonetheless, reference conditions are always an expression of a set of
   characteristics or conditions that should be valid for a given river
   type.
   One of the expected results of applying this method is the creation of a
   reference site database for assessing hydromorphological status of
   streams. This database will serve for proposing stream restoration
   measures and also as a database of stream sections in need of protection
   from potential human impact. (C) 2017 European Regional Centre for
   Ecohydrology of the Polish Academy of Sciences. Published by Elsevier
   Sp. zo.o. All rights reserved.}},
DOI = {{10.1016/j.ecohyd.2017.06.002}},
ISSN = {{1642-3593}},
EISSN = {{2080-3397}},
ResearcherID-Numbers = {{Kujanova, Katerina/D-5659-2018}},
ORCID-Numbers = {{Kujanova, Katerina/0000-0001-8547-9011}},
Unique-ID = {{ISI:000408234300006}},
}

@article{ ISI:000406619800023,
Author = {Liu, Xiaoping and Chang, Xiao and Liu, Rui and Yu, Xiangtian and Chen,
   Luonan and Aihara, Kazuyuki},
Title = {{Quantifying critical states of complex diseases using single-sample
   dynamic network biomarkers}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2017}},
Volume = {{13}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Dynamic network biomarkers (DNB) can identify the critical state or
   tipping point of a disease, thereby predicting rather than diagnosing
   the disease. However, it is difficult to apply the DNB theory to
   clinical practice because evaluating DNB at the critical state required
   the data of multiple samples on each individual, which are generally not
   available, and thus limit the applicability of DNB. In this study, we
   developed a novel method, i.e., single-sample DNB (sDNB), to detect
   early-warning signals or critical states of diseases in individual
   patients with only a single sample for each patient, thus opening a new
   way to predict diseases in a personalized way. In contrast to the
   information of differential expressions used in traditional biomarkers
   to ``diagnose disease{''}, sDNB is based on the information of
   differential associations, thereby having the ability to ``predict
   disease{''} or ``diagnose near-future disease{''}. Applying this method
   to datasets for influenza virus infection and cancer metastasis led to
   accurate identification of the critical states or correct prediction of
   the immediate diseases based on individual samples. We successfully
   identified the critical states or tipping points just before the
   appearance of disease symptoms for influenza virus infection and the
   onset of distant metastasis for individual patients with cancer, thereby
   demonstrating the effectiveness and efficiency of our method for
   quantifying critical states at the single-sample level.}},
DOI = {{10.1371/journal.pcbi.1005633}},
Article-Number = {{e1005633}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
ResearcherID-Numbers = {{Yu, Xiangtian/S-9058-2018
   }},
ORCID-Numbers = {{liu, Xiaoping/0000-0002-3246-4227
   Liu, Rui/0000-0002-4547-8695}},
Unique-ID = {{ISI:000406619800023}},
}

@article{ ISI:000404054700018,
Author = {Magee, Rogan and Loher, Phillipe and Londin, Eric and Rigoutsos, Isidore},
Title = {{Threshold-seq: a tool for determining the threshold in short RNA-seq
   datasets}},
Journal = {{BIOINFORMATICS}},
Year = {{2017}},
Volume = {{33}},
Number = {{13}},
Pages = {{2034-2036}},
Month = {{JUL 1}},
Abstract = {{We present `Threshold-seq,' a new approach for determining thresholds in
   deep-sequencing datasets of short RNA transcripts. Threshold-seq
   addresses the critical question of how many reads need to support a
   short RNA molecule in a given dataset before it can be considered
   different from `background.' The proposed scheme is easy to implement
   and incorporate into existing pipelines.}},
DOI = {{10.1093/bioinformatics/btx073}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ORCID-Numbers = {{Rigoutsos, Isidore/0000-0003-1529-8631}},
Unique-ID = {{ISI:000404054700018}},
}

@article{ ISI:000403635000005,
Author = {Fassoni, Artur C. and Yang, Hyun M.},
Title = {{An ecological resilience perspective on cancer: Insights from a toy
   model}},
Journal = {{ECOLOGICAL COMPLEXITY}},
Year = {{2017}},
Volume = {{30}},
Number = {{SI}},
Pages = {{34-46}},
Month = {{JUN}},
Abstract = {{In this paper we propose an ecological resilience point of view on
   cancer. This view is based on the analysis of a simple ODE model for the
   interactions between cancer and normal cells. The model presents two
   regimes for tumor growth. In the first, cancer arises due to three
   reasons: a partial corruption of the functions that avoid the growth of
   mutated cells, an aggressive phenotype of tumor cells and exposure to
   external carcinogenic factors. In this case, treatments may be effective
   if they drive the system to the basin of attraction of the cancer cure
   state. In the second regime, cancer arises because the repair system is
   intrinsically corrupted. In this case, the complete cure is not possible
   since the cancer cure state is no more stable, but tumor recurrence may
   be delayed if treatment is prolongued. We review three indicators of the
   resilience of a stable equilibrium, related with size and shape of its
   basin of attraction: latitude, precariousness and resistance. A novel
   method to calculate these indicators is proposed. This method is simpler
   and more efficient than those currently used, and may be easily applied
   to other population dynamics models. We apply this method to the model
   and investigate how these indicators behave with parameters changes.
   Finally, we present some simulations to illustrate how the resilience
   analysis can be applied to validated models in order to obtain
   indicators for personalized cancer treatments. (C) 2016 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.ecocom.2016.10.003}},
ISSN = {{1476-945X}},
EISSN = {{1476-9840}},
ResearcherID-Numbers = {{Fassoni, Artur/V-5740-2017
   Yang, Hyun/K-6873-2014}},
ORCID-Numbers = {{Fassoni, Artur/0000-0002-3634-0061
   Yang, Hyun/0000-0002-1711-363X}},
Unique-ID = {{ISI:000403635000005}},
}

@article{ ISI:000403033000003,
Author = {Lischke, Heike and Loffler, Thomas J.},
Title = {{Finding all multiple stable fixpoints of n-species Lotka-Volterra
   competition models}},
Journal = {{THEORETICAL POPULATION BIOLOGY}},
Year = {{2017}},
Volume = {{115}},
Pages = {{24-34}},
Month = {{JUN}},
Abstract = {{One way to explore assembly of extant and novel communities from species
   pools, and by that biodiversity and species ranges, is to study the
   equilibrium behavior of dynamic competition models such as the
   Lotka-Volterra competition (LVC) model. We present a novel method
   (COMMUSTIX) to determine all stable fixpoints of the general LVC model
   with abundances x from a given pool of n species. To that purpose, we
   split the species in potentially surviving species (x(i) > 0) and in
   others going extinct (x(i) = 0). We derived criteria for the stability
   of x(i) = 0 and for the equilibrium of x(i) > 0 to determine possible
   combinations of extinct and surviving species by iteratively applying a
   mixed binary linear optimization algorithm.
   We tested this new method against (a) the numerical solution at
   equilibrium of the LVC ordinary differential equations (ODEs) and (b)
   the fixpoints of all combinations of surviving and extinct species
   (possible only for small n), tested for stability and non-negativity.
   The tests revealed that COMMUSTIX is reliable, it detects all multiple
   stable fixpoints (SFPs), which is not guaranteed by solving the ODEs,
   and more efficient than the combinations method.
   With COMMUSTIX, we studied the dependence of the fixpoint behavior on
   the competition strengths relative to the intra-specific competition. If
   inter-specific competition was considerably lower than intra-specific
   competition, only globally SFPs occurred. In contrast, if all
   inter-specific was higher than intra-specific competition, multiple SFPs
   consisting of only one species occurred. If competition strengths in the
   species pool ranged from below to above the intra-specific competition,
   either global or multiple SFPs strongly differing in species composition
   occurred. The species richness over all SFPs was high for pools of
   species with similar, either weak or strong competition, and lower for
   species with dissimilar or close to intra-specific competition
   strengths.
   The new approach is a reliable and efficient tool for further extensive
   examinations of the dependence of community compositions on parameter
   settings of the LVC model. (C) 2017 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.tpb.2017.02.001}},
ISSN = {{0040-5809}},
EISSN = {{1096-0325}},
ResearcherID-Numbers = {{Lischke, Heike/J-5719-2013}},
Unique-ID = {{ISI:000403033000003}},
}

@article{ ISI:000404358200057,
Author = {Huang, Hong and Wang, Zhenfeng and Xia, Fang and Shang, Xu and Liu,
   YuanYuan and Zhang, Minghua and Dahlgren, Randy A. and Mei, Kun},
Title = {{Water quality trend and change-point analyses using integration of
   locally weighted polynomial regression and segmented regression}},
Journal = {{ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH}},
Year = {{2017}},
Volume = {{24}},
Number = {{18}},
Pages = {{15827-15837}},
Month = {{JUN}},
Abstract = {{Trend and change-point analyses of water quality time series data have
   important implications for pollution control and environmental
   decision-making. This paper developed a new approach to assess trends
   and change-points of water quality parameters by integrating locally
   weighted polynomial regression (LWPR) and segmented regression (SegReg).
   Firstly, LWPR was used to pretreat the original water quality data into
   a smoothed time series to represent the long-term trend of water
   quality. Then, SegReg was used to identify the long-term trends and
   change-points of the smoothed time series. Finally, statistical tests
   were applied to determine the significance of the long-term trends and
   change-points. The efficacy of this approach was validated using a
   10-year record of total nitrogen (TN) and chemical oxygen demand (CODMn)
   from Shanxi Reservoir watershed in eastern China. Results showed that
   this approach was straightforward and reliable for assessment of
   long-term trends and change-points on irregular water quality datasets.
   The reliability was verified by statistical tests and practical
   considerations for Shanxi Reservoir watershed. The newly developed
   integrated LWPR-SegReg approach is not only limited to the assessment of
   trends and change-points of water quality parameters but also has a
   broad application to other fields with long-term time series records.}},
DOI = {{10.1007/s11356-017-9188-x}},
ISSN = {{0944-1344}},
EISSN = {{1614-7499}},
ORCID-Numbers = {{SHANG, XU/0000-0002-3998-8971}},
Unique-ID = {{ISI:000404358200057}},
}

@article{ ISI:000403478400018,
Author = {Su, Yu-Ru and Di, Chong-Zhi and Hsu, Li},
Title = {{Hypothesis testing in functional linear models}},
Journal = {{BIOMETRICS}},
Year = {{2017}},
Volume = {{73}},
Number = {{2}},
Pages = {{551-561}},
Month = {{JUN}},
Abstract = {{Functional data arise frequently in biomedical studies, where it is
   often of interest to investigate the association between functional
   predictors and a scalar response variable. While functional linear
   models (FLM) are widely used to address these questions, hypothesis
   testing for the functional association in the FLM framework remains
   challenging. A popular approach to testing the functional effects is
   through dimension reduction by functional principal component (PC)
   analysis. However, its power performance depends on the choice of the
   number of PCs, and is not systematically studied. In this article, we
   first investigate the power performance of the Wald-type test with
   varying thresholds in selecting the number of PCs for the functional
   covariates, and show that the power is sensitive to the choice of
   thresholds. To circumvent the issue, we propose a new method of ordering
   and selecting principal components to construct test statistics. The
   proposed method takes into account both the association with the
   response and the variation along each eigenfunction. We establish its
   theoretical properties and assess the finite sample properties through
   simulations. Our simulation results show that the proposed test is more
   robust against the choice of threshold while being as powerful as, and
   often more powerful than, the existing method. We then apply the
   proposed method to the cerebral white matter tracts data obtained from a
   diffusion tensor imaging tractography study.}},
DOI = {{10.1111/biom.12624}},
ISSN = {{0006-341X}},
EISSN = {{1541-0420}},
Unique-ID = {{ISI:000403478400018}},
}

@article{ ISI:000402131500006,
Author = {Kapli, P. and Lutteropp, S. and Zhang, J. and Kobert, K. and Pavlidis,
   P. and Stamatakis, A. and Flouri, T.},
Title = {{Multi-rate Poisson tree processes for single-locus species delimitation
   under maximum likelihood and Markov chain Monte Carlo}},
Journal = {{BIOINFORMATICS}},
Year = {{2017}},
Volume = {{33}},
Number = {{11}},
Pages = {{1630-1638}},
Month = {{JUN 1}},
Abstract = {{Motivation: In recent years, molecular species delimitation has become a
   routine approach for quantifying and classifying biodiversity. Barcoding
   methods are of particular importance in large-scale surveys as they
   promote fast species discovery and biodiversity estimates. Among those,
   distance-based methods are the most common choice as they scale well
   with large datasets; however, they are sensitive to similarity threshold
   parameters and they ignore evolutionary relationships. The recently
   introduced ``Poisson Tree Processes{''} (PTP) method is a
   phylogeny-aware approach that does not rely on such thresholds. Yet, two
   weaknesses of PTP impact its accuracy and practicality when applied to
   large datasets; it does not account for divergent intraspecific
   variation and is slow for a large number of sequences.
   Results: We introduce the multi-rate PTP (mPTP), an improved method that
   alleviates the theoretical and technical shortcomings of PTP. It
   incorporates different levels of intraspecific genetic diversity
   deriving from differences in either the evolutionary history or sampling
   of each species. Results on empirical data suggest that mPTP is superior
   to PTP and popular distance-based methods as it, consistently yields
   more accurate delimitations with respect to the taxonomy (i.e.,
   identifies more taxonomic species, infers species numbers closer to the
   taxonomy). Moreover, mPTP does not require any similarity threshold as
   input. The novel dynamic programming algorithm attains a speedup of at
   least five orders of magnitude compared to PTP, allowing it to delimit
   species in large (meta-) barcoding data. In addition, Markov Chain Monte
   Carlo sampling provides a comprehensive evaluation of the inferred
   delimitation in just a few seconds for millions of steps, independently
   of tree size.}},
DOI = {{10.1093/bioinformatics/btx025}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Stamatakis, Alexandros/B-8740-2009}},
Unique-ID = {{ISI:000402131500006}},
}

@article{ ISI:000402077400001,
Author = {Jain, Priyamvada and Chakma, Babina and Singh, Naveen and Patra,
   Sanjukta and Goswami, Pranab},
Title = {{Metal-DNA Interactions Improve signal in High-Resolution Melting of DNA
   for Species Differentiation of Plasmodium Parasite}},
Journal = {{MOLECULAR BIOTECHNOLOGY}},
Year = {{2017}},
Volume = {{59}},
Number = {{6}},
Pages = {{179-191}},
Month = {{JUN}},
Abstract = {{The success of high-resolution melting (HRM) analysis for distinguishing
   similar DNAs with minor base mismatch differences is limited. Here,
   metal-mediated structural change in DNA has been exploited to amplify
   HRM signals leading to differentiation of target DNAs in an orthologous
   gene corresponding to four Plasmodium species. Conserved 26-mer ssDNAs
   from ldh gene of the four Plasmodium species were employed as targets. A
   capture probe (CP) that is fully complementary to the Plasmodium
   falciparum target (FT) and has two base mismatches each, with the
   targets of Plasmodium vivax (VT), Plasmodium malariae, (MT), and
   Plasmodium ovale (OT), was considered. The DNA duplexes were treated
   with metal ions for structural perturbation and analyzed by HRM.
   Distinct resolution of melting fluorescence signal in otherwise
   identical HRM profiles for each of the DNA duplexes was achieved by
   using Ca+2 or Mg+2 ions, where, Ca+2 conferred higher resolution. The
   increase in resolution for CP-FT versus CP-OT, CP-FT versus CP-VT, CP-FT
   versus CP-MT, CP-VT versus CP-OT, and CP-MT versus CP-OT with Ca-DNA as
   compared to control was 67.3-, 20.4-, 22.0-, 10.9-, and 8.3-fold,
   respectively. The signal resolution was the highest at pH 8. The method
   could detect 0.25 pmol/mu l of the target DNA. Structural analysis
   showed that Ca+2 and Mg+2 ions perturbed the structure of DNA. This
   perturbation helped to improve HRM signal resolution among DNA targets
   corresponding to the orthologous gene of four Plasmodium species. This
   novel approach has potential application not only for Plasmodium
   species-specific diagnosis but also for differentiation of DNAs with
   minor sequence variation.}},
DOI = {{10.1007/s12033-017-0004-0}},
ISSN = {{1073-6085}},
EISSN = {{1559-0305}},
ORCID-Numbers = {{jain, Priyamvada/0000-0003-4331-9903
   Chakma, Babina/0000-0001-9316-2620}},
Unique-ID = {{ISI:000402077400001}},
}

@article{ ISI:000401110400015,
Author = {Tonini, Francesco and Shoemaker, Douglas and Petrasova, Anna and Harmon,
   Brendan and Petras, Vaclav and Cobb, Richard C. and Mitasova, Helena and
   Meentemeyer, Ross K.},
Title = {{Tangible geospatial modeling for collaborative solutions to invasive
   species management}},
Journal = {{ENVIRONMENTAL MODELLING \& SOFTWARE}},
Year = {{2017}},
Volume = {{92}},
Pages = {{176-188}},
Month = {{JUN}},
Abstract = {{Managing landscape-scale environmental problems, such as biological
   invasions, can be facilitated by integrating realistic geospatial models
   with user-friendly interfaces that stakeholders can use to make critical
   management decisions. However, gaps between scientific theory and
   application have typically limited opportunities for model-based
   knowledge to reach the stakeholders responsible for problem-solving. To
   address this challenge, we introduce Tangible Landscape, an open-source
   participatory modeling tool providing an interactive, shared arena for
   consensus-building and development of collaborative solutions for
   landscape-scale problems. Using Tangible Landscape, stakeholders gather
   around a geographically realistic 3D visualization and explore
   management scenarios with instant feedback; users direct model
   simulations with intuitive tangible gestures and compare alternative
   strategies with an output dashboard. We applied Tangible Landscape to
   the complex problem of managing the emerging infectious disease, sudden
   oak death, in California and explored its potential to generate
   co-learning and collaborative management strategies among actors
   representing stakeholders with competing management aims. (C) 2017 The
   Authors. Published by Elsevier Ltd.}},
DOI = {{10.1016/j.envsoft.2017.02.020}},
ISSN = {{1364-8152}},
EISSN = {{1873-6726}},
ORCID-Numbers = {{Shoemaker, Douglas/0000-0002-5354-2562
   Harmon, Brendan/0000-0002-6218-9318}},
Unique-ID = {{ISI:000401110400015}},
}

@article{ ISI:000399259300008,
Author = {Mesgaran, Mohsen B. and Onofri, Andrea and Mashhadi, Hamid R. and
   Cousens, Roger D.},
Title = {{Water availability shifts the optimal temperatures for seed germination:
   A modelling approach}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2017}},
Volume = {{351}},
Pages = {{87-95}},
Month = {{MAY 10}},
Abstract = {{Hydrothermal time (HTT) models have been widely applied to describe the
   germination responses of seeds to temperature and water. Some HIT models
   assume that the thermoinhibition of germination is caused by an upward
   shift in base water potential psi(b(g)) that only happens above the
   optimum temperature To, and that To is a parameter independent of the
   water stress level. However, the available data suggest that these
   assumptions may be invalid in practice. We introduce a new HIT model
   that uses a log logistic distribution for psi(b(g))and assumes a linear
   increase in psi(b(g)) across both sub-and supra-optimal temperature
   ranges. We also provide explicit mathematical solutions for estimating
   To and other cardinal temperatures within a HTT modelling framework.
   Germination data were obtained for two winter annual plant species
   (Hordeum spontaneum and Phalaris minor) and used to build and test the
   model. For both species, the linear upward shift in psi(b(g))) was
   confirmed across both sub-and supra-optimal ranges, while the hydrotime
   constant theta(H) decreased nonlinearly with temperature. This interplay
   between OH and psi(b(g)) resulted in a curvilinear germination rate
   response to temperature that accounts for thermoinhibition of
   germination at high temperatures. The optimal temperature decreased
   proportionally with decreasing water potential and became cooler for
   higher (slower) germination fractions than for the lower (faster) ones.
   The new modelling approach not only gave good fits to germination data
   (with root mean square errors <10\%), but also provided useful insights
   about the adaptive strategies evolved by plant species to optimize their
   germination timing under the various temperature and moisture
   environments. (C) 2017 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2017.02.020}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
ORCID-Numbers = {{Mesgaran, Mohsen/0000-0002-0841-6188
   ONOFRI, Andrea/0000-0002-6603-329X}},
Unique-ID = {{ISI:000399259300008}},
}

@article{ ISI:000400597300001,
Author = {McCurdy, S. R. and Pacal, M. and Ahmad, M. and Bremner, R.},
Title = {{A CDK2 activity signature predicts outcome in CDK2-low cancers}},
Journal = {{ONCOGENE}},
Year = {{2017}},
Volume = {{36}},
Number = {{18}},
Pages = {{2491-2502}},
Month = {{MAY 4}},
Abstract = {{The role of cyclin-dependent kinase 2 (CDK2) in cancer is controversial.
   A major hurdle is the availability of tools to easily assess its
   activity across many samples. Here, we introduce a transcriptional
   signature to specifically track CDK2 activity. It responds to genetic
   and chemical perturbations in the CDK-RB-E2F axis, correlates with
   mitotic rate in vitro and in vivo and reacts rapidly to changes in CDK2
   activity during cell cycle progression. We find that CDK2 activity is
   specifically elevated in human testes, mirroring its critical function
   in mice, and report very distinct profiles across human cancers.
   Increased CDK2 activity decreases risk in colon cancer, but elevates
   poor outcome two-to fivefold in specific tumors, including low grade
   glioma, kidney, thyroid, adrenocortical and prostate cancer. These are
   typically `CDK2-low' cancers, suggesting that above a certain threshold
   CDK2 promotes progression, but further increases do not influence
   outcome. Multivariate analysis revealed that the CDK2 signature is the
   most important predictive feature in these cancers versus dozens of
   other clinical parameters, such as tumor grade or mitotic index. Thus,
   transcriptome data provides a novel, straightforward method to monitor
   CDK2 activity, implicates key roles for the kinase in a subset of human
   tissues and tumors and enhances cancer risk prediction. The strategy
   used here for CDK2 could be applied to other kinases that influence
   transcription.}},
DOI = {{10.1038/onc.2016.409}},
ISSN = {{0950-9232}},
EISSN = {{1476-5594}},
ResearcherID-Numbers = {{Bremner, Rod/I-6490-2012}},
ORCID-Numbers = {{Bremner, Rod/0000-0001-9184-7212}},
Unique-ID = {{ISI:000400597300001}},
}

@article{ ISI:000404385100022,
Author = {Dougherty, Brian and Gray, Myles and Johnson, Mark G. and Kleber, Markus},
Title = {{Can Biochar Covers Reduce Emissions from Manure Lagoons While Capturing
   Nutrients?}},
Journal = {{JOURNAL OF ENVIRONMENTAL QUALITY}},
Year = {{2017}},
Volume = {{46}},
Number = {{3}},
Pages = {{659-666}},
Month = {{MAY-JUN}},
Abstract = {{The unique physical and chemical properties of biochars make them
   promising materials for odor, gas, and nutrient sorption. Floating
   covers made from organic materials (biocovers) are one option for
   reducing odor and gas emissions from livestock manure lagoons. This
   study evaluated the potential of floating biochar covers to reduce odor
   and gas emissions while simultaneously sorbing nutrients from liquid
   dairy manure. This new approach has the potential to mitigate multiple
   environmental problems. Two biochars were tested: one made via
   gasification of Douglas fir chips at 650 degrees C (FC650), and the
   other made from a mixture of Douglas fir {[}Pseudotsuga menziesii
   (Mirb.) Franco] bark and center wood pyrolyzed at 600 degrees C (HF600).
   The HF600 biocover reduced mean headspace ammonia concentration by 72 to
   80\%. No significant reduction was found with the FC650 biocover.
   Nutrient uptake ranged from 0.21 to 4.88 mg N g(-1) biochar and 0.64 to
   2.70 mg P g(-1) biochar for the HF600 and FC650 biochars, respectively.
   Potassium ranged from a loss of 4.52 to a gain of 2.65 mg g(-1) biochar
   for the FC650 and HF600 biochars, respectively. The biochars also sorbed
   Ca, Mg, Na, Fe, Al, and Si. In a separate sensory evaluation, judges
   assessed odor offensiveness and odor threshold of five biocover
   treatments including four biochars applied over dairy manure. Reductions
   in mean odor offensiveness and mean odor threshold were observed in
   three treatments compared with the control. These results show that
   biochar covers hold promise as an effective practice for reducing odor
   and gas emissions while sorbing nutrients from liquid dairy manure.}},
DOI = {{10.2134/jeq2016.12.0478}},
ISSN = {{0047-2425}},
EISSN = {{1537-2537}},
Unique-ID = {{ISI:000404385100022}},
}

@article{ ISI:000401377700017,
Author = {Banchhor, Sumit K. and Londhe, Narendra D. and Araki, Tadashi and Saba,
   Luca and Radeva, Petia and Laird, John R. and Suri, Jasjit S. and AIMBE},
Title = {{Well-balanced system for coronary calcium detection and volume
   measurement in a low resolution intravascular ultrasound videos}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2017}},
Volume = {{84}},
Pages = {{168-181}},
Month = {{MAY 1}},
Abstract = {{Background: Accurate and fast quantitative assessment of calcium volume
   is required during the planning of percutaneous coronary interventions
   procedures. Low resolution in intravascular ultrasound (IVUS) coronary
   videos poses a threat to calcium detection causing over-estimation in
   volume measurement. We introduce a correction block that
   counter-balances the bias introduced during the calcium detection
   process.
   Method: Nineteen patients image dataset (around 40,090 frames), IRB
   approved, were collected using 40 MHz IVUS catheter (Atlantis (R) SR
   Pro, Boston Scientific (R), pullback speed of 0.5 mm/sec). A new set of
   20 generalized and well-balanced systems each consisting of three
   stages: (i) calcium detection, (ii) calibration and (iii) measurement,
   while ensuring accuracy of four soft classifiers (Threshold, FCM,
   K-means and HMRF) and workflow speed using five multiresolution
   techniques (bilinear, bicubic, wavelet, Lanczos, Gaussian Pyramid) were
   designed. Results of the three calcium detection methods were
   benchmarked against the Threshold-based method.
   Results: All 20 well-balanced systems with calibration block show
   superior performance. Using calibration block, FCM versus
   Threshold-based method shows the highest cross-correlation 0.99 (P <
   0.0001), Jaccard index 0.984 +/- 0.013 (P < 0.0001), and Dice similarity
   0.992 +/- 0.007 (P < 0.0001). The corresponding area under the curve for
   four calcium detection techniques is: 1.0, 1.0, 0.97 and 0.93,
   respectively. The mean overall performance improvement is 38.54\% and
   when adapting calibration block. The mean workflow speed improvement is
   62.14\% when adapting multiresolution paradigm. Three clinical tests
   shows consistency, reliability, and stability of our well-balanced
   system.
   Conclusions: A well-balanced system with a combination of Threshold
   embedded with Lanczos multiresolution was optimal and can be useable in
   clinical settings.}},
DOI = {{10.1016/j.compbiomed.2017.03.026}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
Unique-ID = {{ISI:000401377700017}},
}

@article{ ISI:000400011100011,
Author = {Perlala, Tommi A. and Swain, Douglas P. and Kuparinen, Anna},
Title = {{Examining nonstationarity in the recruitment dynamics of fishes using
   Bayesian change point analysis}},
Journal = {{CANADIAN JOURNAL OF FISHERIES AND AQUATIC SCIENCES}},
Year = {{2017}},
Volume = {{74}},
Number = {{5}},
Pages = {{751-765}},
Month = {{MAY}},
Abstract = {{Marine ecosystems can undergo regime shifts, which result in
   nonstationarity in the dynamics of the fish populations inhabiting them.
   The assumption of time-invariant parameters in stock-recruitment models
   can lead to severe errors when forecasting renewal ability of stocks
   that experience shifts in their recruitment dynamics. We present a novel
   method for fitting stock-recruitment models using the Bayesian online
   change point detection algorithm, which is able to cope with sudden
   changes in the model parameters. We validate our method using
   simulations and apply it to empirical data of four demersal fishes in
   the southern Gulf of St. Lawrence. We show that all of the stocks have
   experienced shifts in their recruitment dynamics that cannot be captured
   by a model that assumes time-invariant parameters. The detected shifts
   in the recruitment dynamics result in clearly different parameter
   distributions and recruitment predictions between the regimes. This
   study illustrates how stock-recruitment relationships can experience
   shifts, which, if not accounted for, can lead to false predictions about
   a stock's recovery ability and resilience to fishing.}},
DOI = {{10.1139/cjfas-2016-0177}},
ISSN = {{0706-652X}},
EISSN = {{1205-7533}},
Unique-ID = {{ISI:000400011100011}},
}

@article{ ISI:000398947900007,
Author = {Martins, Jose and Pinto, Alberto},
Title = {{Bistability of Evolutionary Stable Vaccination Strategies in the
   Reinfection SIRI Model}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{2017}},
Volume = {{79}},
Number = {{4}},
Pages = {{853-883}},
Month = {{APR}},
Abstract = {{We use the reinfection SIRI epidemiological model to analyze the impact
   of education programs and vaccine scares on individuals decisions to
   vaccinate or not. The presence of the reinfection provokes the novelty
   of the existence of three Nash equilibria for the same level of the
   morbidity relative risk instead of a single Nash equilibrium as occurs
   in the SIR model studied by Bauch and Earn (PNAS 101:13391-13394, 2004).
   The existence of three Nash equilibria, with two of them being
   evolutionary stable, introduces two scenarios with relevant and opposite
   features for the same level of the morbidity relative risk: the
   low-vaccination scenario corresponding to the evolutionary stable
   vaccination strategy, where individuals will vaccinate with a low
   probability; and the high-vaccination scenario corresponding to the
   evolutionary stable vaccination strategy, where individuals will
   vaccinate with a high probability. We introduce the evolutionary
   vaccination dynamics for the SIRI model and we prove that it is
   bistable. The bistability of the evolutionary dynamics indicates that
   the damage provoked by false scares on the vaccination perceived
   morbidity risks can be much higher and much more persistent than in the
   SIR model. Furthermore, the vaccination education programs to be
   efficient they need to implement a mechanism to suddenly increase the
   vaccination coverage level.}},
DOI = {{10.1007/s11538-017-0257-6}},
ISSN = {{0092-8240}},
EISSN = {{1522-9602}},
ResearcherID-Numbers = {{Pinto, Alberto/O-2796-2013
   }},
ORCID-Numbers = {{Pinto, Alberto/0000-0003-2953-6688
   Gouveia Martins, Jose Maria/0000-0002-0556-7861}},
Unique-ID = {{ISI:000398947900007}},
}

@article{ ISI:000396017400002,
Author = {Thiault, Lauric and Kernaleguen, Laetitia and Osenberg, Craig W. and
   Claudet, Joachim},
Title = {{Progressive-Change BACIPS: a flexible approach for environmental impact
   assessment}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2017}},
Volume = {{8}},
Number = {{3}},
Pages = {{288-296}},
Month = {{MAR}},
Abstract = {{The Before-After Control-Impact Paired Series (BACIPS) design
   distinguishes natural spatial and temporal variability from variation
   induced by an environmental impact (or intervention) of interest. BACIPS
   is a powerful tool to derive inferences about interventions when classic
   experimental approaches (e.g. which rely on spatial replicates and
   random assignment of treatments) are not feasible or desirable.
   Previously applied BACIPS designs generally assume that effects are
   sudden, constant and long-lived: that is, that systems exhibit
   step-changes' in response to interventions. However, complex ecological
   interactions or gradual interventions may create delayed and/or
   progressive responses, potentially impeding the reliability of classic
   (step-change) analyses. We propose a novel approach, the
   Progressive-Change BACIPS, which generalizes and expands the scope of
   BACIPS analyses. We evaluate the relative performance of this approach
   using both simulated and real data that exhibit step-change, linear,
   asymptotic and sigmoid responses following an intervention. We quantify
   the statistical power and accuracy of the Progressive-Change BACIPS
   under varying initial population densities, intensity of spatial
   sampling, effect sizes and number of sampling dates After the
   intervention. We show that Progressive-Change BACIPS identified the
   correct model among the set of candidate models under most conditions
   and led to accurate estimates of the parameters that were used to
   generate the simulated data. When data were sparse, and the dynamics
   complex, simpler (more parsimonious) models were favoured over the more
   complex models that actually generated the simulated data. Application
   of the Progressive-Change BACIPS to existing data sets from the
   literature led to strong support for specific models (over alternatives)
   and led to more specific inferences than possible under the classic
   BACIPS approach. The Progressive-Change BACIPS proposed here is more
   flexible than the original BACIPS formulation because the data are used
   to inform the form of the final model, rather than having the form of
   the model imposed on the data. This leads to better estimates of the
   effects of environmental impacts and the time-scales over which they
   operate. As a result, the Progressive-Change BACIPS should be applicable
   to a wide range of studies and should help improve investigation of
   time-dependent effects. R code to perform Progressive-Change BACIPS
   analysis is provided.}},
DOI = {{10.1111/2041-210X.12655}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ResearcherID-Numbers = {{Claudet, Joachim/C-6335-2008
   }},
ORCID-Numbers = {{Claudet, Joachim/0000-0001-6295-1061
   Osenberg, Craig/0000-0003-1918-7904}},
Unique-ID = {{ISI:000396017400002}},
}

@article{ ISI:000395582600006,
Author = {Gigrich, James and Sarkani, Shahryar and Holzer, Thomas},
Title = {{A New Approach in Applying Systems Engineering Tools and Analysis to
   Determine Hepatocyte Toxicogenomics Risk Levels to Human Health}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2017}},
Volume = {{24}},
Number = {{3}},
Pages = {{238-254}},
Month = {{MAR}},
Abstract = {{There is an increasing backlog of potentially toxic compounds that
   cannot be evaluated with current animal-based approaches in a
   cost-effective and expeditious manner, thus putting human health at
   risk. Extrapolation of animal-based test results for human risk
   assessment often leads to different physiological outcomes. This article
   introduces the use of quantitative tools and methods from systems
   engineering to evaluate the risk of toxic compounds by the analysis of
   the amount of stress that human hepatocytes undergo in vitro when
   metabolizing GW7647(1) over extended times and concentrations.
   Hepatocytes are exceedingly connected systems that make it challenging
   to understand the highly varied dimensional genomics data to determine
   risk of exposure. Gene expression data of peroxisome
   proliferator-activated receptor- (PPAR)(2) binding was measured over
   multiple concentrations and varied times of GW7647 exposure and
   leveraging mahalanombis distance to establish toxicity threshold risk
   levels. The application of these novel systems engineering tools
   provides new insight into the intricate workings of human hepatocytes to
   determine risk threshold levels from exposure. This approach is
   beneficial to decision makers and scientists, and it can help reduce the
   backlog of untested chemical compounds due to the high cost and
   inefficiency of animal-based models.}},
DOI = {{10.1089/cmb.2016.0073}},
ISSN = {{1066-5277}},
EISSN = {{1557-8666}},
Unique-ID = {{ISI:000395582600006}},
}

@article{ ISI:000395600200006,
Author = {Rasmussen, Jerod M. and Kruggel, Frithjof and Gilmore, John H. and
   Styner, Martin and Entringer, Sonja and Consing, Kirsten N. Z. and
   Potkin, Steven G. and Wadhwa, Pathik D. and Buss, Claudia},
Title = {{A novel maturation index based on neonatal diffusion tensor imaging
   reflects typical perinatal white matter development in humans}},
Journal = {{INTERNATIONAL JOURNAL OF DEVELOPMENTAL NEUROSCIENCE}},
Year = {{2017}},
Volume = {{56}},
Pages = {{42-51}},
Month = {{FEB}},
Abstract = {{Human birth presents an abrupt transition from intrauterine to
   extrauterine life. Here we introduce a novel Maturation Index (MI) that
   considers the relative importance of gestational age at birth and
   postnatal age at scan in a General Linear Model. The MI is then applied
   to Diffusion Tensor Imaging (DTI) in newborns for characterizing typical
   white matter development in neonates. DTI was performed
   cross-sectionally in 47 neonates (gestational age at birth = 39.1 +/-
   11.6 weeks {[}GA], postnatal age at scan= 25.5 +/- 112.2 days {[}SA]).
   Radial diffusivity (RD), axial diffusivity (AD) and fractional
   anisotropy (FA) along 27 white matter fiber tracts were considered. The
   MI was used to characterize inflection in maturation at the time of
   birth using GLM estimated rates of change before and after birth. It is
   proposed that the sign (positive versus negative) of MI reflects the
   period of greatest maturation rate. Two general patterns emerged from
   the MI analysis. First, RD and AD (but not FA) had positive MI on
   average across the whole brain (average MIAD = 0.31 +/- 0.42, average
   MIRD = 0.22 +/- 10.34). Second, significant regions of negative MI in RD
   and FA (but not AD) were observed in the inferior corticospinal regions,
   areas known to myelinate early. Observations using the proposed method
   are consistent with proposed models of the white matter maturation
   process in which pre-myelination is described by changes in AD and RD
   due to oligodendrocyte proliferation while true myelination is
   characterized by changes in RD and FA due to myelin formation. (C) 2016
   ISDN. Published by Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ijdevneu.2016.12.004}},
ISSN = {{0736-5748}},
EISSN = {{1873-474X}},
Unique-ID = {{ISI:000395600200006}},
}

@article{ ISI:000394156900012,
Author = {Shen Yang and Wang Xin and Lin Han-long and Du Wen-han and Wang
   Bao-guang and Xu Hai-dong},
Title = {{Influence of principal stress rotation of unequal tensile and
   compressive stress amplitudes on characteristics of soft clay}},
Journal = {{JOURNAL OF MOUNTAIN SCIENCE}},
Year = {{2017}},
Volume = {{14}},
Number = {{2}},
Pages = {{369-381}},
Month = {{FEB}},
Abstract = {{Soil behavior can reflect the characteristics of principal stress
   rotation under dynamic wave and traffic loads. Unequal amplitudes of
   tensile and compressive stresses applied to soils have complex effects
   on foundation soils in comparison with the pure principal stress
   rotation path. A series of undrained cyclic hollow torsional shear tests
   were performed on typical remolded soft clay from the Ilexi area of
   Nanjing, China. The main control parameters were the tensile and
   compressive stress amplitude ratio (alpha) and the cyclic dynamic stress
   ratio (eta). It was found that the critical eta tended to remain
   constant at 0.13, when the value of the compressive stress amplitude was
   higher than the tensile stress amplitude. However, the influence of the
   tensile stress was limited by the dynamic stress level when alpha= 1.
   For obvious structural change in the soil, the corresponding numbers of
   cyclic vibration cycles were found to be independent of alpha at low
   stress levels and were only related to eta. Finally, a new method for
   evaluating the failure of remolded soft clay was presented. It considers
   the influence of the tensile and compressive stresses which caused by
   complex paths of the principal stress rotation. This criterion can
   distinguish stable, critical, and destructive states based on the
   pore-water-pressure-strain coupling curve while also providing a range
   of failure strain and vibration cycles. These results provide the
   theoretical support for systematic studies of principal stress rotation
   using constitu tive models.}},
DOI = {{10.1007/s11629-016-4000-9}},
ISSN = {{1672-6316}},
EISSN = {{1993-0321}},
Unique-ID = {{ISI:000394156900012}},
}

@article{ ISI:000395718800008,
Author = {Harper, Angela F. and Leuthaeuser, Janelle B. and Babbitt, Patricia C.
   and Morris, John H. and Ferrin, Thomas E. and Poole, Leslie B. and
   Fetrow, Jacquelyn S.},
Title = {{An Atlas of Peroxiredoxins Created Using an Active Site Profile-Based
   Approach to Functionally Relevant Clustering of Proteins}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2017}},
Volume = {{13}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Peroxiredoxins (Prxs or Prdxs) are a large protein superfamily of
   antioxidant enzymes that rapidly detoxify damaging peroxides and/or
   affect signal transduction and, thus, have roles in proliferation,
   differentiation, and apoptosis. Prx superfamily members are widespread
   across phylogeny and multiple methods have been developed to classify
   them. Here we present an updated atlas of the Prx superfamily identified
   using a novel method called MISST (Multi-level Iterative Sequence
   Searching Technique). MISST is an iterative search process developed to
   be both agglomerative, to add sequences containing similar functional
   site features, and divisive, to split groups when functional site
   features suggest distinct functionally-relevant clusters. Superfamily
   members need not be identified initially-MISST begins with a minimal
   representative set of known structures and searches GenBank iteratively.
   Further, the method's novelty lies in the manner in which isofunctional
   groups are selected; rather than use a single or shifting threshold to
   identify clusters, the groups are deemed isofunctional when they pass a
   self-identification criterion, such that the group identifies itself and
   nothing else in a search of GenBank. The method was preliminarily
   validated on the Prxs, as the Prxs presented challenges of both
   agglomeration and division. For example, previous sequence analysis
   clustered the Prx functional families Prx1 and Prx6 into one group.
   Subsequent expert analysis clearly identified Prx6 as a distinct
   functionally relevant group. The MISST process distinguishes these two
   closely related, though functionally distinct, families. Through MISST
   search iterations, over 38,000 Prx sequences were identified, which the
   method divided into six isofunctional clusters, consistent with previous
   expert analysis. The results represent the most complete computational
   functional analysis of proteins comprising the Prx superfamily. The
   feasibility of this novel method is demonstrated by the Prx superfamily
   results, laying the foundation for potential functionally relevant
   clustering of the universe of protein sequences.}},
DOI = {{10.1371/journal.pcbi.1005284}},
Article-Number = {{e1005284}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
ORCID-Numbers = {{Morris, John "Scooter"/0000-0003-0290-7979
   Poole, Leslie/0000-0003-0334-7328
   Fetrow, Jacquelyn/0000-0002-0528-2049}},
Unique-ID = {{ISI:000395718800008}},
}

@article{ ISI:000397642600002,
Author = {Gomez-Chova, Luis and Amoros-Lopez, Julia and Mateo-Garcia, Gonzalo and
   Munoz-Mari, Jordi and Camps-Valls, Gustau},
Title = {{Cloud masking and removal in remote sensing image time series}},
Journal = {{JOURNAL OF APPLIED REMOTE SENSING}},
Year = {{2017}},
Volume = {{11}},
Month = {{JAN 12}},
Abstract = {{Automatic cloud masking of Earth observation images is one of the first
   required steps in optical remote sensing data processing since the
   operational use and product generation from satellite image time series
   might be hampered by undetected clouds. The high temporal revisit of
   current and forthcoming missions and the scarcity of labeled data force
   us to cast cloud screening as an unsupervised change detection problem
   in the temporal domain. We introduce a cloud screening method based on
   detecting abrupt changes along the time dimension. The main assumption
   is that image time series follow smooth variations over land
   (background) and abrupt changes will be mainly due to the presence of
   clouds. The method estimates the background surface changes using the
   information in the time series. In particular, we propose linear and
   nonlinear least squares regression algorithms that minimize both the
   prediction and the estimation error simultaneously. Then, significant
   differences in the image of interest with respect to the estimated
   background are identified as clouds. The use of kernel methods allows
   the generalization of the algorithm to account for higher-order
   (nonlinear) feature relations. After the proposed cloud masking and
   cloud removal, cloud-free time series at high spatial resolution can be
   used to obtain a better monitoring of land cover dynamics and to
   generate more elaborated products. The method is tested in a dataset
   with 5-day revisit time series from SPOT-4 at high resolution and with
   Landsat-8 time series. Experimental results show that the proposed
   method yields more accurate cloud masks when confronted with
   state-of-the-art approaches typically used in operational settings. In
   addition, the algorithm has been implemented in the Google Earth Engine
   platform, which allows us to access the full Landsat-8 catalog and work
   in a parallel distributed platform to extend its applicability to a
   global planetary scale. (C) 2017 Society of Photo-Optical
   Instrumentation Engineers (SPIE)}},
DOI = {{10.1117/1.JRS.11.015005}},
Article-Number = {{015005}},
ISSN = {{1931-3195}},
ResearcherID-Numbers = {{Gomez-Chova, Luis/M-7367-2015
   }},
ORCID-Numbers = {{Gomez-Chova, Luis/0000-0003-3924-1269
   Camps-Valls, Gustau/0000-0003-1683-2138}},
Unique-ID = {{ISI:000397642600002}},
}

@article{ ISI:000396246700019,
Author = {Shin, Yongdae and Berry, Joel and Pannucci, Nicole and Haataja, Mikko P.
   and Toettcher, Jared E. and Brangwynne, Clifford P.},
Title = {{Spatiotemporal Control of Intracellular Phase Transitions Using
   Light-Activated optoDroplets}},
Journal = {{CELL}},
Year = {{2017}},
Volume = {{168}},
Number = {{1-2}},
Pages = {{159+}},
Month = {{JAN 12}},
Abstract = {{Phase transitions driven by intrinsically disordered protein regions
   (IDRs) have emerged as a ubiquitous mechanism for assembling liquid-like
   RNA/protein (RNP) bodies and other membrane-less organelles. However, a
   lack of tools to control intracellular phase transitions limits our
   ability to understand their role in cell physiology and disease. Here,
   we introduce an optogenetic platform that uses light to activate
   IDR-mediated phase transitions in living cells. We use this
   ``optoDroplet'' system to study condensed phases driven by the IDRs of
   various RNP body proteins, including FUS, DDX4, and HNRNPA1. Above a
   concentration threshold, these constructs undergo light-activated phase
   separation, forming spatiotemporally definable liquid optoDroplets. FUS
   optoDroplet assembly is fully reversible even after multiple activation
   cycles. However, cells driven deep within the phase boundary form
   solid-like gels that undergo aging into irreversible aggregates. This
   system can thus elucidate not only physiological phase transitions but
   also their link to pathological aggregates.}},
DOI = {{10.1016/j.cell.2016.11.054}},
ISSN = {{0092-8674}},
EISSN = {{1097-4172}},
Unique-ID = {{ISI:000396246700019}},
}

@article{ ISI:000403848200019,
Author = {Huynh, Quang C. and Gedamke, Todd and Porch, Clay E. and Hoenig, John M.
   and Walter, John F. and Bryan, Meaghan and Brodziak, Jon},
Title = {{Estimating Total Mortality Rates from Mean Lengths and Catch Rates in
   Nonequilibrium Situations}},
Journal = {{TRANSACTIONS OF THE AMERICAN FISHERIES SOCIETY}},
Year = {{2017}},
Volume = {{146}},
Number = {{4}},
Pages = {{803-815}},
Abstract = {{A series of estimates of the total mortality rate (Z) can be obtained by
   using the Beverton-Holt nonequilibrium-based approach of Gedamke and
   Hoenig (2006) on observations of population mean length over time (ML
   model). In contrast, only relative mortality rates (not absolute values)
   can be obtained from a time series of catch rates. We derived the
   transitional behavior of the catch rate following a change in total
   mortality in the population. From this derivation, we developed a
   new-method to estimate Z that utilizes both mean lengths and catch rates
   (MLCR model). Both theML model and the MLCR model assume constant
   recruitment in the population. We used a simulation study to test
   performance when recruitment is variable. Simulations over various
   scenarios of Z and recruitment variability showed that there may be
   correlated residuals in the mean lengths and catch rates arising from
   fluctuations in recruitment. However, the root mean square errors of the
   Z estimates and the change point (i.e., the year when mortality changed)
   were smaller in the MLCR model than in the ML model, indicating that the
   MLCR model can better account for variable recruitment. Both methods
   were then applied to Mutton Snapper Lutjanus analis in Puerto Rico to
   illustrate their potential application to assess data-limited stocks.
   The ML model estimated an increase in Z, but the MLCR model also
   estimated a subsequent reduction in Z when the catch rate data were
   considered.}},
DOI = {{10.1080/00028487.2017.1308881}},
ISSN = {{0002-8487}},
EISSN = {{1548-8659}},
Unique-ID = {{ISI:000403848200019}},
}

@article{ ISI:000402927800001,
Author = {Chen, Yu and Chen, Dong and Zou, Xiufen},
Title = {{Inference of Biochemical S-Systems via Mixed-Variable Multiobjective
   Evolutionary Optimization}},
Journal = {{COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE}},
Year = {{2017}},
Abstract = {{Inference of the biochemical systems (BSs) via experimental data is
   important for understanding how biochemical components in vivo interact
   with each other. However, it is not a trivial task because BSs usually
   function with complex and nonlinear dynamics. As a popular ordinary
   equation (ODE) model, the S-System describes the dynamical properties of
   BSs by incorporating the power rule of biochemical reactions but behaves
   as a challenge because it has a lot of parameters to be confirmed. This
   work is dedicated to proposing a general method for inference of
   S-Systems by experimental data, using a biobjective optimization (BOO)
   model and a specially mixed-variable multiobjective evolutionary
   algorithm (mv-MOEA). Regarding that BSs are sparse in common sense, we
   introduce binary variables indicating network connections to eliminate
   the difficulty of threshold presetting and take data fitting error and
   the L-0-norm as two objectives to be minimized in the BOO model. Then, a
   selection procedure that automatically runs tradeoff between two
   objectives is employed to choose final inference results from the
   obtained nondominated solutions of the mv-MOEA. Inference results of the
   investigated networks demonstrate that our method can identify their
   dynamical properties well, although the automatic selection procedure
   sometimes ignores some weak connections in BSs.}},
DOI = {{10.1155/2017/3020326}},
Article-Number = {{3020326}},
ISSN = {{1748-670X}},
EISSN = {{1748-6718}},
ResearcherID-Numbers = {{Chen(), Yu()/N-5675-2016
   }},
ORCID-Numbers = {{Chen(), Yu()/0000-0002-8118-7262
   zou, xiufen/0000-0001-5294-0764}},
Unique-ID = {{ISI:000402927800001}},
}

@article{ ISI:000402431300005,
Author = {Moschini, P. and Bisanzio, D. and Pugliese, A.},
Title = {{A Seasonal Model for West Nile Virus}},
Journal = {{MATHEMATICAL MODELLING OF NATURAL PHENOMENA}},
Year = {{2017}},
Volume = {{12}},
Number = {{2, SI}},
Pages = {{58-83}},
Abstract = {{West Nile virus (WNV) is maintained in transmission cycles involving
   bird reservoir hosts and mosquito vectors. While several aspects of the
   infection cycle have been explored through mathematical models,
   relatively little attention has been paid to the theoretical effect of
   seasonal changes in host and vector densities. Here we consider a model
   for the transmission dynamics of WNV in a temperate climate, where
   mosquitoes are not active during winters, so that infection dynamics can
   be described through a sequence of discrete growing seasons.
   Within-season host and vector demography is described through
   phenomenological functions of time describing fertility, mortality and
   migration. Over-wintering of infection is assumed to occur through
   diapausing mosquito females, with or without vertical transmission.
   We introduce a parameter S-0 that, similarly to R-0 but easier to
   compute, yields a threshold condition for infection persistence in this
   semi-discrete setting. Then we study the possible dynamical behavior of
   the model, by exploring parameter values through a Latin Hypercube
   Sampling and accepting only those values yielding solutions respecting a
   few conditions obtained from the qualitative patterns observed in yearly
   patterns of mosquito abundance and virus prevalence.
   For some parameters the posterior distribution is rather narrow,
   implying that simple qualitative agreement with data can yield
   information on parameter difficult to estimate directly. For other
   parameters, the posterior distribution is instead similar to the prior.
   Simulations of multi-year dynamics after a first introduction of the
   virus always asymptotically result, if S-0 > 1, in a pattern of yearly
   identical infections; however, their amplitude may be very different,
   even for the same value of S-0, in correspondence to the uncertainties
   about several parameters.}},
DOI = {{10.1051/mmnp/20171220}},
ISSN = {{0973-5348}},
EISSN = {{1760-6101}},
ResearcherID-Numbers = {{Pugliese, Andrea/E-1905-2011}},
ORCID-Numbers = {{Pugliese, Andrea/0000-0002-3512-8560}},
Unique-ID = {{ISI:000402431300005}},
}

@article{ ISI:000400888400003,
Author = {Veghari, Gholamreza and Sedaghat, Mehdi and Banihashem, Samieh and
   Moharloei, Pooneh and Angizeh, Abdolhamid and Tazik, Ebrahim and
   Moghaddami, Abbas and Kordy, Khadijeh and Honarvar, Mohammadreza},
Title = {{Age Cut-off Point for the Diagnosis of Metabolic Syndrome in Northern
   Adult Iranians: A New Approach in Prevention}},
Journal = {{AMBIENT SCIENCE}},
Year = {{2017}},
Volume = {{4}},
Number = {{1}},
Abstract = {{The Metabolic syndrome (MetS) is one of the main risk factor for
   cardiovascular disease and the aim of this study was to compare the
   discriminative capacity of age in a prediction of non-adipose components
   and to determine its relevant optimal cut-off in Turkman and non-Turkman
   residents of the northern part of Iran. This study has been conducted on
   the 248 subjects of 25-70 years age. ATP-III method was used for
   diagnosis of MetS. The optimal cut-off, the corresponding sensitivity
   and specificity for age have estimated in the threshold that maximizes
   the sum of sensitivity and specificity or equivalently maximizes in ROC
   curve operating points. Finally, the prevalence of MetS was evidenced in
   37.6\% of subjects and significantly was 14.7\% more in non-Turkman than
   in Turkman group. Compared with Turkman group, the means of fasting
   plasma glucose, Triglycerides, WC and diastolic blood pressure (DBP)
   were statistical significantly more in non-Turkman group, respectively.
   Age cut-off values for predicting of MetS was 33.5 years in Turkman,
   50.5 years in non-Turkman and 42.5 years in total of subjects. The AUCs
   (Area Under Curve) ranged from 0.546 in Turkman group to 0.726 for the
   non-Turkman group.}},
DOI = {{10.21276/ambi.2017.04.sp1.ra02}},
ISSN = {{2348-5191}},
EISSN = {{2348-8980}},
Unique-ID = {{ISI:000400888400003}},
}

@article{ ISI:000394557800017,
Author = {Silva, Maristella Borges and Silva, Andrei Nakagawa and Martins Naves,
   Eduardo Lazaro and Palomari, Evanisi Teresa and Soares, Alcimar Barbosa},
Title = {{An improved approach for measuring the tonic stretch reflex response of
   spastic muscles}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2017}},
Volume = {{80}},
Pages = {{166-174}},
Month = {{JAN 1}},
Abstract = {{We propose a new method for detecting the onset of the stretch reflex
   response for assessment of spasticity based on the Tonic Stretch Reflex
   Threshold (TSRT). Our strategy relies on a three-stage approach to
   detect the onset of the reflex EMG activity: (i) Reduction of baseline
   activity by means of Empirical Mode Decomposition; (ii) Extraction of
   the complex envelope of the EMG signal by means of Hilbert Transform
   (HT) and; iii) A double threshold decision rule. Simulated and real EMG
   data were used to evahiate and compare our method (TSRT-EHD) against
   three other popular methods described in the literature to assess TSRT
   `Ferreira' and. `Blanchette'). Four different groups of signals
   containing simulated evoked stretch reflex EMG activities were
   generated: groups A and B without spontaneous EMG activity at rest and
   signal-to-noise ratio (SNR) of 10 dB and 20 dB respectively; groups C
   and D with spontaneous EMG activity at rest, as observed frequently in
   spastic muscles, and SNR of 10 dB and 20 dB respectively. The results
   with simulated data showed a significantly higher accuracy of TSRT-EHD
   for detecting the onset of the reflex EMG activity in groups C and D
   when compared to the other methods. Analyses using real data from five
   post stroke spastic subjects demonstrated that the TSRTs generated by
   each method were dramatically different from one another. Nevertheless,
   only TSRT-EHD provided valid measures across all subjects.}},
DOI = {{10.1016/j.compbiomed.2016.12.001}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{Soares, Alcimar/G-3269-2012}},
ORCID-Numbers = {{Soares, Alcimar/0000-0003-1100-3533}},
Unique-ID = {{ISI:000394557800017}},
}

@article{ ISI:000394670600012,
Author = {Yin, Deyi and Leroux, Shawn J. and He, Fangliang},
Title = {{Methods and models for identifying thresholds of habitat loss}},
Journal = {{ECOGRAPHY}},
Year = {{2017}},
Volume = {{40}},
Number = {{1}},
Pages = {{131-143}},
Month = {{JAN}},
Abstract = {{There is mounting evidence that many taxa respond in non-linear ways to
   perturbation (i.e. deviations from a natural trajectory brought on by an
   external agent), and many statistical, physical and ecological methods
   have been developed to detect the critical points or thresholds of
   perturbation. The majority of these methods define thresholds as the
   perturbation points causing abrupt ecological response, but in reality
   most species or ecosystems do not show a break point response but more
   gradual transitional change to perturbation. We develop a new method
   which delineates thresholds as a region in which the slope of the
   relationship between ecological response (y) and perturbation (x; e.g.
   habitat loss) is larger than 1: vertical bar|dy/dx vertical bar >= 1,
   where both x-and y-axes are scaled to (0, 1) range. The lower end of
   threshold zones so defined is of particular ecological interest because
   it is the smallest x that may trigger impending catastrophic response to
   a small change in x. We derived two landscape models (edge length and
   the number of patches of species distribution) and two biodiversity
   models (endemics-area relationship and half-population curve) to test
   this method. We applied our zonal thresholding method to these four
   models fit to empirical data of two forest plots to detect thresholds of
   species distribution to habitat loss. The two landscape metric models
   predict that no species could tolerate more than 40\% of habitat loss
   and these thresholds can be much lower for relatively rare species with
   occupancy <0.4 and for aggregated habitat loss compared to random
   habitat loss. The half-population model leads to a similar threshold
   level of 40\% habitat loss. Overall, we suggest the maximum permissible
   habitat loss threshold to be between 0-40\%, depending on the
   pre-disturbed occupancy (or abundance) of a species. This habitat loss
   threshold falls within the otherwise wide range of thresholds calculated
   from conventional methods. Our study contributes novel methods and
   models to quantify the effect of habitat loss on species distribution
   and diversity in landscapes with potential for conservation
   applications.}},
DOI = {{10.1111/ecog.02557}},
ISSN = {{0906-7590}},
EISSN = {{1600-0587}},
Unique-ID = {{ISI:000394670600012}},
}

@article{ ISI:000395756300001,
Author = {Bhutiani, N. and Kimbrough, C. W. and Burton, N. C. and Morscher, S. and
   Egger, M. and McMasters, K. and Woloszynska-Read, A. and El-baz, A. and
   McNally, L. R.},
Title = {{Detection of microspheres in vivo using multispectral optoacoustic
   tomography}},
Journal = {{BIOTECHNIC \& HISTOCHEMISTRY}},
Year = {{2017}},
Volume = {{92}},
Number = {{1}},
Pages = {{1-6}},
Abstract = {{We introduce a new approach to detect individual microparticles that
   contain NIR fluorescent dye by multispectral optoacoustic tomography in
   the context of the hemoglobin-rich environment within murine liver. We
   encapsulated a near infrared (NIR) fluorescent dye within polystyrene
   microspheres, then injected them into the ileocolic vein, which drains
   to the liver. NIR absorption was determined using multispectral
   optoacoustic tomography. To quantitate the minimum diameter of
   microspheres, we used both colorimetric and spatial information to
   segment the regions in which the microspheres appear. Regional diameter
   was estimated by doubling the maximum regional distance. We found that
   the minimum microsphere size threshold for detection by multispectral
   optoacoustic tomography images is 78.9 mu m.}},
DOI = {{10.1080/10520295.2016.1251611}},
ISSN = {{1052-0295}},
EISSN = {{1473-7760}},
Unique-ID = {{ISI:000395756300001}},
}

@article{ ISI:000395634300013,
Author = {Wagenhoff, Annika and Clapcott, Joanne E. and Lau, Kelvin E. M. and
   Lewis, Gillian D. and Young, Roger G.},
Title = {{Identifying congruence in stream assemblage thresholds in response to
   nutrient and sediment gradients for limit setting}},
Journal = {{ECOLOGICAL APPLICATIONS}},
Year = {{2017}},
Volume = {{27}},
Number = {{2}},
Pages = {{469-484}},
Abstract = {{The setting of numeric instream objectives (effects-based criteria) and
   catchment limits for major agricultural stressors, such as nutrients and
   fine sediment, is a promising policy instrument to prevent or reduce
   degradation of stream ecosystem health. We explored the suitability of
   assemblage thresholds, defined as a point at which a small increase in a
   stressor will result in a disproportionally large change in assemblage
   structure relative to other points across the stressor gradient, to
   inform instream nutrient and sediment objectives. Identification and
   comparison of thresholds for macroinvertebrate, periphyton, and
   bacterial assemblages aimed at making the setting of objectives more
   robust and may further provide a better understanding of the underlying
   mechanisms of nutrient and fine sediment effects. Gradient forest, a
   novel approach to assemblage threshold identification based on
   regression-tree-based random forest models for individual taxa, allowed
   inclusion of multiple predictors to strengthen the evidence of cause and
   effect between stressors and multispecies responses. The most prominent
   macroinvertebrate and periphyton assemblage threshold across the
   nitrogen (N) gradient was located at very low levels and mainly
   attributed to declines of multiple taxa. This provided strong evidence
   for stream assemblages being significantly affected when N
   concentrations exceed reference conditions and for effects cascading
   through the ecosystem. The most prominent macroinvertebrate assemblage
   threshold across a gradient of suspended fine sediment was also located
   at very low levels and attributed to declines of multiple taxa. However,
   this threshold did not correspond with periphyton assemblage thresholds,
   suggesting that the sensitivity of macroinvertebrate assemblages is
   unrelated to sediment effects on periphyton assemblages. Overall, the
   spectrum of N concentrations and fine sediment levels within which these
   stream assemblages changed most dramatically were relatively narrow
   given the wide gradients tested. We conclude that assemblage thresholds
   can inform the setting of generic instream nutrient and sediment
   objectives for stream ecosystem health. For example, the most stringent
   objective for instream N concentration should be set at values similar
   to reference concentrations for full protection of sensitive taxa or
   overall stream biodiversity. To avoid severe degradation of stream
   biodiversity, the least stringent N objective should stay well below the
   point where significant turnover subsided.}},
DOI = {{10.1002/eap.1457}},
ISSN = {{1051-0761}},
EISSN = {{1939-5582}},
ResearcherID-Numbers = {{Young, Roger/B-6534-2008}},
ORCID-Numbers = {{Young, Roger/0000-0002-0999-4887}},
Unique-ID = {{ISI:000395634300013}},
}

@article{ ISI:000394144400042,
Author = {Liang, Hualou and Wang, Hongbin},
Title = {{Structure-Function Network Mapping and Its Assessment via Persistent
   Homology}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2017}},
Volume = {{13}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Understanding the relationship between brain structure and function is a
   fundamental problem in network neuroscience. This work deals with the
   general method of structure-function mapping at the whole-brain level.
   We formulate the problem as a topological mapping of structure-function
   connectivity via matrix function, and find a stable solution by
   exploiting a regularization procedure to cope with large matrices. We
   introduce a novel measure of network similarity based on persistent
   homology for assessing the quality of the network mapping, which enables
   a detailed comparison of network topological changes across all possible
   thresholds, rather than just at a single, arbitrary threshold that may
   not be optimal. We demonstrate that our approach can uncover the direct
   and indirect structural paths for predicting functional connectivity,
   and our network similarity measure outperforms other currently available
   methods. We systematically validate our approach with (1) a comparison
   of regularized vs. non-regularized procedures, (2) a null model of the
   degree-preserving random rewired structural matrix, (3) different
   network types (binary vs. weighted matrices), and (4) different brain
   parcellation schemes (low vs. high resolutions). Finally, we evaluate
   the scalability of our method with relatively large matrices (2514x2514)
   of structural and functional connectivity obtained from 12 healthy human
   subjects measured non-invasively while at rest. Our results reveal a
   nonlinear structure-function relationship, suggesting that the
   resting-state functional connectivity depends on direct structural
   connections, as well as relatively parsimonious indirect connections via
   polysynaptic pathways.}},
DOI = {{10.1371/journal.pcbi.1005325}},
Article-Number = {{e1005325}},
ISSN = {{1553-7358}},
Unique-ID = {{ISI:000394144400042}},
}

@article{ ISI:000393833400011,
Author = {McGrath, E. O. and Neumann, N. N. and Nichol, C. F.},
Title = {{A Statistical Model for Managing Water Temperature in Streams with
   Anthropogenic Influences}},
Journal = {{RIVER RESEARCH AND APPLICATIONS}},
Year = {{2017}},
Volume = {{33}},
Number = {{1}},
Pages = {{123-134}},
Month = {{JAN}},
Abstract = {{Streams in the Pacific Northwest (Oregon, Washington, British Columbia)
   face rising summer temperatures and increasing anthropogenic influence,
   with consequences for fish populations. Guidance is needed in small
   managed watersheds for setting reservoir release rates or for the
   restriction of water extractions to meet the needs of fish and aquatic
   ecosystems. Existing environmental flow methods focus on discharge rates
   and do not typically consider water temperatures, and detailed thermal
   models are too complex for widespread implementation. We used multiple
   logistic regression to develop statistical models for estimating the
   probability of exceeding a salmonid stream temperature threshold of 22
   degrees C as a function of discharge and maximum daily air temperatures.
   Data required are air temperature, stream temperature and stream
   discharge over a minimum of one summer. The models are used to make
   minimum discharge recommendations under varying forecast weather
   conditions. The method was applied to nine streams in the Pacific
   Northwest. Minimum recommended discharge generally ranged from 23\% to
   86\% of mean annual discharge and was higher than observed low flows in
   most streams. Comparison of the new method to existing methods for
   Fortune Creek in British Columbia indicated that total season discharge
   volumes could be reduced while meeting thermal requirements. For other
   streams, it was evident that high water temperatures cannot be managed
   by increasing discharge, as the discharge required would be greater than
   natural discharge and higher than achievable by management. The
   statistical method described in this paper allows for a risk-based
   approach to discharge management for fish habitat needs.}},
DOI = {{10.1002/rra.3057}},
ISSN = {{1535-1459}},
EISSN = {{1535-1467}},
ORCID-Numbers = {{Nichol, Craig/0000-0002-4685-2116}},
Unique-ID = {{ISI:000393833400011}},
}

@article{ ISI:000392631200015,
Author = {Gaieb, Zied and Morikis, Dimitrios},
Title = {{Detection of Side Chain Rearrangements Mediating the Motions of
   Transmembrane Helices in Molecular Dynamics Simulations of G
   Protein-Coupled Receptors}},
Journal = {{COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL}},
Year = {{2017}},
Volume = {{15}},
Pages = {{131-137}},
Abstract = {{Structure and dynamics are essential elements of protein function.
   Protein structure is constantly fluctuating and undergoing
   conformational changes, which are captured by molecular dynamics (MD)
   simulations. We introduce a computational framework that provides a
   compact representation of the dynamic conformational space of
   biomolecular simulations. This method presents a systematic approach
   designed to reduce the large MD simulation spatiotemporal datasets into
   a manageable set in order to guide our understanding of how protein
   mechanics emerge from side chain organization and dynamic
   reorganization. We focus on the detection of side chain interactions
   that undergo rearrangements mediating global domain motions and vice
   versa. Side chain rearrangements are extracted from side chain
   interactions that undergo well-defined abrupt and persistent changes in
   distance time series using Gaussian mixture models, whereas global
   domain motions are detected using dynamic cross-correlation. Both side
   chain rearrangements and global domain motions represent the dynamic
   components of the protein MD simulation, and are both mapped into a
   network where they are connected based on their degree of coupling. This
   method allows for the study of allosteric communication in proteins by
   mapping out the protein dynamics into an intramolecular network to
   reduce the large simulation data into a manageable set of communities
   composed of coupled side chain rearrangements and global domain motions.
   This computational framework is suitable for the study of tightly packed
   proteins, such as G protein-coupled receptors, and we present an
   application on a seven microseconds MD trajectory of CC chemokine
   receptor 7 (CCR7) bound to its ligand CCL21. (C) 2017 The Authors.
   Published by Elsevier B.V.}},
DOI = {{10.1016/j.csbj.2017.01.001}},
ISSN = {{2001-0370}},
ResearcherID-Numbers = {{Morikis, Dimitrios/L-8527-2013}},
ORCID-Numbers = {{Morikis, Dimitrios/0000-0003-0083-4665}},
Unique-ID = {{ISI:000392631200015}},
}

@article{ ISI:000392578200056,
Author = {Nakata, Hokuto and Nakayama, Shouta M. M. and Oroszlany, Balazs and
   Ikenaka, Yoshinori and Mizukawa, Hazuki and Tanaka, Kazuyuki and
   Harunari, Tsunehito and Tanikawa, Tsutomu and Darwish, Wageh Sobhy and
   Yohannes, Yared B. and Saengtienchai, Aksorn and Ishizuka, Mayumi},
Title = {{Monitoring Lead (Pb) Pollution and Identifying Pb Pollution Sources in
   Japan Using Stable Pb Isotope Analysis with Kidneys of Wild Rats}},
Journal = {{INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH}},
Year = {{2017}},
Volume = {{14}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Although Japan has been considered to have little lead (Pb) pollution in
   modern times, the actual pollution situation is unclear. The present
   study aims to investigate the extent of Pb pollution and to identify the
   pollution sources in Japan using stable Pb isotope analysis with kidneys
   of wild rats. Wild brown (Rattus norvegicus, n = 43) and black (R.
   rattus, n = 98) rats were trapped from various sites in Japan. Mean Pb
   concentrations in the kidneys of rats from Okinawa (15.58 mg/kg, dry
   weight), Aichi (10.83), Niigata (10.62), Fukuoka (8.09), Ibaraki (5.06),
   Kyoto (4.58), Osaka (4.57), Kanagawa (3.42), and Tokyo (3.40) were above
   the threshold (2.50) for histological kidney changes. Similarly,
   compared with the previous report, it was regarded that even structural
   and functional kidney damage as well as neurotoxicity have spread among
   rats in Japan. Additionally, the possibility of human exposure to a high
   level of Pb was assumed. In regard to stable Pb isotope analysis,
   distinctive values of stable Pb isotope ratios (Pb-IRs) were detected in
   some kidney samples with Pb levels above 5.0 mg/kg. This result
   indicated that composite factors are involved in Pb pollution. However,
   the identification of a concrete pollution source has not been
   accomplished due to limited differences among previously reported values
   of Pb isotope composition in circulating Pb products. Namely, the
   current study established the limit of Pb isotope analysis for source
   identification. Further detailed research about monitoring Pb pollution
   in Japan and the demonstration of a novel method to identify Pb sources
   are needed.}},
DOI = {{10.3390/ijerph14010056}},
Article-Number = {{56}},
ISSN = {{1660-4601}},
ORCID-Numbers = {{Darwish, Wageh/0000-0002-4399-1401}},
Unique-ID = {{ISI:000392578200056}},
}

@article{ ISI:000390454300030,
Author = {Milligan, Gregg and Scott, Richard and Young, Damian and Connor, Leslie
   and Blackbird, Sabena and Marrs, Rob},
Title = {{Reducing soil fertility to enable ecological restoration: A new method
   to test the efficacy of Full-Inversion Tillage}},
Journal = {{ECOLOGICAL ENGINEERING}},
Year = {{2017}},
Volume = {{98}},
Pages = {{257-263}},
Month = {{JAN}},
Abstract = {{Ecological restoration of high-quality, semi-natural communities of
   conservation value on ex-arable soils with raised fertility has been a
   major problem in northern Europe. One suggested way of tackling this
   problem is to use Full-Inversion Tillage (FIT) where the fertile
   top-soil is moved below infertile subsurface layers. This should provide
   infertile conditions at least in the short- to medium-term for the
   establishment of communities that require these conditions. Here, we
   pioneer a rigorous test of the efficacy of this approach. Our new
   method, using Principal Response Curves (PRCs), overcomes the difficulty
   in testing for change in soil physico-chemical properties down soil
   profiles with the inherent problem of autocorrelations between soil
   layers. Principal Response Curves (PRCs) is a multivariate technique,
   usually used to test for effects of treatment effects through time on a
   community matrix. We propose an extension of their use for the
   multivariate analysis of properties down a soil-depth profile. We tested
   the effects of FIT in two contrasting soils, before and after treatment.
   In a clay loam soil, FIT was effective in reducing the soil fertility in
   the surface 12 cm. Indeed the soil available P concentration, a key
   variable, was more than halved in the surface layers, and was below the
   lowest literature target threshold for the establishment of semi-natural
   grassland. In contrast, in the sandy soil, soil properties increased
   throughout the profile after FIT, primarily because of the pre-treatment
   nutrient distribution within the soil. Before FIT treatment, the maximum
   concentrations of most measured variables were at mid-depth and FIT
   redistributed these to the surface and bottom layers. Our results
   demonstrate the potential for FIT in ecological restoration, but
   indicate that its efficacy depends on soil type and the site history. We
   recommend that in future a pre-treatment assessment of soil properties
   with depth is undertaken before FIT is implemented, and that afterwards
   our PRC approach can be used to test efficacy immediately after
   treatment, and has the potential for measuring soil resilience to
   perturbation through time. (C) 2016 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecoleng.2016.11.003}},
ISSN = {{0925-8574}},
EISSN = {{1872-6992}},
ORCID-Numbers = {{Milligan, Gregg/0000-0001-9398-4026
   Blackbird, Sabena/0000-0003-0942-6836}},
Unique-ID = {{ISI:000390454300030}},
}

@article{ ISI:000390181600022,
Author = {Xiao, Xi and He, Junyu and Huang, Haomin and Miller, Todd R. and
   Christakos, George and Reichwaldt, Elke S. and Ghadouani, Anas and Lin,
   Shengpan and Xu, Xinhua and Shi, Jiyan},
Title = {{A novel single-parameter approach for forecasting algal blooms}},
Journal = {{WATER RESEARCH}},
Year = {{2017}},
Volume = {{108}},
Pages = {{222-231}},
Month = {{JAN 1}},
Abstract = {{Harmful algal blooms frequently occur globally, and forecasting could
   constitute an essential proactive strategy for bloom control. To
   decrease the cost of aquatic environmental monitoring and increase the
   accuracy of bloom forecasting, a novel single-parameter approach
   combining wavelet analysis with artificial neural networks (WNN) was
   developed and verified based on daily online monitoring datasets of
   algal density in the Siling Reservoir, China and Lake Winnebago, U.S.A.
   Firstly, a detailed modeling process was illustrated using the
   forecasting of cyanobacterial cell density in the Chinese reservoir as
   an example. Three WNN models occupying various prediction time intervals
   were optimized through model training using an early stopped training
   approach. All models performed well in fitting historical data and
   predicting the dynamics of cyanobacterial cell density, with the best
   model predicting cyanobacteria density one-day ahead (r = 0.986 and mean
   absolute error = 0.103 x 10(4) cells mL(-1)). Secondly, the potential of
   this novel approach was further confirmed by the precise predictions of
   algal biomass dynamics measured as chl a in both study sites,
   demonstrating its high performance in forecasting algal blooms,
   including cyanobacteria as well as other blooming species. Thirdly, the
   WNN model was compared to current algal forecasting methods (i.e.
   artificial neural networks, autoregressive integrated moving average
   model), and was found to be more accurate. In addition, the application
   of this novel single-parameter approach is cost effective as it requires
   only a buoy-mounted fluorescent probe, which is merely a fraction
   (-15\%) of the cost of a typical auto-monitoring system. As such, the
   newly developed approach presents a promising and cost-effective tool
   for the future prediction and management of harmful algal blooms. (C)
   2016 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.watres.2016.10.076}},
ISSN = {{0043-1354}},
ResearcherID-Numbers = {{xiao, xi/K-1027-2014
   }},
ORCID-Numbers = {{xiao, xi/0000-0002-9753-6586
   Ghadouani, Anas/0000-0002-1252-4851
   He, Junyu/0000-0003-1873-3125
   Miller, Todd/0000-0002-2113-1662}},
Unique-ID = {{ISI:000390181600022}},
}

@article{ ISI:000389958700001,
Author = {Bulashevska, Svetlana and Priest, Colin and Speicher, Daniel and
   Zimmermann, Joerg and Westermann, Frank and Cremers, Armin B.},
Title = {{SwitchFinder - a novel method and query facility for discovering dynamic
   gene expression patterns}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2016}},
Volume = {{17}},
Month = {{DEC 15}},
Abstract = {{Background: Biological systems and processes are highly dynamic. To gain
   insights into their functioning time-resolved measurements are
   necessary. Time-resolved gene expression data captures temporal
   behaviour of the genes genome-wide under various biological conditions:
   in response to stimuli, during cell cycle, differentiation or
   developmental programs. Dissecting dynamic gene expression patterns from
   this data may shed light on the functioning of the gene regulatory
   system. The present approach facilitates this discovery. The fundamental
   idea behind it is the following: there are change-points (switches) in
   the gene behaviour separating intervals of increasing and decreasing
   activity, whereas the intervals may have different durations.
   Elucidating the switch-points is important for the identification of
   biologically meanigfull features and patterns of the gene dynamics.
   Results: We developed a statistical method, called SwitchFinder, for the
   analysis of time-series data, in particular gene expression data, based
   on a change-point model. Fitting the model to the gene expression
   time-courses indicates switch-points between increasing and decreasing
   activities of each gene. Two types of the model - based on linear and on
   generalized logistic function -were used to capture the data between the
   switch-points. Model inference was facilitated with the Bayesian
   methodology using Markov chain Monte Carlo (MCMC) technique Gibbs
   sampling. Further on, we introduced features of the switch-points:
   growth, decay, spike and cleft, which reflect important dynamic aspects.
   With this, the gene expression profiles are represented in a qualitative
   manner - as sets of the dynamic features at their onset-times. We
   developed a Web application of the approach, enabling to put queries to
   the gene expression time-courses and to deduce groups of genes with
   common dynamic patterns.
   SwitchFinder was applied to our original data -the gene expression
   time-series measured in neuroblastoma cell line upon treatment with
   all-trans retinoic acid (ATRA). The analysis revealed eight patterns of
   the gene expression responses to ATRA, indicating the induction of the
   BMP, WNT, Notch, FGF and NTRK-receptor signaling pathways involved in
   cell differentiation, as well as the repression of the cell-cycle
   related genes.
   Conclusions: SwitchFinder is a novel approach to the analysis of
   biological time-series data, supporting inference and interactive
   exploration of its inherent dynamic patterns, hence facilitating
   biological discovery process. SwitchFinder is freely available at
   https://newbioinformatics.eu/switchfinder.}},
DOI = {{10.1186/s12859-016-1391-0}},
Article-Number = {{532}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000389958700001}},
}

@article{ ISI:000391932100018,
Author = {Wang, Fei and Wang, Lu and Song, Peter X. -K.},
Title = {{Fused Lasso with the Adaptation of Parameter Ordering in Combining
   Multiple Studies with Repeated Measurements}},
Journal = {{BIOMETRICS}},
Year = {{2016}},
Volume = {{72}},
Number = {{4}},
Pages = {{1184-1193}},
Month = {{DEC}},
Abstract = {{Combining multiple studies is frequently undertaken in biomedical
   research to increase sample sizes for statistical power improvement. We
   consider the marginal model for the regression analysis of repeated
   measurements collected in several similar studies with potentially
   different variances and correlation structures. It is of great
   importance to examine whether there exist common parameters across
   study-specific marginal models so that simpler models, sensible
   interpretations, and meaningful efficiency gain can be obtained.
   Combining multiple studies via the classical means of hypothesis testing
   involves a large number of simultaneous tests for all possible subsets
   of common regression parameters, in which it results in unduly large
   degrees of freedom and low statistical power. We develop a new method of
   fused lasso with the adaptation of parameter ordering (FLAPO) to
   scrutinize only adjacent-pair parameter differences, leading to a
   substantial reduction for the number of involved constraints. Our method
   enjoys the oracle properties as does the full fused lasso based on all
   pairwise parameter differences. We show that FLAPO gives estimators with
   smaller error bounds and better finite sample performance than the full
   fused lasso. We also establish a regularized inference procedure based
   on bias-corrected FLAPO. We illustrate our method through both
   simulation studies and an analysis of HIV surveillance data collected
   over five geographic regions in China, in which the presence or absence
   of common covariate effects is reflective to relative effectiveness of
   regional policies on HIV control and prevention.}},
DOI = {{10.1111/biom.12496}},
ISSN = {{0006-341X}},
EISSN = {{1541-0420}},
ResearcherID-Numbers = {{Wang, Fei/J-7651-2017}},
Unique-ID = {{ISI:000391932100018}},
}

@article{ ISI:000391460900003,
Author = {Hatmal, Ma'mon M. and Jaber, Shadi and Taha, Mutasem O.},
Title = {{Combining molecular dynamics simulation and ligand-receptor contacts
   analysis as a new approach for pharmacophore modeling: beta-secretase 1
   and check point kinase 1 as case studies}},
Journal = {{JOURNAL OF COMPUTER-AIDED MOLECULAR DESIGN}},
Year = {{2016}},
Volume = {{30}},
Number = {{12}},
Pages = {{1149-1163}},
Month = {{DEC}},
Abstract = {{Ligand-based pharmacophore modeling require relatively long lists of
   active compounds, while a pharmacophore based on a single
   ligand-receptor crystallographic structure is often promiscuous. These
   problems prompted us to combine molecular dynamics (MD) simulation with
   ligand-receptor contacts analysis as means to develop valid
   pharmacophore model(s). The particular ligand-receptor complex is
   allowed to perturb over a few nano-seconds using MD simulation.
   Subsequently, ligand-receptor contact points (<= 2.5 angstrom) are
   identified. Ligand-receptor contacts maintained above certain threshold
   during molecular dynamics simulation are considered critical and used to
   guide pharmacophore development. We termed this method as
   Molecular-Dynamics Based Ligand-Receptor Contact Analysis. We
   implemented this new methodology to develop valid pharmacophore models
   for check point kinase 1 (Chk1) and beta-secretase 1 (BACE1) inhibitors
   as case studies. The resulting pharmacophore models were validated by
   receiver operating characteristic curved analysis against inhibitors
   obtained from CHEMBL database.}},
DOI = {{10.1007/s10822-016-9984-2}},
ISSN = {{0920-654X}},
EISSN = {{1573-4951}},
ORCID-Numbers = {{TAHA, MUTASEM/0000-0002-4453-072X}},
Unique-ID = {{ISI:000391460900003}},
}

@article{ ISI:000390008700004,
Author = {Voyles, Angela C. and Kiorpes, Lynne},
Title = {{A Window into Brain Development: hdEEG Methods to Track Visual
   Development in Nonhuman Primates}},
Journal = {{DEVELOPMENTAL NEUROBIOLOGY}},
Year = {{2016}},
Volume = {{76}},
Number = {{12}},
Pages = {{1342-1359}},
Month = {{DEC}},
Abstract = {{Electroencephalography (EEG) is widely used to study human brain
   activity, and is a useful tool for bridging the gap between invasive
   neural recording assays and behavioral data. High-density EEG (hdEEG)
   methods currently used for human subjects for use with infant macaque
   monkeys, a species that exhibits similar visual development to humans
   over a shorter time course was adapted. Unlike monkeys, human subjects
   were difficult to study longitudinally and were not appropriate for
   direct within-species comparison to neuronal data. About 27-channel
   electrode caps, which allowed collection of hdEEG data from infant
   monkeys across development were designed. Acuity and contrast sweep VEP
   responses to grating stimuli was obtained and a new method for objective
   threshold estimation based on response signal-to-noise ratios at
   different stimulus levels was established. The developmental
   trajectories of VEP-measured contrast sensitivity and acuity to
   previously collected behavioral and neuronal data were compared. The VEP
   measures showed similar rates of development to behavioral measures,
   both of which were slower than direct neuronal measures; VEP thresholds
   were higher than other measures. This is the first usage of non-invasive
   technology in non-human primates. Other means to assess neural
   sensitivity in infants were all invasive. Use of hdEEG with infant
   monkeys opens many possibilities for tracking development of vision and
   other functions in non-human primates, and can expand our understanding
   of the relationship between neuronal activity and behavioral
   capabilities across various sensory and cognitive domains. (C) 2016
   Wiley Periodicals, Inc.}},
DOI = {{10.1002/dneu.22396}},
ISSN = {{1932-8451}},
EISSN = {{1932-846X}},
Unique-ID = {{ISI:000390008700004}},
}

@article{ ISI:000388612700002,
Author = {Gado, Tamer A. and Van-Thanh-Van Nguyen},
Title = {{Regional Estimation of Floods for Ungauged Sites Using Partial Duration
   Series and Scaling Approach}},
Journal = {{JOURNAL OF HYDROLOGIC ENGINEERING}},
Year = {{2016}},
Volume = {{21}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{One of the main obstacles in making reliable predictions of extreme
   events is the apparent shortness of the time series available in
   hydrology. There is a common advantage of both regional flood-frequency
   analysis and partial duration series (PDS) in which both of them allow a
   reduction of uncertainty by introducing more data to the site of
   interest. Therefore, a new regional index flood method for ungauged
   sites based on the PDS model is presented. The PDS model considered in
   this case assumes a Poisson-distributed number of threshold exceedances
   and generalized Pareto-distributed peak magnitudes. A new objective
   approach for the selection of the threshold in the context of
   regionalization is introduced. This approach estimates a range of
   reasonable thresholds (or an average annual number of events) for every
   site. Consequently, the regional average annual number of events can be
   determined as a common value for all sites in the homogeneous region.
   The delineation of hydrologically homogeneous regions is determined
   using the scaling approach. The feasibility of the proposed method was
   assessed using the available daily flow series from 57 watersheds in
   Quebec (Canada). Furthermore, the new method is compared with two
   existing methods for regionalization: the region of influence and
   canonical correlation analysis methods. Results of the numerical
   application indicate that the quantile estimates obtained from the new
   method provide the best values of the performance criteria
   (e.g.,root-mean-square error). Hence, the new method not only eliminates
   subjective decisions but also greatly improves the predicted floods for
   ungauged sites. (C) 2016 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)HE.1943-5584.0001439}},
Article-Number = {{04016044}},
ISSN = {{1084-0699}},
EISSN = {{1943-5584}},
ORCID-Numbers = {{Gado, Tamer/0000-0002-4367-2902}},
Unique-ID = {{ISI:000388612700002}},
}

@article{ ISI:000402376300004,
Author = {Reuchlin-Hugenholtz, Emilie and Shackell, Nancy L. and Hutchings,
   Jeffrey A.},
Title = {{Spatial reference points for groundfish}},
Journal = {{ICES JOURNAL OF MARINE SCIENCE}},
Year = {{2016}},
Volume = {{73}},
Number = {{10}},
Pages = {{2468-2478}},
Month = {{NOV}},
Abstract = {{According to density-dependent habitat selection theory, areas of high
   density can be indicative of high population productivity and have
   positive individual fitness consequences. Here, we explore six
   groundfish populations on the Scotian Shelf, Canada, where a decline in
   areas of high density beyond a certain threshold is associated with
   disproportionately large declines in Spawning Stock Biomass (SSB). This
   is evidenced by empirical, concave, positive relationships between
   high-density areas (HDAs) and SSB. We introduce a methodology to
   estimate the threshold below which SSB declines increasingly faster per
   unit of HDA decline. The spatial threshold among these six stocks was
   remarkably consistent; when stocks lose 70-80\% of HDAs,
   disproportionately large SSB declines are likely to occur. We propose
   that spatial thresholds could serve as spatial reference points to
   complement existing SSB limit reference points (LRPs). For some stocks
   we identify spatial thresholds which correspond to SSB levels that
   exceed those associated with the designated SSB LRP, suggesting that a
   review of these SSB LRPs warrants merit. For other stocks, spatial
   reference points can be used in concert with SSB reference points,
   strengthening efforts to incorporate a precautionary approach to
   fisheries management. Our results warrant further research into the
   general application of HDA as spatial limit and target reference points
   for fisheries management in addition to other population status
   indicators within a broad recovery framework.}},
DOI = {{10.1093/icesjms/fsw123}},
ISSN = {{1054-3139}},
EISSN = {{1095-9289}},
Unique-ID = {{ISI:000402376300004}},
}

@article{ ISI:000387300300048,
Author = {Smith, Sarah Josephine and Wei, Max and Sohn, Michael D.},
Title = {{A retrospective analysis of compact fluorescent lamp experience curves
   and their correlations to deployment programs}},
Journal = {{ENERGY POLICY}},
Year = {{2016}},
Volume = {{98}},
Number = {{SI}},
Pages = {{505-512}},
Month = {{NOV}},
Abstract = {{Experience curves are useful for understanding technology development
   and can aid in the design and analysis of market transformation
   programs. Here, we employ a novel approach to create experience curves,
   to examine both global and North American compact fluorescent lamp (CFL)
   data for the years 1990-2007. We move away from the prevailing method of
   fitting a single, constant, exponential curve to data and instead search
   for break points where changes in the learning rate may have occurred.
   Our analysis suggests a learning rate of approximately 21\% for the
   period of 1990-1997, and 51\% and 79\% in global and North American
   datasets, respectively, after 1998. We use price data for this analysis;
   therefore our learning rates encompass developments beyond typical
   ``learning by doing{''}, including supply chain impacts such as market
   competition. We examine correlations between North American learning
   rates and the initiation of new programs, abrupt technological advances,
   and economic and political events, and find an increased learning rate
   associated with design advancements and federal standards programs. Our
   findings support the use of segmented experience curves for
   retrospective and prospective technology analysis, and may imply that
   investments in technology programs have contributed to an increase of
   the CFL learning rate. Published by Elsevier Ltd. This is an open access
   article under the CC BY-NC-ND license.}},
DOI = {{10.1016/j.enpol.2016.09.023}},
ISSN = {{0301-4215}},
EISSN = {{1873-6777}},
Unique-ID = {{ISI:000387300300048}},
}

@article{ ISI:000385595800006,
Author = {Arancibia, Ada Liz and Marques, Guilherme Fernandes and Bulhoes Mendes,
   Carlos Andre},
Title = {{Systems capacity expansion planning: Novel approach for environmental
   and energy policy change analysis}},
Journal = {{ENVIRONMENTAL MODELLING \& SOFTWARE}},
Year = {{2016}},
Volume = {{85}},
Pages = {{70-79}},
Month = {{NOV}},
Abstract = {{Planning for power systems generation expansion follows environmental
   policies incorporating technologies based on renewables to reduce CO2
   emissions. These policies are susceptible to unpredictable changes,
   given dynamic economic and political contexts. This paper analyzes the
   impact of changes in energy policies, motivated by different
   environmental objectives. The analysis is done through a novel approach
   coupling Dynamic Programming and Multi-objective programming to generate
   several energy policy scenarios and their trade-offs, representing
   plausible policy changes in the different stages of the planning
   horizon. The results indicate a clear Pareto front and that energy
   policy scenarios with abrupt changes should be avoided in favor of
   scenarios with gradual changes. ``Greener{''} energy policies in a given
   planning stage are not necessarily the best ones considering the full
   planning horizon, considering the unfolding impacts of current decisions
   into the future. The approach is useful in improving planners' future
   vision from myopic into a perspicacious one. (C) 2016 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.envsoft.2016.08.010}},
ISSN = {{1364-8152}},
EISSN = {{1873-6726}},
ORCID-Numbers = {{Arancibia, Ada/0000-0002-1637-8230}},
Unique-ID = {{ISI:000385595800006}},
}

@article{ ISI:000385856200002,
Author = {Yuan, Ying and He, Xiao-Song and Xi, Bei-Dou and Wei, Zi-Min and Tan,
   Wen-Bing and Gao, Ru-Tai},
Title = {{Novel method of vulnerability assessment of simple landfills area using
   the multimedia, multipathway and multireceptor risk assessment (3MRA)
   model, China}},
Journal = {{WASTE MANAGEMENT \& RESEARCH}},
Year = {{2016}},
Volume = {{34}},
Number = {{11}},
Pages = {{1099-1108}},
Month = {{NOV}},
Abstract = {{Vulnerability assessment of simple landfills was conducted using the
   multimedia, multipathway and multireceptor risk assessment (3MRA) model
   for the first time in China. The minimum safe threshold of six
   contaminants (benzene, arsenic (As), cadmium (Cd), hexavalent chromium
   {[}Cr(VI)], divalent mercury {[}Hg(II)] and divalent nickel {[}Ni(II)])
   in landfill and waste pile models were calculated by the 3MRA model.
   Furthermore, the vulnerability indexes of the six contaminants were
   predicted based on the model calculation. The results showed that the
   order of health risk vulnerability index was As > Hg(II) > Cr(VI) >
   benzene > Cd > Ni(II) in the landfill model, whereas the ecology risk
   vulnerability index was in the order of As > Hg(II) > Cr(VI) > Cd >
   benzene > Ni(II). In the waste pile model, the order of health risk
   vulnerability index was benzene > Hg(II) > Cr(VI) > As > Cd and Ni(II),
   whereas the ecology risk vulnerability index was in the order of Hg(II)
   > Cd > Cr(VI) > As > benzene > Ni(II). These results indicated that As,
   Hg(II) and Cr(VI) were the high risk contaminants for the case of a
   simple landfill in China; the concentration of these in soil and
   groundwater around the simple landfill should be strictly monitored, and
   proper mediation is also recommended for simple landfills with a high
   concentration of contaminants.}},
DOI = {{10.1177/0734242X16665912}},
ISSN = {{0734-242X}},
EISSN = {{1096-3669}},
Unique-ID = {{ISI:000385856200002}},
}

@article{ ISI:000387507000001,
Author = {Zylberberg, Ariel and Fetsch, Christopher R. and Shadlen, Michael N.},
Title = {{The influence of evidence volatility on choice, reaction time and
   confidence in a perceptual decision}},
Journal = {{ELIFE}},
Year = {{2016}},
Volume = {{5}},
Month = {{OCT 27}},
Abstract = {{Many decisions are thought to arise via the accumulation of noisy
   evidence to a threshold or bound. In perception, the mechanism explains
   the effect of stimulus strength, characterized by signal-to-noise ratio,
   on decision speed, accuracy and confidence. It also makes intriguing
   predictions about the noise itself. An increase in noise should lead to
   faster decisions, reduced accuracy and, paradoxically, higher
   confidence. To test these predictions, we introduce a novel sensory
   manipulation that mimics the addition of unbiased noise to
   motion-selective regions of visual cortex, which we verified with
   neuronal recordings from macaque areas MT/MST. For both humans and
   monkeys, increasing the noise induced faster decisions and greater
   confidence over a range of stimuli for which accuracy was minimally
   impaired. The magnitude of the effects was in agreement with predictions
   of a bounded evidence accumulation model.}},
DOI = {{10.7554/eLife.17688}},
Article-Number = {{e17688}},
ISSN = {{2050-084X}},
ORCID-Numbers = {{Zylberberg, Ariel/0000-0002-2572-4748
   Shadlen, Michael/0000-0002-2002-2210}},
Unique-ID = {{ISI:000387507000001}},
}

@article{ ISI:000402048800011,
Author = {Kaya, Sertan and Bayraktar, Mustafa and Kockara, Sinan and Mete, Mutlu
   and Halic, Tansel and Field, Halle E. and Wong, Henry K.},
Title = {{Abrupt skin lesion border cutoff measurement for malignancy detection in
   dermoscopy images}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2016}},
Volume = {{17}},
Number = {{13}},
Month = {{OCT 6}},
Note = {{13th Annual Conference of the
   MidSouth-Computational-Biology-and-Bioinformatics-Society (MCBIOS),
   Memphis, TN, MAY 03-05, 2016}},
Organization = {{MidSouth Computat Biol \& Bioinformat Soc}},
Abstract = {{Background: Automated skin lesion border examination and analysis
   techniques have become an important field of research for distinguishing
   malignant pigmented lesions from benign lesions. An abrupt pigment
   pattern cutoff at the periphery of a skin lesion is one of the most
   important dermoscopic features for detection of neoplastic behavior. In
   current clinical setting, the lesion is divided into a virtual pie with
   eight sections. Each section is examined by a dermatologist for abrupt
   cutoff and scored accordingly, which can be tedious and subjective.
   Methods: This study introduces a novel approach to objectively quantify
   abruptness of pigment patterns along the lesion periphery. In the
   proposed approach, first, the skin lesion border is detected by the
   density based lesion border detection method. Second, the detected
   border is gradually scaled through vector operations. Then, along
   gradually scaled borders, pigment pattern homogeneities are calculated
   at different scales. Through this process, statistical texture features
   are extracted. Moreover, different color spaces are examined for the
   efficacy of texture analysis.
   Results: The proposed method has been tested and validated on 100 (31
   melanoma, 69 benign) dermoscopy images. Analyzed results indicate that
   proposed method is efficient on malignancy detection. More specifically,
   we obtained specificity of 0.96 and sensitivity of 0.86 for malignancy
   detection in a certain color space. The F-measure, harmonic mean of
   recall and precision, of the framework is reported as 0.87.
   Conclusions: The use of texture homogeneity along the periphery of the
   lesion border is an effective method to detect malignancy of the skin
   lesion in dermoscopy images. Among different color spaces tested, RGB
   color space's blue color channel is the most informative color channel
   to detect malignancy for skin lesions. That is followed by YCbCr color
   spaces Cr channel, and Cr is closely followed by the green color channel
   of RGB color space.}},
DOI = {{10.1186/s12859-016-1221-4}},
Article-Number = {{367}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000402048800011}},
}

@article{ ISI:000387216300025,
Author = {Fuda, Rebecca K. and Ryan, Sadie J. and Cohen, Jonathan B. and Hartter,
   Joel and Frair, Jacqueline L.},
Title = {{Assessing impacts to primary productivity at the park edge in Murchison
   Falls Conservation Area, Uganda}},
Journal = {{ECOSPHERE}},
Year = {{2016}},
Volume = {{7}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Human activities around parks can alter vegetation patterns within them,
   resulting in edge effects that degrade their ability to sustain
   ecological processes and support biodiversity. We quantified vegetation
   patterns and edge effects over a large geographic extent in Murchison
   Falls Conservation Area, Uganda, using freely available remotely sensed
   data. Over a 13-yr period, we quantified seasonal patterns in
   productivity using the Normalized Difference Vegetation Index (NDVI),
   comparing the park exterior, a boundary zone <= 10 km from the border
   inside the park, and the park interior. To identify the extent of edge
   effects, we further fit mixed models by vegetation type within 1-km
   bands within the boundary zone. Productivity was higher in the park
   interior than exterior in both wet and dry seasons, and in the wet
   season, it was also lower in the boundary zone than the interior. NDVI
   variability differed between seasons; it declined from exterior to
   interior in the wet season, but was highest in the boundary zone and
   similar between interior and exterior during the dry season. Within the
   boundary zone, edge effects varied by land cover type and extended 4-6
   km into MFCA. Abrupt differences in vegetation patterns between the park
   and adjacent unprotected areas indicated a ``hard edge{''} in this
   system. While hard edges are readily apparent, the subtle changes in
   productivity that extend into and degrade park systems are harder to
   detect. We demonstrated a low-cost and novel approach to detect such
   effects using readily available satellite imagery, which indicated human
   influence affecting 29-40\% of the park. As human populations grow,
   parks will become further isolated, and measuring and managing edge
   effects may be crucial to achieving conservation objectives.}},
DOI = {{10.1002/ecs2.1486}},
ISSN = {{2150-8925}},
ORCID-Numbers = {{Ryan, Sadie/0000-0002-4308-6321
   Frair, Jacqueline/0000-0002-8055-2213}},
Unique-ID = {{ISI:000387216300025}},
}

@article{ ISI:000383827700016,
Author = {Romero-Guiza, Ms. and Mata-Alvarez, J. and Chimenos, J. M. and Astals,
   S.},
Title = {{The effect of magnesium as activator and inhibitor of anaerobic
   digestion}},
Journal = {{WASTE MANAGEMENT}},
Year = {{2016}},
Volume = {{56}},
Pages = {{137-142}},
Month = {{OCT}},
Abstract = {{Anaerobic digestion stands as a key technology in the emerging green
   energy economy. Mg2+ has been identified as an important element to
   improve digesters methane production; however the inhibition risk that
   high Mg2+ concentrations can cause to the AD process must also be
   considered when dosing Mg reagents and wastes containing Mg2+. Despite
   its importance, Mg2+ stimulation and inhibition mechanisms as well as
   threshold values are scarce in the literature. This research paper
   investigates the impact (stimulation and inhibition) of Mg2+ on pig
   manure anaerobic digestion. Mathematical modelling was used to better
   understand the interaction between substrate, inoculum and magnesium,
   where Mg2+ inhibition was modelled by a n-component non-competitive
   inhibition function. Modelling was done on absolute curves rather than
   specific methane productions curves (new approach) to account for the
   lower background methane production of the inoculum as the Mg2+
   concentration increased. Results showed that no stimulation or
   inhibition occurred between 40 (native concentration) and 400 mg Mg2+
   L-1, while minor and major inhibition were observed at 750 and 1000 mg
   Mg2+ L-1, and at 2000 and 4000 mg Mg2+ L-1, respectively. mg(2+) half
   maximal inhibition concentration was estimated at 2140 mg Mg2+ L-1 with
   an inhibition order of 2. The latter indicates that Mg2+ inhibition is a
   progressive rather than a steep inhibition mechanism. (C) 2016 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.wasman.2016.06.037}},
ISSN = {{0956-053X}},
ResearcherID-Numbers = {{Astals-Garcia, Sergi/H-2591-2016}},
ORCID-Numbers = {{Astals-Garcia, Sergi/0000-0003-4749-0919}},
Unique-ID = {{ISI:000383827700016}},
}

@article{ ISI:000386497000002,
Author = {Wang, Zhenzhou},
Title = {{A New Approach for Automatic Identification of the Ventricular Boundary
   in Cine Magnetic Resonance Images}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2016}},
Volume = {{6}},
Number = {{5}},
Pages = {{1139-1154}},
Month = {{SEP}},
Abstract = {{Identification of the endocardial borders remains challenging in
   cardiology. In this paper, we propose a new approach named `deformation
   flow tracking' which couples the obtained boundary from the previous
   frame and the extracted edges in the current frame by energy
   minimization. Firstly, the edges are extracted accurately by an
   effective threshold selection method. Then, the boundary in the previous
   frame is driven toward the extracted edges to form a deformation
   boundary by minimizing the energy between the deformation boundary and
   extracted edge while keeping the deformation boundary smooth.
   Deformation thresholds are defined and used to constrain the motions of
   the boundary points and eliminate outliers effectively. The proposed
   approach was tested on complete short-axis cine MRI datasets from 5
   normal subjects and 5 patients with heart failure (total of 1660 images)
   randomly chosen from a much larger dataset (100 cases). As it turned
   out, the proposed approach is efficient and robust for automatic
   identification of the ventricular endocardial boundary that moves
   directionally and regularly, which is true in most cases.}},
DOI = {{10.1166/jmihi.2016.1926}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000386497000002}},
}

@article{ ISI:000386497000023,
Author = {Shi, Zhenghao and Ma, Jiejue and Zhao, Minghua and Liu, Yonghong and
   Feng, Yaning and Zhang, Ming},
Title = {{Novel Method Using Multiple Strategies for Accurate Lung Segmentation in
   Computed Tomography Images}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2016}},
Volume = {{6}},
Number = {{5}},
Pages = {{1271-1275}},
Month = {{SEP}},
Abstract = {{In this paper, a novel method using multiple strategies for accurate
   lung segmentation in CT images was proposed. The method consists of six
   key operation: firstly, in order to avoid noise, the input CT slice was
   smoothed using the guided filter. Then, the smoothed slice was
   transformed into a binary image using an optimized threshold. Next, a
   region-growing strategy was employed to extract thorax regions. Then,
   lung regions were segmented from the thorax regions using a seed-based
   random walk algorithm. The segmented lung contour was then smoothed and
   corrected with a curvature-based correction method on each axis slice.
   Finally, with the lung masks, the lung region was automatically
   segmented from a CT slice. Experimental results show that the proposed
   method accurately segmented lung regions in CT slices.}},
DOI = {{10.1166/jmihi.2016.1911}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000386497000023}},
}

@article{ ISI:000381587400009,
Author = {Chong, Nyuk Sian and Dionne, Benoit and Smith, Robert},
Title = {{An avian-only Filippov model incorporating culling of both susceptible
   and infected birds in combating avian influenza}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2016}},
Volume = {{73}},
Number = {{3}},
Pages = {{751-784}},
Month = {{SEP}},
Abstract = {{Depopulation of birds has always been an effective method not only to
   control the transmission of avian influenza in bird populations but also
   to eliminate influenza viruses. We introduce a Filippov avian-only model
   with culling of susceptible and/or infected birds. For each susceptible
   threshold level , we derive the phase portrait for the dynamical system
   as we vary the infected threshold level , focusing on the existence of
   endemic states; the endemic states are represented by real equilibria,
   pseudoequilibria and pseudo-attractors. We show generically that all
   solutions of this model will approach one of the endemic states. Our
   results suggest that the spread of avian influenza in bird populations
   is tolerable if the trajectories converge to the equilibrium point that
   lies in the region below the threshold level or if they converge to one
   of the pseudoequilibria or a pseudo-attractor on the surface of
   discontinuity. However, we have to cull birds whenever the solution of
   this model converges to an equilibrium point that lies in the region
   above the threshold level in order to control the outbreak. Hence a good
   threshold policy is required to combat bird flu successfully and to
   prevent overkilling birds.}},
DOI = {{10.1007/s00285-016-0971-y}},
ISSN = {{0303-6812}},
EISSN = {{1432-1416}},
Unique-ID = {{ISI:000381587400009}},
}

@article{ ISI:000381843800009,
Author = {Richter, Otto and Langemann, Dirk and Beffa, Roland},
Title = {{Genetics of metabolic resistance}},
Journal = {{MATHEMATICAL BIOSCIENCES}},
Year = {{2016}},
Volume = {{279}},
Pages = {{71-82}},
Month = {{SEP}},
Abstract = {{Herbicide resistance has become a major issue for many weeds. Metabolic
   resistance refers to the biochemical processes within organisms that
   degrade herbicides to less toxic compounds, resulting in a shift of the
   dose response curve. This type of resistance involves polygenic
   inheritance. A model is presented linking the biochemical pathway of
   amino acid synthesis and the detoxifying pathway of an inhibitor of the
   key enzyme ALS. From this model, resistance factors for each biotype are
   derived, which are then applied to a polygenic population genetic model
   for an annual weed plant. Polygenic inheritance is described by a new
   approach based on tensor products of heredity matrices. Important
   results from the model are that low dose regimes favour fast emergence
   of resistant biotypes and that the emergence of resistant biotypes
   occurs as abrupt outbreaks. The model is used to evaluate strategies for
   the management of metabolic resistance. (C) 2016 Elsevier Inc. All
   rights reserved.}},
DOI = {{10.1016/j.mbs.2016.07.005}},
ISSN = {{0025-5564}},
EISSN = {{1879-3134}},
Unique-ID = {{ISI:000381843800009}},
}

@article{ ISI:000388785300003,
Author = {Modica, Larissa and Cordoba, Pilar and Rodriguez-Cabello, Cristina and
   Sanchez, Francisco and Velasco, Francisco},
Title = {{A new approach to species distributional indicators for the Marine
   Strategy Framework Directive (MSFD)}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2016}},
Volume = {{67}},
Pages = {{21-30}},
Month = {{AUG}},
Abstract = {{We propose alternative fish-populations spatial indicators for use in
   the Marine Strategy Framework Directive (MSFD). Following Commission
   Decision 2010/477, we have applied two different spatial indicators to
   three fish populations with ``slow type{''} life-history traits, i.e.
   slow growing like Helicolenus daclylopterus, or large bodied like
   Merluccius merluccius and Lophius budegassa. We tested their efficiency
   separately and combined. One of these indicators, the presence/absence
   of the population in sampling squares, had already been applied during
   the initial assessment of the MSFD in Spain. Another indicator, the
   geographical spread, is proposed here as a new monitoring tool for the
   MSFD in Spanish waters. `` The results demonstrate for the three
   populations analyzed that neither indicator was sufficient alone to
   describe the population spatial pattern or its evolution. Thus, the
   approach to implementing the MSFD indicated in Commission Decision
   2010/477 is not sufficient to provide integrated information about the
   spatial behavior of the fish populations analyzed. Although numerical
   targets or threshold values cannot be set, directional targets could be
   proposed, based on the results of both indicators, if evaluation of them
   is extended to more species and more geographical areas. The analysis
   could be extended to other ``slow type{''} populations within the fish
   community and also to different ecoregions. We propose an approach
   including the estimation of two different indicators to monitoring both
   the area occupied and the geographical spread of fish populations within
   communities, interpreting them together to generate a more complete
   picture of the spatial patterns of those populations. In spite of the
   difficulties in fixing numerical targets or thresholds, or in
   distinguishing between environmentally and human driven changes in the
   population spatial distributions, this approach helps to summarize fish
   spatial behavior. It improves information from the indicators applied
   alone and reduces the requirement for a large number of maps (except for
   some particular event or population). The proposed indicators can be
   readily used by managers and politicians. (C) 2016 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.ecolind.2016.02.010}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
Unique-ID = {{ISI:000388785300003}},
}

@article{ ISI:000381716200014,
Author = {Zeng, Yan-Syun and Gao, Ruo-Cing and Wu, Ting-Wei and Cho, Chien and
   Tan, Kui-Thong},
Title = {{Fluorescent Probe Encapsulated in SNAP-Tag Protein Cavity To Eliminate
   Nonspecific Fluorescence and Increase Detection Sensitivity}},
Journal = {{BIOCONJUGATE CHEMISTRY}},
Year = {{2016}},
Volume = {{27}},
Number = {{8}},
Pages = {{1872-1879}},
Month = {{AUG}},
Abstract = {{Despite the promising improvements made recently on fluorescence probes
   for the detection of enzymes and reactive small molecules, two
   fundamental problems remain: weaker fluorescence of many dyes in aqueous
   buffers and strong nonspecific signals in samples containing high
   protein levels. In this paper, we introduce a novel fluorescent probe
   encapsulated in protein cavity (FPEPC) concept as demonstrated by
   SNAP-tag protein and three environment-sensitive fluorescence probes to
   overcome these two problems. The probes were constructed by following
   the current probe design for enzymes and reactive small molecules but
   with an additional benzylguanine moiety for selective SNAP-tag
   conjugation. The SNAP-tag conjugated probes achieved quantitative
   nitroreductase and hydrogen sulfide detection in blood plasma, whereas
   analyte concentrations were overestimated up to 700-fold when bare
   fluorescent probes were employed for detection. Furthermore, detection
   sensitivity was increased dramatically, as our probes displayed 390-fold
   fluorescence enhancement upon SNAP-tag conjugation, in stark contrast to
   the weak fluorescence of the free probes in aqueous solutions. Compared
   with the conventional approaches where fluorescent probes are
   encapsulated into polymers and nanoparticles, our simple and general
   approach successfully overcame many key issues such as dye leakage, long
   preparation steps, inconsistent dye-host ratios, difficulty in
   constructing in situ in a complex medium, and limited application to
   detect only small metabolites.}},
DOI = {{10.1021/acs.bioconjchem.6b00290}},
ISSN = {{1043-1802}},
ResearcherID-Numbers = {{Tan, Kui-Thong/A-6265-2018}},
ORCID-Numbers = {{Tan, Kui-Thong/0000-0002-0091-8546}},
Unique-ID = {{ISI:000381716200014}},
}

@article{ ISI:000381231600020,
Author = {Szweda, Roza and Trzebicka, Barbara and Dworak, Andrzej and Otulakowski,
   Lukasz and Kosowski, Dominik and Hertlein, Justyna and Haladjova, Emi
   and Rangelov, Stanislav and Szweda, Dawid},
Title = {{Smart Polymeric Nanocarriers of Met-enkephalin}},
Journal = {{BIOMACROMOLECULES}},
Year = {{2016}},
Volume = {{17}},
Number = {{8}},
Pages = {{2691-2700}},
Month = {{AUG}},
Abstract = {{This study describes a novel approach to polymeric nanocarriers of the
   therapeutic peptide metenkephalin based on the aggregation of
   thermoresponsive polymers. Thermoresponsive bioconjugate
   poly((di(ethylene glycol) monomethyl ether
   methacrylate)-ran-(oligo(ethylene glycol) monomethyl ether methacrylate)
   is synthesized by AGET ATRP using modified met-enkephalin as a macro
   initiator. The abrupt heating of bioconjugate water solution leads to
   the self-assembly of bioconjugate chains and the formation of
   mesoglobules of controlled sizes. Mesoglobules formed by bioconjugates
   are stabilized by coating with cross linked two-layer shell via
   nucleated radical polymerization of N-isopropylacrylamide using a
   degradable cross-linker. The targeting peptide RGD, containing the
   fluorescence marker carboxyfluorescein, is linked to a nanocarrier
   during the formation of the outer shell layer. In the presence of
   glutathione, the whole shell is completely degradable and the
   met-enkephalin conjugate is released. It is anticipated that precisely
   engineered nanoparticles protecting their cargo will emerge as the
   next-generation platform for cancer therapy and many other biomedical
   applications.}},
DOI = {{10.1021/acs.biomac.6b00725}},
ISSN = {{1525-7797}},
EISSN = {{1526-4602}},
ResearcherID-Numbers = {{Dworak, Andrzej/D-1460-2017
   Szweda, Roza/X-7417-2018
   }},
ORCID-Numbers = {{Szweda, Roza/0000-0003-2152-7656}},
Unique-ID = {{ISI:000381231600020}},
}

@article{ ISI:000373930200012,
Author = {Tamborrino, Massimiliano},
Title = {{APPROXIMATION OF THE FIRST PASSAGE TIME DENSITY OF A WIENER PROCESS TO
   AN EXPONENTIALLY DECAYING BOUNDARY BY TWO-PIECEWISE LINEAR THRESHOLD.
   APPLICATION TO NEURONAL SPIKING ACTIVITY}},
Journal = {{MATHEMATICAL BIOSCIENCES AND ENGINEERING}},
Year = {{2016}},
Volume = {{13}},
Number = {{3}},
Pages = {{613-629}},
Month = {{JUN}},
Note = {{11th Workshop on Neural Coding (NC), Versailles, FRANCE, OCT 06-12, 2014}},
Abstract = {{The first passage time density of a diffusion process to a time varying
   threshold is of primary interest in different fields. Here, we consider
   a Brownian motion in presence of an exponentially decaying threshold to
   model the neuronal spiking activity. Since analytical expressions of the
   first passage time density are not available, we propose to approximate
   the curved boundary by means of a continuous two-piecewise linear
   threshold. Explicit expressions for the first passage time density
   towards the new boundary are provided. First, we introduce different
   approximating linear thresholds. Then, we describe how to choose the
   optimal one minimizing the distance to the curved boundary, and hence
   the error in the corresponding passage time density. Theoretical means,
   variances and coefficients of variation given by our method are compared
   with empirical quantities from simulated data. Moreover, a further
   comparison with firing statistics derived under the assumption of a
   small amplitude of the time-dependent change in the threshold, is also
   carried out. Finally, maximum likelihood and moment estimators of the
   parameters of the model are derived and applied on simulated data.}},
DOI = {{10.3934/mbe.2016011}},
ISSN = {{1547-1063}},
EISSN = {{1551-0018}},
ResearcherID-Numbers = {{Tamborrino, Massimiliano/L-2322-2014}},
ORCID-Numbers = {{Tamborrino, Massimiliano/0000-0002-4661-8071}},
Unique-ID = {{ISI:000373930200012}},
}

@article{ ISI:000376609500005,
Author = {Seibold, Heidi and Zeileis, Achim and Hothorn, Torsten},
Title = {{Model-Based Recursive Partitioning for Subgroup Analyses}},
Journal = {{INTERNATIONAL JOURNAL OF BIOSTATISTICS}},
Year = {{2016}},
Volume = {{12}},
Number = {{1, SI}},
Pages = {{45-63}},
Month = {{MAY}},
Abstract = {{The identification of patient subgroups with differential treatment
   effects is the first step towards individualised treatments. A current
   draft guideline by the EMA discusses potentials and problems in subgroup
   analyses and formulated challenges to the development of appropriate
   statistical procedures for the data-driven identification of patient
   subgroups. We introduce model-based recursive partitioning as a
   procedure for the automated detection of patient subgroups that are
   identifiable by predictive factors. The method starts with a model for
   the overall treatment effect as defined for the primary analysis in the
   study protocol and uses measures for detecting parameter instabilities
   in this treatment effect. The procedure produces a segmented model with
   differential treatment parameters corresponding to each patient
   subgroup. The subgroups are linked to predictive factors by means of a
   decision tree. The method is applied to the search for subgroups of
   patients suffering from amyotrophic lateral sclerosis that differ with
   respect to their Riluzole treatment effect, the only currently approved
   drug for this disease.}},
DOI = {{10.1515/ijb-2015-0032}},
ISSN = {{2194-573X}},
EISSN = {{1557-4679}},
ResearcherID-Numbers = {{Zeileis, Achim/K-9226-2015
   }},
ORCID-Numbers = {{Zeileis, Achim/0000-0003-0918-3766
   Seibold, Heidi/0000-0002-8960-9642}},
Unique-ID = {{ISI:000376609500005}},
}

@article{ ISI:000375339700023,
Author = {de Fouw, Jimmy and Govers, Laura L. and van de Koppel, Johan and van
   Belzen, Jim and Dorigo, Wouter and Cheikh, Mohammed A. Sidi and
   Christianen, Marjolijn J. A. and van der Reijden, Karin J. and van der
   Geest, Matthijs and Piersma, Theunis and Smolders, Alfons J. P. and
   Olff, Han and Lamers, Leon P. M. and van Gils, Jan A. and van der Heide,
   Tjisse},
Title = {{Drought, Mutualism Breakdown, and Landscape-Scale Degradation of
   Seagrass Beds}},
Journal = {{CURRENT BIOLOGY}},
Year = {{2016}},
Volume = {{26}},
Number = {{8}},
Pages = {{1051-1056}},
Month = {{APR 25}},
Abstract = {{In many marine ecosystems, biodiversity critically depends on foundation
   species such as corals and seagrasses that engage in mutualistic
   interactions {[}1-3]. Concerns grow that environmental disruption of
   marine mutualisms exacerbates ecosystem degradation, with breakdown of
   the obligate coral mutualism ({''}coral bleaching{''}) being an iconic
   example {[}2, 4, 5]. However, as these mutualisms are mostly facultative
   rather than obligate, it remains unclear whether mutualism breakdown is
   a common risk in marine ecosystems, and thus a potential accelerator of
   ecosystem degradation. Here, we provide evidence that. drought triggered
   landscape-scale seagrass degradation and show the consequent failure of
   a facultative mutualistic feedback between seagrass and
   sulfide-consuming lucinid bivalves that in turn appeared to exacerbate
   the observed collapse. Local climate and remote sensing analyses
   revealed seagrass collapse after a summer with intense low-tide drought
   stress. Potential analysis a novel approach to detect feedback-mediated
   state shifts-revealed two attractors (healthy and degraded states)
   during the collapse, suggesting that the drought disrupted internal
   feedbacks to cause abrupt, patch-wise degradation. Field measurements
   comparing degraded patches that were healthy before the collapse with
   patches that remained healthy demonstrated that bivalves declined
   dramatically in degrading patches with associated high sediment sulfide
   concentrations, confirming the breakdown of the mutualistic
   seagrass-lucinid feedback. Our findings indicate that drought triggered
   mutualism breakdown, resulting in toxic sulfide concentrations that
   aggravated seagrass degradation. We conclude that external disturbances
   can cause sudden breakdown of facultative marine mutualistic feedbacks.
   As this may amplify ecosystem degradation, we suggest including
   mutualisms in marine conservation and restoration approaches.}},
DOI = {{10.1016/j.cub.2016.02.023}},
ISSN = {{0960-9822}},
EISSN = {{1879-0445}},
ResearcherID-Numbers = {{Lamers, Leon/A-8718-2012
   Olff, Han/A-8516-2008
   Smolders, Alfons/H-2583-2012
   van Gils, Jan/B-5544-2008
   Christianen, Marjolijn/B-8847-2011
   van der Heide, Tjisse/M-3000-2013
   van der Geest, Matthijs/A-4744-2013
   }},
ORCID-Numbers = {{Lamers, Leon/0000-0003-3769-2154
   Olff, Han/0000-0003-2154-3576
   van Gils, Jan/0000-0002-4132-8243
   Christianen, Marjolijn/0000-0001-5839-2981
   van der Geest, Matthijs/0000-0002-9837-3803
   van Belzen, Jim/0000-0003-2099-1545}},
Unique-ID = {{ISI:000375339700023}},
}

@article{ ISI:000373186400001,
Author = {Beltman, Joost B. and Urbanus, Jos and Velds, Arno and van Rooij, Nienke
   and Rohr, Jan C. and Naik, Shalin H. and Schumacher, Ton N.},
Title = {{Reproducibility of Illumina platform deep sequencing errors allows
   accurate determination of DNA barcodes in cells}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2016}},
Volume = {{17}},
Month = {{APR 2}},
Abstract = {{Background: Next generation sequencing (NGS) of amplified DNA is a
   powerful tool to describe genetic heterogeneity within cell populations
   that can both be used to investigate the clonal structure of cell
   populations and to perform genetic lineage tracing. For applications in
   which both abundant and rare sequences are biologically relevant, the
   relatively high error rate of NGS techniques complicates data analysis,
   as it is difficult to distinguish rare true sequences from spurious
   sequences that are generated by PCR or sequencing errors. This issue,
   for instance, applies to cellular barcoding strategies that aim to
   follow the amount and type of offspring of single cells, by supplying
   these with unique heritable DNA tags.
   Results: Here, we use genetic barcoding data from the Illumina HiSeq
   platform to show that straightforward read threshold-based filtering of
   data is typically insufficient to filter out spurious barcodes.
   Importantly, we demonstrate that specific sequencing errors occur at an
   approximately constant rate across different samples that are sequenced
   in parallel. We exploit this observation by developing a novel approach
   to filter out spurious sequences.
   Conclusions: Application of our new method demonstrates its value in the
   identification of true sequences amongst spurious sequences in
   biological data sets.}},
DOI = {{10.1186/s12859-016-0999-4}},
Article-Number = {{151}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Velds, Arno/0000-0003-4333-8872}},
Unique-ID = {{ISI:000373186400001}},
}

@article{ ISI:000376696800012,
Author = {Lin, Wei-Chih and Lin, Yu-Pin and Wang, Yung-Chieh},
Title = {{A decision-making approach for delineating sites which are potentially
   contaminated by heavy metals via joint simulation}},
Journal = {{ENVIRONMENTAL POLLUTION}},
Year = {{2016}},
Volume = {{211}},
Pages = {{98-110}},
Month = {{APR}},
Abstract = {{This work develops a new approach for delineating sites that are
   contaminated by multiple soil heavy metals and applies it to a case
   study. First a number of contaminant sample data are transformed into
   multiple spatially un-correlated factors using Uniformly Weighted
   Exhaustive Diagonalization with Gauss iterations (U-WEDGE). Sequential
   Gaussian simulation (sGs) is then used to generate sets of realizations
   of each resultant factor. These are then transformed into sets of sGs
   contaminant distribution realizations, which are then used to analyze
   the local and spatial (global) uncertainties in the distribution and
   concentration of contaminants via joint simulation. Finally, Info-Gap
   Decision Theory (IGDT) is used to consider different monitoring and or
   remediation regimes based on the analysis of contaminant realization
   spatial uncertainty. In our case study each heavy metal contaminant was
   considered individually and together with all other heavy metals; as the
   number of heavy metals considered increased, higher critical proportion
   values of local probability were chosen to obtain a low global
   uncertainty (to provide high reliability). Info-Gap Decision Theory
   (IGDT) yielded the most appropriate critical proportion values which
   minimized information loss in terms of specific goals. When the false
   negative rate is set to zero, meaning that it is necessary to monitor
   all potentially polluted areas, the corresponding false positive rates
   are at least 63\%, 65\%, 66\%, 68\%, 70\%, and 78\% to yield robustness
   levels of 0.50, 0.60, 0.70, 0.80, 0.90, and 1.00 respectively. However,
   when the false negative rate tolerance threshold is raised to 50\%, the
   false positive rate tolerance which yields robustness levels of 0.50,
   0.60, 0.70, 0.80, 0.90 and 1.00 drop to 12\%, 14\%, 15\%, 18\%, 20\%,
   and 39\%. The case study demonstrates the effectiveness of the developed
   approach at making robust decisions concerning the delineation of sites
   contaminated by multiple heavy metals. (C) 2015 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.envpol.2015.12.030}},
ISSN = {{0269-7491}},
EISSN = {{1873-6424}},
ORCID-Numbers = {{Lin, Yu-Pin/0000-0003-1954-334X}},
Unique-ID = {{ISI:000376696800012}},
}

@article{ ISI:000374145800018,
Author = {Kumar, Amit and Singh, Mandeep},
Title = {{Statistical Analysis of ST Segments for Ischemia Detection in
   Electrocardiogram Signals}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2016}},
Volume = {{6}},
Number = {{2}},
Pages = {{431-440}},
Month = {{APR}},
Abstract = {{A novel method has been proposed for the detection of ischemia using
   statistical features resulting from ST segment deviations in
   Electrocardiogram (ECG) signals. The method consists of five stages:
   pre-processing, region of interest (ROI), measurement of ST segment
   deviation, thresholding and statistical analysis. The mean threshold of
   ST segment deviation is used to classify the ischemic beats from normal
   beats and then ischemic episode recognition is made through enduring
   beats. The statistical analysis has been performed, which involves
   coefficient of variation (COV), kurtosis and form factor. A bell shaped
   normal distribution is generated for stable and reliable measured ST
   segments for normal, depressed and elevated records. The preprocessing
   and delineation algorithms have been implemented in MATLAB 2012a and IBM
   SPSS 22 has been used for statistical analysis. The results show 97.83\%
   average sensitivity (S-E) and 97.56\% average specificity (S-P) for
   86,384 segments of annotated European ST-T database (EDB) after
   validation. These results are significantly better than the available
   methods in the literature. The proposed algorithm covers the advantages
   of automatic discarding of irrelevant data, noisy beats.}},
DOI = {{10.1166/jmihi.2016.1717}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000374145800018}},
}

@article{ ISI:000373750200008,
Author = {Roman-Jimenez, Geoffrey and De Crevoisier, Renaud and Leseur, Julie and
   Devillers, Anne and Ospina, Juan David and Simon, Antoine and Terve,
   Pierre and Acosta, Oscar},
Title = {{Detection of bladder metabolic artifacts in F-18-FDG PET imaging}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2016}},
Volume = {{71}},
Pages = {{77-85}},
Month = {{APR 1}},
Abstract = {{Positron emission tomography using F-18-fiuorodeoxyglucose
   (F-18-FDG-PET) is a widely used imaging modality in oncology. It enables
   significant functional information to be included in analyses of
   anatomical data provided by other image modalities. Although PET offers
   high sensitivity in detecting suspected malignant metabolism, F-18-FDG
   uptake is not tumor-specific and can also be fixed in surrounding
   healthy tissue, which may consequently be mistaken as cancerous. PET
   analyses may be particularly hampered in pelvic-located cancers by the
   bladder's physiological uptake potentially obliterating the tumor
   uptake. In this paper, we propose a novel method for detecting F-18-FDG
   bladder artifacts based on a multi-feature double-step classification
   approach. Using two manually defined seeds (tumor and bladder), the
   method consists of a semi-automated double-step clustering strategy that
   simultaneously takes into consideration standard uptake values (SUV) on
   PET, Hounsfield values on computed tomography (CT), and the distance to
   the seeds. This method was performed on 52 PET/CT images from patients
   treated for locally advanced cervical cancer. Manual delineations of the
   bladder on CT images were used in order to evaluate bladder uptake
   detection capability. Tumor preservation was evaluated using a manual
   segmentation of the tumor, with a threshold of 42\% of the maximal
   uptake within the tumor. Robustness was assessed by randomly selecting
   different initial seeds. The classification averages were 0.94 +/- 0.09
   for sensitivity, 0.98 +/- 0.01 specificity, and 0.98 +/- 0.01 accuracy.
   These results suggest that this method is able to detect most 18F-FDG
   bladder metabolism artifacts while preserving tumor uptake, and could
   thus be used as a pre-processing step for further non-parasitized PET
   analyses. (C) 2016 Published by Elsevier Ltd.}},
DOI = {{10.1016/j.compbiomed.2016.02.002}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ResearcherID-Numbers = {{SIMON, Antoine/N-5717-2015}},
ORCID-Numbers = {{SIMON, Antoine/0000-0001-6023-6427}},
Unique-ID = {{ISI:000373750200008}},
}

@article{ ISI:000372750100037,
Author = {Panahi, Davood and Azari, Mansour and Akbari, Mohammad Esmaeil and
   Zendehdel, Rezvan and Mirzaei, Hamid Reza and Hatami, Hossein and
   Mehrabi, Yadollah},
Title = {{Development of a new method for sampling and monitoring oncology staff
   exposed to cyclophosphamide drug}},
Journal = {{ENVIRONMENTAL MONITORING AND ASSESSMENT}},
Year = {{2016}},
Volume = {{188}},
Number = {{4}},
Month = {{APR}},
Abstract = {{Treatment using cytotoxic drugs is considered to be the most common
   treatment for cancers. However, the widespread use of these drugs on the
   health status of the staff at the oncology department has become a great
   concern. Due to challenges of sampling and analysis of cytotoxic drugs,
   the aim of this study was to development a novel practical method called
   Needle trap devices (NTD) for sampling and analysis of personal exposure
   to cyclophosphamide drug. The sampler consisted of a stainless steel
   hyper needle gauge 21 of length 9 cm packed with Carboxen 1000 for
   adsorbing cyclophosphamide. A total of 41 samples of staff's air
   breathing zone in different wards of the oncology department were taken
   with the sampler. Samples were analyzed by gas chromatography coupled
   with electron capture detector (ECD), Linear range concentration was
   212-1062 mu g/m(3). and LOD and LOQ were 100 and 191 mu g/m(3),
   respectively. The mean inter-day and intra-day coefficient variations
   for standards within linear range concentration were 8.9 and 4.8 \%.
   respectively. Detectable levels of cyclophosphamide were measured in
   31.7 \% of air samples. The developed method is user-friendly, quick,
   and precise for sampling of airborne cyclophosphamide. The results
   showed that some staff of the oncology department were exposed to the
   carcinogenic drug and their health were at risk. Since carcinogens do
   not have a threshold and oncology staffs with their continuous exposure
   might be at risk, therefore, proper work practice and adequate control
   measures are essential to ensure their wellbeing.}},
DOI = {{10.1007/s10661-016-5255-x}},
Article-Number = {{UNSP 238}},
ISSN = {{0167-6369}},
EISSN = {{1573-2959}},
ResearcherID-Numbers = {{Zendehdel, Rezvan/M-4027-2017
   hatami, hossein/M-2962-2017
   }},
ORCID-Numbers = {{Zendehdel, Rezvan/0000-0002-1886-6713
   hatami, hossein/0000-0002-1448-3598
   panahi, davoud/0000-0002-1620-808X
   Mehrabi, Yadollah/0000-0001-9837-4956}},
Unique-ID = {{ISI:000372750100037}},
}

@article{ ISI:000372975000008,
Author = {Kim, Sangjin and Schliekelman, Paul},
Title = {{Prioritizing hypothesis tests for high throughput data}},
Journal = {{BIOINFORMATICS}},
Year = {{2016}},
Volume = {{32}},
Number = {{6}},
Pages = {{850-858}},
Month = {{MAR 15}},
Abstract = {{Motivation: The advent of high throughput data has led to a massive
   increase in the number of hypothesis tests conducted in many types of
   biological studies and a concomitant increase in stringency of
   significance thresholds. Filtering methods, which use independent
   information to eliminate less promising tests and thus reduce multiple
   testing, have been widely and successfully applied. However, key
   questions remain about how to best apply them: When is filtering
   beneficial and when is it detrimental? How good does the independent
   information need to be in order for filtering to be effective? How
   should one choose the filter cutoff that separates tests that pass the
   filter from those that don't?
   Result: We quantify the effect of the quality of the filter information,
   the filter cutoff and other factors on the effectiveness of the filter
   and show a number of results: If the filter has a high probability (e.g.
   70\%) of ranking true positive features highly (e.g. top 10\%), then
   filtering can lead to dramatic increase (e.g. 10-fold) in discovery
   probability when there is high redundancy in information between
   hypothesis tests. Filtering is less effective when there is low
   redundancy between hypothesis tests and its benefit decreases rapidly as
   the quality of the filter information decreases. Furthermore, the
   outcome is highly dependent on the choice of filter cutoff. Choosing the
   cutoff without reference to the data will often lead to a large loss in
   discovery probability. However, naive optimization of the cutoff using
   the data will lead to inflated type I error. We introduce a data-based
   method for choosing the cutoff that maintains control of the family-wise
   error rate via a correction factor to the significance threshold.
   Application of this approach offers as much as a several-fold advantage
   in discovery probability relative to no filtering, while maintaining
   type I error control. We also introduce a closely related method of
   P-value weighting that further improves performance.}},
DOI = {{10.1093/bioinformatics/btv608}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000372975000008}},
}

@article{ ISI:000372005600010,
Author = {Xu, Lei and Ronnlund, Daniel and Aspenstrom, Pontus and Braun, Laura J.
   and Gad, Annica K. B. and Widengren, Jerker},
Title = {{Resolution, target density and labeling effects in colocalization
   studies - suppression of false positives by nanoscopy and modified
   algorithms}},
Journal = {{FEBS JOURNAL}},
Year = {{2016}},
Volume = {{283}},
Number = {{5}},
Pages = {{882-898}},
Month = {{MAR}},
Abstract = {{Colocalization analyses of fluorescence images are extensively used to
   quantify molecular interactions in cells. In recent years, fluorescence
   nanoscopy has approached resolutions close to molecular dimensions.
   However, the extent to which image resolution influences different
   colocalization estimates has not been systematically investigated. In
   this work, we applied simulations and resolution-tunable stimulated
   emission depletion microscopy to evaluate how the resolution, molecular
   density and label size of targeted molecules influence estimates of the
   most commonly used colocalization algorithms (Pearson correlation
   coefficient, Manders' M1 and M2 coefficients), as well as estimates by
   the image cross-correlation spectroscopy method. We investigated the
   practically measureable extents of colocalization for stimulated
   emission depletion microscopy with positive and negative control samples
   with an aim to identifying the strengths and weaknesses of nanoscopic
   techniques for colocalization studies. At a typical optical resolution
   of a confocal microscope (200-300 nm), our results indicate that the
   extent of colocalization is typically overestimated by the tested
   algorithms, especially at high molecular densities. Only minor effects
   of this kind were observed at higher resolutions (< 60 nm). By contrast,
   underestimation of colocalization may occur if the resolution is close
   to the size of the label/affinity molecules themselves. To suppress
   false positives at confocal resolutions and high molecular densities, we
   introduce a statistical variant of Costes' threshold searching
   algorithm, used in combination with correlation-based methods like the
   Pearson coefficient and the image cross-correlation spectroscopy
   approach, to set intensity thresholds separating background noise from
   signals.}},
DOI = {{10.1111/febs.13652}},
ISSN = {{1742-464X}},
EISSN = {{1742-4658}},
ResearcherID-Numbers = {{Gad, Annica/H-8758-2017
   }},
ORCID-Numbers = {{Gad, Annica/0000-0002-1098-9129
   Widengren, Jerker/0000-0003-3200-0374}},
Unique-ID = {{ISI:000372005600010}},
}

@article{ ISI:000369525200004,
Author = {Calabrese, Edward J. and Shamoun, Dima Yazji and Hanekamp, Jaap C.},
Title = {{THE INTEGRATION OF LNT AND HORMESIS FOR CANCER RISK ASSESSMENT OPTIMIZES
   PUBLIC HEALTH PROTECTION}},
Journal = {{HEALTH PHYSICS}},
Year = {{2016}},
Volume = {{110}},
Number = {{3}},
Pages = {{256-259}},
Month = {{MAR}},
Note = {{60th Annual Meeting of the Health-Physics-Society, Indianapolis, IN, JUL
   12-16, 2015}},
Organization = {{Hlth Phys Soc}},
Abstract = {{This paper proposes a new cancer risk assessment strategy and
   methodology that optimizes population-based responses by yielding the
   lowest disease/tumor incidence across the entire dose continuum. The
   authors argue that the optimization can be achieved by integrating two
   seemingly conflicting models; i.e., the linear no-threshold (LNT) and
   hormetic dose-response models. The integration would yield the optimized
   response at a risk of 10(-4) with the LNT model. The integrative
   functionality of the LNT and hormetic dose response models provides an
   improved estimation of tumor incidence through model uncertainty
   analysis and major reductions in cancer incidence via hormetic model
   estimates. This novel approach to cancer risk assessment offers
   significant improvements over current risk assessment approaches by
   revealing a regulatory sweet spot that maximizes public health benefits
   while incorporating practical approaches for model validation.}},
DOI = {{10.1097/HP.0000000000000382}},
ISSN = {{0017-9078}},
EISSN = {{1538-5159}},
Unique-ID = {{ISI:000369525200004}},
}

@article{ ISI:000374207800015,
Author = {Yletyinen, Johanna and Bodin, Orjan and Weigel, Benjamin and Nordstrom,
   Marie C. and Bonsdorff, Erik and Blenckner, Thorsten},
Title = {{Regime shifts in marine communities: a complex systems perspective on
   food web dynamics}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2016}},
Volume = {{283}},
Number = {{1825}},
Month = {{FEB 24}},
Abstract = {{Species composition and habitats are changing at unprecedented rates in
   the world's oceans, potentially causing entire food webs to shift to
   structurally and functionally different regimes. Despite the severity of
   these regime shifts, elucidating the precise nature of their underlying
   processes has remained difficult. We address this challenge with a new
   analytic approach to detect and assess the relative strength of
   different driving processes in food webs. Our study draws on complexity
   theory, and integrates the network-centric exponential random graph
   modelling (ERGM) framework developed within the social sciences with
   community ecology. In contrast to previous research, this approach makes
   dear assumptions of direction of causality and accommodates a dynamic
   perspective on the emergence of food webs. We apply our approach to
   analysing food webs of the Baltic Sea before and after a previously
   reported regime shift. Our results show that the dominant food web
   processes have remained largely the same, although we detect changes in
   their magnitudes. The results indicate that the reported regime shift
   may not be a system-wide shift, but instead involve a limited number of
   species. Our study emphasizes the importance of community-wide analysis
   on marine regime shifts and introduces a novel approach to examine food
   webs.}},
DOI = {{10.1098/rspb.2015.2569}},
Article-Number = {{20152569}},
ISSN = {{0962-8452}},
EISSN = {{1471-2954}},
ResearcherID-Numbers = {{Nordstrom, Marie/C-8956-2012
   Bodin, Orjan/A-5098-2010
   }},
ORCID-Numbers = {{Nordstrom, Marie/0000-0001-5763-1813
   Bodin, Orjan/0000-0002-8218-1153
   Weigel, Benjamin/0000-0003-2302-5529
   Blenckner, Thorsten/0000-0002-6991-7680}},
Unique-ID = {{ISI:000374207800015}},
}

@article{ ISI:000367474300016,
Author = {Schonmann, Roberto H. and Boyd, Robert},
Title = {{A simple rule for the evolution of contingent cooperation in large
   groups}},
Journal = {{PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2016}},
Volume = {{371}},
Number = {{1687}},
Month = {{FEB 5}},
Abstract = {{Humans cooperate in large groups of unrelated individuals, and many
   authors have argued that such cooperation is sustained by contingent
   reward and punishment. However, such sanctioning systems can also
   stabilize a wide range of behaviours, including mutually deleterious
   behaviours. Moreover, it is very likely that large-scale cooperation is
   derived in the human lineage. Thus, understanding the evolution of
   mutually beneficial cooperative behaviour requires knowledge of when
   strategies that support such behaviour can increase when rare. Here, we
   derive a simple formula that gives the relatedness necessary for
   contingent cooperation in n-person iterated games to increase when rare.
   This rule applies to a wide range of pay-off functions and assumes that
   the strategies supporting cooperation are based on the presence of a
   threshold fraction of cooperators. This rule suggests that modest levels
   of relatedness are sufficient for invasion by strategies that make
   cooperation contingent on previous cooperation by a small fraction of
   group members. In contrast, only high levels of relatedness allow the
   invasion by strategies that require near universal cooperation. In order
   to derive this formula, we introduce a novel methodology for studying
   evolution in group structured populations including local and global
   group-size regulation and fluctuations in group size.}},
DOI = {{10.1098/rstb.2015.0099}},
Article-Number = {{20150099}},
ISSN = {{0962-8436}},
EISSN = {{1471-2970}},
Unique-ID = {{ISI:000367474300016}},
}

@article{ ISI:000371317700007,
Author = {Yang, Haidong and Shao, Dongguo and Liu, Biyu and Huang, Jianhua and Ye,
   Xianbao},
Title = {{Multi-point source identification of sudden water pollution accidents in
   surface waters based on differential evolution and
   Metropolis-Hastings-Markov Chain Monte Carlo}},
Journal = {{STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT}},
Year = {{2016}},
Volume = {{30}},
Number = {{2}},
Pages = {{507-522}},
Month = {{FEB}},
Abstract = {{Sudden water pollution accidents in surface waters occur with increasing
   frequency. These accidents significantly threaten people's health and
   lives. To prevent the diffusion of pollutants, identifying these
   pollution sources is necessary. The identification problem of pollution
   source, especially for multi-point source, is one of the difficulties in
   the inverse problem area. This study examines this issue. A new method
   is designed by combining differential evolution algorithm (DEA) and
   Metropolis-Hastings-Markov Chain Monte Carlo (MH-MCMC) based on Bayesian
   inference to identify multi-point sudden water pollution sources. The
   effectiveness and accuracy of this proposed method is verified through
   outdoor experiments and comparison between DEA and MH-MCMC. The average
   absolute error of the sources' position and intensity, the relative
   error and the average standard deviations obtained using the proposed
   method are less than those of DEA and MH-MCMC. Moreover, the relative
   error and the sampling relative error under four different standard
   deviations of measurement error (r = 0.01, 0.05, 0.1, 0.15) are less
   than 2 and 0.11 \%, respectively. The proposed method (i. e., DEMH-MCMC)
   is effective even when the standard deviation of the measurement error
   increases to 0.15. Therefore, the proposed method can identify sources
   of multi-point sudden water pollution accidents efficiently and
   accurately.}},
DOI = {{10.1007/s00477-015-1191-5}},
ISSN = {{1436-3240}},
EISSN = {{1436-3259}},
Unique-ID = {{ISI:000371317700007}},
}

@article{ ISI:000370770700032,
Author = {Wu, Chuandong and Liu, Jiemin and Zhao, Peng and Piringer, Martin and
   Schauberger, Guenther},
Title = {{Conversion of the chemical concentration of odorous mixtures into odour
   concentration and odour intensity: A comparison of methods}},
Journal = {{ATMOSPHERIC ENVIRONMENT}},
Year = {{2016}},
Volume = {{127}},
Pages = {{283-292}},
Month = {{FEB}},
Abstract = {{Continuous odour measurements both of emissions as well as ambient
   concentrations are seldom realised, mainly because of their high costs.
   They are therefore often substituted by concentration measurements of
   odorous substances. Then a conversion of the chemical concentrations C
   (mg m(-3)) into odour concentrations Cop (out m-3) and odour intensities
   OI is necessary. Four methods to convert the concentrations of single
   substances to the odour concentrations and odour intensities of an
   odorous mixture are investigated: (1) direct use of measured
   concentrations, (2) the sum of the odour activity value SOAV, (3) the
   sum of the odour intensities SOI, and (4) the equivalent odour
   concentration EOC, as a new method. The methods are evaluated with
   olfactometric measurements of seven substances as well as their
   mixtures. The results indicate that the SDI and EOC conversion methods
   deliver reliable values. These methods use not only the odour threshold
   concentration but also the slope of the Weber Fechner law to include the
   sensitivity of the odour perception of the individual substances. They
   fulfil the criteria of an objective conversion without the need of a
   further calibration by additional olfactometric measurements. (C) 2015
   The Authors. Published by Elsevier Ltd. This is an open access article
   under the CC BY license}},
DOI = {{10.1016/j.atmosenv.2015.12.051}},
ISSN = {{1352-2310}},
EISSN = {{1873-2844}},
ORCID-Numbers = {{Piringer, Martin/0000-0002-7583-8700
   Schauberger, Gunther/0000-0003-2418-3692}},
Unique-ID = {{ISI:000370770700032}},
}

@article{ ISI:000370955300009,
Author = {Rayfield, Bronwyn and Pelletier, David and Dumitru, Maria and Cardille,
   Jeffrey A. and Gonzalez, Andrew},
Title = {{Multipurpose habitat networks for short-range and long-range
   connectivity: a new method combining graph and circuit connectivity}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2016}},
Volume = {{7}},
Number = {{2}},
Pages = {{222-231}},
Month = {{FEB}},
Abstract = {{Biodiversity conservation in landscapes undergoing climate and land-use
   changes requires designing multipurpose habitat networks that connect
   the movements of organisms at multiple spatial scales. Short-range
   connectivity within habitat networks provides organisms access to
   spatially distributed resources, reduces local extinctions and increases
   recolonization of habitat fragments. Long-range connectivity across
   habitat networks facilitates annual migrations and climate-driven range
   shifts. We present a method for identifying a multipurpose network of
   forest patches that promotes both short- and long-range connectivity.
   Our method uses both graph-theoretic analyses that quantify network
   connectedness and circuit-based analyses that quantify network
   traversability as the basis for identifying spatial conservation
   priorities on the landscape. We illustrate our approach in the
   agroecosystem, bordered by the Laurentian and Appalachian mountain
   ranges, that surrounds the metropolis of Montreal, Canada. We
   established forest conservation priorities for the ovenbird, a
   Neotropical migrant, sensitive to habitat fragmentation that breeds in
   our study area. All connectivity analyses were based on the same
   empirically informed resistance surface for ovenbird, but habitat pixels
   that facilitated short- and long-range connectivity requirements had low
   spatial correlation. The trade-off between connectivity requirements in
   the final ranking of conservation priorities showed a pattern of
   diminishing returns such that beyond a threshold, additional
   conservation of long-range connectivity had decreased effectiveness on
   the conservation of short-range connectivity. Highest conservation
   priority was assigned to a series of stepping stone forest patches
   across the study area that promote traversability between the bordering
   mountain ranges and to a collection of small forest fragments scattered
   throughout the study area that provide connectivity within the
   agroecosystem. Landscape connectivity is important for the ecology and
   genetics of populations threatened by climate change and habitat
   fragmentation. Our method has been illustrated as a means to conserve
   two critical dimensions of connectivity for a single species, but it is
   designed to incorporate a variety of connectivity requirements for many
   species. Our approach can be tailored to local, regional and continental
   conservation initiatives to protect essential species movements that
   will allow biodiversity to persist in a changing climate.}},
DOI = {{10.1111/2041-210X.12470}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ORCID-Numbers = {{Cardille, Jeffrey/0000-0002-4667-9085
   Gonzalez, Andrew/0000-0001-6075-8081}},
Unique-ID = {{ISI:000370955300009}},
}

@article{ ISI:000370685000017,
Author = {Parodi, S. and Muselli, M. and Carlini, B. and Fontana, V. and Haupt, R.
   and Pistoia, V. and Corrias, M. V.},
Title = {{Restricted ROC curves are useful tools to evaluate the performance of
   tumour markers}},
Journal = {{STATISTICAL METHODS IN MEDICAL RESEARCH}},
Year = {{2016}},
Volume = {{25}},
Number = {{1}},
Pages = {{294-313}},
Month = {{FEB}},
Abstract = {{In Clinical Epidemiology, receiver operating characteristic (ROC)
   analysis is a standard approach for the evaluation of the performance of
   diagnostic tests for binary classification based on a tumour marker
   distribution. The area under a ROC curve is a popular indicator of test
   accuracy, but its use has been questioned when the curve is asymmetric.
   This situation often happens when the marker concentrations overlap in
   the two groups under study in the range of low specificity,
   corresponding to a subset of values useless for classification purposes
   (non-informative values). The partial area under the curve at a high
   specificity threshold has been proposed as an alternative, but a method
   to identify an optimal cut-off that separates informative from
   non-informative values is not yet available. In this study, a new
   statistical approach is proposed to perform this task. Furthermore, a
   statistical test associated with the area under a ROC curve
   corresponding to informative values only (restricted ROC curve) is
   provided and its properties are explored by extensive simulations.
   Finally, the proposed method is applied to a real data set containing
   peripheral blood levels of six tumour markers proposed for the diagnosis
   of neuroblastoma. A new approach to combine couples of markers for
   classification purposes is also illustrated.}},
DOI = {{10.1177/0962280212452199}},
ISSN = {{0962-2802}},
EISSN = {{1477-0334}},
ResearcherID-Numbers = {{Corrias, Maria Valeria/E-4571-2011
   }},
ORCID-Numbers = {{Corrias, Maria Valeria/0000-0002-7316-0772
   Parodi, Stefano/0000-0002-9193-1622}},
Unique-ID = {{ISI:000370685000017}},
}

@article{ ISI:000369786700001,
Author = {Szymczak, Silke and Holzinger, Emily and Dasgupta, Abhijit and Malley,
   James D. and Molloy, Anne M. and Mills, James L. and Brody, Lawrence C.
   and Stambolian, Dwight and Bailey-Wilson, Joan E.},
Title = {{r2VIM: A new variable selection method for random forests in genome-wide
   association studies}},
Journal = {{BIODATA MINING}},
Year = {{2016}},
Volume = {{9}},
Month = {{FEB 1}},
Abstract = {{Background: Machine learning methods and in particular random forests
   (RFs) are a promising alternative to standard single SNP analyses in
   genome-wide association studies (GWAS). RFs provide variable importance
   measures (VIMs) to rank SNPs according to their predictive power.
   However, in contrast to the established genome-wide significance
   threshold, no clear criteria exist to determine how many SNPs should be
   selected for downstream analyses.
   Results: We propose a new variable selection approach, recurrent
   relative variable importance measure (r2VIM). Importance values are
   calculated relative to an observed minimal importance score for several
   runs of RF and only SNPs with large relative VIMs in all of the runs are
   selected as important. Evaluations on simulated GWAS data show that the
   new method controls the number of false-positives under the null
   hypothesis. Under a simple alternative hypothesis with several
   independent main effects it is only slightly less powerful than logistic
   regression. In an experimental GWAS data set, the same strong signal is
   identified while the approach selects none of the SNPs in an
   underpowered GWAS.
   Conclusions: The novel variable selection method r2VIM is a promising
   extension to standard RF for objectively selecting relevant SNPs in GWAS
   while controlling the number of false-positive results.}},
DOI = {{10.1186/s13040-016-0087-3}},
Article-Number = {{7}},
ISSN = {{1756-0381}},
ResearcherID-Numbers = {{Szymczak, Silke/C-6625-2013
   }},
ORCID-Numbers = {{Szymczak, Silke/0000-0002-8897-9035
   Bailey-Wilson, Joan/0000-0002-9153-2920
   Dasgupta, Abhijit/0000-0001-6062-9896
   Mills, James/0000-0003-4496-332X}},
Unique-ID = {{ISI:000369786700001}},
}

@article{ ISI:000368207300020,
Author = {Li, Bo and Zhu, Mengyan and Jiang, Yushan and Li, Zhenhong},
Title = {{Pricing policies of a competitive dual-channel green supply chain}},
Journal = {{JOURNAL OF CLEANER PRODUCTION}},
Year = {{2016}},
Volume = {{112}},
Number = {{3}},
Pages = {{2029-2042}},
Month = {{JAN 20}},
Abstract = {{This study examines a dual-channel supply chain in which the
   manufacturer makes green products for the environmental conscious. We
   discuss the pricing and greening strategies for the chain members in
   both centralized and decentralized cases using the Stackelberg game
   model under a consistent pricing strategy. Furthermore, we compare the
   results of the single channel and dual-channel supply chains. We obtain
   that when the greening cost is greater than a threshold, the
   manufacturer does not open direct channel. However, when the degree of
   customer loyalty to the retail channel and the greening cost satisfy
   certain conditions, the dual-channel green supply chain does exist.
   Interestingly, we find that the retail price in the centralized green
   supply chain is higher than that in the decentralized supply chain,
   which contrasts with the result of `double marginalization'. We also
   propose a contract to coordinate the decentralized dual-channel green
   supply chain. Finally, extensions of consistent pricing strategy are
   discussed and different results are achieved. Our main contributions are
   that we introduce e-commerce into green supply chain management and
   obtain pricing and greening strategies for chain members. (C) 2015
   Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jclepro.2015.05.017}},
ISSN = {{0959-6526}},
EISSN = {{1879-1786}},
Unique-ID = {{ISI:000368207300020}},
}

@article{ ISI:000368413800002,
Author = {van Reenen, Mari and Reinecke, Carolus J. and Westerhuis, Johan A. and
   Venter, J. Hendrik},
Title = {{Variable selection for binary classification using error rate p-values
   applied to metabolomics data}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2016}},
Volume = {{17}},
Month = {{JAN 14}},
Abstract = {{Background: Metabolomics datasets are often high-dimensional though only
   a limited number of variables are expected to be informative given a
   specific research question. The important task of selecting informative
   variables can therefore become complex. In this paper we look at
   discriminating between two groups. Two tasks need to be performed: (i)
   finding variables which differ between the two groups; and (ii)
   determining how the selected variables can be used to classify new
   subjects. We introduce an approach using minimum classification error
   rates as test statistics to find discriminatory and therefore
   informative variables. The thresholds resulting in the minimum error
   rates can be used to classify new subjects. This approach transforms
   error rates into p-values and is referred to as ERp.
   Results: We show that non-parametric hypothesis testing, based on
   minimum classification error rates as test statistics, can find
   statistically significantly shifted variables. The discriminatory
   ability of variables becomes more apparent when error rates are
   evaluated based on their corresponding p-values, as relatively high
   error rates can still be statistically significant. ERp can handle
   unequal and small group sizes, as well as account for the cost of
   misclassification. ERp retains (if known) or reveals (if unknown) the
   shift direction, aiding in biological interpretation. The threshold
   resulting in the minimum error rate can immediately be used to classify
   new subjects. We use NMR generated metabolomics data to illustrate how
   ERp is able to discriminate subjects diagnosed with Mycobacterium
   tuberculosis infected meningitis from a control group. The list of
   discriminatory variables produced by ERp contains all biologically
   relevant variables with appropriate shift directions discussed in the
   original paper from which this data is taken.
   Conclusions: ERp performs variable selection and classification, is
   non-parametric and aids biological interpretation while handling unequal
   group sizes and misclassification costs. All this is achieved by a
   single approach which is easy to perform and interpret. ERp has the
   potential to address many other characteristics of metabolomics data.
   Future research aims to extend ERp to account for a large proportion of
   observations below the detection limit, as well as expand on
   interactions between variables.}},
DOI = {{10.1186/s12859-015-0867-7}},
Article-Number = {{33}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{van Wyk, Mari/0000-0002-5856-3258}},
Unique-ID = {{ISI:000368413800002}},
}

@article{ ISI:000368323000017,
Author = {Rashin, Alexander A. and Jernigan, Robert L.},
Title = {{Clusters of Structurally Similar MHC I HLA-A2 Molecules, Found with a
   New Method, Suggest Mechanisms of T-Cell Receptor Avidity}},
Journal = {{BIOCHEMISTRY}},
Year = {{2016}},
Volume = {{55}},
Number = {{1}},
Pages = {{167-185}},
Month = {{JAN 12}},
Abstract = {{Only alpha 1 and alpha 2 domains of the alpha-chain of the major
   histocompatibility complex (MHC) directly bind peptide antigens (Ag-s)
   and the T-cell receptor (TCR). Significant plasticity was found in the
   TCR but only minor in (alpha 1 + alpha 2). The alpha 3-domain position
   variation was noted only in connection to its binding the coreceptor
   CD8. We apply our methods for identifying functional conformational
   changes in proteins to a systematic study of similarities between 43
   X-ray structures of the entire a chains of MHC-I HLA-A2. Out of 903
   different alpha HLA-A2 pairs 203 show similarities within the earlier
   determined uncertainty threshold and unexpectedly form three similarity
   clusters (SCs) with all/most structures in a cluster similar within the
   uncertainty threshold. Pairs from different SCs always differ above the
   threshold, mainly due to variations in the alpha 3 position/structure.
   All structures in SC3 cannot bind the CD8 coreceptor. Strong hydrogen
   bonds between (alpha 1 + alpha 2) and alpha 3 differ between SC1 and SC2
   but are nearly invariant within each SC. Small conformational changes in
   the (alpha 1 + alpha 2), caused by Ag-s differences, act as an alpha 3
   ``allosteric switch{''} between SC2 and SC1. Binding of CD8 to
   SC2-HLA-A2 (Tax-type Ag-s) changes it to SC1-HLA-A2 (HuD-type Ag-s). HuD
   binding to HLA-A2 is much less stable than Tax binding. CD8-liganded
   HLA-A2 preference for binding HuD suggests that CD8-HLA-A2 may present a
   weakly binding peptide for TCR recognition, supporting the hypothesis
   that CD8 increases TCR avidity to weak Ag-s. Other HLA-A2 functions may
   involve alpha 3. TCR-A6-liganded-Tax-type-HLA-A2s form two small
   clusters, similar to either A6-liganded-HuD or A6-liganded-native-Tax
   HLA-A2s.}},
DOI = {{10.1021/acs.biochem.5b01077}},
ISSN = {{0006-2960}},
Unique-ID = {{ISI:000368323000017}},
}

@article{ ISI:000367413500013,
Author = {Collins, O. C. and Duffy, K. J.},
Title = {{Consumption threshold used to investigate stability and ecological
   dominance in consumer-resource dynamics}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2016}},
Volume = {{319}},
Number = {{SI}},
Pages = {{155-162}},
Month = {{JAN 10}},
Abstract = {{Understanding consumer resource population dynamics can be important to
   an understanding of the overall ecology of systems. For example, the
   tree-grass continuum dynamics of savannas, an important ecological
   biome, is influenced by the population dynamics. Here we investigate
   herbivory driven population dynamics of a savanna using a simple model
   of the interactions of the dominant players, namely: trees, grasses,
   browsers, grazers and mixed browsers-grazers. We introduce a consumption
   threshold that summarises some of the parameters and this is used as a
   guide to understanding the dynamics. This number is used in
   investigating system stability and sensitivity to parameter
   fluctuations. It is also used to identify degrees of ecological
   dominance. (C) 2015 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2015.03.021}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
ORCID-Numbers = {{Collins, Obiora/0000-0001-9426-6763
   Duffy, Kevin/0000-0002-2580-8984}},
Unique-ID = {{ISI:000367413500013}},
}

@article{ ISI:000371261700016,
Author = {Kulakovskiy, Ivan V. and Vorontsov, Ilya E. and Yevshin, Ivan S. and
   Soboleva, Anastasiia V. and Kasianov, Artem S. and Ashoor, Haitham and
   Ba-alawi, Wail and Bajic, Vladimir B. and Medvedeva, Yulia A. and
   Kolpakov, Fedor A. and Makeev, Vsevolod J.},
Title = {{HOCOMOCO: expansion and enhancement of the collection of transcription
   factor binding sites models}},
Journal = {{NUCLEIC ACIDS RESEARCH}},
Year = {{2016}},
Volume = {{44}},
Number = {{D1}},
Pages = {{D116-D125}},
Month = {{JAN 4}},
Abstract = {{Models of transcription factor (TF) binding sites provide a basis for a
   wide spectrum of studies in regulatory genomics, from reconstruction of
   regulatory networks to functional annotation of transcripts and sequence
   variants. While TFs may recognize different sequence patterns in
   different conditions, it is pragmatic to have a single generic model for
   each particular TF as a baseline for practical applications. Here we
   present the expanded and enhanced version of HOCOMOCO
   (http://hocomoco.autosome.ru and
   http://www.cbrc.kaust.edu.sa/hocomoco10), the collection of models of
   DNA patterns, recognized by transcription factors. HOCOMOCO now provides
   position weight matrix (PWM) models for binding sites of 601 human TFs
   and, in addition, PWMs for 396 mouse TFs. Furthermore, we introduce the
   largest up to date collection of dinucleotide PWM models for 86 (52)
   human (mouse) TFs. The update is based on the analysis of massive
   ChIP-Seq and HT-SELEX datasets, with the validation of the resulting
   models on in vivo data. To facilitate a practical application, all
   HOCOMOCO models are linked to gene and protein databases (Entrez Gene,
   HGNC, UniProt) and accompanied by precomputed score thresholds. Finally,
   we provide command-line tools for PWM and diPWM threshold estimation and
   motif finding in nucleotide sequences.}},
DOI = {{10.1093/nar/gkv1249}},
ISSN = {{0305-1048}},
EISSN = {{1362-4962}},
ResearcherID-Numbers = {{Makeev, Vsevolod/E-8015-2013
   Medvedva, Yulia/H-5947-2016
   Vorontsov, Ilya/C-6333-2014
   Kasianov, Artem/R-3495-2016
   Bajic, Vladimir/D-2810-2009
   Ba alawi, Wail/E-6680-2016
   }},
ORCID-Numbers = {{Makeev, Vsevolod/0000-0001-9405-9748
   Medvedva, Yulia/0000-0002-7587-1666
   Vorontsov, Ilya/0000-0001-8888-0804
   Kasianov, Artem/0000-0001-8086-392X
   Bajic, Vladimir/0000-0001-5435-4750
   Ba alawi, Wail/0000-0002-5218-6113
   Ba Alawi, Wail/0000-0002-2747-4703
   Kolpakov, Fedor/0000-0002-0396-0256}},
Unique-ID = {{ISI:000371261700016}},
}

@article{ ISI:000392117500004,
Author = {Clairambault, J. and Fercoq, O.},
Title = {{Physiologically Structured Cell Population Dynamic Models with
   Applications to Combined Drug Delivery Optimisation in Oncology}},
Journal = {{MATHEMATICAL MODELLING OF NATURAL PHENOMENA}},
Year = {{2016}},
Volume = {{11}},
Number = {{6}},
Pages = {{45-70}},
Abstract = {{In this paper, we introduce a model for drug delivery optimisation in a
   chronotherapeutics framework. We present a pharmacokinetics and
   pharmacodynamics model for oxaliplatin and 5-Fluorouracil, a classic
   therapeutic association in the treatment of colorectal cancer. We derive
   the pharmacokinetics model using law of mass action and enzyme kinetics.
   We design an age-structured cell cycle PDE model with drug damage and
   repair phases to account for the effect of the drugs on proliferating
   cell populations, with different parameters for healthy and cancer cell
   populations focused on their different synchronisation responses to
   circadian clock triggering. Our goal is to minimise the growth rate of
   cancerous cells while maintaining the growth rate of healthy cells above
   a given toxicity threshold. We numerically optimise the drug delivery
   schedules under our model and obtain theoretically efficient infusion
   schemes in a chronotherapeutics framework, with as well as without
   circadian clock involvement in the molecular pharmacological model.}},
DOI = {{10.1051/mmnp/201611604}},
ISSN = {{0973-5348}},
EISSN = {{1760-6101}},
Unique-ID = {{ISI:000392117500004}},
}

@article{ ISI:000388737600003,
Author = {Jayapriya, J. and Arock, Michael},
Title = {{Aligning two molecular sequences using genetic operators in grey wolf
   optimiser technique}},
Journal = {{INTERNATIONAL JOURNAL OF DATA MINING AND BIOINFORMATICS}},
Year = {{2016}},
Volume = {{15}},
Number = {{4}},
Pages = {{328-349}},
Abstract = {{Sequence analysis is one of the most important concepts in the domain of
   bioinformatics. Molecular sequence alignment is a predominant problem in
   sequence analysis. In this paper, we proposed a new approach for
   pairwise sequence alignment using a recent meta-heuristic algorithm
   called Grey Wolf Optimiser (GWO) technique in which genetic operators
   are integrated to yield efficient solutions. This algorithm obtains the
   initial set of alignments by inserting gaps randomly and uses the search
   agents in GWO for exploration and exploitation. In addition to this,
   genetic operators like crossover and mutation are applied for faster
   convergence. A novel horizontal crossover and a single-point crossover
   that suits, particularly for sequence alignment problem, are employed in
   this paper. Here, two mutations are used depending upon their threshold
   value. This threshold value depends on a novel fitness function FF,
   which gives maximum matched counts for a new representation of the
   molecular sequences. When the FF of the sequence is less than the
   threshold value, the global gap swap mutation is used and if it is
   greater, then aligned block gap swap mutation is employed. The results
   are compared with the state-of-the-art techniques and statistical
   evaluation done to prove that the proposed algorithm yields better
   solution.}},
DOI = {{10.1504/IJDMB.2016.078151}},
ISSN = {{1748-5673}},
EISSN = {{1748-5681}},
Unique-ID = {{ISI:000388737600003}},
}

@article{ ISI:000384881600096,
Author = {Sun, L. and Xu, J. -C. and Wang, W. and Yin, Y.},
Title = {{Locally linear embedding and neighborhood rough set-based gene selection
   for gene expression data classification}},
Journal = {{GENETICS AND MOLECULAR RESEARCH}},
Year = {{2016}},
Volume = {{15}},
Number = {{3}},
Abstract = {{Cancer subtype recognition and feature selection are important problems
   in the diagnosis and treatment of tumors. Here, we propose a novel gene
   selection approach applied to gene expression data classification.
   First, two classical feature reduction methods including locally linear
   embedding (LLE) and rough set (RS) are summarized. The advantages and
   disadvantages of these algorithms were analyzed and an optimized model
   for tumor gene selection was developed based on LLE and neighborhood RS
   (NRS). Bhattacharyya distance was introduced to delete irrelevant genes,
   pair-wise redundant analysis was performed to remove strongly correlated
   genes, and the wavelet soft threshold was determined to eliminate noise
   in the gene datasets. Next, prior optimized search processing was
   carried out. A new approach combining dimension reduction of LLE and
   feature reduction of NRS (LLE-NRS) was developed for selecting gene
   subsets, and then an open source software Weka was applied to
   distinguish different tumor types and verify the cross-validation
   classification accuracy of our proposed method. The experimental results
   demonstrated that the classification performance of the proposed LLE-NRS
   for selecting gene subset outperforms those of other related models in
   terms of accuracy, and our proposed approach is feasible and effective
   in the field of high-dimensional tumor classification.}},
DOI = {{10.4238/gmr.15038990}},
Article-Number = {{15038990}},
ISSN = {{1676-5680}},
Unique-ID = {{ISI:000384881600096}},
}

@article{ ISI:000383969700001,
Author = {Wong, Ka-Chun},
Title = {{A Novel Approach to Predict Core Residues on Cancer-Related DNA-Binding
   Domains}},
Journal = {{CANCER INFORMATICS}},
Year = {{2016}},
Volume = {{15}},
Number = {{2}},
Pages = {{1-7}},
Abstract = {{Protein-DNA interactions are involved in different cancer pathways. In
   particular, the DNA-binding domains of proteins can determine where and
   how gene regulatory regions are bound in different cell lines at
   different stages. Therefore, it is essential to develop a method to
   predict and locate the core residues on cancer-related DNA-binding
   domains. In this study, we propose a computational method to predict and
   locate core residues on DNA-binding domains. In particular, we have
   selected the cancer-related DNA-binding domains for in-depth studies,
   namely, winged Helix Turn Helix family, homeodomain family, and basic
   Helix-Loop-Helix family. The results demonstrate that the proposed
   method can predict the core residues involved in protein-DNA
   interactions, as verified by the existing structural data. Given its
   good performance, various aspects of the method are discussed and
   explored: for instance, different uses of prediction algorithm,
   different protein domains, and hotspot threshold setting.}},
DOI = {{10.4137/CIN.S39366}},
ISSN = {{1176-9351}},
Unique-ID = {{ISI:000383969700001}},
}

@article{ ISI:000379435400001,
Author = {Ying Ma and Yang Sheng and Tian Ruixia and Chen Xun},
Title = {{Three-Dimensional Visualization of Myocardial Ischemia Based on the
   Standard Twelve-Lead Electrocardiogram}},
Journal = {{COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE}},
Year = {{2016}},
Abstract = {{A novel method was proposed for transforming the ischemic information in
   the 12-lead electrocardiogram(ECG) into the pseudo-color pattern
   displayed on a 3D heart model based on the projection of a ST injury
   vector in this study. The projection of the ST injury vector at a point
   on the heart surface was used for identifying the presence of myocardial
   ischemia by the difference between the projection value and the
   detection threshold. Supposing that myocardial ischemia was uniform and
   continuous, the location and range of myocardial ischemia could be
   accurately calculated and visually displayed in a color-encoding way.
   The diagnoses of the same patient were highly consistent (kappa
   coefficient k = 0.9030) between the proposed method used by ordinary
   people lacking medical knowledge and the standard 12-lead ECG used by
   experienced cardiologists. In addition, the diagnostic accuracy of the
   proposed method was further confirmed by the coronary angiography. The
   results of this study provide a new way to promote the development of
   the 3D visualization of the standard 12-lead ECG, which has a great help
   for inexperienced doctors or ordinary family members in their diagnosis
   of patients with myocardial ischemia.}},
DOI = {{10.1155/2016/7697980}},
Article-Number = {{7697980}},
ISSN = {{1748-670X}},
EISSN = {{1748-6718}},
Unique-ID = {{ISI:000379435400001}},
}

@article{ ISI:000372629700019,
Author = {Joshi, Deepak and Nakamura, Bryson H. and Hahn, Michael E.},
Title = {{A Novel Approach for Toe Off Estimation During Locomotion and
   Transitions on Ramps and Level Ground}},
Journal = {{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}},
Year = {{2016}},
Volume = {{20}},
Number = {{1}},
Pages = {{153-157}},
Month = {{JAN}},
Abstract = {{Identification of the toe off event is critical in many gait
   applications. Accelerometer threshold-based algorithms lack adaptability
   and have not been tested for transitions between locomotion states. We
   describe a new approach for toe off identification using one
   accelerometer in over ground and ramp walking, including transitions.
   The method uses invariant foot acceleration features in the segment of
   gait, where toe off is probable. Wavelet analysis of foot acceleration
   is used to derive a unique feature in a particular frequency band,
   yielding estimated toe off occurrence. We tested the new method for five
   conditions: over ground walking (W), ramp ascending (RA), ramp
   descending (RD); transitions between states (W-RA, W-RD). Mean absolute
   estimation error was 17.4 +/- 12.5, 13.8 +/- 8.5, and 22.0 +/- 16.4 ms
   for steady statesW, RA, and RD, 20.1 +/- 15.5, and 17.1 +/- 13.7 ms for
   transitions W-RA and W-RD, respectively. Algorithm performance was
   equivalent across all pairs of transition and locomotion state except
   between RA and RD (p = 0.03), demonstrating adaptability. The db1
   wavelet outperformed db2 across states and transitions (p < 0.01). The
   presented algorithm is a simple, robust approach for toe off detection.}},
DOI = {{10.1109/JBHI.2014.2377749}},
ISSN = {{2168-2194}},
Unique-ID = {{ISI:000372629700019}},
}

@article{ ISI:000369206200009,
Author = {Kumar, Amit and Singh, Mandeep},
Title = {{Ischemia detection using Isoelectric Energy Function}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2016}},
Volume = {{68}},
Pages = {{76-83}},
Month = {{JAN 1}},
Abstract = {{A novel method has been proposed for the detection of ischemia using an
   isoelectric energy function (IEEF) resulting from ST segment deviations
   in ECG signals. The method consists of five stages: pre-processing,
   delineation, measurement of isoelectric energy, a beat characterization
   algorithm and detection of ischemia. The isoelectric energy threshold is
   used to differentiate ischemic beats from normal beats for ischemic
   episode detection. Then, ischemic episodes are classified as transmural
   or subendocardial. The method is validated for recordings of the
   annotated European ST-T database (EDB). The results show 98.12\% average
   sensitivity (S-E) and 98.16\% average specificity (S-P). These results
   are significantly better than those of existing methods cited in the
   literature. The advantage of the proposed method includes simplicity,
   ruggedness and automatic discarding of noisy beats. (C) 2015 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiomed.2015.11.002}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
Unique-ID = {{ISI:000369206200009}},
}

@article{ ISI:000368501000006,
Author = {Lewis, Mary and Shapland, Fiona and Watts, Rebecca},
Title = {{On the Threshold of Adulthood: A New Approach for the Use of Maturation
   Indicators to Assess Puberty in Adolescents from Medieval England}},
Journal = {{AMERICAN JOURNAL OF HUMAN BIOLOGY}},
Year = {{2016}},
Volume = {{28}},
Number = {{1}},
Pages = {{48-56}},
Month = {{JAN-FEB}},
Abstract = {{Objectives: This study provides the first large scale analysis of the
   age at which adolescents in medieval England entered and completed the
   pubertal growth spurt. This new method has implications for expanding
   our knowledge of adolescent maturation across different time periods and
   regions.
   Methods: In total, 994 adolescent skeletons (10-25 years) from four
   urban sites in medieval England (AD 900-1550) were analyzed for evidence
   of pubertal stage using new osteological techniques developed from the
   clinical literature (i.e., hamate hook development, cervical vertebral
   maturation (CVM), canine mineralization, iliac crest ossification, and
   radial fusion).
   Results: Adolescents began puberty at a similar age to modern children
   at around 10-12 years, but the onset of menarche in girls was delayed by
   up to 3 years, occurring around 15 for most in the study sample and 17
   years for females living in London. Modern European males usually
   complete their maturation by 16-18 years; medieval males took longer
   with the deceleration stage of the growth spurt extending as late as 21
   years.
   Conclusions: This research provides the first attempt to directly assess
   the age of pubertal development in adolescents during the 10th-17th
   centuries. Poor diet, infections, and physical exertion may have
   contributed to delayed development in the medieval adolescents,
   particularly for those living in the city of London. This study sheds
   new light on the nature of adolescence in the medieval period,
   highlighting an extended period of physical and social transition. (C)
   2015 Wiley Periodicals, Inc.}},
DOI = {{10.1002/ajhb.22761}},
ISSN = {{1042-0533}},
EISSN = {{1520-6300}},
ORCID-Numbers = {{Lewis, Mary/0000-0001-6224-0278}},
Unique-ID = {{ISI:000368501000006}},
}

@article{ ISI:000366764500010,
Author = {Hamunyela, Eliakim and Verbesselt, Jan and Herold, Martin},
Title = {{Using spatial context to improve early detection of deforestation from
   Landsat time series}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2016}},
Volume = {{172}},
Pages = {{126-138}},
Month = {{JAN}},
Abstract = {{Mapping deforestation using medium spatial resolution satellite data
   (e.g. Landsat) is increasingly shifting from decadal and annual scales
   to sub-annual scales in recent years, but this shift has brought new
   challenges on how to account for seasonality in the satellite data when
   detecting deforestation. A seasonal model is typically used to account
   for seasonality, but fitting a seasonal model is difficult when there
   are not enough data in the time series. Here, we propose a new approach
   that reduces seasonality in satellite image time series using spatial
   context. With this spatial context approach, each pixel value in the
   image is spatially normalised using the median value calculated from
   neighbouring pixels whose pixel values are above the 90th percentile.
   Using Landsat data, we compared our spatial context approach to a
   seasonal model approach at a humid tropical forest in Brazil and a thy
   tropical forest with strong seasonality in Bolivia. After reducing
   seasonal variations in Landsat data, we detected deforestation from the
   same data using the Breaks For Additive Season and Trend (BFAST) method.
   We show that, in dry tropical forest, deforestation events are detected
   much earlier when the spatial context approach is used to reduce
   seasonal variations in Landsat data than when a seasonal model is used.
   In the dry tropical forest, the median temporal detection delay for
   deforestation from the spatial context approach was two observations,
   seven times shorter than the median temporal detection delay from the
   seasonal model approach (15 observations). In the humid tropical forest,
   the difference in the temporal detection delay between the spatial
   context and seasonal model approach was not significant. The differences
   in overall spatial accuracy between the spatial context and seasonal
   model were also not significant in both dry and humid tropical forests.
   The main benefit for using spatial context is early detection of
   deforestation events in forests with strong seasonality. Therefore, the
   spatial context approach we propose here provides opportunity to monitor
   deforestation events in dry tropical forests at sub-annual scales using
   Landsat data. (C) 2015 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2015.11.006}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Verbesselt, Jan/B-8029-2015
   Herold, Martin/F-8553-2012}},
ORCID-Numbers = {{Verbesselt, Jan/0000-0001-7923-4309
   Herold, Martin/0000-0003-0246-6886}},
Unique-ID = {{ISI:000366764500010}},
}

@article{ ISI:000424479600005,
Author = {Cerqueira, Fabio Ribeiro and Ricardo, Adilson Mendes and Oliveira,
   Alcione de Paiva and Graber, Armin and Baumgartner, Christian},
Title = {{MUMAL2: Improving sensitivity in shotgun proteomics using cost sensitive
   artificial neural networks and a threshold selector algorithm}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2016}},
Volume = {{17}},
Number = {{18}},
Abstract = {{Background: This work presents a machine learning strategy to increase
   sensitivity in tandem mass spectrometry (MS/MS) data analysis for
   peptide/protein identification. MS/MS yields thousands of spectra in a
   single run which are then interpreted by software. Most of these
   computer programs use a protein database to match peptide sequences to
   the observed spectra. The peptide- spectrum matches (PSMs) must also be
   assessed by computational tools since manual evaluation is not
   practicable. The target- decoy database strategy is largely used for
   error estimation in PSM assessment. However, in general, that strategy
   does not account for sensitivity.
   Results: In a previous study, we proposed the method MUMAL that applies
   an artificial neural network to effectively generate a model to classify
   PSMs using decoy hits with increased sensitivity. Nevertheless, the
   present approach shows that the sensitivity can be further improved with
   the use of a cost matrix associated with the learning algorithm. We also
   demonstrate that using a threshold selector algorithm for probability
   adjustment leads to more coherent probability values assigned to the
   PSMs. Our new approach, termed MUMAL2, provides a two- fold contribution
   to shotgun proteomics. First, the increase in the number of correctly
   interpreted spectra in the peptide level augments the chance of
   identifying more proteins. Second, the more appropriate PSM probability
   values that are produced by the threshold selector algorithm impact the
   protein inference stage performed by programs that take probabilities
   into account, such as ProteinProphet. Our experiments demonstrate that
   MUMAL2 reached around 15\% of improvement in sensitivity compared to the
   best current method. Furthermore, the area under the ROC curve obtained
   was 0.93, demonstrating that the probabilities generated by our model
   are in fact appropriate. Finally, Venn diagrams comparing MUMAL2 with
   the best current method show that the number of exclusive peptides found
   by our method was nearly 4- fold higher, which directly impacts the
   proteome coverage.
   Conclusions: The inclusion of a cost matrix and a probability threshold
   selector algorithm to the learning task further improves the
   target-decoy database analysis for identifying peptides, which optimally
   contributes to the challenging task of protein level identification,
   resulting in a powerful computational tool for shotgun proteomics.}},
DOI = {{10.1186/s12859-016-1341-x}},
Article-Number = {{472}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Baumgartner, Christian/0000-0002-3763-5195}},
Unique-ID = {{ISI:000424479600005}},
}

@article{ ISI:000366872300021,
Author = {Pyle, Lacey A. and Hockaday, William C. and Boutton, Thomas and
   Zygourakis, Kyriacos and Kinney, Timothy J. and Masiello, Caroline A.},
Title = {{Chemical and Isotopic Thresholds in Charring: Implications for the
   Interpretation of Charcoal Mass and Isotopic Data}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2015}},
Volume = {{49}},
Number = {{24}},
Pages = {{14057-14064}},
Month = {{DEC 15}},
Abstract = {{Charcoal plays a significant role in the long-term carbon cycle, and its
   use as a soil amendment is promoted as a C sequestration strategy
   (biochar). One challenge in this research area is understanding the
   heterogeneity of charcoal properties. Although the maximum reaction
   temperature is often used as a gauge of pyrolysis conditions, pyrolysis
   duration also changes charcoal physicochemical qualities. Here, we
   introduce a formal definition of charring intensity (CI) to more
   accurately characterize pyrolysis, and we document variation in charcoal
   chemical properties with variation in CI. We find two types of responses
   to CI: either linear or threshold relationships. Mass yield decreases
   linearly with CI, while a threshold exists across which \% C, \% N, and
   delta N-15 exhibit large changes. This CI threshold co-occurs with an
   increase in charcoal aromaticity. C isotopes do not change from original
   biomass values, supporting the use of charcoal delta C-13 signatures to
   infer paleoecological conditions. Fractionation of N isotopes indicates
   that fire may be enriching soils in N-15 through pyrolytic N isotope
   fractionation. This influx of ``black N{''} could have a significant
   impact on soil N isotopes, which we show theoretically using a simple
   mass-balance model.}},
DOI = {{10.1021/acs.est.5b03087}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Boutton, Thomas/C-5821-2016
   Pyle, Lacey/I-2981-2015
   Masiello, Caroline/A-2653-2011
   }},
ORCID-Numbers = {{Boutton, Thomas/0000-0002-7522-5728
   Pyle, Lacey/0000-0001-5928-8609
   Masiello, Caroline/0000-0003-2102-6229
   Zygourakis, Kyriacos/0000-0002-1044-1139
   Hockaday, William/0000-0002-0501-0393}},
Unique-ID = {{ISI:000366872300021}},
}

@article{ ISI:000369761500009,
Author = {Akhmedov, Murodzhon and Catay, Bulent and Apaydin, Mehmet Serkan},
Title = {{Automating unambiguous NOE data usage in NVR for NMR protein
   structure-based assignments}},
Journal = {{JOURNAL OF BIOINFORMATICS AND COMPUTATIONAL BIOLOGY}},
Year = {{2015}},
Volume = {{13}},
Number = {{6, SI}},
Month = {{DEC}},
Abstract = {{Nuclear Magnetic Resonance (NMR) Spectroscopy is an important technique
   that allows determining protein structure in solution. An important
   problem in protein structure determination using NMR spectroscopy is the
   mapping of peaks to corresponding amino acids, also known as the
   assignment problem. Structure-Based Assignment (SBA) is an approach to
   solve this problem using a template structure that is homologous to the
   target. Our previously developed approach Nuclear Vector
   Replacement-Binary Integer Programming (NVR-BIP) computed the optimal
   solution for small proteins, but was unable to solve the assignments of
   large proteins. NVR-Ant Colony Optimization (ACO) extended the
   applicability of the NVR approach for such proteins. One of the input
   data utilized in these approaches is the Nuclear Overhauser Effect (NOE)
   data. NOE is an interaction observed between two protons if the protons
   are located close in space. These protons could be amide protons,
   protons attached to the alpha-carbon atom in the backbone of the
   protein, or side chain protons. NVR only uses backbone protons. In this
   paper, we reformulate the NVR-BIP model to distinguish the type of
   proton in NOE data and use the corresponding proton coordinates in the
   extended formulation. In addition, the threshold value over interproton
   distances is set in a standard manner for all proteins by extracting the
   NOE upper bound distance information from the data. We also convert NOE
   intensities into distance thresholds. Our new approach thus handles the
   NOE data correctly and without manually determined parameters. We
   accordingly adapt NVR-ACO solution methodology to these changes.
   Computational results show that our approaches obtain optimal solutions
   for small proteins. For the large proteins our ant colony
   optimization-based approach obtains promising results.}},
DOI = {{10.1142/S0219720015500201}},
Article-Number = {{1550020}},
ISSN = {{0219-7200}},
EISSN = {{1757-6334}},
Unique-ID = {{ISI:000369761500009}},
}

@article{ ISI:000368564700053,
Author = {Zhao, D. X. and Ma, Z. and Zhang, D. G. and Li, W. B. and Liu, S.},
Title = {{A New Method of Bio-Medical Image Fusion with Multi-Scale Edge}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2015}},
Volume = {{5}},
Number = {{8}},
Pages = {{1896-1901}},
Month = {{DEC}},
Abstract = {{Discrete wavelet transform (DWT) provides a fast, local, sparse
   multi-resolution analysis of signals. It can reconstruct high quality
   images. By comparison, we find the shortcoming of current bio-medical
   image fusion method, and a new method for bio-medical image fusion based
   on combination of graph formed by its multi-scale edge of DWT has been
   presented in this paper. Dyadic wavelet transform is used to generate
   different scale images, fusion is made with directed graph of multiple
   scanned images, in the processing, it leverages the inherent de-noising
   properties of wavelet transform under the control of hard threshold
   function and soft threshold function, thus good edge detection can also
   be obtained in the multi-scale domain, which subsequently are used for
   graph generation and thus fusion. As an example, the CT\&MR fusion steps
   of the process have been shown. By many experiments and application, the
   correctness of our method has been tested. This method is valid for
   application of high belief degree of image process fields for
   bio-medical decision and analysis.}},
DOI = {{10.1166/jmihi.2015.1666}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000368564700053}},
}

@article{ ISI:000368521900052,
Author = {Buder, Thomas and Deutsch, Andreas and Klink, Barbara and Voss-Boehme,
   Anja},
Title = {{Model-Based Evaluation of Spontaneous Tumor Regression in Pilocytic
   Astrocytoma}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2015}},
Volume = {{11}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Pilocytic astrocytoma (PA) is the most common brain tumor in children.
   This tumor is usually benign and has a good prognosis. Total resection
   is the treatment of choice and will cure the majority of patients.
   However, often only partial resection is possible due to the location of
   the tumor. In that case, spontaneous regression, regrowth, or
   progression to a more aggressive form have been observed. The dependency
   between the residual tumor size and spontaneous regression is not
   understood yet. Therefore, the prognosis is largely unpredictable and
   there is controversy regarding the management of patients for whom
   complete resection cannot be achieved. Strategies span from pure
   observation (wait and see) to combinations of surgery, adjuvant
   chemotherapy, and radiotherapy. Here, we introduce a mathematical model
   to investigate the growth and progression behavior of PA. In particular,
   we propose a MARKOV chain model incorporating cell proliferation and
   death as well as mutations. Our model analysis shows that the tumor
   behavior after partial resection is essentially determined by a risk
   coefficient., which can be deduced from epidemiological data about PA.
   Our results quantitatively predict the regression probability of a
   partially resected benign PA given the residual tumor size and lead to
   the hypothesis that this dependency is linear, implying that removing
   any amount of tumor mass will improve prognosis. This finding stands in
   contrast to diffuse malignant glioma where an extent of resection
   threshold has been experimentally shown, below which no benefit for
   survival is expected. These results have important implications for
   future therapeutic studies in PA that should include residual tumor
   volume as a prognostic factor.}},
DOI = {{10.1371/journal.pcbi.1004662}},
Article-Number = {{e1004662}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
ORCID-Numbers = {{Voss-Bohme, Anja/0000-0001-6510-8694}},
Unique-ID = {{ISI:000368521900052}},
}

@article{ ISI:000366637300078,
Author = {Liu, Geng and Niu, Junjie and Zhang, Chao and Guo, Guanlin},
Title = {{Accuracy and uncertainty analysis of soil Bbf spatial distribution
   estimation at a coking plant-contaminated site based on normalization
   geostatistical technologies}},
Journal = {{ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH}},
Year = {{2015}},
Volume = {{22}},
Number = {{24}},
Pages = {{20121-20130}},
Month = {{DEC}},
Abstract = {{Data distribution is usually skewed severely by the presence of hot
   spots in contaminated sites. This causes difficulties for accurate
   geostatistical data transformation. Three types of typical normal
   distribution transformation methods termed the normal score, Johnson,
   and Box-Cox transformations were applied to compare the effects of
   spatial interpolation with normal distribution transformation data of
   benzo(b)fluoranthene in a large-scale coking plant-contaminated site in
   north China. Three normal transformation methods decreased the skewness
   and kurtosis of the benzo(b)fluoranthene, and all the transformed data
   passed the Kolmogorov-Smirnov test threshold. Cross validation showed
   that Johnson ordinary kriging has a minimum root-mean-square error of
   1.17 and a mean error of 0.19, which was more accurate than the other
   two models. The area with fewer sampling points and that with high
   levels of contamination showed the largest prediction standard errors
   based on the Johnson ordinary kriging prediction map. We introduce an
   ideal normal transformation method prior to geostatistical estimation
   for severely skewed data, which enhances the reliability of risk
   estimation and improves the accuracy for determination of remediation
   boundaries.}},
DOI = {{10.1007/s11356-015-5122-2}},
ISSN = {{0944-1344}},
EISSN = {{1614-7499}},
Unique-ID = {{ISI:000366637300078}},
}

@article{ ISI:000366379000008,
Author = {Thibaud, Emeric and Opitz, Thomas},
Title = {{Efficient inference and simulation for elliptical Pareto processes}},
Journal = {{BIOMETRIKA}},
Year = {{2015}},
Volume = {{102}},
Number = {{4}},
Pages = {{855-870}},
Month = {{DEC}},
Abstract = {{Recent advances in extreme value theory have established l-Pareto
   processes as the natural limits for extreme events defined in terms of
   exceedances of a risk functional. In this paper we provide methods for
   the practical modelling of data based on a tractable yet flexible
   dependence model. We introduce the class of elliptical l-Pareto
   processes, which arise as the limits of threshold exceedances of certain
   elliptical processes characterized by a correlation function and a shape
   parameter. An efficient inference method based on maximizing a full
   likelihood with partial censoring is developed. Novel procedures for
   exact conditional and unconditional simulation are proposed. These ideas
   are illustrated using precipitation extremes in Switzerland.}},
DOI = {{10.1093/biomet/asv045}},
ISSN = {{0006-3444}},
EISSN = {{1464-3510}},
ORCID-Numbers = {{Thibaud, Emeric/0000-0002-1899-3990}},
Unique-ID = {{ISI:000366379000008}},
}

@article{ ISI:000364621200017,
Author = {Xu, Zhen-Zhong and Kim, Yong Ho and Bang, Sangsu and Zhang, Yi and
   Berta, Temugin and Wang, Fan and Oh, Seog Bae and Ji, Ru-Rong},
Title = {{Inhibition of mechanical allodynia in neuropathic pain by TLR5-mediated
   A-fiber blockade}},
Journal = {{NATURE MEDICINE}},
Year = {{2015}},
Volume = {{21}},
Number = {{11}},
Pages = {{1326-1331}},
Month = {{NOV}},
Abstract = {{Mechanical allodynia, induced by normally innocuous low-threshold
   mechanical stimulation, represents a cardinal feature of neuropathic
   pain. Blockade or ablation of high-threshold, small-diameter
   unmyelinated group C nerve fibers (C-fibers) has limited effects on
   mechanical allodynia(1-4). Although large, myelinated group A fibers, in
   particular A beta-fibers, have previously been implicated in mechanical
   allodynia(5-7), an A-fiber-selective pharmacological blocker is still
   lacking. Here we report a new method for targeted silencing of A-fibers
   in neuropathic pain. We found that Toll-like receptor 5 (TLR5) is
   co-expressed with neurofilament-200 in large-diameter A-fiber neurons in
   the dorsal root ganglion (DRG). Activation of TLR5 with its ligand
   flagellin results in neuronal entry of the membrane-impermeable
   lidocaine derivative QX-314, leading to TLR5-dependent blockade of
   sodium currents, predominantly in A-fiber neurons of mouse DRGs.
   Intraplantar co-application of flagellin and QX-314 (flagellin/QX-314)
   dose-dependently suppresses mechanical allodynia after chemotherapy,
   nerve injury, and diabetic neuropathy, but this blockade is abrogated in
   Tlr5-deficient mice. In vivo electrophysiology demonstrated that
   co-application of flagellin/QX-314 selectively suppressed A beta-fiber
   conduction in naive and chemotherapy-treated mice. TLR5-mediated A
   beta-fiber blockade, but not capsaicin-mediated C-fiber blockade, also
   reduced chemotherapy-induced ongoing pain without impairing motor
   function. Finally, flagellin/QX-314 co-application suppressed sodium
   currents in large-diameter human DRG neurons. Thus, our findings provide
   a new tool for targeted silencing of A beta-fibers and neuropathic pain
   treatment.}},
DOI = {{10.1038/nm.3978}},
ISSN = {{1078-8956}},
EISSN = {{1546-170X}},
ResearcherID-Numbers = {{bang, sangsu/U-4313-2017
   Berta, Temugin/A-6908-2016
   }},
ORCID-Numbers = {{bang, sangsu/0000-0002-9092-0075
   Berta, Temugin/0000-0002-4486-8288
   Ji, Ru-Rong/0000-0002-9355-3688}},
Unique-ID = {{ISI:000364621200017}},
}

@article{ ISI:000362397300001,
Author = {Kryven, Ivan and Roeblitz, Susanna and Schuette, Christof},
Title = {{Solution of the chemical master equation by radial basis functions
   approximation with interface tracking}},
Journal = {{BMC SYSTEMS BIOLOGY}},
Year = {{2015}},
Volume = {{9}},
Month = {{OCT 8}},
Abstract = {{Background: The chemical master equation is the fundamental equation of
   stochastic chemical kinetics. This differential-difference equation
   describes temporal evolution of the probability density function for
   states of a chemical system. A state of the system, usually encoded as a
   vector, represents the number of entities or copy numbers of interacting
   species, which are changing according to a list of possible reactions.
   It is often the case, especially when the state vector is
   high-dimensional, that the number of possible states the system may
   occupy is too large to be handled computationally. One way to get around
   this problem is to consider only those states that are associated with
   probabilities that are greater than a certain threshold level.
   Results: We introduce an algorithm that significantly reduces
   computational resources and is especially powerful when dealing with
   multi-modal distributions. The algorithm is built according to two key
   principles. Firstly, when performing time integration, the algorithm
   keeps track of the subset of states with significant probabilities
   (essential support). Secondly, the probability distribution that solves
   the equation is parametrised with a small number of coefficients using
   collocation on Gaussian radial basis functions. The system of basis
   functions is chosen in such a way that the solution is approximated only
   on the essential support instead of the whole state space.
   Discussion: In order to demonstrate the effectiveness of the method, we
   consider four application examples: a) the self-regulating gene model,
   b) the 2-dimensional bistable toggle switch, c) a generalisation of the
   bistable switch to a 3-dimensional tristable problem, and d) a
   3-dimensional cell differentiation model that, depending on parameter
   values, may operate in bistable or tristable modes. In all
   multidimensional examples the manifold containing the system states with
   significant probabilities undergoes drastic transformations over time.
   This fact makes the examples especially challenging for numerical
   methods.
   Conclusions: The proposed method is a new numerical approach permitting
   to approximately solve a wide range of problems that have been hard to
   tackle until now. A full representation of multi-dimensional
   distributions is recovered. The method is especially attractive when
   dealing with models that yield solutions of a complex structure, for
   instance, featuring multi-stability.}},
DOI = {{10.1186/s12918-015-0210-y}},
Article-Number = {{67}},
ISSN = {{1752-0509}},
ORCID-Numbers = {{Roblitz, Susanna/0000-0002-2735-0030
   Kryven, Ivan/0000-0002-3964-2196}},
Unique-ID = {{ISI:000362397300001}},
}

@article{ ISI:000359505200001,
Author = {Liu, L. J. and Schlesinger, M.},
Title = {{Interstitial hydraulic conductivity and interstitial fluid pressure for
   avascular or poorly vascularized tumors}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2015}},
Volume = {{380}},
Pages = {{1-8}},
Month = {{SEP 7}},
Abstract = {{A correct description of the hydraulic conductivity is essential for
   determining the actual tumor interstitial fluid pressure (TIFF)
   distribution. Traditionally, it has been assumed that the hydraulic
   conductivities both in a tumor and normal tissue are constant, and that
   a tumor has a much larger interstitial hydraulic conductivity than
   normal tissue. The abrupt transition of the hydraulic conductivity at
   the tumor surface leads to non-physical results (the hydraulic
   conductivity and the slope of the TIFP are not continuous at tumor
   surface). For the sake of simplicity and the need to represent reality,
   we focus our analysis on avascular or poorly vascularized tumors, which
   have a necrosis that is mostly in the center and vascularization that is
   mostly on the periphery. We suggest that there is an intermediary region
   between the tumor surface and normal tissue. Through this region, the
   interstitium (including the structure and composition of solid
   components and interstitial fluid) transitions from tumor to normal
   tissue. This process also causes the hydraulic conductivity to do the
   same. We introduce a continuous variation of the hydraulic conductivity,
   and show that the interstitial hydraulic conductivity in the
   intermediary region should be monotonically increasing up to the value
   of hydraulic conductivity in the normal tissue in order for the model to
   correspond to the actual TIFP distribution. The value of the hydraulic
   conductivity at the tumor surface should be the lowest in value. (C)
   2015 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2015.05.012}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
ORCID-Numbers = {{Liu, Longjian/0000-0002-0493-9272}},
Unique-ID = {{ISI:000359505200001}},
}

@article{ ISI:000366042200002,
Author = {Ozel, Gamze and Cakmakyapan, Selen},
Title = {{A new approach to the prediction of PM10 concentrations in Central
   Anatolia Region, Turkey}},
Journal = {{ATMOSPHERIC POLLUTION RESEARCH}},
Year = {{2015}},
Volume = {{6}},
Number = {{5}},
Pages = {{735-741}},
Month = {{SEP}},
Abstract = {{Environmental pollution control is one of the most important goals in
   pollution risk assessment. In this sense, modern and precise tools that
   allow scientists to quantify and predict air pollution are of particular
   interest. In this study, we describe an air quality evaluation in terms
   of particulate matter having a diameter < 10 mu m (PM10), using a
   gamma-Poisson process. PM10 measurements from 2007 to 2013 at 24 air
   quality monitoring stations of Ministry of Environment and Urbanization
   (Turkey), was used to predict air pollution levels in Central Anatolia
   Region of Turkey. We assume that the number of PM10 peaks follows a
   Poisson process and PM10 amount of each peak fits a gamma distribution.
   The findings indicate that daily average of number for the threshold
   exceedance of PM10 concentration is 1.3 for all monitoring stations. The
   probability of exceedance the PM10 concentration threshold once a day is
   0.35 for all monitoring stations. The results also show that the
   probabilities of total PM10 concentrations exceeding threshold are
   rapidly decreasing after 158 mu g/m(3) which will occur in one day. The
   average of daily total PM10 concentration is 148 mu g/m(3), the average
   of monthly total PM10 concentration is 4 437 mu g/m(3), and the average
   of yearly total PM10 concentrations is 53 984 mu g/m(3). It is found
   that there is a moderate correlation between the number of threshold
   exceedances and the total PM10 concentrations in Central Anatolia
   Region. Thus, it was concluded that gamma-Poisson process could be
   promising for air pollutant prediction for a given period of time.}},
DOI = {{10.5094/APR.2015.082}},
ISSN = {{1309-1042}},
Unique-ID = {{ISI:000366042200002}},
}

@article{ ISI:000361527000007,
Author = {Li, Chenxi and Dowling, N. Maritza and Chappell, Rick},
Title = {{Quantile Regression with a Change-Point Model for Longitudinal Data: An
   Application to the Study of Cognitive Changes in Preclinical Alzheimer's
   Disease}},
Journal = {{BIOMETRICS}},
Year = {{2015}},
Volume = {{71}},
Number = {{3}},
Pages = {{625-635}},
Month = {{SEP}},
Abstract = {{Progressive and insidious cognitive decline that interferes with daily
   life is the defining characteristic of Alzheimer's disease (AD).
   Epidemiological studies have found that the pathological process of AD
   begins years before a clinical diagnosis is made and can be highly
   variable within a given population. Characterizing cognitive decline in
   the preclinical phase of AD is critical for the development of early
   intervention strategies when disease-modifying therapies may be most
   effective. In the last decade, there has been an increased interest in
   the application of change-point models to longitudinal cognitive
   outcomes prior to and after diagnosis. Most of the proposed statistical
   methodology for describing decline relies upon distributional
   assumptions that may not hold. In this article, we introduce a quantile
   regression with a change-point model for longitudinal data of cognitive
   function in persons bound to develop AD. A change-point in our model
   reflects the transition from the cognitive decline due to normal aging
   to the accelerated decline due to disease progression. Quantile
   regression avoids common distributional assumptions on cognitive
   outcomes and allows the covariate effects and the change-point to vary
   for different quantiles of the response. We provided an approach for
   estimating the model parameters, including the change-point, and
   presented inferential procedures based on the asymptotic properties of
   the estimators. A simulation study showed that the estimation and
   inferential procedures perform reasonably well in finite samples. The
   practical use of our model was illustrated by an application to
   longitudinal episodic memory outcomes from two cohort studies of aging
   and AD.}},
DOI = {{10.1111/biom.12313}},
ISSN = {{0006-341X}},
EISSN = {{1541-0420}},
Unique-ID = {{ISI:000361527000007}},
}

@article{ ISI:000357548300002,
Author = {Lechner, Alex M. and Doerr, Veronica and Harris, Rebecca M. B. and
   Doerr, Erik and Lefroy, Edward C.},
Title = {{A framework for incorporating fine-scale dispersal behaviour into
   biodiversity conservation planning}},
Journal = {{LANDSCAPE AND URBAN PLANNING}},
Year = {{2015}},
Volume = {{141}},
Pages = {{11-23}},
Month = {{SEP}},
Abstract = {{Fine-scale landscape features such as scattered trees are increasingly
   thought to be critical for dispersal, and need to be considered in
   connectivity modelling and planning. Yet existing modelling approaches
   struggle to adequately take fine-scale features and threshold dynamics
   of dispersal behaviour into account, in part because of computational
   limitations. We present a framework for modelling connectivity at fine
   spatial resolutions over large spatial extents. Our framework involves a
   novel approach to characterising fine-scale dispersal behaviour within
   the context of existing modelling methods, and uses key parameters of
   dispersal behaviour to link models and their interpretation at multiple
   scales. We address computational limitations by creating a gap-crossing
   threshold layer, which identifies areas where dispersal is possible
   because of the presence and spacing of fine-scale connectivity elements.
   This layer is combined with a dispersal-cost layer within a
   graph-network analysis to identify the optimal least-cost path between
   patches. Graph metrics are used to assess the importance of specific
   patches at the regional-scale and to describe connectivity for the whole
   landscape. A local-scale connectivity model using the Circuitscape
   software complements the regional analysis outputs by considering all
   possible pathways across a landscape simultaneously rather than a single
   least-cost path. The framework was designed specifically to be applied
   by land use planners who need to quantify the impacts of property
   development on fine-scale connectivity, yet need to assess implications
   at the regional scale. We demonstrate the framework by applying it in
   the Lower Hunter region, Australia. (C) 2015 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.landurbplan.2015.04.008}},
ISSN = {{0169-2046}},
EISSN = {{1872-6062}},
ResearcherID-Numbers = {{Doerr, Veronica/A-2150-2011
   Doerr, Erik/A-9797-2011
   }},
ORCID-Numbers = {{Doerr, Veronica/0000-0003-4330-4576
   Doerr, Erik/0000-0003-4511-9972
   Harris, Rebecca/0000-0002-6426-2179
   Lechner, Alex/0000-0003-2050-9480
   Lefroy, Edward/0000-0002-3164-8948}},
Unique-ID = {{ISI:000357548300002}},
}

@article{ ISI:000358625600033,
Author = {Duan, Lijuan and Xu, Yanhui and Yang, Zhen and Ma, Wei and Powers, David},
Title = {{Transition Detection and Sample Purification for EEG Based Brain
   Computer Interface Classification}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2015}},
Volume = {{5}},
Number = {{4}},
Pages = {{871-875}},
Month = {{AUG}},
Abstract = {{This paper develops a novel method combining transition detection with
   the sample purification to filter noises in the raw EEG signal data,
   which helps to improve the precision of EEG based motor imagery
   classification. Note that the EEG samples belonging to the same class
   are time sequences across multiple electrodes, and these signals are in
   varying degrees contaminated by noise and artifact while also attention
   lapses by subjects during data acquisition. To overcome this problem,
   firstly, the transitions of EEG signals, the Euclidean distances between
   adjacent samples are larger than a given threshold, are extracted. Next,
   the sample purification is performed to filter the between-class noises
   based on the statistics of EEG signal. Finally, the purified EEG signals
   are treated as the input to the classifiers for BC! classification.
   Experimental results show that the proposed method is effective for the
   BCI competition III data (Data Set V), beating the winner.}},
DOI = {{10.1166/jmihi.2015.1472}},
ISSN = {{2156-7018}},
EISSN = {{2156-7026}},
Unique-ID = {{ISI:000358625600033}},
}

@article{ ISI:000358336700001,
Author = {Xia, Kelin and Wei, Guo-Wei},
Title = {{Persistent topology for cryo-EM data analysis}},
Journal = {{INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING}},
Year = {{2015}},
Volume = {{31}},
Number = {{8}},
Month = {{AUG}},
Abstract = {{In this work, we introduce persistent homology for the analysis of
   cryo-electron microscopy (cryo-EM) density maps. We identify the
   topological fingerprint or topological signature of noise, which is
   widespread in cryo-EM data. For low signal-to-noise ratio (SNR)
   volumetric data, intrinsic topological features of biomolecular
   structures are indistinguishable from noise. To remove noise, we employ
   geometric flows that are found to preserve the intrinsic topological
   fingerprints of cryo-EM structures and diminish the topological
   signature of noise. In particular, persistent homology enables us to
   visualize the gradual separation of the topological fingerprints of
   cryo-EM structures from those of noise during the denoising process,
   which gives rise to a practical procedure for prescribing a noise
   threshold to extract cryo-EM structure information from noise
   contaminated data after certain iterations of the geometric flow
   equation. To further demonstrate the utility of persistent homology for
   cryo-EM data analysis, we consider a microtubule intermediate structure
   Electron Microscopy Data (EMD 1129). Three helix models, an
   alpha-tubulin monomer model, an alpha-tubulin and beta-tubulin model,
   and an alpha-tubulin and beta-tubulin dimer model, are constructed to
   fit the cryo-EM data. The least square fitting leads to similarly high
   correlation coefficients, which indicates that structure determination
   via optimization is an ill-posed inverse problem. However, these models
   have dramatically different topological fingerprints. Especially,
   linkages or connectivities that discriminate one model from another,
   play little role in the traditional density fitting or optimization but
   are very sensitive and crucial to topological fingerprints. The
   intrinsic topological features of the microtubule data are identified
   after topological denoising. By a comparison of the topological
   fingerprints of the original data and those of three models, we found
   that the third model is topologically favored. The present work offers
   persistent homology based new strategies for topological denoising and
   for resolving ill-posed inverse problems. Copyright (c) 2015John Wiley
   \& Sons, Ltd.}},
DOI = {{10.1002/cnm.2719}},
Article-Number = {{e02719}},
ISSN = {{2040-7939}},
EISSN = {{2040-7947}},
ResearcherID-Numbers = {{XIA, KELIN/I-7880-2016}},
ORCID-Numbers = {{XIA, KELIN/0000-0003-4183-0943}},
Unique-ID = {{ISI:000358336700001}},
}

@article{ ISI:000360180000001,
Author = {Oster, Andrew and Faure, Philippe and Gutkin, Boris S.},
Title = {{Mechanisms for multiple activity modes of VTA dopamine neurons}},
Journal = {{FRONTIERS IN COMPUTATIONAL NEUROSCIENCE}},
Year = {{2015}},
Volume = {{9}},
Month = {{JUL 28}},
Abstract = {{Midbrain ventral segmental area (VIA) dopaminergic neurons send numerous
   projections to cortical and sub-cortical areas, and diffusely release
   dopamine (DA) to their targets. DA neurons display a range of activity
   modes that vary in frequency and degree of burst firing. Importantly, DA
   neuronal bursting is associated with a significantly greater degree of
   DA release than an equivalent tonic activity pattern. Here, we introduce
   a single compartmental, conductance-based computational model for DA
   cell activity that captures the behavior of DA neuronal dynamics and
   examine the multiple factors that underlie DA firing modes: the strength
   of the SK conductance, the amount of drive, and GABA inhibition. Our
   results suggest that neurons with low SK conductance fire in a fast
   firing mode, are correlated with burst firing, and require higher levels
   of applied current before undergoing depolarization block. We go on to
   consider the role of GABAergic inhibition on an ensemble of dynamical
   classes of DA neurons and find that strong GABA inhibition suppresses
   burst firing. Our studies suggest differences in the distribution of the
   SK conductance and GABA inhibition levels may indicate subclasses of DA
   neurons within the VIA. We further identify, that by considering
   alternate potassium dynamics, the dynamics display burst patterns that
   terminate via depolarization block, akin to those observed in vivo in
   VIA DA neurons and in substantia nigra pars compacta (SNc) DA cell
   preparations under apamin application. In addition, we consider the
   generation of transient burst firing events that are NMDA-initiated or
   elicited by a sudden decrease of GABA inhibition, that is,
   disinhibition.}},
DOI = {{10.3389/fncom.2015.00095}},
Article-Number = {{95}},
ISSN = {{1662-5188}},
ORCID-Numbers = {{Faure, Philippe/0000-0003-3573-4971
   Gutkin, Boris/0000-0001-6409-979X}},
Unique-ID = {{ISI:000360180000001}},
}

@article{ ISI:000355672400009,
Author = {Sattar, Abdus and Sinha, Sanjoy K. and Wang, Xiao-Feng and Li, Yehua},
Title = {{Frailty models for pneumonia to death with a left-censored covariate}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2015}},
Volume = {{34}},
Number = {{14}},
Pages = {{2266-2280}},
Month = {{JUN 30}},
Abstract = {{Frailty models are multiplicative hazard models for studying association
   between survival time and important clinical covariates. When some
   values of a clinical covariate are unobserved but known to be below a
   threshold called the limit of detection (LOD), naive approaches ignoring
   this problem, such as replacing the undetected value by the LOD or half
   of the LOD, often produce biased parameter estimate with larger mean
   squared error of the estimate. To address the LOD problem in a frailty
   model, we propose a flexible smooth nonparametric density estimator
   along with Simpson's numerical integration technique. This is an
   extension of an existing method in the likelihood framework for the
   estimation and inference of the model parameters. The proposed new
   method shows the estimators are asymptotically unbiased and gives
   smaller mean squared error of the estimates. Compared with the existing
   method, the proposed new method does not require distributional
   assumptions for the underlying covariates. Simulation studies were
   conducted to evaluate the performance of the new method in realistic
   scenarios. We illustrate the use of the proposed method with a data set
   from Genetic and Inflammatory Markers of Sepsis study in which
   interlekuin-10 was subject to LOD. Copyright (c) 2015John Wiley \& Sons,
   Ltd.}},
DOI = {{10.1002/sim.6466}},
ISSN = {{0277-6715}},
EISSN = {{1097-0258}},
Unique-ID = {{ISI:000355672400009}},
}

@article{ ISI:000356930100021,
Author = {Monteil, Arnaud and Chausson, Patrick and Boutourlinsky, Katia and
   Mezghrani, Alexandre and Huc-Brandt, Sylvaine and Blesneac, Iulia and
   Bidaud, Isabelle and Lemmers, Celine and Leresche, Nathalie and Lambert,
   Regis C. and Lory, Philippe},
Title = {{Inhibition of Cav3.2 T-type Calcium Channels by Its Intracellular I-II
   Loop}},
Journal = {{JOURNAL OF BIOLOGICAL CHEMISTRY}},
Year = {{2015}},
Volume = {{290}},
Number = {{26}},
Pages = {{16168-16176}},
Month = {{JUN 26}},
Abstract = {{Background: Novel strategies are needed to characterize the properties
   of T-type (Cav3) calcium channel isoforms. Results: The I-II loop of the
   Cav3.2 protein potently inhibits both recombinant and neuronal Cav3.1
   and Cav3.2 channels. Conclusion: This I-II loop region can be used to
   selectively silence Cav3.1/Cav3.2 channels. Significance: This study
   reveals a new approach to differentiate among the activity of native
   Cav3 channels.
   Voltage-dependent calcium channels (Cav) of the T-type family (Cav3.1,
   Cav3.2, and Cav3.3) are activated by low threshold membrane
   depolarization and contribute greatly to neuronal network excitability.
   Enhanced T-type channel activity, especially Cav3.2, contributes to
   disease states, including absence epilepsy. Interestingly, the
   intracellular loop connecting domains I and II (I-II loop) of Cav3.2
   channels is implicated in the control of both surface expression and
   channel gating, indicating that this I-II loop plays an important
   regulatory role in T-type current. Here we describe that co-expression
   of this I-II loop or its proximal region (1-Cav3.2; Ser(423)-Pro(542))
   together with recombinant full-length Cav3.2 channel inhibited T-type
   current without affecting channel expression and membrane incorporation.
   Similar T-type current inhibition was obtained in NG 108-15
   neuroblastoma cells that constitutively express Cav3.2 channels. Of
   interest, 1-Cav3.2 inhibited both Cav3.2 and Cav3.1 but not Cav3.3
   currents. Efficacy of 1-Cav3.2 to inhibit native T-type channels was
   assessed in thalamic neurons using viral transduction. We describe that
   T-type current was significantly inhibited in the ventrobasal neurons
   that express Cav3.1, whereas in nucleus reticularis thalami neurons that
   express Cav3.2 and Cav3.3 channels, only the fast inactivating T-type
   current (Cav3.2 component) was significantly inhibited. Altogether,
   these data describe a new strategy to differentially inhibit Cav3
   isoforms of the T-type calcium channels.}},
DOI = {{10.1074/jbc.M114.634261}},
ISSN = {{0021-9258}},
EISSN = {{1083-351X}},
ORCID-Numbers = {{Leresche, Nathalie/0000-0001-6705-9769
   Monteil, Arnaud/0000-0003-1632-7318}},
Unique-ID = {{ISI:000356930100021}},
}

@article{ ISI:000356233300022,
Author = {Verma, Sunita and Prakash, Divya and Ricaud, Philippe and Payra, Swagata
   and Attie, Jean-Luc and Soni, Manish},
Title = {{A New Classification of Aerosol Sources and Types as Measured over
   Jaipur, India}},
Journal = {{AEROSOL AND AIR QUALITY RESEARCH}},
Year = {{2015}},
Volume = {{15}},
Number = {{3}},
Pages = {{985-993}},
Month = {{JUN}},
Abstract = {{The aerosol properties retrieved from the AErosol RObotic NETwork
   (AERONET) measurements during the period 2009 to 2012 over Jaipur (26.9
   degrees N, 75.8 degrees E, 450m asl) in Northwestern India are used for
   the first time to identify the types of aerosols. In order to consider
   the appropriate threshold of aerosol optical thickness (tau) at 500 nm
   (tau(500)) and Angstrom exponent (a) in the spectral band 440-870 nm, a
   novel approach has been conducted and applied for the identification
   process. Five prevailing aerosol classes are identified: desert dust,
   biomass, maritime, arid background and mixed aerosols. Arid background
   and desert dust type aerosols are the most common at Jaipur (34.7\% and
   13.6\%, respectively), with a wide variability in both t and a. In about
   8.4\% of the cases, aerosols can be classified as maritime, although
   mixing with other aerosols (33.6\%) is substantial. The ground-based
   spectral optical thickness and the refractive index estimated at visible
   and near-infrared wavelengths are used to account for the type of
   atmospheric aerosols. They are compared with four more AERONET sites
   located in India based upon their geographical distribution and
   extensive data availability. Simultaneously, single scattering albedo of
   dust is also inferred for all the available AERONET sites for the same
   period over India. The comparison results suggest that Jaipur arid
   background is more scattering in nature than Northern and Western
   regions in India. Finally, the absorption is less in summer than in
   winter over the Jaipur site.}},
DOI = {{10.4209/aaqr.2014.07.0143}},
ISSN = {{1680-8584}},
EISSN = {{2071-1409}},
ORCID-Numbers = {{Verma, Sunita/0000-0002-5514-6360
   Soni, Manish/0000-0002-9443-0284}},
Unique-ID = {{ISI:000356233300022}},
}

@article{ ISI:000356108400015,
Author = {Gomez-Tames, Jose and Fukuhara, Yuto and He, Siyu and Saito, Kazuyuki
   and Ito, Koichi and Yu, Wenwei},
Title = {{A human-phantom coupling experiment and a dispersive simulation model
   for investigating the variation of dielectric properties of biological
   tissues}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2015}},
Volume = {{61}},
Pages = {{144-149}},
Month = {{JUN 1}},
Abstract = {{Variation of the dielectric properties of tissues could happen due to
   aging, moisture of the skin, muscle denervation, and variation of blood
   flow by temperature. Several studies used burst-modulated alternating
   stimulation to improve activation and comfort by reducing tissue
   impedance as a possible mechanism to generate muscle activation with
   less energy. The study of the effect of dielectric properties of
   biological tissues in nerve activation presents a fundamental problem,
   which is the difficulty of systematically changing the morphological
   factors and dielectric properties of the subjects under study. We tackle
   this problem by using a simulation and an experimental study. The
   experimental study is a novel method that combines a fat
   tissue-equivalent phantom, with known and adjustable dielectric
   properties, with the human thigh. In this way, the dispersion of the
   tissue under study could be modified to observe its effects
   systematically in muscle activation. We observed that, to generate a
   given amount of muscle or nerve activation under conditions of decreased
   impedance, the magnitude of the current needs to be increased while the
   magnitude of the voltage needs to be decreased. (C) 2015 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.compbiomed.2015.03.029}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
Unique-ID = {{ISI:000356108400015}},
}

@article{ ISI:000355664600003,
Author = {Shi, Tailong and Kim, Hyun June and Murry, Thomas and Woo, Peak and Yan,
   Yuling},
Title = {{Tracing vocal fold vibrations using level set segmentation method}},
Journal = {{INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING}},
Year = {{2015}},
Volume = {{31}},
Number = {{6}},
Month = {{JUN}},
Abstract = {{High-speed digital imaging (HSDI) of the larynx can provide important
   information on the vocal fold kinematics. This information is useful and
   may provide a better understanding of the mechanism of phonation and
   assist clinical assessment of voice disorders. Automatic tracing of the
   vocal fold vibration is a key step in the kinematic analysis and for
   correlative characterization of vocal fold vibrations and voice quality
   in normal and diseased states. In this study, we introduce a new
   approach for image segmentation and automatic tracing of vocal fold
   motion that combines the level set method and motion cue. This approach
   is applied to videokymogram (VKG)-form images, which are obtained from a
   sequence of laryngeal images captured using the HSDI. To utilize the
   motion cue for a more effective level set based segmentation on the VKG,
   we first construct a so-called standard deviation (STD) image by mapping
   the pixel-based measure of temporal intensity dispersion from the
   initial HSDI sequence. The STD image maps the extent of vocal fold
   motion, and followed by threshold operation, a region of interest (ROI)
   that encloses vocal fold motion, or glottal region, is identified. The
   performance and effectiveness of our approach are evaluated by using
   clinical datasets representing both normal and pathological voice
   conditions. Copyright (c) 2015 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/cnm.2715}},
Article-Number = {{e02715}},
ISSN = {{2040-7939}},
EISSN = {{2040-7947}},
Unique-ID = {{ISI:000355664600003}},
}

@article{ ISI:000352667500012,
Author = {Browne, Nicola K. and Tay, Jason and Todd, Peter A.},
Title = {{Recreating pulsed turbidity events to determine coral-sediment
   thresholds for active management}},
Journal = {{JOURNAL OF EXPERIMENTAL MARINE BIOLOGY AND ECOLOGY}},
Year = {{2015}},
Volume = {{466}},
Pages = {{98-109}},
Month = {{MAY}},
Abstract = {{Active management of anthropogenically driven sediment resuspension
   events near coral reefs relies on an accurate assessment of coral
   thresholds to both suspended and deposited sediments. Yet the range of
   coral responses to sediments both within and amongst species has limited
   our ability to determine representative threshold values. This study
   reviews information available on coral physiological responses to a
   range of sediment loads at varying time frames and provides a novel
   approach to assess coral thresholds to suspended and deposited
   sediments. The new approach replicates natural turbidity regimes by
   creating pulsed turbidity events at two environmentally realistic levels
   (moderate = similar to 50 mg l(-1), peaks at 100 mg l(-1); severe =
   similar to 100 mg l(-1), peaks at 250 mg l(-1)). Corals (Merulina
   ampliata, Pachyseris speciosa, Platygyra sinensis) were subjected to two
   exposure regimes: pulsed turbidity events for four weeks followed by two
   months of recovery (constant regime) or pulsed turbidity events every
   other week followed by one month of recovery (periodic regime). Coral
   thresholds were greater than commonly used estimates with little to no
   effect on corals at moderate sediment levels. At extreme sediment
   levels, species morphological differences were potentially key
   determinants of coral survival. The periodic exposure regime was less
   detrimental to all coral species than the constant exposure regime as
   demonstrated by elevated yields and lower tissue morality rates. To
   improve knowledge on coral-sediment threshold values, research needs to
   expand to incorporate a broader range of species and exposure regimes.
   Realistic threshold values combined with modelling efforts would improve
   prediction of reef health and enable managers to react to declines in
   health before coral mortality occurs. (C) 2015 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.jembe.2015.02.010}},
ISSN = {{0022-0981}},
EISSN = {{1879-1697}},
ResearcherID-Numbers = {{Todd, Peter/H-8410-2012}},
Unique-ID = {{ISI:000352667500012}},
}

@article{ ISI:000366608200001,
Author = {Negahbani, Ehsan and Steyn-Ross, D. Alistair and Steyn-Ross, Moira L.
   and Wilson, Marcus T. and Sleigh, Jamie W.},
Title = {{Noise-Induced Precursors of State Transitions in the Stochastic
   Wilson-Cowan Model}},
Journal = {{JOURNAL OF MATHEMATICAL NEUROSCIENCE}},
Year = {{2015}},
Volume = {{5}},
Month = {{APR 8}},
Abstract = {{The Wilson-Cowan neural field equations describe the dynamical behavior
   of a 1-D continuum of excitatory and inhibitory cortical neural
   aggregates, using a pair of coupled integro-differential equations. Here
   we use bifurcation theory and small-noise linear stochastics to study
   the range of a phase transitions-sudden qualitative changes in the state
   of a dynamical system emerging from a bifurcation-accessible to the
   Wilson-Cowan network. Specifically, we examine saddle-node, Hopf,
   Turing, and Turing-Hopf instabilities. We introduce stochasticity by
   adding small-amplitude spatio-temporal white noise, and analyze the
   resulting subthreshold fluctuations using an Ornstein-Uhlenbeck
   linearization. This analysis predicts divergent changes in correlation
   and spectral characteristics of neural activity during close approach to
   bifurcation from below. We validate these theoretical predictions using
   numerical simulations. The results demonstrate the role of noise in the
   emergence of critically slowed precursors in both space and time, and
   suggest that these early-warning signals are a universal feature of a
   neural system close to bifurcation. In particular, these precursor
   signals are likely to have neurobiological significance as early
   warnings of impending state change in the cortex. We support this claim
   with an analysis of the in vitro local field potentials recorded from
   slices of mouse-brain tissue. We show that in the period leading up to
   emergence of spontaneous seizure-like events, the mouse field potentials
   show a characteristic spectral focusing toward lower frequencies
   concomitant with a growth in fluctuation variance, consistent with
   critical slowing near a bifurcation point. This observation of
   biological criticality has clear implications regarding the feasibility
   of seizure prediction.}},
DOI = {{10.1186/s13408-015-0021-x}},
Article-Number = {{UNSP 9}},
ISSN = {{2190-8567}},
Unique-ID = {{ISI:000366608200001}},
}

@article{ ISI:000353004600003,
Author = {Shatnawi, Maad and Zaki, Nazar},
Title = {{Inter-domain linker prediction using amino acid compositional index}},
Journal = {{COMPUTATIONAL BIOLOGY AND CHEMISTRY}},
Year = {{2015}},
Volume = {{55}},
Pages = {{23-30}},
Month = {{APR}},
Abstract = {{Protein chains are generally long and consist of multiple domains.
   Domains are distinct structural units of a protein that can evolve and
   function independently. The accurate and reliable prediction of protein
   domain linkers and boundaries is often considered to be the initial step
   of protein tertiary structure and function predictions. In this paper,
   we introduce CISA as a method for predicting inter-domain linker regions
   solely from the amino acid sequence information. The method first
   computes the amino acid compositional index from the protein sequence
   dataset of domain-linker segments and the amino acid composition. A
   preference profile is then generated by calculating the average
   compositional index values along the amino acid sequence using a sliding
   window. Finally, the protein sequence is segmented into intetvals and a
   simulated annealing algorithm is employed to enhance the prediction by
   finding the optimal threshold value for each segment that separates
   domains from inter-domain linkers. The method was tested on two standard
   protein datasets and showed considerable improvement over the
   state-of-the-art domain linker prediction methods. (C) 2015 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiolchem.2015.01.006}},
ISSN = {{1476-9271}},
EISSN = {{1476-928X}},
ORCID-Numbers = {{Zaki, Nazar/0000-0002-6259-9843
   Shatnawi, Maad/0000-0003-3885-1065}},
Unique-ID = {{ISI:000353004600003}},
}

@article{ ISI:000351586500044,
Author = {Wang, Yeyao and Zhang, Lingsong and Meng, Fansheng and Zhou, Yuexi and
   Jin, Xiaowei and Giesy, John P. and Liu, Fang},
Title = {{Improvement on species sensitivity distribution methods for deriving
   site-specific water quality criteria}},
Journal = {{ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH}},
Year = {{2015}},
Volume = {{22}},
Number = {{7}},
Pages = {{5271-5282}},
Month = {{APR}},
Abstract = {{Species sensitivity distribution (SSD) is the most common method used to
   derive water quality criteria, but there are still issues to be
   resolved. Here, issues associated with application of SSD methods,
   including species selection, plotting position, and cutoff point
   setting, are addressed. A preliminary improvement to the SSD approach
   based on post-stratified sampling theory is proposed. In the improved
   method, selection of species is based on biota of a specific basin, and
   the whole species in the specific ecosystem are considered. After
   selecting species to be included and calculating the cumulative
   probability, a new method to set the critical threshold for protection
   of ecosystem-level structure and function is proposed. The alternative
   method was applied in a case study in which a water quality criterion
   (WQC) was derived for ammonia in the Songhua River (SHR), China.}},
DOI = {{10.1007/s11356-014-3783-x}},
ISSN = {{0944-1344}},
EISSN = {{1614-7499}},
Unique-ID = {{ISI:000351586500044}},
}

@article{ ISI:000351228900024,
Author = {Ji, Zhou and Card, Kyle J. and Dazzo, Frank B.},
Title = {{CMEIAS JFrad: A Digital Computing Tool to Discriminate the Fractal
   Geometry of Landscape Architectures and Spatial Patterns of Individual
   Cells in Microbial Biofilms}},
Journal = {{MICROBIAL ECOLOGY}},
Year = {{2015}},
Volume = {{69}},
Number = {{3}},
Pages = {{710-720}},
Month = {{APR}},
Abstract = {{Image analysis of fractal geometry can be used to gain deeper insights
   into complex ecophysiological patterns and processes occurring within
   natural microbial biofilm landscapes, including the scale-dependent
   heterogeneities of their spatial architecture, biomass, and cell-cell
   interactions, all driven by the colonization behavior of optimal spatial
   positioning of organisms to maximize their efficiency in utilization of
   allocated nutrient resources. Here, we introduce CMEIAS JFrad, a new
   computing technology that analyzes the fractal geometry of complex
   biofilm architectures in digital landscape images. The software uniquely
   features a data-mining opportunity based on a comprehensive collection
   of 11 different mathematical methods to compute fractal dimension that
   are implemented into a wizard design to maximize ease-of-use for
   semi-automatic analysis of single images or fully automatic analysis of
   multiple images in a batch process. As examples of application,
   quantitative analyses of fractal dimension were used to optimize the
   important variable settings of brightness threshold and minimum object
   size in order to discriminate the complex architecture of freshwater
   microbial biofilms at multiple spatial scales, and also to differentiate
   the spatial patterns of individual bacterial cells that influence their
   cooperative interactions, resource use, and apportionment in situ.
   Version 1.0 of JFrad is implemented into a software package containing
   the program files, user manual, and tutorial images that will be freely
   available at http://cme.msu.edu/cmeias/. This improvement in
   computational image informatics will strengthen microscopy-based
   approaches to analyze the dynamic landscape ecology of microbial biofilm
   populations and communities in situ at spatial resolutions that range
   from single cells to microcolonies.}},
DOI = {{10.1007/s00248-014-0495-1}},
ISSN = {{0095-3628}},
EISSN = {{1432-184X}},
ORCID-Numbers = {{Card, Kyle/0000-0002-0462-2777}},
Unique-ID = {{ISI:000351228900024}},
}

@article{ ISI:000349882300008,
Author = {Lai, Shu-mei and Liu, Wei-chung and Jordan, Ferenc},
Title = {{A trophic overlap-based measure for species uniqueness in ecological
   networks}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2015}},
Volume = {{299}},
Pages = {{95-101}},
Month = {{MAR 10}},
Abstract = {{Species importance is often defined by how central the role a species
   plays in a food web. Here we argue that species importance should also
   incorporate the concept of uniqueness of its network position. By
   developing a previous methodology, we propose a new approach for species
   uniqueness. Our methodology quantifies the interaction structure of
   species, separates strong and weak interactors in a threshold-dependent
   way and measures the similarity of strong interactors' identity for
   pairs of species. By exploring various threshold values systematically,
   the profile of a species' trophic overlap can be constructed. Summing up
   the extent of trophic overlap across the whole profile for a species, we
   then obtain a new measure for species uniqueness. A unique species
   should overlap less with other species and therefore has a low value for
   this new measure. We demonstrate our methodology by using the food web
   representation of Prince Williams Sound ecosystem. Drastic differences
   between our new result and that derived from the previous approach are
   found. We further compare the result with other network indices and
   found that the information generated from our new approach differs from
   the majority others. This in turn implies our approach offer an
   alternative view on species importance that may complement the existing
   methodologies in the literature. (C) 2014 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.ecolmodel.2014.12.014}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
Unique-ID = {{ISI:000349882300008}},
}

@article{ ISI:000351046000001,
Author = {Hoffmann, Steve and Stadler, Peter F. and Strimmer, Korbinian},
Title = {{A simple data-adaptive probabilistic variant calling model}},
Journal = {{ALGORITHMS FOR MOLECULAR BIOLOGY}},
Year = {{2015}},
Volume = {{10}},
Month = {{MAR 4}},
Abstract = {{Background: Several sources of noise obfuscate the identification of
   single nucleotide variation (SNV) in next generation sequencing data.
   For instance, errors may be introduced during library construction and
   sequencing steps. In addition, the reference genome and the algorithms
   used for the alignment of the reads are further critical factors
   determining the efficacy of variant calling methods. It is crucial to
   account for these factors in individual sequencing experiments.
   Results: We introduce a simple data-adaptive model for variant calling.
   This model automatically adjusts to specific factors such as alignment
   errors. To achieve this, several characteristics are sampled from sites
   with low mismatch rates, and these are used to estimate empirical
   log-likelihoods. The likelihoods are then combined to a score that
   typically gives rise to a mixture distribution. From this we determine a
   decision threshold to separate potentially variant sites from the noisy
   background.
   Conclusions: In simulations we show that our simple model is competitive
   with frequently used much more complex SNV calling algorithms in terms
   of sensitivity and specificity. It performs specifically well in cases
   with low allele frequencies. The application to next-generation
   sequencing data reveals stark differences of the score distributions
   indicating a strong influence of data specific sources of noise. The
   proposed model is specifically designed to adjust to these differences.}},
DOI = {{10.1186/s13015-015-0037-5}},
Article-Number = {{10}},
ISSN = {{1748-7188}},
ResearcherID-Numbers = {{Strimmer, Korbinian/C-1522-2009
   Stadler, Peter F./L-7857-2015}},
ORCID-Numbers = {{Strimmer, Korbinian/0000-0001-7917-2056
   Stadler, Peter F./0000-0002-5016-5191}},
Unique-ID = {{ISI:000351046000001}},
}

@article{ ISI:000352268500022,
Author = {Renaud, Gabriel and Stenzel, Udo and Maricic, Tomislav and Wiebe, Victor
   and Kelso, Janet},
Title = {{deML: robust demultiplexing of Illumina sequences using a
   likelihood-based approach}},
Journal = {{BIOINFORMATICS}},
Year = {{2015}},
Volume = {{31}},
Number = {{5}},
Pages = {{770-772}},
Month = {{MAR 1}},
Abstract = {{Motivation: Pooling multiple samples increases the efficiency and lowers
   the cost of DNA sequencing. One approach to multiplexing is to use short
   DNA indices to uniquely identify each sample. After sequencing, reads
   must be assigned in silico to the sample of origin, a process referred
   to as demultiplexing. Demultiplexing software typically identifies the
   sample of origin using a fixed number of mismatches between the read
   index and a reference index set. This approach may fail or misassign
   reads when the sequencing quality of the indices is poor.
   Results: We introduce deML, a maximum likelihood algorithm that
   demultiplexes Illumina sequences. deML computes the likelihood of an
   observed index sequence being derived from a specified sample. A quality
   score which reflects the probability of the assignment being correct is
   generated for each read. Using these quality scores, even very
   problematic datasets can be demultiplexed and an error threshold can be
   set.}},
DOI = {{10.1093/bioinformatics/btu719}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ORCID-Numbers = {{Maricic, Tomislav/0000-0003-3267-0474
   Kelso, Janet/0000-0002-3618-322X}},
Unique-ID = {{ISI:000352268500022}},
}

@article{ ISI:000348879100005,
Author = {Barnie, Talfan and Oppenheimer, Clive},
Title = {{Extracting High Temperature Event radiance from satellite images and
   correcting for saturation using Independent Component Analysis}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2015}},
Volume = {{158}},
Pages = {{56-68}},
Month = {{MAR 1}},
Abstract = {{We present a novel method for extracting the radiance from High
   Temperature Events (HTEs) recorded by geo-stationary imagers using
   Independent Component Analysis (ICA). We use ICA to decompose the image
   cube collected by the instrument into a sum of the outer products of
   independent, maximally non-Gaussian time series and images of their
   spatial distribution, and then reassemble the image cube using only
   sources that appear to be HTEs. Integrating spatially gives the time
   series of total HTE radiance emission. In this study we test the
   technique on a number of simulated HTE events, and then apply it to a
   number of volcanic HTEs observed by the SEVIRI instrument. We find that
   the technique performs well on small localised eruptions and can be used
   to correct for saturation. The technique offers the advantage of
   obviating the need for a priori knowledge of the area being imaged,
   beyond some basic assumptions about the nature of the processes
   affecting radiance in the scene, namely that (i) HTE sources are
   statistically independent from other processes, (ii) the radiance
   registered at the sensor is a linear mixture of the HTE signal and those
   from other processes, and (iii) HTE sources can be reliably identified
   for the reconstruction process. This results in only five free
   parameters the dimensions of the image cube, an estimate of the data
   dimensionality and a threshold for distinguishing between HTE and nonHTE
   sources. While we have focused here on volcanic HTEs, the methodology
   can, in principle, be extended to studies of other kinds of HTEs such as
   those associated with biomass burning. (C) 2014 The Authors. Published
   by Elsevier Inc.}},
DOI = {{10.1016/j.rse.2014.10.023}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
Unique-ID = {{ISI:000348879100005}},
}

@article{ ISI:000351876500002,
Author = {Chen, Hao and Bell, John M. and Zavala, Nicolas A. and Ji, Hanlee P. and
   Zhang, Nancy R.},
Title = {{Allele-specific copy number profiling by next-generation DNA sequencing}},
Journal = {{NUCLEIC ACIDS RESEARCH}},
Year = {{2015}},
Volume = {{43}},
Number = {{4}},
Month = {{FEB 27}},
Abstract = {{The progression and clonal development of tumors often involve
   amplifications and deletions of genomic DNA. Estimation of
   allele-specific copy number, which quantifies the number of copies of
   each allele at each variant loci rather than the total number of
   chromosome copies, is an important step in the characterization of tumor
   genomes and the inference of their clonal history. We describe a new
   method, falcon, for finding somatic allele-specific copy number changes
   by next generation sequencing of tumors with matched normals. falcon is
   based on a change-point model on a bivariate mixed Binomial process,
   which explicitly models the copy numbers of the two chromosome
   haplotypes and corrects for local allele-specific coverage biases. By
   using the Binomial distribution rather than a normal approximation,
   falcon more effectively pools evidence from sites with low coverage. A
   modified Bayesian information criterion is used to guide model selection
   for determining the number of copy number events. Falcon is evaluated on
   in silico spike-in data and applied to the analysis of a pre-malignant
   colon tumor sample and late-stage colorectal adenocarcinoma from the
   same individual. The allele-specific copy number estimates obtained by
   falcon allows us to draw detailed conclusions regarding the clonal
   history of the individual's colon cancer.}},
DOI = {{10.1093/nar/gku1252}},
Article-Number = {{e23}},
ISSN = {{0305-1048}},
EISSN = {{1362-4962}},
Unique-ID = {{ISI:000351876500002}},
}

@article{ ISI:000349504100009,
Author = {Srinivasan, Venkatraman and Kumar, Praveen},
Title = {{Emergent and divergent resilience behavior in catastrophic shift systems}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2015}},
Volume = {{298}},
Number = {{SI}},
Pages = {{87-105}},
Month = {{FEB 24}},
Abstract = {{Resilience in dynamic ecological systems has been intuitively associated
   with the ability to withstand disturbances in system drivers represented
   as shocks. Typically shocks are characterized as instantaneous, and
   isolated non-interacting events with the system dynamics corresponding
   to a fixed potential well. However, ecological systems are subject to
   continuous variation in environmental drivers such as rainfall,
   temperature, etc. and these interact with the ecosystem dynamics to
   alter the potential well. These variations are typically represented as
   a stochastic process. Furthermore, with climate change, the stochastic
   characteristics of the environmental drivers also change, thereby
   impacting the well dynamics. To characterize the resilience behavior
   under continuous variation in system drivers, and contrast it with that
   subject to instantaneous shock in system drivers, we employ the
   canonical catastrophic shift system as an example, and demonstrate
   emergent and contrasting divergent resilience behavior of the measures
   as the properties of the system-driver couple change. These behaviors
   include variability induced stabilization or enhancement of dynamic
   regimes, regions of sensitivity to dynamic regime transitions and
   existence of trap or escape regions. Furthermore, we introduce the
   concept of iso-resilience curves which are employed to design travel
   paths in resilience landscapes. These results provide valuable insights
   for managing resilience attributes associated with dynamic regime
   transitions in catastrophic shift systems under instantaneous shock and
   continuous variability in system drivers. (C) 2013 Elsevier B.V. All
   rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2013.12.003}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
ResearcherID-Numbers = {{Kumar, Praveen/D-2036-2010
   }},
ORCID-Numbers = {{Kumar, Praveen/0000-0002-4787-0308
   Srinivasan, Venkatraman/0000-0003-4586-8893}},
Unique-ID = {{ISI:000349504100009}},
}

@article{ ISI:000350146600007,
Author = {Rotgeri, Andrea and Korolainen, Henriikka and Sundholm, Oskari and
   Schmitz, Heinz and Fuhrmann, Ulrike and Prelles, Katja and Sacher, Frank},
Title = {{Characterization of anastrozole effects, delivered by an intravaginal
   ring in cynomolgus monkeys}},
Journal = {{HUMAN REPRODUCTION}},
Year = {{2015}},
Volume = {{30}},
Number = {{2}},
Pages = {{308-314}},
Month = {{FEB}},
Abstract = {{STUDY QUESTION: Is it feasible to deliver anastrozole (ATZ), an
   aromatase inhibitor (AI), by a vaginal polymer-based drug delivery
   system in the cynomolgus monkey (Macaca fascicularis) to describe the
   pharmacokinetic profile?
   SUMMARY ANSWER: The present study showed the effective release of ATZ
   into the systemic circulation from intravaginal rings in cynomolgus
   monkeys.
   WHAT IS KNOWN ALREADY: ATZ is a marketed drug with well documented
   pharmacological and safety profiles for oral administration. Aromatase
   is the key enzyme catalyzing estrogen biosynthesis and is overexpressed
   in endometriotic lesions. AIs show therapeutic efficacy in endometriosis
   in exploratory clinical trials.
   STUDY DESIGN, SIZE, DURATION: The pharmacokinetics of the in vivo
   release and the pharmacodynamic activity of ATZ released by intravaginal
   rings (IVR) were investigated in healthy cycling female cynomolgus
   monkeys in three different dose groups (n = 5) for one menstrual cycle.
   PARTICIPANTS/MATERIALS, SETTING, METHODS: IVRs for the cynomolgus
   monkey, releasing three different doses of ATZ were designed and tested
   for in vitro/in vivo release for up to 42 days. For pharmacokinetic and
   pharmacodynamic evaluation, plasma samples were taken once daily from
   Day 1 to 3 and then every third day until menses occurred (17-42 days).
   MAIN RESULTS AND THE ROLE OF CHANCE: ATZ was shown to be compatible with
   the IVR drug delivery system. An average in vivo release of 277
   mg/day/animal of ATZ for one menstrual cycle was effective in causing a
   decrease of systemic estradiol (E2) levels by similar to 30\% without
   inducing counter regulation such as the elevation of FSH or the
   formation of ovarian cysts.
   LIMITATIONS, REASONS FOR CAUTION: The study was limited to three dose
   groups in which only the highest dose decreased the E2 level. Hence,
   additional research with IVRs releasing higher amounts of ATZ is
   required to define the threshold for an ATZ-dependent ovarian
   stimulation in cynomolgus monkeys.
   WIDER IMPLICATIONS OF THE FINDINGS: The release rate administered from
   IVRs is sufficient and in a range that supports feasibility of IVR
   administration of ATZ as a new approach for long-term therapy of
   estrogen-dependent diseases such as endometriosis in human.}},
DOI = {{10.1093/humrep/deu315}},
ISSN = {{0268-1161}},
EISSN = {{1460-2350}},
Unique-ID = {{ISI:000350146600007}},
}

@article{ ISI:000349446900005,
Author = {Souto-Maior, Caetano and Lopes, Joao S. and Gjini, Erida and Struchiner,
   Claudio J. and Teixeira, Luis and Gomes, M. Gabriela M.},
Title = {{Heterogeneity in symbiotic effects facilitates Wolbachia establishment
   in insect populations}},
Journal = {{THEORETICAL ECOLOGY}},
Year = {{2015}},
Volume = {{8}},
Number = {{1}},
Pages = {{53-65}},
Month = {{FEB}},
Abstract = {{Facultative vertically transmitted bacterial symbionts often manipulate
   its host's reproductive biology and thus facilitate their persistence.
   Wolbachia is one such symbiont where frequency-dependent reproductive
   benefits are opposed by frequency-independent fitness costs leading to
   bistable dynamics. Introduction of carriers does not assure invasion
   unless the initial frequency is above a threshold determined by the
   balance of costs and benefits. Recent laboratory experiments have
   uncovered that Wolbachia also protects their hosts from pathogens. The
   expected consequence of this phenotype in natural environments is to
   lower the invasion threshold by a factor that increases with the extent
   of pathogen exposure. Here, we introduce a series of mathematical models
   to address how pathogen protection affects Wolbachia invasion. First,
   under homogeneous symbiotic effects, we obtain an analytical expression
   for the invasion threshold in terms of pathogen exposure, and find a
   regime where symbiont releases may result in elimination of the entire
   host population provided that abundance of virulent pathogens is high.
   Second, we distribute Wolbachia effects such that some carriers are
   totally protected and others not at all, and explore how this interplays
   with different pathogen intensities, to conclude that heterogeneity
   further lowers the threshold for Wolbachia invasion. Third, we replicate
   the analysis using a realistic distribution of protective effects and
   confirm that heterogeneity increases system resilience by reducing the
   odds of population collapse.}},
DOI = {{10.1007/s12080-014-0235-7}},
ISSN = {{1874-1738}},
EISSN = {{1874-1746}},
ResearcherID-Numbers = {{Struchiner, Claudio/M-9360-2013
   Gomes, Gabriela/H-7218-2014
   Teixeira, Luis/H-6106-2013
   }},
ORCID-Numbers = {{Struchiner, Claudio/0000-0003-2114-847X
   Gomes, Gabriela/0000-0002-1454-4979
   Teixeira, Luis/0000-0001-8326-6645
   Souto-Maior, Caetano/0000-0002-0271-2576
   Lopes, Joao/0000-0001-5670-2069
   Gjini, Erida/0000-0002-0493-3102}},
Unique-ID = {{ISI:000349446900005}},
}

@article{ ISI:000348900200014,
Author = {Goetz, T. and Janik, V. M.},
Title = {{Target-specific acoustic predator deterrence in the marine environment}},
Journal = {{ANIMAL CONSERVATION}},
Year = {{2015}},
Volume = {{18}},
Number = {{1}},
Pages = {{102-111}},
Month = {{FEB}},
Abstract = {{Acoustic deterrent devices (ADDs) have often been considered a benign
   solution to managing pinniped predation. However, ADDs have also been
   highlighted as a conservation concern since they can inflict large-scale
   habitat exclusion in toothed whales (odontocetes). We tested a new
   method that selectively inflicted startle responses in harbour seals
   (Phoca vitulina) at close ranges to the loudspeaker but not in a
   non-target species, the harbour porpoise (Phocoena phocoena), by using a
   frequency range where porpoise hearing was less sensitive than that of
   phocid seals. The sound exposure consisted of isolated 200ms long, 2-3
   octave-band noise pulses with a peak frequency of 1kHz, which were
   presented at a source level of approximate to 180dB re 1Pa. Field tests
   were carried out within a 2-month period on a fish farm on the west
   coast of Scotland where marine mammal behaviour was observed within
   three distance categories. Seal numbers dropped sharply during sound
   exposure compared with control observation periods within 250m of the
   sound source but were unaffected at distances further away from the
   farm. A Poisson regression model revealed that the number of seal tracks
   within 250m of the device decreased by approximate to 91\% during sound
   exposure and was primarily influenced by sound exposure with no evidence
   for a change in the effect of treatment such as habituation, throughout
   the experiment. In contrast to seals, there was no shift in the number
   of porpoise groups in each distance category as a result of sound
   exposure and porpoises were regularly seen close to the device. We also
   sighted six common minke whales during sound exposure while only one was
   seen during control periods. Our data demonstrate that the startle
   method can be used to selectively deter seals without affecting
   porpoises.}},
DOI = {{10.1111/acv.12141}},
ISSN = {{1367-9430}},
EISSN = {{1469-1795}},
Unique-ID = {{ISI:000348900200014}},
}

@article{ ISI:000348051200003,
Author = {Danziger, Zachary and Grill, Warren M.},
Title = {{A neuron model of stochastic resonance using rectangular pulse trains}},
Journal = {{JOURNAL OF COMPUTATIONAL NEUROSCIENCE}},
Year = {{2015}},
Volume = {{38}},
Number = {{1}},
Pages = {{53-66}},
Month = {{FEB}},
Abstract = {{Stochastic resonance (SR) is the enhanced representation of a weak input
   signal by the addition of an optimal level of broadband noise to a
   nonlinear (threshold) system. Since its discovery in the 1980s the
   domain of input signals shown to be applicable to SR has greatly
   expanded, from strictly periodic inputs to now nearly any aperiodic
   forcing function. The perturbations (noise) used to generate SR have
   also expanded, from white noise to now colored noise or vibrational
   forcing. This study demonstrates that a new class of perturbations can
   achieve SR, namely, series of stochastically generated biphasic pulse
   trains. Using these pulse trains as `noise' we show that a Hodgkin
   Huxley model neuron exhibits SR behavior when detecting weak input
   signals. This result is of particular interest to neuroscience because
   nearly all artificial neural stimulation is implemented with square
   current or voltage pulses rather than broadband noise, and this new
   method may facilitate the translation of the performance gains
   achievable through SR to neural prosthetics.}},
DOI = {{10.1007/s10827-014-0526-4}},
ISSN = {{0929-5313}},
EISSN = {{1573-6873}},
ORCID-Numbers = {{Grill, Warren/0000-0001-5240-6588
   Danziger, Zachary/0000-0003-1474-1259}},
Unique-ID = {{ISI:000348051200003}},
}

@article{ ISI:000348816400001,
Author = {Johnson, Nicholas and Zhang, Huanan and Fang, Gang and Kumar, Vipin and
   Kuang, Rui},
Title = {{SubPatCNV: approximate subspace pattern mining for mapping copy-number
   variations}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2015}},
Volume = {{16}},
Month = {{JAN 16}},
Abstract = {{Background: Many DNA copy-number variations (CNVs) are known to lead to
   phenotypic variations and pathogenesis. While CNVs are often only common
   in a small number of samples in the studied population or patient
   cohort, previous work has not focused on customized identification of
   CNV regions that only exhibit in subsets of samples with advanced data
   mining techniques to reliably answer questions such as ``Which are all
   the chromosomal fragments showing nearly identical deletions or
   insertions in more than 30\% of the individuals?{''}.
   Results: We introduce a tool for mining CNV subspace patterns, namely
   SubPatCNV, which is capable of identifying all aberrant CNV regions
   specific to arbitrary sample subsets larger than a support threshold. By
   design, SubPatCNV is the implementation of a variation of approximate
   association pattern mining algorithm under a spatial constraint on the
   positional CNV probe features. In benchmark test, SubPatCNV was applied
   to identify population specific germline CNVs from four populations of
   HapMap samples. In experiments on the TCGA ovarian cancer dataset,
   SubPatCNV discovered many large aberrant CNV events in patient
   subgroups, and reported regions enriched with cancer relevant genes. In
   both HapMap data and TCGA data, it was observed that SubPatCNV employs
   approximate pattern mining to more effectively identify CNV subspace
   patterns that are consistent within a subgroup from high-density array
   data.
   Conclusions: SubPatCNV available through
   http://sourceforge.net/projects/subpatcnv/ is a unique scalable
   open-source software tool that provides the flexibility of identifying
   CNV regions specific to sample subgroups of different sizes from
   high-density CNV array data.}},
DOI = {{10.1186/s12859-014-0426-7}},
Article-Number = {{16}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000348816400001}},
}

@article{ ISI:000346473000006,
Author = {Huang, Wei and Zeng, Yongnian and Li, Songnian},
Title = {{An analysis of urban expansion and its associated thermal
   characteristics using Landsat imagery}},
Journal = {{GEOCARTO INTERNATIONAL}},
Year = {{2015}},
Volume = {{30}},
Number = {{1}},
Pages = {{93-103}},
Month = {{JAN 2}},
Abstract = {{There has been an increasing interest in mapping and monitoring urban
   land use/land cover using remote sensing techniques. However, there
   still exist quite a number of challenges in deriving urban extent and
   its expansion density from remote sensing data quantitatively. This
   study utilized Landsat TM/ETM+ remote sensing data to assess urban
   expansion and its thermal characteristics with a case study in the city
   of Changsha, China. We proposed a new approach for quantitatively
   determining built-up area, its expansion density and their respective
   relationship with land surface temperature (LST) patterns. An urban
   expansion metric was also developed using a moving window mechanism to
   identify urban built-up area and its expansion density based on selected
   threshold values. The study suggested that urban extent and its
   expansion density, as well as surface thermal characteristics and
   patterns could be identified through quantitatively derived remotely
   sensed indices and LST, which offer meaningful characteristics in
   quantifying urban expansion density and urban thermal pattern. Results
   from the case study demonstrated that: (1) the built-up area and urban
   expansion density have significantly increased in the city of Changsha
   from 1990 to 2001; and (2) the differences of urban expansion densities
   correspond to thermal effects, where a high percentage of imperviousness
   is usually associated with the area covered by high surface temperature.}},
DOI = {{10.1080/10106049.2014.965756}},
ISSN = {{1010-6049}},
EISSN = {{1752-0762}},
ORCID-Numbers = {{Li, Songnian/0000-0002-8244-5681
   Huang, Wei/0000-0001-8324-8877}},
Unique-ID = {{ISI:000346473000006}},
}

@article{ ISI:000362913100016,
Author = {Singh, Riddhi and Reed, Patrick M. and Keller, Klaus},
Title = {{Many-objective robust decision making for managing an ecosystem with a
   deeply uncertain threshold response}},
Journal = {{ECOLOGY AND SOCIETY}},
Year = {{2015}},
Volume = {{20}},
Number = {{3}},
Abstract = {{Managing ecosystems with deeply uncertain threshold responses and
   multiple decision makers poses nontrivial decision analytical
   challenges. The problem is imbued with deep uncertainties because
   decision makers do not know or cannot converge on a single probability
   density function for each key parameter, a perfect model structure, or a
   single adequate objective. The existing literature on managing
   multistate ecosystems has generally followed a normative decision-making
   approach based on expected utility maximization (MEU). This approach has
   simple and intuitive axiomatic foundations, but faces at least two
   limitations. First, a prespecified utility function is often unable to
   capture the preferences of diverse decision makers. Second, decision
   makers' preferences depart from MEU in the presence of deep uncertainty.
   Here, we introduce a framework that allows decision makers to pose
   multiple objectives, explore the trade-offs between potentially
   conflicting preferences of diverse decision makers, and to identify
   strategies that are robust to deep uncertainties. The framework,
   referred to as many-objective robust decision making (MORDM), employs
   multiobjective evolutionary search to identify trade-offs between
   strategies, re-evaluates their performance under deep uncertainty, and
   uses interactive visual analytics to support the selection of robust
   management strategies. We demonstrate MORDM on a stylized decision
   problem posed by the management of a lake in which surpassing a
   pollution threshold causes eutrophication. Our results illustrate how
   framing the lake problem in terms of MEU can fail to represent key
   trade-offs between phosphorus levels in the lake and expected economic
   benefits. Moreover, the MEU strategy deteriorates severely in
   performance for all objectives under deep uncertainties. Alternatively,
   the MORDM framework enables the discovery of strategies that balance
   multiple preferences and perform well under deep uncertainty. This
   decision analytic framework allows the decision makers to select
   strategies with a better understanding of their expected trade-offs
   (traditional uncertainty) as well as their robustness (deep
   uncertainty).}},
DOI = {{10.5751/ES-07687-200312}},
Article-Number = {{12}},
ISSN = {{1708-3087}},
ResearcherID-Numbers = {{Keller, Klaus/A-6742-2013
   Reed, Patrick/E-4435-2014
   }},
ORCID-Numbers = {{Reed, Patrick/0000-0002-7963-6102
   Singh, Riddhi/0000-0002-1670-1229}},
Unique-ID = {{ISI:000362913100016}},
}

@article{ ISI:000356635800018,
Author = {Dung Tri Phung and Connell, Des and Chu, Cordia},
Title = {{A new method for setting guidelines to protect human health from
   agricultural exposure by using chlorpyrifos as an example}},
Journal = {{ANNALS OF AGRICULTURAL AND ENVIRONMENTAL MEDICINE}},
Year = {{2015}},
Volume = {{22}},
Number = {{2}},
Pages = {{275-280}},
Abstract = {{Introduction and Objectives. Guidelines set by various agencies for the
   control and management of chlorpyrifos cover a wide range of values
   reflecting difficulties in the procedures for their development. To
   overcome these difficulties a new method to set guidelines would be
   developed. Published data derived from epidemiological investigations on
   human populations would be used to develop a dose-response relationship
   for chlorpyrifos allowing the calculation of threshold values which can
   be used as guidelines.
   Materials and Method. Data from the scientific literature on human
   populations were collected to evaluate the adverse response doses for a
   range of health effects. The Cumulative Frequency Distribution (CFD) for
   the minimum levels of adverse effects measured in terms of the Lifetime
   Average Daily Dose (LADD(D)) and the Absorbed Daily Dose for
   neurological (ADD(DN)) and non-neurological effects were used.
   Results. Linear regression equations were fitted to the CFD plots giving
   R-2 values of 0.93 and 0.86 indicating a normal distribution of the
   data. Using these CFD plots, the chronic and acute threshold values were
   calculated at the 5\% cumulative frequency level for chlorpyrifos
   exposure giving values at 0.5 mu g/kg/d and 3 mu g/kg/d respectively.
   Conclusions. Guidelines set using this technique at the values at 0.5 mu
   g/kg/d and 3 mu g/kg/d for chronic and acute exposure respectively
   provide an alternative to the currently used biological endpoint and
   safety factor method.}},
DOI = {{10.5604/12321966.1152080}},
ISSN = {{1232-1966}},
EISSN = {{1898-2263}},
Unique-ID = {{ISI:000356635800018}},
}

@article{ ISI:000355823400001,
Author = {Wu, Jian and Xu, Yingke and Feng, Zhouyan and Zheng, Xiaoxiang},
Title = {{Automatically Identifying Fusion Events between GLUT4 Storage Vesicles
   and the Plasma Membrane in TIRF Microscopy Image Sequences}},
Journal = {{COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE}},
Year = {{2015}},
Abstract = {{Quantitative analysis of the dynamic behavior about membrane-bound
   secretory vesicles has proven to be important in biological research.
   This paper proposes a novel approach to automatically identify the
   elusive fusion events between VAMP2-pHluorin labeled GLUT4 storage
   vesicles (GSVs) and the plasma membrane. The differentiation is
   implemented to detect the initiation of fusion events by modified
   forward subtraction of consecutive frames in the TIRFM image sequence.
   Spatially connected pixels in difference images brighter than a
   specified adaptive threshold are grouped into a distinct fusion spot.
   The vesicles are located at the intensity-weighted centroid of their
   fusion spots. To reveal the true in vivo nature of a fusion event, 2D
   Gaussian fitting for the fusion spot is used to derive the
   intensity-weighted centroid and the spot size during the fusion process.
   The fusion event and its termination can be determined according to the
   change of spot size. The method is evaluated on real experiment data
   with ground truth annotated by expert cell biologists. The evaluation
   results show that it can achieve relatively high accuracy comparing
   favorably to the manual analysis, yet at a small fraction of time.}},
DOI = {{10.1155/2015/610482}},
Article-Number = {{610482}},
ISSN = {{1748-670X}},
EISSN = {{1748-6718}},
Unique-ID = {{ISI:000355823400001}},
}

@article{ ISI:000352513000005,
Author = {Guerrero, P. and Alarcon, T.},
Title = {{Stochastic Multiscale Models of Cell Population Dynamics: Asymptotic and
   Numerical Methods}},
Journal = {{MATHEMATICAL MODELLING OF NATURAL PHENOMENA}},
Year = {{2015}},
Volume = {{10}},
Number = {{1}},
Pages = {{64-93}},
Abstract = {{In this paper we present a new methodology that allows us to formulate
   and analyse stochastic multiscale models of the dynamics of cell
   populations. In the spirit of existing hybrid multiscale models, we set
   up our model in a hierarchical way according to the characteristic time
   scales involved, where the stochastic population dynamics is governed by
   the birth and death rates as prescribed by the corresponding
   intracellular pathways (e.g. stochastic cell-cycle model). ``file
   feed-back loop is closed by the coupling between the dynamics of the
   population and the intracellular dynamics via the concentration of
   oxygen: Cells consume oxygen which, in turn, regulate the rate at which
   cells proceed through their cell-cycle. The coupling between
   intracellular and population dynainics is carried out through a novel
   method to obtain the birth rate from the stochastic cell-cycle model,
   based on a mean-first passage time approach. Cell proliferation is
   assumed to be activated when one or more of the proteins involved in the
   cell-cycle regulatory pathway hit a threshold. This view allows us to
   calculate the birth rate as a function of the age of the cell and the
   extracellular oxygen in terms of the corresponding mean-first passage
   time. We then proceed to formulate the stochastic dynamics of the
   population of cells in terms of an age-structured Master Equation.
   Further, we have developed generalisations of asymptotic (WKB) methods
   for our age-structured Master Equation as well as a tau-leap method to
   simulate the evolution of our age-structured population. Finally, we
   illustrate Otis general methodology with a particular example of a cell
   population where progression through the cell-cycle is regulated by the
   availability of oxygen.}},
DOI = {{10.1051/mmnp/201510104}},
ISSN = {{0973-5348}},
EISSN = {{1760-6101}},
ResearcherID-Numbers = {{Alarcon, Tomas/D-9083-2013
   Guerrero, Pilar/R-8234-2018}},
ORCID-Numbers = {{Alarcon, Tomas/0000-0002-8566-3676
   Guerrero, Pilar/0000-0002-5522-7339}},
Unique-ID = {{ISI:000352513000005}},
}

@article{ ISI:000351968200004,
Author = {Dupuis, D. J. and Sun, Y. and Wang, Huixia Judy},
Title = {{Detecting change-points in extremes}},
Journal = {{STATISTICS AND ITS INTERFACE}},
Year = {{2015}},
Volume = {{8}},
Number = {{1}},
Pages = {{19-31}},
Abstract = {{Even though most work on change-point estimation focuses on changes in
   the mean, changes in the variance or in the tail distribution can lead
   to more extreme events. In this paper, we develop a new method of
   detecting and estimating the change-points in the tail of multiple time
   series data. In addition, we adapt existing tail change-point detection
   methods to our specific problem and conduct a thorough comparison of
   different methods in terms of performance on the estimation of
   change-points and computational time. We also examine three locations on
   the U.S. northeast coast and demonstrate that the methods are useful for
   identifying changes in seasonally extreme warm temperatures.}},
DOI = {{10.4310/SII.2015.v8.n1.a3}},
ISSN = {{1938-7989}},
EISSN = {{1938-7997}},
ResearcherID-Numbers = {{Sun, Ying/N-2009-2017}},
ORCID-Numbers = {{Sun, Ying/0000-0001-6703-4270}},
Unique-ID = {{ISI:000351968200004}},
}

@article{ ISI:000350813000017,
Author = {Wang, Lei and Peng, Hui and Zheng, Jinhua},
Title = {{Similarities/Dissimilarities Analysis of Protein Sequences Based on
   Recurrence Quantification Analysis}},
Journal = {{CURRENT BIOINFORMATICS}},
Year = {{2015}},
Volume = {{10}},
Number = {{1}},
Pages = {{112-119}},
Abstract = {{To facilitate the similarities/dissimilarities analysis of the protein
   sequences, we introduce a novel approach based on the recurrence
   quantification analysis (RQA), in which, based on a selected pair of
   physicochemical properties of amino acids, the primary structure of
   proteins is considered as two time series, with the amino acid order
   playing the role of subsequent time intervals, and then, we adopt RQA to
   analyze these two time series, and utilize 6 characteristic parameters
   calculated with RQA as feature representation of protein sequence to
   analyze the similarities/dissimilarities of the nine ND5 protein
   sequences. The analysis results show that our method is effective, and
   in addition, different from existing RQA based methods, in our method,
   after the two parameters such as the embedding dimension m and the time
   delay tau have been predetermined based on given algorithms, the range
   of the threshold epsilon can be determined efficiently, and more
   interesting, the variation of epsilon in the determined range almost
   will not influence the rationality of the results of the
   similarities/dissimilarities analysis.}},
DOI = {{10.2174/157489361001150309144955}},
ISSN = {{1574-8936}},
EISSN = {{2212-392X}},
ResearcherID-Numbers = {{Wang, Lei/I-9064-2018}},
ORCID-Numbers = {{Wang, Lei/0000-0002-5065-8447}},
Unique-ID = {{ISI:000350813000017}},
}

@article{ ISI:000347536500009,
Author = {Castro-Conde, Irene and de Una-Alvarez, Jacobo},
Title = {{Adjusted p-values for SGoF multiple test procedure}},
Journal = {{BIOMETRICAL JOURNAL}},
Year = {{2015}},
Volume = {{57}},
Number = {{1, SI}},
Pages = {{108-122}},
Month = {{JAN}},
Note = {{8th International Conference on Multiple Comparison Procedures
   (MCP2013), Univ Southampton, Southampton, UNITED KINGDOM, JUL 08-11,
   2013}},
Abstract = {{In the field of multiple comparison procedures, adjusted p-values are an
   important tool to evaluate the significance of a test statistic while
   taking the multiplicity into account. In this paper, we introduce
   adjusted p-values for the recently proposed Sequential Goodness-of-Fit
   (SGoF) multiple test procedure by letting the level of the test vary on
   the unit interval. This extends previous research on the SGoF method,
   which is a method of high interest when one aims to increase the
   statistical power in a multiple testing scenario. The adjusted p-value
   is the smallest level at which the SGoF procedure would still reject the
   given null hypothesis, while controlling for the multiplicity of tests.
   The main properties of the adjusted p-values are investigated. In
   particular, we show that they are a subset of the original p-values,
   being equal to 1 for p-values above a certain threshold. These are very
   useful properties from a numerical viewpoint, since they allow for a
   simplified method to compute the adjusted p-values. We introduce a
   modification of the SGoF method, termed majorant version, which rejects
   the null hypotheses with adjusted p-values below the level. This
   modification rejects more null hypotheses as the level increases,
   something which is not in general the case for the original SGoF.
   Adjusted p-values for the conservative version of the SGoF procedure,
   which estimates the variance without assuming that all the null
   hypotheses are true, are also included. The situation with ties among
   the p-values is discussed too. Several real data applications are
   investigated to illustrate the practical usage of adjusted p-values,
   ranging from a small to a large number of tests.}},
DOI = {{10.1002/bimj.201300238}},
ISSN = {{0323-3847}},
EISSN = {{1521-4036}},
ResearcherID-Numbers = {{de Una-Alvarez, Jacobo/K-5667-2014}},
ORCID-Numbers = {{de Una-Alvarez, Jacobo/0000-0002-4686-8417}},
Unique-ID = {{ISI:000347536500009}},
}

@article{ ISI:000347248500020,
Author = {Majid, Abdul and Ali, Safdar},
Title = {{HBC-Evo: predicting human breast cancer by exploiting amino acid
   sequence-based feature spaces and evolutionary ensemble system}},
Journal = {{AMINO ACIDS}},
Year = {{2015}},
Volume = {{47}},
Number = {{1}},
Pages = {{217-226}},
Month = {{JAN}},
Abstract = {{We developed genetic programming (GP)-based evolutionary ensemble system
   for the early diagnosis, prognosis and prediction of human breast
   cancer. This system has effectively exploited the diversity in feature
   and decision spaces. First, individual learners are trained in different
   feature spaces using physicochemical properties of protein amino acids.
   Their predictions are then stacked to develop the best solution during
   GP evolution process. Finally, results for HBC-Evo system are obtained
   with optimal threshold, which is computed using particle swarm
   optimization. Our novel approach has demonstrated promising results
   compared to state of the art approaches.}},
DOI = {{10.1007/s00726-014-1871-3}},
ISSN = {{0939-4451}},
EISSN = {{1438-2199}},
Unique-ID = {{ISI:000347248500020}},
}

@article{ ISI:000346896200002,
Author = {Shenker, Sol and Miura, Pedro and Sanfilippo, Piero and Lao, Eric C.},
Title = {{IsoSCM: improved and alternative 3 ` UTR annotation using multiple
   change-point inference}},
Journal = {{RNA}},
Year = {{2015}},
Volume = {{21}},
Number = {{1}},
Pages = {{14-27}},
Month = {{JAN}},
Abstract = {{Major applications of RNA-seq data include studies of how the
   transcriptome is modulated at the levels of gene expression and RNA
   processing, and how these events are related to cellular identity,
   environmental condition, and/or disease status. While many excellent
   tools have been developed to analyze RNA-seq data, these generally have
   limited efficacy for annotating 3' UTRs. Existing assembly strategies
   often fragment long 3' UTRs, and importantly, none of the algorithms in
   popular use can apportion data into tandem 3' UTR isoforms, which are
   frequently generated by alternative cleavage and polyadenylation (APA).
   Consequently, it is often not possible to identify patterns of
   differential APA using existing assembly tools. To address these
   limitations, we present a new method for transcript assembly, Isoform
   Structural Change Model (IsoSCM) that incorporates change-point analysis
   to improve the 3' UTR annotation process. Through evaluation on
   simulated and genuine data sets, we demonstrate that IsoSCM annotates 3'
   termini with higher sensitivity and specificity than can be achieved
   with existing methods. We highlight the utility of IsoSCM by
   demonstrating its ability to recover known patterns of tissue-regulated
   APA. IsoSCM will facilitate future efforts for 3' UTR annotation and
   genome-wide studies of the breadth, regulation, and roles of APA
   leveraging RNA-seq data.}},
DOI = {{10.1261/rna.046037.114}},
ISSN = {{1355-8382}},
EISSN = {{1469-9001}},
ORCID-Numbers = {{Miura, Pedro/0000-0002-8434-5027
   Lai, Eric/0000-0002-8432-5851
   Sanfilippo, Piero/0000-0002-9650-5243}},
Unique-ID = {{ISI:000346896200002}},
}

@article{ ISI:000346236800001,
Author = {Shayegh, Soheil and Thomas, Valerie M.},
Title = {{Adaptive stochastic integrated assessment modeling of optimal greenhouse
   gas emission reductions}},
Journal = {{CLIMATIC CHANGE}},
Year = {{2015}},
Volume = {{128}},
Number = {{1-2}},
Pages = {{1-15}},
Month = {{JAN}},
Abstract = {{We develop a method for finding optimal greenhouse gas reduction rates
   under ongoing uncertainty and re-evaluation of climate parameters over
   future decades. Uncertainty about climate change includes both overall
   climate sensitivity and the risk of extreme tipping point events. We
   incorporate both types of uncertainty into a stochastic model of climate
   and the economy that has the objective of reducing global greenhouse gas
   emissions at lowest overall cost over time. Solving this problem is
   computationally challenging; we introduce a two-step-ahead approximate
   dynamic programming algorithm to solve the finite time horizon
   stochastic problem. The uncertainty in climate sensitivity may narrow in
   the future as the behavior of the climate continues to be observed and
   as climate science progresses. To incorporate this future knowledge, we
   use a Bayesian framework to update the two correlated uncertainties over
   time. The method is illustrated with the DICE integrated assessment
   model, adding in current estimates of climate sensitivity uncertainty
   and tipping point risk with an endogenous updating of climate
   sensitivity based on the occurrence of tipping point events; the method
   could also be applied to other integrated assessment models with
   different characterizations of uncertainties and risks.}},
DOI = {{10.1007/s10584-014-1300-3}},
ISSN = {{0165-0009}},
EISSN = {{1573-1480}},
ORCID-Numbers = {{Shayegh, Soheil/0000-0002-8960-8244
   Thomas, Valerie/0000-0002-0968-8863}},
Unique-ID = {{ISI:000346236800001}},
}

@article{ ISI:000346469900005,
Author = {Moore, Jason H. and Amos, Ryan and Kiralis, Jeff and Andrews, Peter C.},
Title = {{Heuristic Identification of Biological Architectures for Simulating
   Complex Hierarchical Genetic Interactions}},
Journal = {{GENETIC EPIDEMIOLOGY}},
Year = {{2015}},
Volume = {{39}},
Number = {{1}},
Pages = {{25-34}},
Month = {{JAN}},
Note = {{Workshop on Genetic Simulation Tools for Post-Genome Wide Association
   Studies of Complex Diseases, Natl Inst Hlth, Bethesda, MD, MAR 11-12,
   2014}},
Organization = {{Natl Canc Inst}},
Abstract = {{Simulation plays an essential role in the development of new
   computational and statistical methods for the genetic analysis of
   complex traits. Most simulations start with a statistical model using
   methods such as linear or logistic regression that specify the
   relationship between genotype and phenotype. This is appealing due to
   its simplicity and because these statistical methods are commonly used
   in genetic analysis. It is our working hypothesis that simulations need
   to move beyond simple statistical models to more realistically represent
   the biological complexity of genetic architecture. The goal of the
   present study was to develop a prototype genotype-phenotype simulation
   method and software that are capable of simulating complex genetic
   effects within the context of a hierarchical biology-based framework.
   Specifically, our goal is to simulate multilocus epistasis or gene-gene
   interaction where the genetic variants are organized within the
   framework of one or more genes, their regulatory regions and other
   regulatory loci. We introduce here the Heuristic Identification of
   Biological Architectures for simulating Complex Hierarchical
   Interactions (HIBACHI) method and prototype software for simulating data
   in this manner. This approach combines a biological hierarchy, a
   flexible mathematical framework, a liability threshold model for
   defining disease endpoints, and a heuristic search strategy for
   identifying high-order epistatic models of disease susceptibility. We
   provide several simulation examples using genetic models exhibiting
   independent main effects and three-way epistatic effects.}},
DOI = {{10.1002/gepi.21865}},
ISSN = {{0741-0395}},
EISSN = {{1098-2272}},
ORCID-Numbers = {{Moore, Jason/0000-0002-5015-1099}},
Unique-ID = {{ISI:000346469900005}},
}

@article{ ISI:000346070700002,
Author = {Madjidi, Faramarz and Mohammadi, Jamshid},
Title = {{A NEW METHOD TO CALCULATE THE THRESHOLD TEMPERATURE OF A PERFECT
   BLACKBODY TO PROTECT CORNEA AND LENS IN THE RANGE OF 780-3,000 nm}},
Journal = {{HEALTH PHYSICS}},
Year = {{2015}},
Volume = {{108}},
Number = {{1}},
Pages = {{8-14}},
Month = {{JAN}},
Abstract = {{Exposure to IR-A and IR-B radiation, in the wavelength region of 780 nm
   to 3,000 nm, may lead to the development of cataractogenesis. Estimation
   of the exposure levels is the first step in controlling adverse health
   effects. In the present study, the irradiance of a hot blackbody emitter
   is replaced by its temperature in the exposure limit values for cornea
   and lens in the range of 780-3,000 nm. This paper explains the
   development and implementation of a computer code to predict a
   temperature, defined as Threshold Temperature, which satisfies the
   exposure limits already proposed by the ICNIRP. To this end, first an
   infinite series was created for the calculation of spectral radiance by
   integration with Planck's law. For calculation of irradiance, the
   initial terms of this infinite series were selected, and integration was
   performed in the wavelength region of 780 nm to 3,000 nm. Finally, using
   a computer code, an unknown source temperature that can emit the same
   irradiance was found. Exposure duration, source area, and observer
   distance from the hot source were entered as input data in this proposed
   code. Consequently, it is possible only by measurement of a Planckian
   emitter temperature and taking into account the distance from source and
   exposure time for an observer to decide whether the exposure to IR
   radiation in the range of 780 to 3,000 nm is permissible or not. It
   seems that the substitution of irradiance by the source temperature is
   an easier and more convenient way for hygienists to evaluate IR
   exposures.}},
DOI = {{10.1097/HP.0000000000000173}},
ISSN = {{0017-9078}},
EISSN = {{1538-5159}},
ResearcherID-Numbers = {{Madjidi, Faramarz/S-1038-2016
   Mohammadi, Jamshid/M-6515-2016}},
ORCID-Numbers = {{Madjidi, Faramarz/0000-0003-3300-1184
   Mohammadi, Jamshid/0000-0001-9472-5130}},
Unique-ID = {{ISI:000346070700002}},
}

@article{ ISI:000347018400061,
Author = {Yao, Linjun and Naeth, M. Anne},
Title = {{Soil and plant response to unused potassium silicate drilling fluid
   application}},
Journal = {{ECOLOGICAL ENGINEERING}},
Year = {{2014}},
Volume = {{73}},
Pages = {{461-468}},
Month = {{DEC}},
Abstract = {{Drilling fluid, also referred to as drilling mud, is a major waste from
   oil and gas drilling. Land application is a novel approach to potassium
   silicate drilling fluid (PSDF) waste recycling, addressing its disposal
   requirements while potentially improving soil quality for land
   reclamation. Inorganic nitrogen (N) and phosphorus (P) fertilizer (0,34
   N: 45 P kg ha(-1)) was added with PSDF (0, 30, 45, 60 m(3) ha(-1)) as
   eight PSDF amendments. PSDF amendments were incorporated or sprayed on
   four reclamation soils (sand, loam, clay loam 1 and 2). Response to PSDF
   application was assessed in the greenhouse with two plant species
   (Hordeum vulgare L. (barley) and Agropyron trachycaulum (Link) Malte
   (slender wheat grass).
   PSDF amendments had no detrimental effects on soil quality
   (macronutrients, pH, salinity, sodicity) and plant growth except in clay
   loam 2 soil. In loam soil, barley height and biomass were greater with
   PSDF at 45 m3 ha(-1) with fertilizer relative to soil without PSDF. In
   sand soil with PSDF at the highest rate without fertilizer, wheat grass
   height was 1.08 times and biomass was 1.76 times greater than the
   control. High electrical conductivity in clay loam 2 soil, and decreased
   density, height and biomass of wheat grass at highest PSDF application
   rates or with PSDF incorporation, suggest a threshold beyond which
   conditions are compromised for PSDF application. Increasing PSDF
   application rate increased soil potassium availability by 1.6-4.1 times
   relative to no PSDF. This initial research demonstrates that PSDF may be
   an appropriate soil amendment for agricultural crops and native plant
   species on land reclamation sites with consideration of substrates
   properties, plant species tolerances and inorganic fertilizer. (C) 2014
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecoleng.2014.09.110}},
ISSN = {{0925-8574}},
EISSN = {{1872-6992}},
ORCID-Numbers = {{Naeth, M Anne/0000-0002-9147-1638}},
Unique-ID = {{ISI:000347018400061}},
}

@article{ ISI:000345723100036,
Author = {Amorosi, Alessandro and Guermandi, Marina and Marchi, Nazaria and
   Sammartino, Irene},
Title = {{Fingerprinting sedimentary and soil units by their natural metal
   contents: A new approach to assess metal contamination}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2014}},
Volume = {{500}},
Pages = {{361-372}},
Month = {{DEC 1}},
Abstract = {{One of the major issues when assessing soil contamination by inorganic
   substances is reliable determination of natural metal concentrations.
   Through integrated sedimentological, pedological and geochemical
   analyses of 1414 (topsoil/subsoil) samples from 707 sampling stations in
   the southern Po Plain (Italy), we document that the natural distribution
   of five potentially toxic metals (Cr, Ni, Cu, Zn and Pb) can be
   spatially predicted as a function of three major factors: source-rock
   composition, grain size variability and degree of soil weathering.
   Thirteen genetic and functional soil units (GFUs), each reflecting a
   unique combination of these three variables, are fingerprinted by
   distinctive geochemical signatures. Where sediment is supplied by
   ultramafic (ophiolite-rich) sources, the natural contents of Cr and Ni
   in soils almost invariably exceed the Italian threshold limits
   designated for contaminated lands (150 mg/kg and 120 mg/kg,
   respectively), with median values around twice the maximum permissible
   levels (345 mg/kg for Cr and 207 mg/kg for Ni in GFU B5). The original
   provenance signal is commonly confounded by soil texture, with general
   tendency toward higher metal concentrations in the finest-grained
   fractions. Once reliable natural metal concentrations in soils are
   established, the anthropogenic contribution can be promptly assessed by
   calculating metal enrichments in topsoil samples. The use of combined
   sedimentological and pedological criteria to fingerprint GFU geochemical
   composition is presented here as a new approach to enhance
   predictability of natural metal contents, with obvious positive
   feedbacks for legislative purposes and environmental protection.
   Particularly, natural metal concentrations inferred directly from a new
   type of pedogeochemical map, built according to the international
   guideline ISO 19258, are proposed as an efficient alternative to the
   pre-determined threshold values for soil contamination commonly
   established by the national regulations. (C) 2014 Elsevier B.V. All
   rights reserved.}},
DOI = {{10.1016/j.scitotenv.2014.08.078}},
ISSN = {{0048-9697}},
EISSN = {{1879-1026}},
Unique-ID = {{ISI:000345723100036}},
}

@article{ ISI:000345329500002,
Author = {Yu, Zhongbo and Fu, Xiaolei and Lue, Haishen and Luo, Lifeng and Liu, Di
   and Ju, Qin and Xiang, Long and Wang, Zongzhi},
Title = {{Evaluating Ensemble Kalman, Particle, and Ensemble Particle Filters
   through Soil Temperature Prediction}},
Journal = {{JOURNAL OF HYDROLOGIC ENGINEERING}},
Year = {{2014}},
Volume = {{19}},
Number = {{12}},
Month = {{DEC}},
Abstract = {{Data assimilation is a useful tool in hydrologic and agricultural
   application studies because of its ability to produce predicted results
   with high accuracy. However, different data-assimilation methods have
   different performances for a given application. Although the popular
   ensemble Kalman filter (EnKF) performs well with Gaussian distribution,
   the error is difficult to conform to the Gaussian distribution. To take
   advantage of the EnKF, this study presents a new data-assimilation
   method, ensemble particle filter (EnPF), which is an integration of the
   EnKF and the particle filter (PF). This new method was evaluated in
   comparison with two existing methods (EnKF and PF) through soil
   temperature predictions. The simple biosphere model (SiB2) and the
   filters were assessed with observations from the Wudaogou experimental
   area in the Huaihe River basin, China. Results show that when the time
   interval increases adequately, all the simulated or assimilated results
   improve significantly. All of these filters tend to be more stable when
   the number of particles reaches a certain amount (e.g.,60) or the
   variance is small (e.g.,less than 0.6) in the study. When the number of
   particles is less than a threshold value (e.g.,30), the advantage among
   these three methods is not appreciable. The error obtained by EnPF is
   smaller than that by EnKF and PF; this means that EnPF performs better
   than EnKF and PF. (C) 2014 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)HE.1943-5584.0000976}},
ISSN = {{1084-0699}},
EISSN = {{1943-5584}},
ORCID-Numbers = {{Luo, Lifeng/0000-0002-2829-7104
   wang, zongzhi/0000-0003-1013-9594}},
Unique-ID = {{ISI:000345329500002}},
}

@article{ ISI:000347430000001,
Author = {Fredriksson, Nils Johan and Hermansson, Malte and Wilen, Britt-Marie},
Title = {{Impact of T-RFLP data analysis choices on assessments of microbial
   community structure and dynamics}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2014}},
Volume = {{15}},
Month = {{NOV 8}},
Abstract = {{Background: Terminal restriction fragment length polymorphism (T-RFLP)
   analysis is a common DNA-fingerprinting technique used for comparisons
   of complex microbial communities. Although the technique is well
   established there is no consensus on how to treat T-RFLP data to achieve
   the highest possible accuracy and reproducibility. This study focused on
   two critical steps in the T-RFLP data treatment: the alignment of the
   terminal restriction fragments (T-RFs), which enables comparisons of
   samples, and the normalization of T-RF profiles, which adjusts for
   differences in signal strength, total fluorescence, between samples.
   Results: Variations in the estimation of T-RF sizes were observed and
   these variations were found to affect the alignment of the T-RFs. A
   novel method was developed which improved the alignment by adjusting for
   systematic shifts in the T-RF size estimations between the T-RF
   profiles. Differences in total fluorescence were shown to be caused by
   differences in sample concentration and by the gel loading. Five
   normalization methods were evaluated and the total fluorescence
   normalization procedure based on peak height data was found to increase
   the similarity between replicate profiles the most. A high peak
   detection threshold, alignment correction, normalization and the use of
   consensus profiles instead of single profiles increased the similarity
   of replicate T-RF profiles, i.e. lead to an increased reproducibility.
   The impact of different treatment methods on the outcome of subsequent
   analyses of T-RFLP data was evaluated using a dataset from a
   longitudinal study of the bacterial community in an activated sludge
   wastewater treatment plant. Whether the alignment was corrected or not
   and if and how the T-RF profiles were normalized had a substantial
   impact on ordination analyses, assessments of bacterial dynamics and
   analyses of correlations with environmental parameters.
   Conclusions: A novel method for the evaluation and correction of the
   alignment of T-RF profiles was shown to reduce the uncertainty and
   ambiguity in alignments of T-RF profiles. Large differences in the
   outcome of assessments of bacterial community structure and dynamics
   were observed between different alignment and normalization methods. The
   results of this study can therefore be of value when considering what
   methods to use in the analysis of T-RFLP data.}},
DOI = {{10.1186/s12859-014-0360-8}},
Article-Number = {{360}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Wilen, Britt-Marie/0000-0001-6155-7759
   Fredriksson, Nils Johan/0000-0002-5763-6777}},
Unique-ID = {{ISI:000347430000001}},
}

@article{ ISI:000348675700006,
Author = {Gao, Lianru and Guo, Qiandong and Plaza, Antonio and Li, Jun and Zhang,
   Bing},
Title = {{Probabilistic anomaly detector for remotely sensed hyperspectral data}},
Journal = {{JOURNAL OF APPLIED REMOTE SENSING}},
Year = {{2014}},
Volume = {{8}},
Month = {{NOV 3}},
Abstract = {{Anomaly detection is an important technique for remotely sensed
   hyperspectral data exploitation. In the last decades, several algorithms
   have been developed for detecting anomalies in hyperspectral images. The
   Reed-Xiaoli detector (RXD) is one of the most widely used approaches for
   this purpose. Since the RXD assumes that the distribution of the
   background is Gaussian, it generally suffers from a high false alarm
   rate. In order to address this issue, we introduce an unsupervised
   probabilistic anomaly detector (PAD) based on estimating the difference
   between the probabilities of the anomalies and the background. The
   proposed PAD takes advantage of the results provided by the RXD to
   estimate statistical information for the targets and background,
   respectively, and then uses an automatic strategy to find the most
   suitable threshold for the separation of targets from the background.
   The proposed technique is validated using a synthetic data set and two
   real hyperspectral data sets with ground-truth information. Our
   experimental results indicate that the proposed method achieves good
   detection ratios with adequate computational complexity as compared with
   other widely used anomaly detectors. (C) 2014 Society of Photo-Optical
   Instrumentation Engineers (SPIE)}},
DOI = {{10.1117/1.JRS.8.083538}},
Article-Number = {{083538}},
ISSN = {{1931-3195}},
Unique-ID = {{ISI:000348675700006}},
}

@article{ ISI:000345631300005,
Author = {Mbah, A. K. and Paothong, A. and Wilson, R. E. and Salihu, H. M.},
Title = {{Utility and application of SEM in delineating cause-effect relationship
   between ambient pollutant and fetal birth outcome}},
Journal = {{ENVIRONMETRICS}},
Year = {{2014}},
Volume = {{25}},
Number = {{7}},
Pages = {{548-556}},
Month = {{NOV}},
Abstract = {{The US Environmental Protection Agency has set air quality standards for
   six criteria pollutants based on threshold levels that could be
   hazardous to human health. Interest in the potential impact of prenatal
   exposure to these pollutants has increased; however, validity of
   modeling approaches has hindered research in this area. To address the
   methodological concerns of the previous studies, we introduce the
   utility of modeling exposure-outcome relationships in maternal and child
   health environmental research using the structural equation modeling
   framework. The dataset for this study comprised all live births in
   Hillsborough County, FL, covering the period from 1998 to 2007
   inclusive. Vital records data were linked to the hospital discharge and
   vital statistic birth data for analysis. Additionally, local air
   pollution data were utilized. Out of 104,967 births, 8813 (rate = 8.4\%)
   were born with low birth weight (LBW), 6757 infants were preterm birth
   (PTB) (rate = 6.4\%), and 9575 (rate = 9.1\%) were born small for
   gestational age (SGA). Our structural equation modeling approach
   indicated the following: (1) five of the six criteria pollutants had a
   positive contribution to the latent variable and (2) all the outcome
   variables (SGA, LBW, and PTB) had a positive relationship with the
   latent variable, thereby, suggesting an association between higher
   pollutant values and adverse birth outcomes. Copyright (C) 2014 John
   Wiley \& Sons, Ltd.}},
DOI = {{10.1002/env.2298}},
ISSN = {{1180-4009}},
EISSN = {{1099-095X}},
Unique-ID = {{ISI:000345631300005}},
}

@article{ ISI:000345621100017,
Author = {Yan Shi-jiang and Tang Guo'an and Li Fa-yuan and Zhang Lei},
Title = {{Snake Model for the Extraction of Loess Shoulder-line from DEMs}},
Journal = {{JOURNAL OF MOUNTAIN SCIENCE}},
Year = {{2014}},
Volume = {{11}},
Number = {{6}},
Pages = {{1552-1559}},
Month = {{NOV}},
Abstract = {{Shoulder lines are the most important landform demarcations for
   geographical analysis, soil erosion modeling and land use planning in
   the Loess Plateau area of China. This paper proposes an automatic,
   effective and accurate method of determining loess shoulder line from
   DEMs by integrating a hydrological D8 algorithm and a snake model. The
   watershed boundary line is adopted as the initial contour which evolves
   to identify the exact position of loess shoulder-line by the guidance of
   an external force of snake model from DEMs. Experiments show that the
   method overcomes the difficulties in both threshold selection for edge
   detection and the disconnecting issues in former extraction approaches.
   The accuracy evaluation of shoulder-line maps from the two test sites of
   the loess plateau area show obvious improvements in the extraction. The
   average contour matching distance of the new method is 12.0 m on 5 m
   resolution DEM, and shows improvement in the accuracy and continuity.
   The comparisons of accuracy evaluations of the two test sites show that
   the snake model method performs better in the loess plain area than in
   the area with high gully density.}},
DOI = {{10.1007/s11629-013-2484-0}},
ISSN = {{1672-6316}},
EISSN = {{1993-0321}},
ResearcherID-Numbers = {{Li, Fayuan/P-8264-2017
   Tang, Guo-An/W-3278-2017}},
ORCID-Numbers = {{Li, Fayuan/0000-0001-5495-5193
   }},
Unique-ID = {{ISI:000345621100017}},
}

@article{ ISI:000345564300076,
Author = {Calderon, Christopher P.},
Title = {{Data-Driven Techniques for Detecting Dynamical State Changes in Noisily
   Measured 3D Single-Molecule Trajectories}},
Journal = {{MOLECULES}},
Year = {{2014}},
Volume = {{19}},
Number = {{11}},
Pages = {{18381-18398}},
Month = {{NOV}},
Abstract = {{Optical microscopes and nanoscale probes (AFM, optical tweezers, etc.)
   afford researchers tools capable of quantitatively exploring how
   molecules interact with one another in live cells. The analysis of in
   vivo single-molecule experimental data faces numerous challenges due to
   the complex, crowded, and time changing environments associated with
   live cells. Fluctuations and spatially varying systematic forces
   experienced by molecules change over time; these changes are obscured by
   ``measurement noise{''} introduced by the experimental probe monitoring
   the system. In this article, we demonstrate how the Hierarchical
   Dirichlet Process Switching Linear Dynamical System (HDP-SLDS) of Fox et
   al. {[}IEEE Transactions on Signal Processing 59] can be used to detect
   both subtle and abrupt state changes in time series containing
   ``thermal{''} and ``measurement{''} noise. The approach accounts for
   temporal dependencies induced by random and ``systematic overdamped{''}
   forces. The technique does not require one to subjectively select the
   number of ``hidden states{''} underlying a trajectory in an a priori
   fashion. The number of hidden states is simultaneously inferred along
   with change points and parameters characterizing molecular motion in a
   data-driven fashion. We use large scale simulations to study and compare
   the new approach to state-of-the-art Hidden Markov Modeling techniques.
   Simulations mimicking single particle tracking (SPT) experiments are the
   focus of this study.}},
DOI = {{10.3390/molecules191118381}},
ISSN = {{1420-3049}},
Unique-ID = {{ISI:000345564300076}},
}

@article{ ISI:000345189800008,
Author = {Chauvet, Pierre E. and Tich, Sylvie Nguyen The and Schang, Daniel and
   Clement, Alain},
Title = {{Evaluation of automatic feature detection algorithms in EEG: Application
   to interburst intervals}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2014}},
Volume = {{54}},
Pages = {{61-71}},
Month = {{NOV 1}},
Abstract = {{In this paper, we present a new method to compare and improve algorithms
   for feature detection in neonatal EEG. The method is based on the
   algorithm's ability to compute accurate statistics to predict the
   results of EEG visual analysis. This method is implemented inside a Java
   software called EEGDiag, as part of an e-health Web portal dedicated to
   neonatal EEG.
   EEGDiag encapsulates a component-based implementation of the detection
   algorithms called analyzers. Each analyzer is defined by a list of
   modules executed sequentially. As the libraries of modules are intended
   to be enriched by its users, we developed a process to evaluate the
   performance of new modules and analyzers using a database of expertized
   and categorized EEGs. The evaluation is based on the Davies-Bouldin
   index (DBI) which measures the quality of cluster separation, so that it
   will ease the building of classifiers on risk categories. For the first
   application we tested this method on the detection of interburst
   intervals (IBI) using a database of 394 EEG acquired on premature
   newborns. We have defined a class of IBI detectors based on a threshold
   of the standard deviation on contiguous short time windows, inspired by
   previous work. Then we determine which detector and what threshold
   values are the best regarding DBI, as well as the robustness of this
   choice. This method allows us to make counter-intuitive choices, such as
   removing the 50 Hz filter (power supply) to save time. (C) 2014 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiomed.2014.08.011}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
Unique-ID = {{ISI:000345189800008}},
}

@article{ ISI:000345326000006,
Author = {Seddon, Alistair W. R. and Froyd, Cynthia A. and Witkowski, Andrzej and
   Willis, Katherine J.},
Title = {{A quantitative framework for analysis of regime shifts in a Galapagos
   coastal lagoon}},
Journal = {{ECOLOGY}},
Year = {{2014}},
Volume = {{95}},
Number = {{11}},
Pages = {{3046-3055}},
Month = {{NOV}},
Abstract = {{Regime shifts are often used to describe sharp changes between two or
   more ecological states, each characterized by their own dynamics,
   stochastic fluctuations, or cycles. Ecological theory indicates they can
   occur either as a result of an abrupt environmental forcing (extrinsic
   regime shift), or be indicative of complex responses to local-scale
   dynamics and thresholds (intrinsic regime shift). One important area of
   ecological research is to develop quantitative tools to analyze regime
   shifts, but there are few studies that have applied these methods to the
   long-term ecological record. In this study, we introduce a framework to
   investigate regime shifts in diatom assemblages and mangrove ecosystem
   dynamics in a coastal lagoon from the Galapagos Islands over the past
   2600 years. The framework integrates a set of established statistical
   methodologies for investigating regime shift dynamics in long-term
   ecological records. We use these methods to (1) identify the presence of
   regime shifts; (2) test for a series of hypothetical relationships
   (i.e., linear through to threshold) between ecological response and
   environment using nonlinear regression; and (3) investigate the relative
   importance of intrinsic and extrinsic dynamics in response to
   environmental perturbations. The transitions in the diatoms closely
   track the sequence of disturbance, recovery, and habitat shifts that
   have occurred in the lagoon over the past 2600 years, demonstrating
   extrinsic responses to environmental forcing. In contrast, the shift
   from a mangrove-to a microbial mat-dominated system; 945 cal yr BP
   provides potential evidence of an intrinsic regime shift. Our framework
   enables robust interpretations into the underlying dynamics of regime
   shifts in the paleoecological record and is widely applicable for
   investigating abrupt ecological changes in a range of systems.}},
ISSN = {{0012-9658}},
EISSN = {{1939-9170}},
ResearcherID-Numbers = {{Froyd, Cynthia/A-8488-2012
   }},
ORCID-Numbers = {{Froyd, Cynthia/0000-0001-5291-9156
   Seddon, Alistair/0000-0002-8266-0947}},
Unique-ID = {{ISI:000345326000006}},
}

@article{ ISI:000345156000002,
Author = {Zhang, Libao and Li, Aoxue},
Title = {{Thresholding-based remote sensing image segmentation using mean absolute
   deviation algorithm}},
Journal = {{JOURNAL OF APPLIED REMOTE SENSING}},
Year = {{2014}},
Volume = {{8}},
Month = {{OCT 27}},
Abstract = {{Simple and effective segmentation algorithms are required for remote
   sensing images because of their mass data and complex texture features.
   An algorithm based on minimum class mean absolute deviation (MCMAD) is
   proposed. First, a two-dimensional (2-D) histogram is constructed by a
   median filter and gray process. Second, by using a diagonal projection,
   the 2-D histogram of remote sensing images is transformed into a
   one-dimensional (1-D) histogram to decrease the computational
   complexity. Finally, class mean absolute deviation of each threshold in
   the 1-D histogram is calculated and the threshold corresponding to the
   MCMAD is considered as the optimal segmentation threshold. To improve
   performance, we introduce spectral information into the MCMAD algorithm
   and the results of spectral bands are combined to get final segmentation
   results. Because most of the background used in our experiment is
   vegetation, we introduce a normalized difference vegetation index band
   into our algorithm and use the MCMAD algorithm on it. Experimental
   results show that our algorithms not only perform better for remote
   sensing images but also meet time requirements. (C) 2014 Society of
   Photo-Optical Instrumentation Engineers (SPIE)}},
DOI = {{10.1117/1.JRS.8.083542}},
Article-Number = {{083542}},
ISSN = {{1931-3195}},
Unique-ID = {{ISI:000345156000002}},
}

@article{ ISI:000344427500005,
Author = {Luo, Jiawei and Kuang, Ling},
Title = {{A new method for predicting essential proteins based on dynamic network
   topology and complex information}},
Journal = {{COMPUTATIONAL BIOLOGY AND CHEMISTRY}},
Year = {{2014}},
Volume = {{52}},
Pages = {{34-42}},
Month = {{OCT}},
Abstract = {{Predicting essential proteins is highly significant because organisms
   can not survive or develop even if only one of these proteins is
   missing. Improvements in high-throughput technologies have resulted in a
   large number of available protein-protein interactions. By taking
   advantage of these interaction data, researchers have proposed many
   computational methods to identify essential proteins at the network
   level. Most of these approaches focus on the topology of a static
   protein interaction network. However, the protein interaction network
   changes with time and condition. This important inherent dynamics of the
   protein interaction network is overlooked by previous methods. In this
   paper, we introduce a new method named CDLC to predict essential
   proteins by integrating dynamic local average connectivity and in-degree
   of proteins in complexes. CDLC is applied to the protein interaction
   network of Saccharomyces cerevisiae. The results show that CDLC
   outperforms five other methods (Degree Centrality (DC), Local Average
   Connectivity-based method (LAC), Sum of ECC (SoECC), PeC and
   Co-Expression Weighted by Clustering coefficient (CoEWC)). In
   particular, CDLC could improve the prediction precision by more than
   45\% compared with DC methods. CDLC is also compared with the latest
   algorithm CEPPK, and a higher precision is achieved by CDLC. CDLC is
   available as Supplementary materials. The default settings of active
   threshold and alpha-parameter are 0.8 and 0.1, respectively. (C) 2014
   Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiolchem.2014.08.022}},
ISSN = {{1476-9271}},
EISSN = {{1476-928X}},
Unique-ID = {{ISI:000344427500005}},
}

@article{ ISI:000343050700002,
Author = {Gulbudak, Hayriye and Martcheva, Maia},
Title = {{A Structured Avian Influenza Model with Imperfect Vaccination and
   Vaccine-Induced Asymptomatic Infection}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{2014}},
Volume = {{76}},
Number = {{10}},
Pages = {{2389-2425}},
Month = {{OCT}},
Abstract = {{We introduce a model of avian influenza in domestic birds with imperfect
   vaccination and age-since-vaccination structure. The model has four
   components: susceptible birds, vaccinated birds (stratified by
   vaccination age), asymptomatically infected birds, and infected birds.
   The model includes reduction in the probability of infection, decreasing
   severity of disease of vaccinated birds and vaccine waning. The basic
   reproduction number, , is calculated. The disease-free equilibrium is
   found to be globally stable under certain conditions when . When ,
   existence of an endemic equilibrium is proved (with uniqueness for the
   ODE case and local stability under stricter conditions) and uniform
   persistence of the disease is established. The inclusion of reduction in
   susceptibility of vaccinated birds, reduction in infectiousness of
   asymptomatically infected birds and vaccine waning can have important
   implications for disease control. We analytically and numerically
   demonstrate that vaccination can paradoxically increase the total number
   of infected, resulting in the ``silent spread{''} of the disease. We
   also study the effects of vaccine efficacy on disease prevalence and the
   minimum critical vaccination coverage, a threshold value for vaccination
   coverage to avoid an increase in total disease prevalence due to
   asymptomatic infection.}},
DOI = {{10.1007/s11538-014-0012-1}},
ISSN = {{0092-8240}},
EISSN = {{1522-9602}},
Unique-ID = {{ISI:000343050700002}},
}

@article{ ISI:000340479400009,
Author = {Yang, Ying and Yin, Xin'an and Chen, He and Yang, Zhifeng},
Title = {{Determining water level management strategies for lake protection at the
   ecosystem level}},
Journal = {{HYDROBIOLOGIA}},
Year = {{2014}},
Volume = {{738}},
Number = {{1}},
Pages = {{111-127}},
Month = {{OCT}},
Abstract = {{To prevent lake degradation, water level management has been a major
   focus of research in the past several decades. There are, however, some
   shortcomings in the traditional studies, and the protection of entire
   ecosystems is difficult to achieve in practice. In this paper, the
   framework of a new method for determining ecosystem-based water level
   regimes (WLRs) for lake protection is proposed. First, historical WLRs
   are divided into several sub-stages. Then, ecosystem statuses
   corresponding to different WLRs are quantified and compared. Finally,
   parameters of optimal and acceptable WLRs are used to determine water
   level management goals. The proposed method was applied to Baiyangdian
   Lake, the largest shallow lake in the North China Plain, to test its
   effectiveness. Results showed that to protect the ecosystem at the
   optimal status, 50\% of the parameter values should fall within the
   range of the 25th and 75th percentiles of Stage I; and to protect the
   ecosystem from reverse succession, 50\% of the water level parameter
   values should fall within the range of the 25th and 75th percentiles of
   Stage II. This method takes ecosystem status into account, and has high
   practicability in water resources management.}},
DOI = {{10.1007/s10750-014-1923-4}},
ISSN = {{0018-8158}},
EISSN = {{1573-5117}},
Unique-ID = {{ISI:000340479400009}},
}

@article{ ISI:000342912400051,
Author = {Bari, Mehrab Ghanat and Ma, Xuepo and Zhang, Jianqiu},
Title = {{PeakLink: a new peptide peak linking method in LC-MS/MS using wavelet
   and SVM}},
Journal = {{BIOINFORMATICS}},
Year = {{2014}},
Volume = {{30}},
Number = {{17}},
Pages = {{2464-2470}},
Month = {{SEP 1}},
Note = {{13th European Conference on Computational Biology (ECCB), Strasbourg,
   FRANCE, SEP 07-10, 2014}},
Organization = {{BioBase; Sbv IMPROVER; Koriscale; Totalinux; Genom, Proteom \&
   Bioinformat}},
Abstract = {{Motivation: In liquid chromatography-mass spectrometry/tandem mass
   spectrometry (LC-MS/MS), it is necessary to link tandem MS-identified
   peptide peaks so that protein expression changes between the two runs
   can be tracked. However, only a small number of peptides can be
   identified and linked by tandem MS in two runs, and it becomes necessary
   to link peptide peaks with tandem identification in one run to their
   corresponding ones in another run without identification. In the past,
   peptide peaks are linked based on similarities in retention time (rt),
   mass or peak shape after rt alignment, which corrects mean rt shifts
   between runs. However, the accuracy in linking is still limited
   especially for complex samples collected from different conditions.
   Consequently, large-scale proteomics studies that require comparison of
   protein expression profiles of hundreds of patients can not be carried
   out effectively.
   Method: In this article, we consider the problem of linking peptides
   from a pair of LC-MS/MS runs and propose a new method, PeakLink (PL),
   which uses information in both the time and frequency domain as inputs
   to a non-linear support vector machine (SVM) classifier. The PL
   algorithm first uses a threshold on an rt likelihood ratio score to
   remove candidate corresponding peaks with excessively large elution time
   shifts, then PL calculates the correlation between a pair of candidate
   peaks after reducing noise through wavelet transformation. After
   converting rt and peak shape correlation to statistical scores, an SVM
   classifier is trained and applied for differentiating corresponding and
   non-corresponding peptide peaks.
   Results: PL is tested in multiple challenging cases, in which LC-MS/MS
   samples are collected from different disease states, different
   instruments and different laboratories. Testing results show significant
   improvement in linking accuracy compared with other algorithms.}},
DOI = {{10.1093/bioinformatics/btu299}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000342912400051}},
}

@article{ ISI:000341313300007,
Author = {Mueller, N. and Kierfeld, J.},
Title = {{Effects of microtubule mechanics on hydrolysis and catastrophes}},
Journal = {{PHYSICAL BIOLOGY}},
Year = {{2014}},
Volume = {{11}},
Number = {{4}},
Month = {{AUG}},
Abstract = {{We introduce a model for microtubule (MT) mechanics containing lateral
   bonds between dimers in neighboring protofilaments, bending rigidity of
   dimers, and repulsive interactions between protofilaments modeling
   steric constraints to investigate the influence of mechanical forces on
   hydrolysis and catastrophes. We use the allosteric dimer model, where
   tubulin dimers are characterized by an equilibrium bending angle, which
   changes from 0 degrees to 22 degrees by hydrolysis of a dimer. This also
   affects the lateral interaction and bending energies and, thus, the
   mechanical equilibrium state of the MT. As hydrolysis gives rise to
   conformational changes in dimers, mechanical forces also influence the
   hydrolysis rates by mechanical energy changes modulating the hydrolysis
   rate. The interaction via the MT mechanics then gives rise to
   correlation effects in the hydrolysis dynamics, which have not been
   taken into account before. Assuming a dominant influence of mechanical
   energies on hydrolysis rates, we investigate the most probable
   hydrolysis pathways both for vectorial and random hydrolysis.
   Investigating the stability with respect to lateral bond rupture, we
   identify initiation configurations for catastrophes along the hydrolysis
   pathways and values for a lateral bond rupture force. If we allow for
   rupturing of lateral bonds between dimers in neighboring protofilaments
   above this threshold force, our model exhibits avalanche-like
   catastrophe events.}},
DOI = {{10.1088/1478-3975/11/4/046001}},
Article-Number = {{046001}},
ISSN = {{1478-3967}},
EISSN = {{1478-3975}},
ResearcherID-Numbers = {{Kierfeld, Jan/A-2659-2009}},
ORCID-Numbers = {{Kierfeld, Jan/0000-0003-4291-0638}},
Unique-ID = {{ISI:000341313300007}},
}

@article{ ISI:000340049100018,
Author = {Park, Sung Kyu Robin and Aslanian, Aaron and McClatchy, Daniel B. and
   Han, Xuemei and Shah, Harshil and Singh, Meha and Rauniyar, Navin and
   Moresco, James J. and Pinto, Antonio F. M. and Diedrich, Jolene K. and
   Delahunty, Claire and Yates, III, John R.},
Title = {{Census 2: isobaric labeling data analysis}},
Journal = {{BIOINFORMATICS}},
Year = {{2014}},
Volume = {{30}},
Number = {{15}},
Pages = {{2208-2209}},
Month = {{AUG 1}},
Abstract = {{We introduce Census 2, an update of a mass spectrometry data analysis
   tool for peptide/protein quantification. New features for analysis of
   isobaric labeling, such as Tandem Mass Tag (TMT) or Isobaric Tags for
   Relative and Absolute Quantification (iTRAQ), have been added in this
   version, including a reporter ion impurity correction, a reporter ion
   intensity threshold filter and an option for weighted normalization to
   correct mixing errors. TMT/iTRAQ analysis can be performed on
   experiments using HCD (High Energy Collision Dissociation) only, CID
   (Collision Induced Dissociation)/HCD (High Energy Collision
   Dissociation) dual scans or HCD triple-stage mass spectrometry data. To
   improve measurement accuracy, we implemented weighted normalization,
   multiple tandem spectral approach, impurity correction and dynamic
   intensity threshold features.}},
DOI = {{10.1093/bioinformatics/btu151}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Rauniyar, Navin/A-3318-2016
   }},
ORCID-Numbers = {{Moresco, James/0000-0003-1178-1642
   Pinto, Antonio/0000-0002-1573-8011}},
Unique-ID = {{ISI:000340049100018}},
}

@article{ ISI:000340798100001,
Author = {Kiani, Narsis A. and Kaderali, Lars},
Title = {{Dynamic probabilistic threshold networks to infer signaling pathways
   from time-course perturbation data}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2014}},
Volume = {{15}},
Month = {{JUL 22}},
Abstract = {{Background: Network inference deals with the reconstruction of molecular
   networks from experimental data. Given N molecular species, the
   challenge is to find the underlying network. Due to data limitations,
   this typically is an ill-posed problem, and requires the integration of
   prior biological knowledge or strong regularization. We here focus on
   the situation when time-resolved measurements of a system's response
   after systematic perturbations are available.
   Results: We present a novel method to infer signaling networks from
   time-course perturbation data. We utilize dynamic Bayesian networks with
   probabilistic Boolean threshold functions to describe protein
   activation. The model posterior distribution is analyzed using
   evolutionary MCMC sampling and subsequent clustering, resulting in
   probability distributions over alternative networks. We evaluate our
   method on simulated data, and study its performance with respect to data
   set size and levels of noise. We then use our method to study
   EGF-mediated signaling in the ERBB pathway.
   Conclusions: Dynamic Probabilistic Threshold Networks is a new method to
   infer signaling networks from time-series perturbation data. It exploits
   the dynamic response of a system after external perturbation for network
   reconstruction. On simulated data, we show that the approach outperforms
   current state of the art methods. On the ERBB data, our approach
   recovers a significant fraction of the known interactions, and predicts
   novel mechanisms in the ERBB pathway.}},
DOI = {{10.1186/1471-2105-15-250}},
Article-Number = {{250}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Kaderali, Lars/0000-0002-2359-2294
   Kiani, Narsis/0000-0002-0949-046X}},
Unique-ID = {{ISI:000340798100001}},
}

@article{ ISI:000339715000080,
Author = {Hach, Faraz and Sarrafi, Iman and Hormozdiari, Farhad and Alkan, Can and
   Eichler, Evan E. and Sahinalp, S. Cenk},
Title = {{mrsFAST-Ultra: a compact, SNP-aware mapper for high performance
   sequencing applications}},
Journal = {{NUCLEIC ACIDS RESEARCH}},
Year = {{2014}},
Volume = {{42}},
Number = {{W1}},
Pages = {{W494-W500}},
Month = {{JUL 1}},
Abstract = {{High throughput sequencing (HTS) platforms generate unprecedented
   amounts of data that introduce challenges for processing and downstream
   analysis. While tools that report the `best' mapping location of each
   read provide a fast way to process HTS data, they are not suitable for
   many types of downstream analysis such as structural variation
   detection, where it is important to report multiple mapping loci for
   each read. For this purpose we introduce mrsFAST-Ultra, a fast, cache
   oblivious, SNP-aware aligner that can handle the multi-mapping of HTS
   reads very efficiently. mrsFAST-Ultra improves mrsFAST, our first cache
   oblivious read aligner capable of handling multi-mapping reads, through
   new and compact index structures that reduce not only the overall memory
   usage but also the number of CPU operations per alignment. In fact the
   size of the index generated by mrsFAST-Ultra is 10 times smaller than
   that of mrsFAST. As importantly, mrsFAST-Ultra introduces new features
   such as being able to (i) obtain the best mapping loci for each read,
   and (ii) return all reads that have at most n mapping loci (within an
   error threshold), together with these loci, for any user specified n.
   Furthermore, mrsFAST-Ultra is SNP-aware, i.e. it can map reads to
   reference genome while discounting the mismatches that occur at common
   SNP locations provided by db-SNP; this significantly increases the
   number of reads that can be mapped to the reference genome. Notice that
   all of the above features are implemented within the index structure and
   are not simple post-processing steps and thus are performed highly
   efficiently. Finally, mrsFAST-Ultra utilizes multiple available cores
   and processors and can be tuned for various memory settings. Our results
   show that mrsFAST-Ultra is roughly five times faster than its
   predecessor mrsFAST. In comparison to newly enhanced popular tools such
   as Bowtie2, it is more sensitive (it can report 10 times or more
   mappings per read) and much faster (six times or more) in the
   multi-mapping mode. Furthermore, mrsFAST-Ultra has an index size of 2GB
   for the entire human reference genome, which is roughly half of that of
   Bowtie2. mrsFAST-Ultra is open source and it can be accessed at
   http://mrsfast.sourceforge.net.}},
DOI = {{10.1093/nar/gku370}},
ISSN = {{0305-1048}},
EISSN = {{1362-4962}},
ResearcherID-Numbers = {{Alkan, Can/D-2982-2009
   Sahinalp, Suleyman Cenk/X-7843-2018}},
ORCID-Numbers = {{Alkan, Can/0000-0002-5443-0706
   Sahinalp, Suleyman Cenk/0000-0002-5050-0682}},
Unique-ID = {{ISI:000339715000080}},
}

@article{ ISI:000339219000003,
Author = {Ruchala, Iwona and Cabra, Vanessa and Solis, Jr., Ernesto and Glennon,
   Richard A. and De Felice, Louis J. and Eltit, Jose M.},
Title = {{Electrical coupling between the human serotonin transporter and
   voltage-gated Ca2+ channels}},
Journal = {{CELL CALCIUM}},
Year = {{2014}},
Volume = {{56}},
Number = {{1}},
Pages = {{25-33}},
Month = {{JUL}},
Abstract = {{Monoamine transporters have been implicated in dopamine or serotonin
   release in response to abused drugs such as methamphetamine or ecstasy
   (MDMA). In addition, monoamine transporters show substrate-induced
   inward currents that may modulate excitability and Ca2+ mobilization,
   which could also contribute to neurotransmitter release. How monoamine
   transporters modulate Cal permeability is currently unknown. We
   investigate the functional interaction between the human serotonin
   transporter (hSERT) and voltage-gated Ca2+ channels (Cav). We introduce
   an excitable expression system consisting of cultured muscle cells
   genetically engineered to express hSERT. Both 5HT and S(+)MDMA
   depolarize these cells and activate the excitation-contraction
   (EC)-coupling mechanism. However, hSERT substrates fail to activate
   EC-coupling in Cav1.1-null muscle cells, thus implicating Ca2+ channels.
   Cav1.3 and Cav2.2 channels are natively expressed in neurons. When these
   channels are co-expressed with hSERT in HEK293T cells, only cells
   expressing the lower-threshold L-type Cav1.3 channel show Ca2+
   transients evoked by 5HT or SHMDMA. In addition, the electrical coupling
   between hSERT and Cav 1.3 takes place at physiological 5HT
   concentrations. The electrical coupling between monoamine
   neurotransmitter transporters and Ca2+ channels such as Cav1.3 is a
   novel mechanism by which endogenous substrates (neurotransmitters) or
   exogenous substrates (like ecstasy) could modulate Ca2+-driven signals
   in excitable cells. (C) 2014 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ceca.2014.04.003}},
ISSN = {{0143-4160}},
EISSN = {{1532-1991}},
ORCID-Numbers = {{De Felice, Louis/0000-0002-4594-3171}},
Unique-ID = {{ISI:000339219000003}},
}

@article{ ISI:000336828500013,
Author = {Li, Yangfan and Shi, Yalou and Qureshi, Salman and Bruns, Antje and Zhu,
   Xiaodong},
Title = {{Applying the concept of spatial resilience to socio-ecological systems
   in the urban wetland interface}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2014}},
Volume = {{42}},
Number = {{SI}},
Pages = {{135-146}},
Month = {{JUL}},
Abstract = {{Resilient socio-ecological systems (SESs) can handle negative
   environmental changes well without regime shifts. In this study, we
   introduce the concept of spatial resilience and apply it to the
   assessment, planning, and ecosystem-based management of the urban
   wetland interface in the Taihu Lake watershed in China. From the
   assumption that spatial indicators in patterns and processes affect SES
   resilience, spatial resilience in this case focuses on the importance of
   ecological sensitivity, water quality, and vegetation cover. We consider
   two criteria in this study, protection and recovery, which are further
   categorized into general and specific types, to examine four resilience
   scenarios, namely, key protection, general protection, general recovery,
   and key recovery. Spatial resilience is assessed with an indicator-based
   system, multi-criteria evaluation method, and spatial visualization
   based on a geographic information system (GIS) to create zones. Spatial
   zonings are evaluated in the context of different degrees of spatial
   resilience. Results are integrated with indicators of ecological
   sensitivity, water quality and vegetation cover, are assessed to
   determine the practical application of spatial resilience. Zoning maps
   that show water quality, vegetation cover, and corresponding plans are
   generated on the basis of spatial resilience assessment, social
   indicators, and the existing administrative region. These maps can be
   used by authorities in protection or restoration activities for
   ecological services in wetlands. (C) 2013 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.ecolind.2013.09.032}},
ISSN = {{1470-160X}},
EISSN = {{1872-7034}},
ResearcherID-Numbers = {{Qureshi, Salman/A-4355-2009
   }},
ORCID-Numbers = {{Li, Chengwei/0000-0002-0208-512X}},
Unique-ID = {{ISI:000336828500013}},
}

@article{ ISI:000337117400015,
Author = {Parnell, S. and Gottwald, T. R. and Riley, T. and van den Bosch, F.},
Title = {{A generic risk-based surveying method for invading plant pathogens}},
Journal = {{ECOLOGICAL APPLICATIONS}},
Year = {{2014}},
Volume = {{24}},
Number = {{4}},
Pages = {{779-790}},
Month = {{JUN}},
Abstract = {{Invasive plant pathogens are increasing with international trade and
   travel, with damaging environmental and economic consequences. Recent
   examples include tree diseases such as sudden oak death in the Western
   United States and ash dieback in Europe. To control an invading pathogen
   it is crucial that newly infected sites are quickly detected so that
   measures can be implemented to control the epidemic. However, since
   sampling resources are often limited, not all locations can be inspected
   and locations must be prioritized for surveying. Existing approaches to
   achieve this are often species specific and rely on detailed data
   collection and parameterization, which is difficult, especially when new
   arrivals are unanticipated. Consequently regulatory sampling responses
   are often ad hoc and developed without due consideration of
   epidemiology, leading to the suboptimal deployment of expensive sampling
   resources. We introduce a flexible risk-based sampling method that is
   pathogen generic and enables available information to be utilized to
   develop epidemiologically informed sampling programs for virtually any
   biologically relevant plant pathogen. By targeting risk we aim to inform
   sampling schemes that identify high-impact locations that can be
   subsequently treated in order to reduce inoculum in the landscape. This
   ``damage limitation{''} is often the initial management objective
   following the first discovery of a new invader. Risk at each location is
   determined by the product of the basic reproductive number (R-0), as a
   measure of local epidemic size, and the probability of infection. We
   illustrate how the risk estimates can be used to prioritize a survey by
   weighting a random sample so that the highest-risk locations have the
   highest probability of selection. We demonstrate and test the method
   using a high-quality spatially and temporally resolved data set on
   Huanglongbing disease (HLB) in Florida, USA. We show that even when
   available epidemiological information is relatively minimal, the method
   has strong predictive value and can result in highly effective targeted
   surveying plans.}},
DOI = {{10.1890/13-0704.1}},
ISSN = {{1051-0761}},
EISSN = {{1939-5582}},
ResearcherID-Numbers = {{parnell, stephen/I-7682-2015}},
ORCID-Numbers = {{parnell, stephen/0000-0002-2625-4557}},
Unique-ID = {{ISI:000337117400015}},
}

@article{ ISI:000335928500004,
Author = {Wang, Linlin and Cummings, Richard D. and Smith, David F. and Huflejt,
   Margaret and Campbell, Christopher T. and Gildersleeve, Jeffrey C. and
   Gerlach, Jared Q. and Kilcoyne, Michelle and Joshi, Lokesh and Serna,
   Sonia and Reichardt, Niels-Christian and Pera, Nuria Parera and Pieters,
   Roland J. and Eng, William and Mahal, Lara K.},
Title = {{Cross-platform comparison of glycan microarray formats}},
Journal = {{GLYCOBIOLOGY}},
Year = {{2014}},
Volume = {{24}},
Number = {{6}},
Pages = {{507-517}},
Month = {{JUN}},
Abstract = {{Carbohydrates participate in almost every aspect of biology from protein
   sorting to modulating cell differentiation and cell-cell interactions.
   To date, the majority of data gathered on glycan expression has been
   obtained via analysis with either anti-glycan antibodies or lectins. A
   detailed understanding of the specificities of these reagents is
   critical to the analysis of carbohydrates in biological systems. Glycan
   microarrays are increasingly used to determine the binding specificity
   of glycan-binding proteins (GBPs). In this study, six different glycan
   microarray platforms with different modes of glycan presentation were
   compared using five well-known lectins; concanavalin A, Helix pomatia
   agglutinin, Maackia amurensis lectin I, Sambucus nigra agglutinin and
   wheat germ agglutinin. A new method (universal threshold) was developed
   to facilitate systematic comparisons across distinct array platforms.
   The strongest binders of each lectin were identified using the universal
   threshold across all platforms while identification of weaker binders
   was influenced by platform-specific factors including presentation of
   determinants, array composition and self-reported thresholding methods.
   This work compiles a rich dataset for comparative analysis of glycan
   array platforms and has important implications for the implementation of
   microarrays in the characterization of GBPs.}},
DOI = {{10.1093/glycob/cwu019}},
ISSN = {{0959-6658}},
EISSN = {{1460-2423}},
ResearcherID-Numbers = {{Pieters, Roland/A-9254-2008
   Gildersleeve, Jeffrey/N-3392-2014
   Gerlach, Jared/K-2698-2013
   Reichardt, Niels-Christian/K-8950-2017
   Serna, Sonia/K-5795-2014
   biomaGUNE, CIC/J-9136-2014
   }},
ORCID-Numbers = {{Pieters, Roland/0000-0003-4723-3584
   Gerlach, Jared/0000-0001-7343-7201
   Reichardt, Niels-Christian/0000-0002-9092-7023
   Serna, Sonia/0000-0002-2085-4412
   biomaGUNE, CIC/0000-0001-7690-0660
   Joshi, Lokesh/0000-0002-3612-0747
   Kilcoyne, Michelle/0000-0002-8870-1308}},
Unique-ID = {{ISI:000335928500004}},
}

@article{ ISI:000342787200003,
Author = {Darbani, Behrooz and Stewart, Jr., Charles Neal},
Title = {{Reproducibility and reliability assays of the gene
   expression-measurements}},
Journal = {{JOURNAL OF BIOLOGICAL RESEARCH-THESSALONIKI}},
Year = {{2014}},
Volume = {{21}},
Month = {{MAY 13}},
Abstract = {{Background: Reliability and reproducibility are key metrics for gene
   expression assays. This report assesses the utility of the correlation
   coefficient in the analysis of reproducibility and reliability of gene
   expression data.
   Results: The correlation coefficient alone is not sufficient to assess
   equality among sample replicates but when coupled with slope and scatter
   plots expression data equality can be better assessed. Narrow-intervals
   of scatter plots should be shown as a tool to inspect the actual level
   of noise within the data. Here we propose a method to examine expression
   data reproducibility, which is based on the ratios of both the means and
   the standard deviations for the inter-treatment expression ratios of
   genes. In addition, we introduce a fold-change threshold with an
   inter-replicate occurrence likelihood lower than 5\% to perform analysis
   even when reproducibility is not acceptable. There is no possibility to
   find a perfect correlation between transcript and protein levels even
   when there is not any post-transcriptional regulatory mechanism. We
   therefore propose an adjustment for protein abundance with that of
   transcript abundance based on open reading frame length.
   Conclusions: Here, we introduce a very efficient reproducibility
   approach. Our method detects very small changes in large datasets which
   was not possible through regular correlation analysis. We also introduce
   a correction on protein quantities which allows us to examine the
   post-transcriptional regulatory effects with a higher accuracy.}},
DOI = {{10.1186/2241-5793-21-3}},
ISSN = {{2241-5793}},
Unique-ID = {{ISI:000342787200003}},
}

@article{ ISI:000336660700023,
Author = {Farhang Ghahremani, Morvarid and Radaelli, Enrico and Haigh, Katharina
   and Bartunkova, Sonia and Haenebalcke, Lieven and Marine,
   Jean-Christophe and Goossens, Steven and Haigh, Jody J.},
Title = {{Loss of autocrine endothelial-derived VEGF significantly reduces
   hemangiosarcoma development in conditional p53-deficient mice}},
Journal = {{CELL CYCLE}},
Year = {{2014}},
Volume = {{13}},
Number = {{9}},
Pages = {{1501-1507}},
Month = {{MAY 1}},
Abstract = {{Malignant transformation of the endothelium is rare, and
   hemangiosarcomas comprise only 1\% of all sarcomas. For this reason and
   due to the lack of appropriate mouse models, the genetic mechanisms of
   malignant endothelial transformation are poorly understood. Here, we
   describe a hemangiosarcoma mouse model generated by deleting p53
   specifically in the endothelial and hematopoietic lineages. This
   strategy led to a high incidence of hemangiosarcoma, with an average
   latency of 25 weeks. To study the in vivo roles of autocrine or
   endothelial cell autonomous VEGF signaling in the initiation and/or
   progression of hemangiosarcomas, we genetically deleted autocrine
   endothelial sources of VEGF in this mouse model. We found that loss of
   even a single conditional VEGF allele results in substantial rescue from
   endothelial cell transformation. These findings highlight the important
   role of threshold levels of autocrine VEGF signaling in endothelial
   malignancies and suggest a new approach for hemangiosarcoma treatment
   using targeted autocrine VEGF inhibition.}},
DOI = {{10.4161/cc.28474}},
ISSN = {{1538-4101}},
EISSN = {{1551-4005}},
ResearcherID-Numbers = {{Marine, Jean Christophe/J-2237-2015
   Marine, Jean-Christophe/K-3292-2016
   }},
ORCID-Numbers = {{Marine, Jean-Christophe/0000-0003-2433-9837
   haigh, jody/0000-0002-7319-8922
   Goossens, Steven/0000-0002-5693-8570}},
Unique-ID = {{ISI:000336660700023}},
}

@article{ ISI:000335783500003,
Author = {Aparicio, P. and Mandaltsi, A. and Boamah, J. and Chen, H. and
   Selimovic, A. and Bratby, M. and Uberoi, R. and Ventikos, Y. and Watton,
   P. N.},
Title = {{Modelling the influence of endothelial heterogeneity on the progression
   of arterial disease: application to abdominal aortic aneurysm evolution}},
Journal = {{INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING}},
Year = {{2014}},
Volume = {{30}},
Number = {{5}},
Pages = {{563-586}},
Month = {{MAY}},
Abstract = {{We sophisticate a fluid-solid growth computational framework for
   modelling aneurysm evolution. A realistic structural model of the
   arterial wall is integrated into a patient-specific geometry of the
   vasculature. This enables physiologically representative distributions
   of haemodynamic stimuli, obtained from a rigid-wall computational fluid
   dynamics analysis, to be linked to growth and remodelling algorithms.
   Additionally, a quasistatic structural analysis quantifies the cyclic
   deformation of the arterial wall so that collagen growth and remodelling
   can be explicitly linked to the cyclic deformation of vascular cells. To
   simulate aneurysm evolution, degradation of elastin is driven by
   reductions in wall shear stress (WSS) below homeostatic thresholds.
   Given that the endothelium exhibits spatial and temporal heterogeneity,
   we propose a novel approach to define the homeostatic WSS thresholds: We
   allow them to be spatially and temporally heterogeneous. We illustrate
   the application of this novel fluid-solid growth framework to model
   abdominal aortic aneurysm (AAA) evolution and to examine how the
   influence of the definition of the WSS homeostatic threshold influences
   AAA progression. We conclude that improved understanding and modelling
   of the endothelial heterogeneity is important for modelling aneurysm
   evolution and, more generally, other vascular diseases where
   haemodynamic stimuli play an important role. Copyright (c) 2014 John
   Wiley \& Sons, Ltd.}},
DOI = {{10.1002/cnm.2620}},
ISSN = {{2040-7939}},
EISSN = {{2040-7947}},
ORCID-Numbers = {{Aparicio, Pedro/0000-0002-8152-3545
   Watton, Paul/0000-0002-5531-5953}},
Unique-ID = {{ISI:000335783500003}},
}

@article{ ISI:000335873000002,
Author = {Potts, Daniel L. and Barron-Gafford, Greg A. and Jenerette, G. Darrel},
Title = {{Metabolic acceleration quantifies biological systems' ability to
   up-regulate metabolism in response to episodic resource availability}},
Journal = {{JOURNAL OF ARID ENVIRONMENTS}},
Year = {{2014}},
Volume = {{104}},
Pages = {{9-16}},
Month = {{MAY}},
Abstract = {{Precipitation often arrives discretely in semi-arid ecosystems. Under
   these conditions, natural selection might favor rapid metabolic
   responses to the sudden availability of otherwise limiting resources. We
   introduce and define metabolic acceleration (alpha) as the first
   derivative of the metabolic rate of a living system with respect to
   time. As such, alpha describes the capacity of a biological system to
   up- and down-regulate metabolism and may be applied across scales and
   processes. To better understand the responses of roots and soil microbes
   to seasonal patterns of rainfall and plant activity, we compared soil
   respiratory acceleration (alpha(soil)) derived from soil respiration
   time-series among three microhabitats (under mesquite, under
   bunchgrasses, and in intercanopy soils) in a semi-arid shrubland near
   Tucson, Arizona. Across microhabitats, alpha(soil) was greatest during
   the warm, wet summer months and lowest during cool winter months.
   Throughout the year, alpha(soil) beneath mesquite was greater than
   beneath bunchgrasses or in intercanopy soils. Finally,
   microhabitat-specific responses of alpha(soil) to spring and monsoonal
   rainfall events were consistent with seasonal contrasts in the
   photosynthetic activity of deeply-rooted mesquite shrubs and warm-season
   bunchgrasses. By quantifying the capacity of living systems to respond
   to episodic resource availability, metabolic acceleration provides a new
   perspective and potentially unifying metric for biological responses to
   environmental heterogeneity. Published by Elsevier Ltd.}},
DOI = {{10.1016/j.jaridenv.2014.01.018}},
ISSN = {{0140-1963}},
EISSN = {{1095-922X}},
Unique-ID = {{ISI:000335873000002}},
}

@article{ ISI:000334693500010,
Author = {Engel, Kelly B. and Vaught, Jim and Moore, Helen M.},
Title = {{National Cancer Institute Biospecimen Evidence-Based Practices: A Novel
   Approach to Pre-analytical Standardization}},
Journal = {{BIOPRESERVATION AND BIOBANKING}},
Year = {{2014}},
Volume = {{12}},
Number = {{2}},
Pages = {{148-150}},
Month = {{APR 1}},
Abstract = {{Variable biospecimen collection, processing, and storage practices may
   introduce variability in biospecimen quality and analytical results.
   This risk can be minimized within a facility through the use of
   standardized procedures; however, analysis of biospecimens from
   different facilities may be confounded by differences in procedures and
   inferred biospecimen quality. Thus, a global approach to standardization
   of biospecimen handling procedures and their validation is needed. Here
   we present the first in a series of procedural guidelines that were
   developed and annotated with published findings in the field of human
   biospecimen science. The series of documents will be known as NCI
   Biospecimen Evidence-Based Practices, or BEBPs. Pertinent literature was
   identified via the National Cancer Institute (NCI) Biospecimen Research
   Database (brd.nci.nih.gov) and findings were organized by specific
   biospecimen pre-analytical factors and analytes of interest (DNA, RNA,
   protein, morphology). Meta-analysis results were presented as annotated
   summaries, which highlight concordant and discordant findings and the
   threshold and magnitude of effects when applicable. The detailed and
   adaptable format of the document is intended to support the development
   and execution of evidence-based standard operating procedures (SOPs) for
   human biospecimen collection, processing, and storage operations.}},
DOI = {{10.1089/bio.2013.0091}},
ISSN = {{1947-5535}},
EISSN = {{1947-5543}},
Unique-ID = {{ISI:000334693500010}},
}

@article{ ISI:000334950000001,
Author = {de Leon, Antonio de la Vega and Hu, Ye and Bajorath, Juergen},
Title = {{Systematic Identification of Matching Molecular Series and Mapping of
   Screening Hits}},
Journal = {{MOLECULAR INFORMATICS}},
Year = {{2014}},
Volume = {{33}},
Number = {{4}},
Pages = {{257-263}},
Month = {{APR}},
Abstract = {{Matching molecular series (MMS) have originally been introduced as an
   extension of the matched molecular pair (MMP) concept to facilitate the
   design of substructure-based structure-activity relationship (SAR)
   networks. An MMP is defined as a pair of compounds that only differ by a
   structural change at a single site. In addition, an MMS is defined as an
   MMP-based series of compounds that have a conserved structural core and
   are distinguished by modifications at a single site. Systematic
   generation of MMS from specifically active compounds generalizes the
   search for series of structural analogs. Potency-ordered MMS provide
   series associated with SAR information. We have systematically extracted
   MMS from publicly available compounds with well-defined activity
   measurements and generated a large database with approx. 40000 single-
   and 13600 multi-target series, which provide a rich source of SAR
   information. As an application, we introduce MMP-based mapping of
   screening hits to MMS to search for initial SAR information and
   determine all SAR environments available for such hits. The MMS database
   is made freely available to the scientific community.}},
DOI = {{10.1002/minf.201400017}},
ISSN = {{1868-1743}},
EISSN = {{1868-1751}},
ORCID-Numbers = {{de la Vega de Leon, Antonio/0000-0003-0927-2099}},
Unique-ID = {{ISI:000334950000001}},
}

@article{ ISI:000333475800005,
Author = {Meng, Yan and Ji, Feng and Jia, Long and Jiang, Xiao-zhen and Lei,
   Ming-tang},
Title = {{A new approach for forecasting the appearance of sinkholes near the
   Jinshazhou tunnel}},
Journal = {{ENVIRONMENTAL EARTH SCIENCES}},
Year = {{2014}},
Volume = {{71}},
Number = {{8}},
Pages = {{3339-3347}},
Month = {{APR}},
Abstract = {{The paper deals with a frequent and known problem of suffusion type of
   sinkhole formation in soil, due to water level oscillations above
   karstified bedrock. The authors measured groundwater pressure
   oscillations caused by pumping groundwater from the excavated tunnel to
   determine the reliable indicators for prediction of sudden soil collapse
   based on risk assessment. The statistical analysis was performed to
   determine suspicious values, which reliably predict the soil collapse.
   The results of analysis of monitored values indicate very good time
   correlation coefficients with the failures. The work comprises the
   following main parts: (1) survey of the geologic environment; (2)
   monitoring of hydrodynamic conditions; and (3) acquisition of a
   forecasting criterion based on the statistical analysis of anomalous
   monitoring data. From this research, it might be concluded that the
   appearance time of sinkholes and the abnormal values were strongly
   linearly dependent with a correlation coefficient of over 99 \%. This
   conclusion will be a contribution to estimate the risk of karst collapse
   in a similar environment.}},
DOI = {{10.1007/s12665-013-2723-2}},
ISSN = {{1866-6280}},
EISSN = {{1866-6299}},
Unique-ID = {{ISI:000333475800005}},
}

@article{ ISI:000332046700012,
Author = {Revell, Liam J.},
Title = {{ANCESTRAL CHARACTER ESTIMATION UNDER THE THRESHOLD MODEL FROM
   QUANTITATIVE GENETICS}},
Journal = {{EVOLUTION}},
Year = {{2014}},
Volume = {{68}},
Number = {{3}},
Pages = {{743-759}},
Month = {{MAR}},
Abstract = {{Evolutionary biology is a study of life's history on Earth. In
   researching this history, biologists are often interested in attempting
   to reconstruct phenotypes for the long extinct ancestors of living
   species. Various methods have been developed to do this on a phylogeny
   from the data for extant taxa. In the present article, I introduce a new
   approach for ancestral character estimation for discretely valued
   traits. This approach is based on the threshold model from evolutionary
   quantitative genetics. Under the threshold model, the value exhibited by
   an individual or species for a discrete character is determined by an
   underlying, unobserved continuous trait called ``liability.{''} In this
   new method for ancestral state reconstruction, I use Bayesian Markov
   chain Monte Carlo (MCMC) to sample the liabilities of ancestral and tip
   species, and the relative positions of two or more thresholds, from
   their joint posterior probability distribution. Using data simulated
   under the model, I find that the method has very good performance in
   ancestral character estimation. Use of the threshold model for ancestral
   state reconstruction relies on a priori specification of the order of
   the discrete character states along the liability axis. I test the use
   of a Bayesian MCMC information theoretic criterion based approach to
   choose among different hypothesized orderings for the discrete
   character. Finally, I apply the method to the evolution of feeding mode
   in centrarchid fishes.}},
DOI = {{10.1111/evo.12300}},
ISSN = {{0014-3820}},
EISSN = {{1558-5646}},
Unique-ID = {{ISI:000332046700012}},
}

@article{ ISI:000331551600006,
Author = {Fang, Weirong and Zhang, Rui and Sha, Lan and Lv, Peng and Shang, Erxin
   and Han, Dan and Wei, Jie and Geng, Xiaohan and Yang, Qichuan and Li,
   Yunman},
Title = {{Platelet activating factor induces transient blood-brain barrier opening
   to facilitate edaravone penetration into the brain}},
Journal = {{JOURNAL OF NEUROCHEMISTRY}},
Year = {{2014}},
Volume = {{128}},
Number = {{5}},
Pages = {{662-671}},
Month = {{MAR}},
Abstract = {{The blood-brain barrier (BBB) greatly limits the efficacy of many
   neuroprotective drugs' delivery to the brain, so improving drug
   penetration through the BBB has been an important focus of research.
   Here we report that platelet activating factor (PAF) transiently opened
   BBB and facilitated neuroprotectant edaravone penetration into the
   brain. Intravenous infusion with PAF induced a transient BBB opening in
   rats, reflected by increased Evans blue leakage and mild edema
   formation, which ceased within 6h. Furthermore, rat regional cerebral
   blood flow (rCBF) declined acutely during PAF infusion, but recovered
   slowly. More importantly, this transient BBB opening significantly
   increased the penetration of edaravone into the brain, evidenced by
   increased edaravone concentrations in tissue interstitial fluid
   collected by microdialysis and analyzed by Ultra-performance liquid
   chromatograph combined with a hybrid quadrupole time-of-flight mass
   spectrometer (UPLC-MS/MS). Similarly, incubation of rat brain
   microvessel endothelial cells monolayer with 1M PAF for 1h significantly
   increased monolayer permeability to I-125-albumin, which recovered 1h
   after PAF elimination. However, PAF incubation with rat brain
   microvessel endothelial cells for 1h did not cause detectable
   cytotoxicity, and did not regulate intercellular adhesion molecule-1,
   matrix-metalloproteinase-9 and P-glycoprotein expression. In conclusion,
   PAF could induce transient and reversible BBB opening through abrupt
   rCBF decline, which significantly improved edaravone penetration into
   the brain.
   Platelet activating factor (PAF) transiently induces BBB dysfunction and
   increases BBB permeability, which may be due to vessel contraction and a
   temporary decline of regional cerebral blood flow (rCBF) triggered by
   PAF. More importantly, the PAF induced transient BBB opening facilitates
   neuroprotectant edaravone penetration into brain. The results of this
   study may provide a new approach to improve drug delivery into the
   brain.}},
DOI = {{10.1111/jnc.12507}},
ISSN = {{0022-3042}},
EISSN = {{1471-4159}},
Unique-ID = {{ISI:000331551600006}},
}

@article{ ISI:000334111600018,
Author = {Liu, Zhe and Khan, Urooj and Sharma, Ashish},
Title = {{A new method for verification of delineated channel networks}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2014}},
Volume = {{50}},
Number = {{3}},
Pages = {{2164-2175}},
Month = {{MAR}},
Abstract = {{Several methods are used to delineate channel networks. The most widely
   used are the contributing area method, area-slope method, and grid
   network ordering method. The number of delineated channels depends on
   the threshold adopted when using each method. However, the appropriate
   threshold value required to delineate channel networks, and their
   corresponding accuracies, are still uncertain. The consistency between
   the delineated channels and actual channels can be evaluated by carrying
   out extensive field surveys, but these require significant time and
   cost. Accurate knowledge of delineated channel networks is vital, and is
   achievable more efficiently and simply. A new method of calculating the
   accuracy of delineated channel networks is introduced in this study.
   Channel cross-section profiles throughout the channel network were
   examined and three new incision indices were derived: an incised channel
   index, a partially incised channel index, and a nonincised channel
   index. The indices were found useful for setting appropriate threshold
   values for actual channel networks. Three small catchments in
   Wellington, New South Wales (NSW), Australia, were investigated in this
   study.}},
DOI = {{10.1002/2013WR014290}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ORCID-Numbers = {{Sharma, Ashish/0000-0002-6758-0519}},
Unique-ID = {{ISI:000334111600018}},
}

@article{ ISI:000334068000023,
Author = {Long, Stephanie and Fatoyinbo, Temilola E. and Policelli, Frederick},
Title = {{Flood extent mapping for Namibia using change detection and thresholding
   with SAR}},
Journal = {{ENVIRONMENTAL RESEARCH LETTERS}},
Year = {{2014}},
Volume = {{9}},
Number = {{3}},
Month = {{MAR}},
Abstract = {{A new method for flood detection change detection and thresholding
   (CDAT) was used with synthetic aperture radar (SAR) imagery to delineate
   the extent of flooding for the Chobe floodplain in the Caprivi region of
   Namibia. This region experiences annual seasonal flooding and has seen a
   recent renewal of severe flooding after a long dry period in the 1990s.
   Flooding in this area has caused loss of life and livelihoods for the
   surrounding communities and has caught the attention of disaster relief
   agencies. There is a need for flood extent mapping techniques that can
   be used to process images quickly, providing near real-time flooding
   information to relief agencies. ENVISAT/ASAR and Radarsat-2 images were
   acquired for several flooding seasons from February 2008 to March 2013.
   The CDAT method was used to determine flooding from these images and
   includes the use of image subtraction, decision-based classification
   with threshold values, and segmentation of SAR images. The total extent
   of flooding determined for 2009, 2011 and 2012 was about 542 km(2), 720
   km2, and 673 km2 respectively. Pixels determined to be flooded in
   vegetation were typically <0.5\% of the entire scene, with the exception
   of 2009 where the detection of flooding in vegetation was much greater
   (almost one third of the total flooded area). The time to maximum
   flooding for the 2013 flood season was determined to be about 27 days.
   Landsat water classification was used to compare the results from the
   new CDAT with SAR method; the results show good spatial agreement with
   Landsat scenes.}},
DOI = {{10.1088/1748-9326/9/3/035002}},
Article-Number = {{035002}},
ISSN = {{1748-9326}},
ResearcherID-Numbers = {{Fatoyinbo, Temilola/G-6104-2012
   Policelli, Frederick/D-8994-2018}},
ORCID-Numbers = {{Fatoyinbo, Temilola/0000-0002-1130-6748
   Policelli, Frederick/0000-0002-7446-7338}},
Unique-ID = {{ISI:000334068000023}},
}

@article{ ISI:000333125700007,
Author = {Ornik, Melkior},
Title = {{MODELING THE ROLE OF MUTATIONS AND DENSITY INDEPENDENT DISPERSAL IN
   EVOLUTIONARY RESCUE}},
Journal = {{JOURNAL OF BIOLOGICAL SYSTEMS}},
Year = {{2014}},
Volume = {{22}},
Number = {{1}},
Pages = {{123-132}},
Month = {{MAR}},
Abstract = {{Faced with a strong and sudden deterioration of environment, a
   population encounters two possible options - adapt or perish. In
   general, it is not known which of those outcomes the environmental
   changes will lead to. Building on experimental research, we introduce a
   discrete-space, discrete-time model for environmental rescue based on
   the influence of population dispersal, as well as, potentially
   beneficial mutations. Numerical results obtained by the model are shown
   to correspond well to experimentally obtained data.}},
DOI = {{10.1142/S0218339014500077}},
ISSN = {{0218-3390}},
EISSN = {{1793-6470}},
Unique-ID = {{ISI:000333125700007}},
}

@article{ ISI:000332324000016,
Author = {Jacobs, Stephanie J. and Pezza, Alexandre B. and Barras, Vaughan and
   Bye, John},
Title = {{A new `bio-comfort' perspective for Melbourne based on heat stress, air
   pollution and pollen}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETEOROLOGY}},
Year = {{2014}},
Volume = {{58}},
Number = {{2, SI}},
Pages = {{263-275}},
Month = {{MAR}},
Abstract = {{Humans are at risk from exposure to extremes in their environment, yet
   there is no consistent way to fully quantify and understand the risk
   when considering more than just meteorological variables. An outdoor
   `bio-comfort' threshold is defined for Melbourne, Australia using a
   combination of heat stress, air particulate concentration and grass
   pollen count, where comfortable conditions imply an ideal range of
   temperature, humidity and wind speed, acceptable levels of air
   particulates and a low pollen count. This is a new approach to defining
   the comfort of human populations. While other works have looked into the
   separate impacts of different variables, this is the first time that a
   unified bio-comfort threshold is suggested. Composite maps of surface
   pressure are used to illustrate the genesis and evolution of the
   atmospheric structures conducive to an uncomfortable day. When there is
   an uncomfortable day due to heat stress conditions in Melbourne, there
   is a high pressure anomaly to the east bringing warm air from the
   northern interior of Australia. This anomaly is part of a slow moving
   blocking high originating over the Indian Ocean. Uncomfortable days due
   to high particulate levels have an approaching cold front. However, for
   air particulate cases during the cold season there are stable
   atmospheric conditions enhanced by a blocking high emanating from
   Australia and linking with the Antarctic continent. Finally, when grass
   pollen levels are high, there are northerly winds carrying the pollen
   from rural grass lands to Melbourne, due to a stationary trough of low
   pressure inland. Analysis into days with multiple types of stress
   revealed that the atmospheric signals associated with each type of
   discomfort are present regardless of whether the day is uncomfortable
   due to one or multiple variables. Therefore, these bio-comfort results
   are significant because they offer a degree of predictability for future
   uncomfortable days in Melbourne.}},
DOI = {{10.1007/s00484-013-0636-0}},
ISSN = {{0020-7128}},
EISSN = {{1432-1254}},
Unique-ID = {{ISI:000332324000016}},
}

@article{ ISI:000331660800004,
Author = {Carpenter, Stephen R. and Brock, William A. and Cole, Jonathan J. and
   Pace, Michael L.},
Title = {{A new approach for rapid detection of nearby thresholds in ecosystem
   time series}},
Journal = {{OIKOS}},
Year = {{2014}},
Volume = {{123}},
Number = {{3}},
Pages = {{290-297}},
Month = {{MAR}},
Abstract = {{Massive changes to ecosystems sometimes cross thresholds from which
   recovery can be difficult, expensive and slow. These thresholds are
   usually discovered in post hoc analyses long after the event occurred.
   Anticipating these changes prior to their occurrence could give managers
   a chance to intervene. Here we present a novel approach for anticipating
   ecosystem thresholds that combines resilience indicators with Quickest
   detection of change points. Unlike existing methods, the Quickest
   detection method is updated every time a data point arrives, and
   minimizes the time to detect an approaching threshold given the users'
   tolerance for false alarms. The procedure accurately detected an
   impending regime shift in an experimentally manipulated ecosystem. An
   ecosystem model was used to determine if the method can detect an
   approaching threshold soon enough to prevent a regime shift. When the
   monitored variable was directly involved in the interaction that caused
   the regime shift, detection was quick enough to avert collapse. When the
   monitored variable was only indirectly linked to the critical
   transition, detection came too late. The procedure is useful for
   assessing changes in resilience as ecosystems approach thresholds.
   However some thresholds cannot be detected in time to prevent regime
   shifts, and surprises will be inevitable in ecosystem management.}},
DOI = {{10.1111/j.1600-0706.2013.00539.x}},
ISSN = {{0030-1299}},
EISSN = {{1600-0706}},
ORCID-Numbers = {{Pace, Michael/0000-0001-5945-6131}},
Unique-ID = {{ISI:000331660800004}},
}

@article{ ISI:000331448900005,
Author = {van Luijtelaar, G. and Zobeiri, M.},
Title = {{Progress and Outlooks in a Genetic Absence Epilepsy Model (WAG/Rij)}},
Journal = {{CURRENT MEDICINAL CHEMISTRY}},
Year = {{2014}},
Volume = {{21}},
Number = {{6}},
Pages = {{704-721}},
Month = {{FEB}},
Abstract = {{The WAG/Rij model is a well characterized and validated genetic animal
   epilepsy model in which the for absence epilepsy highly characteristic
   spike-wave discharges (SWDs) develop spontaneously. In this review we
   discuss first some older and many new studies, with an emphasis on
   pharmacological and neurochemical studies towards the role of GABA and
   glutamate and the ion channels involved in the pathological firing
   patterns. Next, new insights and highlights from the last 5-10 years of
   reaearch in WAG/Rij rats are discussed. First, early environmental
   factors modulate SWD characteristics and antiepileptogenesis is
   possible. Also new is that the classically assumed association between
   sleep spindles and SWDs seems no longer valid as an explanatory role for
   the occurrence of SWDs in the genetic rodent models. A role of cortical
   and thalamic glial cells has been revealed, indicating a putative role
   for inflammatory cytokines. Neurophysiologic and signal analytical
   studies in this and in another rodent model (GAERS) point towards a
   cortical site of origin, that SWDs do not have a sudden onset, and
   propose a more important role for the posterior thalamus than was
   previously assumed. Finally it is proposed that the reticular nucleus of
   the thalamus might be heterogeneous with respect to its role in
   propagation and maintenance of SWDs. The presence of a well-established
   cortical region in which SWDs are elicited allows for research towards
   new non-invasive treatment options, such as transcranial direct current
   stimulation (tDCS) and transcranial magnetic stimulation (TMS). The
   first results show the feasibility of this new approach.}},
ISSN = {{0929-8673}},
EISSN = {{1875-533X}},
ResearcherID-Numbers = {{van Luijtelaar, Gilles/D-2290-2010}},
Unique-ID = {{ISI:000331448900005}},
}

@article{ ISI:000331021100004,
Author = {Sung, Lung-Yu and Shie, Ruei-Hou and Lu, Chia-Jung},
Title = {{Locating sources of hazardous gas emissions using dual pollution rose
   plots and open path Fourier transform infrared spectroscopy}},
Journal = {{JOURNAL OF HAZARDOUS MATERIALS}},
Year = {{2014}},
Volume = {{265}},
Pages = {{30-40}},
Month = {{JAN 30}},
Abstract = {{A new approach employing two pollution rose plots to locate the sources
   of multiple hazardous gas emissions was proposed and tested in an
   industrial area. The data used for constructing the pollution rose plots
   were obtained from two side-by-side measurements of open-path Fourier
   Transform Infrared (OP-FTIR) spectrometers during one week of continuous
   analysis on the rooftop of a semiconductor plant. Hazardous gases such
   as CF4, C2F6, CH3OH, NH3, NO2, and SF6 were found and quantified at the
   ppb level by both OP-FTIR measurement sites. The data of the top 20\%
   highest concentrations and associated wind directions were used to
   construct the pollution rose plots. Pollution source probability
   contours for each compound were constructed using the
   probability-product of directional probability from two pollution rose
   plots. Hot spots for SF6, CF4, NO2, and C2F6 pointed to the stack area
   of the plant, but the sources of CH3OH and NH3 were found outside of
   this plant. The influences of parameters for this approach such as the
   variation in wind direction, lower limit concentration threshold and the
   nearby buildings were discussed. (C) 2013 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.jhazmat.2013.11.006}},
ISSN = {{0304-3894}},
EISSN = {{1873-3336}},
ResearcherID-Numbers = {{Lu, Chia-Jung/S-9804-2018}},
ORCID-Numbers = {{Lu, Chia-Jung/0000-0001-5147-6840}},
Unique-ID = {{ISI:000331021100004}},
}

@article{ ISI:000332457900001,
Author = {Bakkum, Douglas J. and Radivojevic, Milos and Frey, Urs and Franke,
   Felix and Hierlemann, Andreas and Takahashi, Hirokazu},
Title = {{Parameters for burst detection}},
Journal = {{FRONTIERS IN COMPUTATIONAL NEUROSCIENCE}},
Year = {{2014}},
Volume = {{7}},
Month = {{JAN 13}},
Abstract = {{Bursts of action potentials within neurons and throughout networks are
   believed to serve roles in how neurons handle and store information,
   both in vivo and in vitro. Accurate detection of burst occurrences and
   durations are therefore crucial for many studies. A number of algorithms
   have been proposed to do so, but a standard method has not been adopted.
   This is due, in part, to many algorithms requiring the adjustment of
   multiple ad-hoc parameters and further post-hoc criteria in order to
   produce satisfactory results. Here, we broadly catalog existing
   approaches and present a new approach requiring the selection of only a
   single parameter: the number of spikes N comprising the smallest burst
   to consider. A burst was identified if N spikes occurred in less than T
   ms, where the threshold T was automatically determined from observing
   aprobability distribution of inter-spike- intervals. Performance was
   compared vs. different classes of detectors on data gathered from in
   vitro neuronal networks grown over microelectrode arrays. Our approach
   offered a number of useful features including: a simple implementation,
   no need for ad-hoc or post-hoc criteria, and precise assignment of burst
   boundary time points. Unlike existing approaches, detection was not
   biased toward larger bursts, allowing identification and analysis of a
   greater range of neuronal and network dynamics.}},
DOI = {{10.3389/fncom.2013.00193}},
Article-Number = {{193}},
ISSN = {{1662-5188}},
ResearcherID-Numbers = {{Frey, Urs/C-5364-2011
   Hierlemann, Andreas/A-7046-2008}},
ORCID-Numbers = {{Frey, Urs/0000-0003-4026-1259
   Hierlemann, Andreas/0000-0002-3838-2468}},
Unique-ID = {{ISI:000332457900001}},
}

@incollection{ ISI:000363461600030,
Author = {Kraberg, Alexandra C. and Wiltshire, Karen H.},
Editor = {{Goffredo, S and Dubinsky, Z}},
Title = {{Regime Shifts in the Marine Environment: How Do They Affect Ecosystem
   Services?}},
Booktitle = {{Mediterranean Sea: Its History and Present Challenges}},
Year = {{2014}},
Pages = {{499-504}},
Abstract = {{Marine ecosystems are facing unprecedented pressures for instance rising
   water temperatures, changing current patterns and ocean acidification.
   Coastal systems in particular are also challenged with additional
   anthropogenic pressures caused by accelerating rates of human settlement
   near the coast. This trend places increasing strains on the delivery of
   ecosystem services associated with recreation but also coastal
   fisheries. Here we introduce the concepts related to ecosystem stability
   and ecosystem services and review the evidence for regime shifts in the
   world's ecosystems with particular reference to the Mediterranean,
   before reviewing mechanisms for ecosystem valuations, ending with
   recommendations for increasing the practical relevance of future
   ecosystem evaluations. We stress the need for regional approaches,
   taking into account the views and needs of local populations, which
   might differ greatly geographically even for the same issue being
   considered.}},
DOI = {{10.1007/978-94-007-6704-1\_29}},
ISBN = {{978-94-007-6704-1; 978-94-007-6703-4}},
ORCID-Numbers = {{Kraberg, Alexandra/0000-0003-2571-2074}},
Unique-ID = {{ISI:000363461600030}},
}

@article{ ISI:000346625800003,
Author = {Myszkowska, Dorota and Majewska, Renata},
Title = {{Pollen grains as allergenic environmental factors - new approach to the
   forecasting of the pollen concentration during the season}},
Journal = {{ANNALS OF AGRICULTURAL AND ENVIRONMENTAL MEDICINE}},
Year = {{2014}},
Volume = {{21}},
Number = {{4}},
Pages = {{681-688}},
Abstract = {{Introduction and objectives. It is important to monitor the threat of
   allergenic pollen during the whole season, because of practical
   application in allergic rhinitis treatment, especially in the specific
   allergen immunotherapy. The aim of the study was to propose the forecast
   models predicting the pollen occurrence in the defined pollen
   concentration categories related to the patient exposure and symptom
   intensity.
   Material and methods. The study was performed in Cracow (southern
   Poland), pollen data were collected using the volumetric method in
   1991-2012. For all independent variables (meteorological elements) and
   the daily pollen concentrations the running mean for periods: 2-, 3-,
   4-, 5-, 6- and 7 days before the predicted day were calculated. The
   multinomial logistic regression was used to find the relation between
   the probability of the pollen concentration occurrence in the selected
   categories and meteorological elements and pollen concentration in days
   preceding the predicted daily concentration. The models were constructed
   for each taxon using data in 1991-2011 (without 1992 and 1996 due to
   missing data in these years) and 1998-2011 pollen seasons.
   Results. The days classified among the lowest category (0-10 PG/m(3))
   (pollen grains/m(3) of air) dominated for all the studied taxa. The
   percentage of the obtained predictions of the pollen occurrence
   fluctuated between 35-78\% which is a sufficient value of model
   predictions. Considering the studied taxon, the best model accuracy was
   obtained for models forecasting Betula pollen concentration (both data
   series), and Poaceae (both data series).
   Conclusions. The application of the recommended threshold values during
   the predictive models construction seems to be really useful to estimate
   the real threat of allergen exposure. It was indicated that the
   polynomial logistic regression models could be a practical tool for
   effective forecasting in biological monitoring of pollen exposure.}},
DOI = {{10.5604/12321966.1129914}},
ISSN = {{1232-1966}},
EISSN = {{1898-2263}},
ResearcherID-Numbers = {{Majewska, Renata/U-7384-2018
   }},
ORCID-Numbers = {{Majewska, Renata/0000-0002-3376-7908
   Myszkowska, Dorota/0000-0002-1493-3990}},
Unique-ID = {{ISI:000346625800003}},
}

@incollection{ ISI:000344735500006,
Author = {Agata, Kiyokazu},
Book-Author = {{Okuda, N
   Watanabe, K
   Fukumori, K
   Nakano, SI
   Nakazawa, T}},
Title = {{A Dynamic Resilience Perspective Toward Integrated Ecosystem Management:
   Biodiversity, Landscape, and Climate}},
Booktitle = {{BIODIVERSITY IN AQUATIC SYSTEMS AND ENVIRONMENTS: LAKE BIWA}},
Series = {{SpringerBriefs in Biology}},
Year = {{2014}},
Pages = {{69-91}},
Abstract = {{Ecosystems often show sudden and drastic shifts in their states
   following relatively small environmental changes, yet the environmental
   restoration does not necessarily easily recover the original state. The
   resilience theory has played a pivotal role in ecosystem management by
   providing a theoretical basis for such abrupt and irreversible phase
   transitions (i.e., regime shifts). However, a major concern remains that
   the existing theory considers ecosystem responses along only a single
   disturbance axis (e.g., eutrophication), despite the fact that natural
   ecosystems are subject to multiple anthropogenic disturbances. In this
   chapter, I introduce ontogenetic niche shifts (i.e., changes in resource
   use or predation vulnerability during individual growth) as a possible
   common mechanism of regime shifts. Based on this framework, I show how
   additional factors not accounted for the basic resilience theory (e.g.,
   species extinction and invasion, habitat loss and fragmentation, and
   phenological shifts) may affect whether or where regime shifts occur
   along environmental gradients. I conclude that these results taken
   together illustrate the importance of interdisciplinary research
   integrating biodiversity conservation, landscape protection, and climate
   change adaptation for more effective management of lake ecosystems.}},
DOI = {{10.1007/978-4-431-54150-9\_4}},
ISSN = {{2192-2179}},
ISBN = {{978-4-431-54149-3; 978-4-431-54150-9}},
Unique-ID = {{ISI:000344735500006}},
}

@article{ ISI:000344153200001,
Author = {Klingberg, J. and Engardt, M. and Karlsson, P. E. and Langner, J. and
   Pleijel, H.},
Title = {{Declining ozone exposure of European vegetation under climate change and
   reduced precursor emissions}},
Journal = {{BIOGEOSCIENCES}},
Year = {{2014}},
Volume = {{11}},
Number = {{19}},
Pages = {{5269-5283}},
Abstract = {{The impacts of changes in ozone precursor emissions as well as climate
   change on the future ozone exposure of the vegetation in Europe were
   investigated. The ozone exposure is expressed as AOT40 (Accumulated
   exposure Over a Threshold of 40 ppb O-3) as well as PODY (Phytotoxic
   Ozone Dose above a threshold Y). A new method is suggested to express
   how the length of the period during the year when coniferous and
   evergreen trees are sensitive to ozone might be affected by climate
   change. Ozone precursor emission changes from the RCP4.5 scenario were
   combined with climate simulations based on the IPCC SRES A1B scenario
   and used as input to the Eulerian Chemistry Transport Model MATCH from
   which projections of ozone concentrations were derived. The ozone
   exposure of vegetation over Europe expressed as AOT40 was projected to
   be substantially reduced between the periods 1990-2009 and 2040-2059 to
   levels which are well below critical levels used for vegetation in the
   EU directive 2008/50/EC as well as for crops and forests used in the
   LRTAP convention, despite that the future climate resulted in prolonged
   yearly ozone sensitive periods. The reduction in AOT40 was mainly driven
   by the emission reductions, not changes in the climate. For the
   toxicologically more relevant POD1 index the projected reductions were
   smaller, but still significant. The values for POD1 for the time period
   2040-2059 were not projected to decrease to levels which are below
   critical levels for forest trees, represented by Norway spruce. This
   study shows that substantial reductions of ozone precursor emissions
   have the potential to strongly reduce the future risk for ozone effects
   on the European vegetation, even if concurrent climate change promotes
   ozone formation.}},
DOI = {{10.5194/bg-11-5269-2014}},
ISSN = {{1726-4170}},
EISSN = {{1726-4189}},
ResearcherID-Numbers = {{Pleijel, Hakan/C-9724-2010
   }},
ORCID-Numbers = {{Pleijel, Hakan/0000-0002-6975-5984
   Klingberg, Jenny/0000-0001-7081-2782}},
Unique-ID = {{ISI:000344153200001}},
}

@article{ ISI:000335946200001,
Author = {Wang, Jinliang and Pang, Jingmei and Liu, Xianning},
Title = {{Modelling diseases with relapse and nonlinear incidence of infection: a
   multi-group epidemic model}},
Journal = {{JOURNAL OF BIOLOGICAL DYNAMICS}},
Year = {{2014}},
Volume = {{8}},
Number = {{1}},
Pages = {{99-116}},
Month = {{JAN 1}},
Abstract = {{In this paper, we introduce a basic reproduction number for a
   multi-group SIR model with general relapse distribution and nonlinear
   incidence rate. We find that basic reproduction number plays the role of
   a key threshold in establishing the global dynamics of the model. By
   means of appropriate Lyapunov functionals, a subtle grouping technique
   in estimating the derivatives of Lyapunov functionals guided by
   graph-theoretical approach and LaSalle invariance principle, it is
   proven that if it is less than or equal to one, the disease-free
   equilibrium is globally stable and the disease dies out; whereas if it
   is larger than one, some sufficient condition is obtained in ensuring
   that there is a unique endemic equilibrium which is globally stable and
   thus the disease persists in the population. Furthermore, our results
   suggest that general relapse distribution are not the reason of
   sustained oscillations. Biologically, our model might be realistic for
   sexually transmitted diseases, such as Herpes, Condyloma acuminatum,
   etc.}},
DOI = {{10.1080/17513758.2014.912682}},
ISSN = {{1751-3758}},
EISSN = {{1751-3766}},
ResearcherID-Numbers = {{Liu, Xianning/K-2578-2013}},
ORCID-Numbers = {{Liu, Xianning/0000-0001-8527-7698}},
Unique-ID = {{ISI:000335946200001}},
}

@article{ ISI:000333291900001,
Author = {Fan, Ran and Jin, Xiaogang},
Title = {{Controllable Edge Feature Sharpening for Dental Applications}},
Journal = {{COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE}},
Year = {{2014}},
Abstract = {{This paper presents a new approach to sharpen blurred edge features in
   scanned tooth preparation surfaces generated by structured-light
   scanners. It aims to efficiently enhance the edge features so that the
   embedded feature lines can be easily identified in dental CAD systems,
   and to avoid unnatural oversharpening geometry. We first separate the
   feature regions using graph-cut segmentation, which does not require a
   user-defined threshold. Then, we filter the face normal vectors to
   propagate the geometry from the smooth region to the feature region. In
   order to control the degree of the sharpness, we propose a feature
   distance measure which is based on normal tensor voting. Finally, the
   vertex positions are updated according to the modified face normal
   vectors. We have applied the approach to scanned tooth preparation
   models. The results show that the blurred edge features are enhanced
   without unnatural oversharpening geometry.}},
DOI = {{10.1155/2014/873635}},
Article-Number = {{873635}},
ISSN = {{1748-670X}},
EISSN = {{1748-6718}},
ORCID-Numbers = {{Jin, Xiaogang/0000-0001-7339-2920}},
Unique-ID = {{ISI:000333291900001}},
}

@article{ ISI:000332074000018,
Author = {Shao, Dongguo and Yang, Haidong and Xiao, Yi and Liu, Biyu},
Title = {{Water quality model parameter identification of an open channel in a
   long distance water transfer project based on finite difference,
   difference evolution and Monte Carlo}},
Journal = {{WATER SCIENCE AND TECHNOLOGY}},
Year = {{2014}},
Volume = {{69}},
Number = {{3}},
Pages = {{587-594}},
Abstract = {{A new method is proposed based on the finite difference method (FDM),
   differential evolution algorithm and Markov Chain Monte Carlo (MCMC)
   simulation to identify water quality model parameters of an open channel
   in a long distance water transfer project. Firstly, this parameter
   identification problem is considered as a Bayesian estimation problem
   and the forward numerical model is solved by FDM, and the posterior
   probability density function of the parameters is deduced. Then these
   parameters are estimated using a sampling method with differential
   evolution algorithm and MCMC simulation. Finally this proposed method is
   compared with FDM-MCMC by a twin experiment. The results show that the
   proposed method can be used to identify water quality model parameters
   of an open channel in a long distance water transfer project under
   different scenarios better with fewer iterations, higher reliability and
   anti-noise capability compared with FDM-MCMC. Therefore, it provides a
   new idea and method to solve the traceability problem in sudden water
   pollution accidents.}},
DOI = {{10.2166/wst.2013.753}},
ISSN = {{0273-1223}},
EISSN = {{1996-9732}},
Unique-ID = {{ISI:000332074000018}},
}

@article{ ISI:000330911400013,
Author = {Campbell, Matthew P. and Nguyen-Khuong, Terry and Hayes, Catherine A.
   and Flowers, Sarah A. and Alagesan, Kathirvel and Kolarich, Daniel and
   Packer, Nicolle H. and Karlsson, Niclas G.},
Title = {{Validation of the curation pipeline of UniCarb-DB: Building a global
   glycan reference MS/MS repository}},
Journal = {{BIOCHIMICA ET BIOPHYSICA ACTA-PROTEINS AND PROTEOMICS}},
Year = {{2014}},
Volume = {{1844}},
Number = {{1, A, SI}},
Pages = {{108-116}},
Month = {{JAN}},
Abstract = {{The UniCarb-DB database is an emerging public glycomics data repository,
   containing over 500 tandem mass spectra (as of March 2013) of glycans
   released from glycoproteins. A major challenge in glycomics research is
   to provide and maintain high-quality datasets that will offer the
   necessary diversity to support the development of accurate
   bioinformatics tools for data deposition and analySis. The role of
   UniCarb-DB, as an archival database, is to provide the glycomics
   community with open-access to a comprehensive LC MS/MS library of N- and
   O- linked glycans released from glycoproteins that have been annotated
   with glycosidic and cross-ring fragmentation ions, retention times, and
   associated experimental metadata descriptions. Here, we introduce the
   UniCarb-DB data submission pipeline and its practical application to
   construct a library of LC MS/MS glycan standards that forms part of this
   database. In this context, an independent consortium of three
   laboratories was established to analyze the same 23 commercially
   available oligosaccharide standards, all by using graphitized
   carbon-liquid chromatography (LC) electrospray ionization (ESI) ion trap
   mass spectrometry in the negative ion mode. A dot product score was
   calculated for each spectrum in the three sets of data as a measure of
   the comparability that is necessary for use of such a collection in
   library-based spectral matching and glycan structural identification.
   The effects of charge state, de-isotoping and threshold levels on the
   quality of the input data are shown. The provision of well-characterized
   oligosaccharide fragmentation data provides the opportunity to identify
   determinants of specific glycan structures, and will contribute to the
   confidence level of algorithms that assign glycan structures to
   experimental MS/MS spectra. This article is part of a Special Issue
   entitled: Computational Proteomics in the Post-Identification Era. Guest
   Editors: Martin Eisenacher and Christian Stephan. (C) 2013 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.bbapap.2013.04.018}},
ISSN = {{1570-9639}},
EISSN = {{0006-3002}},
ORCID-Numbers = {{Kolarich, Daniel/0000-0002-8452-1350
   Campbell, Matthew/0000-0002-9525-792X
   Karlsson, Niclas/0000-0002-3045-2628
   Flowers, Sarah/0000-0002-3513-2260
   Packer, Nicolle/0000-0002-7532-4021
   Alagesan, Kathirvel/0000-0002-7596-5558}},
Unique-ID = {{ISI:000330911400013}},
}

@article{ ISI:000329766200003,
Author = {Feyisa, Gudina L. and Meilby, Henrik and Fensholt, Rasmus and Proud,
   Simon R.},
Title = {{Automated Water Extraction Index: A new technique for surface water
   mapping using Landsat imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2014}},
Volume = {{140}},
Pages = {{23-35}},
Month = {{JAN}},
Abstract = {{Classifying surface cover types and analyzing changes are among the most
   common applications of remote sensing. One of the most basic
   classification tasks is to distinguish water bodies from dry land
   surfaces. Landsat imagery is among the most widely used sources of data
   in remote sensing of water resources; and although several techniques of
   surface water extraction using Landsat data are described in the
   literature, their application is constrained by low accuracy in various
   situations. Besides, with the use of techniques such as single band
   thresholding and two-band indices, identifying an appropriate threshold
   yielding the highest possible accuracy is a challenging and time
   consuming task, as threshold values vary with location and time of image
   acquisition. The purpose of this study was therefore to devise an index
   that consistently improves water extraction accuracy in the presence of
   various sorts of environmental noise and at the same time offers a
   stable threshold value. Thus we introduced a new Automated Water
   Extraction Index (AWEI) improving classification accuracy in areas that
   include shadow and dark surfaces that other classification methods often
   fail to classify correctly. We tested the accuracy and robustness of the
   new method using Landsat 5 TM images of several water bodies in Denmark,
   Switzerland, Ethiopia, South Africa and New Zealand. Kappa coefficient,
   omission and commission errors were calculated to evaluate accuracies.
   The performance of the classifier was compared with that of the Modified
   Normalized Difference Water Index (MNDWI) and Maximum Likelihood (ML)
   classifiers. In four out of five test sites, classification accuracy of
   AWEI was significantly higher than that of MNDWI and ML (P-value <
   0.01). AWEI improved accuracy by lessening commission and omission
   errors by 50\% compared to those resulting from MNDWI and about 25\%
   compared to ML classifiers. Besides, the new method was shown to have a
   fairly stable optimal threshold value. Therefore, AWEI can be used for
   extracting water with high accuracy, especially in mountainous areas
   where deep shadow caused by the terrain is an important source of
   classification error. (C) 2013 Elsevier Inc All rights reserved.}},
DOI = {{10.1016/j.rse.2013.08.029}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Fensholt, Rasmus/L-7951-2014
   Meilby, Henrik/E-1404-2015
   Feyisa, Gudina/E-4988-2013
   }},
ORCID-Numbers = {{Fensholt, Rasmus/0000-0003-3067-4527
   Meilby, Henrik/0000-0002-3770-3880
   Proud, Simon/0000-0003-3880-6774}},
Unique-ID = {{ISI:000329766200003}},
}

@article{ ISI:000329629000004,
Author = {Friedman, Avner and Yakubu, Abdul-Aziz},
Title = {{A Bovine Babesiosis Model with Dispersion}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{2014}},
Volume = {{76}},
Number = {{1}},
Pages = {{98-135}},
Month = {{JAN}},
Abstract = {{Bovine Babesiosis (BB) is a tick borne parasitic disease with worldwide
   over 1.3 billion bovines at potential risk of being infected. The
   disease, also called tick fever, causes significant mortality from
   infection by the protozoa upon exposure to infected ticks. An important
   factor in the spread of the disease is the dispersion or migration of
   cattle as well as ticks. In this paper, we study the effect of this
   factor. We introduce a number, , a ``proliferation index,{''} which
   plays the same role as the basic reproduction number with respect to the
   stability/instability of the disease-free equilibrium, and observe that
   decreases as the dispersion coefficients increase. We prove,
   mathematically, that if then the tick fever will remain endemic. We also
   consider the case where the birth rate of ticks undergoes seasonal
   oscillations. Based on data from Colombia, South Africa, and Brazil, we
   use the model to determine the effectiveness of several intervention
   schemes to control the progression of BB.}},
DOI = {{10.1007/s11538-013-9912-8}},
ISSN = {{0092-8240}},
EISSN = {{1522-9602}},
Unique-ID = {{ISI:000329629000004}},
}

@article{ ISI:000329243700073,
Author = {Xue, Qiaoyun and Lu, Lingli and Zhou, Yuanqing and Qi, Lingyu and Dai,
   Peibin and Liu, Xiaoxia and Sun, Chengliang and Lin, Xianyong},
Title = {{Deriving sorption indices for the prediction of potential phosphorus
   loss from calcareous soils}},
Journal = {{ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH}},
Year = {{2014}},
Volume = {{21}},
Number = {{2}},
Pages = {{1564-1571}},
Month = {{JAN}},
Abstract = {{The aim of this study was to develop techniques to evaluate soil
   phosphorus (P) sorption capacity (PSC) and determine critical soil P
   levels to predict P loss potential for calcareous soils. Seventy-five
   soils mostly from Northern China were analyzed for soil P using four
   extraction methods (water, P-w; carbonate, P-Ols; ammonium oxalate,
   P-ox; and Mehlich 3, P-M3) as well as PSC derived from single-point
   (PSC150) and multipoint sorption (S (t)) isotherms. Strong correlation
   was found between PSC150 and S (t) (r (2)=0.89, p < 0.001). The sum of
   alpha Ca-M3 and beta Mg-M3 as an index of PSC (PSC(CaM3 + MgM3)) was
   most closely related to the maximum amount of P sorbed (S (max)) as
   given by the sum of S (t) and soil initial P setting alpha=0.039 and
   beta=0.462 (r (2)=0.80, p < 0.001). The degree of P saturation (DPS) was
   thereafter calculated from PSC(CaM3 + MgM3) (DPS(CaM3 + MgM3)), to which
   Olsen P (P-Ols) was significantly correlated (r (2)=0.82, p < 0.001). In
   a split-line regression from P-w against DPS(CaM3 + MgM3) (r (2)=0.87, p
   < 0.05), a change point was identified at 28.1\% DPS(CaM3 + MgM3), which
   was equivalent to 49.2 mg kg(-1) P-Ols and corresponded to a P-w
   concentration of 8.8 mg kg(-1). After the change point, a sharp increase
   in P-w was observed. Our results reveal a new approach to approximating
   DPS from Ca-M3 and Mg-M3 for calcareous soils without the need to
   generate a S (max). We conclude that in the absence of an environmental
   soil test criteria for P, the DPS(CaM3 + MgM3) and P-Ols could be used
   to predict P loss potential from calcareous soils.}},
DOI = {{10.1007/s11356-013-2045-7}},
ISSN = {{0944-1344}},
EISSN = {{1614-7499}},
Unique-ID = {{ISI:000329243700073}},
}

@incollection{ ISI:000328647400019,
Author = {Emmer, Adam and Vilimek, Vit and Klimes, Jan and Cochachin, Alejo},
Editor = {{Shan, W and Guo, Y and Wang, F and Marui, H and Strom, A}},
Title = {{Glacier Retreat, Lakes Development and Associated Natural Hazards in
   Cordilera Blanca, Peru}},
Booktitle = {{LANDSLIDES IN COLD REGIONS IN THE CONTEXT OF CLIMATE CHANGE}},
Series = {{Environmental Science and Engineering-Environmental Engineering}},
Year = {{2014}},
Pages = {{231-252}},
Abstract = {{Cordillera Blanca is the heaviest glacierized tropical range in the
   world. Due to the global climate change, most of glaciers are retreating
   and thinning. Glacier retreat leads to the formation and development of
   all types of potentially hazardous glacial lakes (bedrock-dammed,
   moraine-dammed, and ice-dammed). Potential hazardousness of glacial
   lakes is strongly interconnected with dynamic slope movements: (1)
   sudden release of water from glacial lakes (also known as glacial lake
   outburst floods-GLOF) is mainly caused by dynamic slope movement into
   the lake (about 80 \% in the Cordillera Blanca); (2) released water may
   easily transform into debris-flow or mud-flow, thanks to its high
   erosion and transport potential. Based on field study and remotely
   sensed images, this contribution documents glacier retreat in the
   Cordillera Blanca with regards to formation and development of new
   potentially hazardous glacial lakes, which evolve mainly in elevations
   of about 4,600-5,000 m a.s.l. We introduce and describe three hazardous
   events associated with glacier retreat in the last decade: (a) sudden
   release of water from moraine-dammed Lake Palcacocha in 2003; (b) sudden
   release of water from bedrock-dammed lake No. 513 in 2010; and (c)
   sudden release of water from bedrock-dammed Lake Artizon Alto and
   subsequent moraine dam failure of downstream situated Lake Artizon Bajo
   in 2012. The first and third events were caused by landslides of lateral
   moraines (which are often nonconsolidated and nearly vertical) into the
   lakes. The second event was caused by ice- and rockfall into the lake.
   These events illustrate that various natural hazards (dynamic slope
   movements, floods) associated with glacier retreat in the Cordillera
   Blanca are closely linked and represent actual threats to urbanization
   and safety of lives and property.}},
DOI = {{10.1007/978-3-319-00867-7\_17}},
ISSN = {{1863-5520}},
ISBN = {{978-3-319-00866-0}},
ResearcherID-Numbers = {{Klimes, Jan/H-4637-2014}},
ORCID-Numbers = {{Klimes, Jan/0000-0003-2539-5451}},
Unique-ID = {{ISI:000328647400019}},
}

@incollection{ ISI:000400252000214,
Author = {Viegas, Domingos Xavier},
Editor = {{Viegas, DX}},
Title = {{Theoretical solution for a logistic problem: how to raise the
   effectiveness of aerial water transport}},
Booktitle = {{ADVANCES IN FOREST FIRE RESEARCH}},
Year = {{2014}},
Pages = {{1911-1919}},
Abstract = {{Introduction: As well-known, aerial firefighting is an effective
   solution suppressing forest fire, however there is no doubt, this tool
   in many cases can be the only one effective solution, even if it is very
   expensive. Following the above idea, any new method that is able to
   reduce the cost of aerial means supporting forest fire management, is
   worth examining as a new aerial solution. Methods: This article used
   practical experiments of the aerial firefighting, created a graphics
   model to understand the logistic problem, made assumptions to
   concentrate on the key problems and with mathematical backgrounds some
   logistic functions meaning distance and capacity axes. Results and
   discussion: The effectiveness of air tankers depends on the distance
   between the fire zone and water resource. The shorter the distance is
   between them, the higher the effectiveness is using aerials. This
   relation can be shown even at a function; however the change is not
   linear. The rate of the curve depends on the distance, larger distance
   means smaller change and vice versa, shorter distance means bigger scale
   of change. This result gives the tipping point where the higher costs of
   suggested solution - aerial water supply system will be balanced by the
   higher amount of bombed water.}},
DOI = {{10.14195/978-989-26-0884-6\_213}},
ISBN = {{978-989-26-0884-6}},
Unique-ID = {{ISI:000400252000214}},
}

@article{ ISI:000329888600048,
Author = {Van Wynsberge, Simon and Gilbert, Antoine and Guillemot, Nicolas and
   Payri, Claude and Andrefouet, Serge},
Title = {{Alert thresholds for monitoring environmental variables: A new approach
   applied to seagrass beds diversity in New Caledonia}},
Journal = {{MARINE POLLUTION BULLETIN}},
Year = {{2013}},
Volume = {{77}},
Number = {{1-2}},
Pages = {{300-307}},
Month = {{DEC 15}},
Abstract = {{Monitoring ecological variables is mandatory to detect abnormal changes
   in ecosystems. When the studied variables exceed predefined alert
   thresholds, management actions may be required. In the past, alert
   thresholds have been typically defined by expert judgments and
   descriptive statistics. Recently, approaches based on statistical power
   were also used. In New Caledonia, seagrass monitoring is a priority
   given their vulnerability to natural and anthropic disturbances. To
   define a suitable monitoring strategy and alert thresholds, we compared
   a Percentile Based Approach (PBA) and a sensitivity analysis of power
   (SAP). Both methods defined statistically relevant alert thresholds, but
   the SAP approach-was more robust to spatial and temporal variability of
   seagrass cover. Moreover, this method characterized the sensitivity of
   threshold values to sampling efforts, a useful knowledge for managers.
   (C) 2013 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.marpolbul.2013.09.035}},
ISSN = {{0025-326X}},
EISSN = {{1879-3363}},
ResearcherID-Numbers = {{PAYRI, Claude/K-5646-2016}},
ORCID-Numbers = {{PAYRI, Claude/0000-0002-0393-6811}},
Unique-ID = {{ISI:000329888600048}},
}

@article{ ISI:000328205900027,
Author = {Li, Jia and Uttarwar, Rohan G. and Huang, Yinlun},
Title = {{CFD-based modeling and design for energy-efficient VOC emission
   reduction in surface coating systems}},
Journal = {{CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY}},
Year = {{2013}},
Volume = {{15}},
Number = {{6}},
Pages = {{1023-1032}},
Month = {{DEC}},
Abstract = {{Volatile organic compounds (VOC's) are among the most hazardous
   substances generated in surface coating operations. Hence, VOC emission
   must be strictly controlled. In this paper, we introduce a CFD-based
   system modeling and analysis approach to investigate VOC emission
   mechanisms and to identify the key design and operation parameters of a
   general surface coating application system for energy-efficient emission
   reduction. A case study on paint spray in different design environments
   demonstrates the efficacy of the introduced modeling and analysis
   approach. It shows that a redesign of the ventilation system of a spray
   booth and an adjustment of the operational parameter can reduce VOC
   emission to the level below the threshold limit value; meanwhile, the
   energy efficiency can be improved significantly. The introduced modeling
   and analysis technique for energy-efficient VOC reduction is applicable
   to various industrial practices.}},
DOI = {{10.1007/s10098-013-0583-9}},
ISSN = {{1618-954X}},
EISSN = {{1618-9558}},
Unique-ID = {{ISI:000328205900027}},
}

@article{ ISI:000327322700006,
Author = {Smith, Steven J. and Rasch, Philip J.},
Title = {{The long-term policy context for solar radiation management}},
Journal = {{CLIMATIC CHANGE}},
Year = {{2013}},
Volume = {{121}},
Number = {{3, SI}},
Pages = {{487-497}},
Month = {{DEC}},
Abstract = {{We examine the potential role of ``solar radiation management{''} or
   ``sunlight reduction methods{''} (SRM) in limiting future climate
   change, focusing on the interplay between SRM deployment and mitigation
   in the context of uncertainty in climate response. We use a
   straightforward scenario analysis to show that the policy and physical
   context determine the potential need, amount, and timing of SRM. SRM
   techniques, along with a substantial emission reduction policy, would be
   needed to meet stated policy goals, such as limiting climate change to 2
   A degrees C above pre-industrial levels, if the climate sensitivity is
   high. The SRM levels examined by current modeling studies are much
   higher than the levels required under an assumption of a consistent
   long-term policy. We introduce a degree-year metric, which quantifies
   the magnitude of SRM that would be needed to keep global temperatures
   under a given threshold.}},
DOI = {{10.1007/s10584-012-0577-3}},
ISSN = {{0165-0009}},
EISSN = {{1573-1480}},
Unique-ID = {{ISI:000327322700006}},
}

@article{ ISI:000326643600008,
Author = {Zhang, Jiajie and Kapli, Paschalia and Pavlidis, Pavlos and Stamatakis,
   Alexandros},
Title = {{A general species delimitation method with applications to phylogenetic
   placements}},
Journal = {{BIOINFORMATICS}},
Year = {{2013}},
Volume = {{29}},
Number = {{22}},
Pages = {{2869-2876}},
Month = {{NOV 15}},
Abstract = {{Motivation: Sequence-based methods to delimit species are central to DNA
   taxonomy, microbial community surveys and DNA metabar-coding studies.
   Current approaches either rely on simple sequence similarity thresholds
   (OTU-picking) or on complex and compute-intensive evolutionary models.
   The OTU-picking methods scale well on large datasets, but the results
   are highly sensitive to the similarity threshold. Coalescent-based
   species delimitation approaches often rely on Bayesian statistics and
   Markov Chain Monte Carlo sampling, and can therefore only be applied to
   small datasets.
   Results: We introduce the Poisson tree processes (PTP) model to infer
   putative species boundaries on a given phylogenetic input tree. We also
   integrate PTP with our evolutionary placement algorithm (EPA-PTP) to
   count the number of species in phylogenetic placements. We compare our
   approaches with popular OTU-picking methods and the General Mixed Yule
   Coalescent (GMYC) model. For de novo species delimitation, the
   stand-alone PTP model generally outperforms GYMC as well as OTU-picking
   methods when evolutionary distances between species are small. PTP
   neither requires an ultrametric input tree nor a sequence similarity
   threshold as input. In the open reference species delimitation approach,
   EPA-PTP yields more accurate results than de novo species delimitation
   methods. Finally, EPA-PTP scales on large datasets because it relies on
   the parallel implementations of the EPA and RAxML, thereby allowing to
   delimit species in high-throughput sequencing data.}},
DOI = {{10.1093/bioinformatics/btt499}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Stamatakis, Alexandros/B-8740-2009}},
Unique-ID = {{ISI:000326643600008}},
}

@article{ ISI:000326711300055,
Author = {Hickie, Brendan E. and Cadieux, Marc A. and Riehl, Kimberly N. and
   Bossart, Gregory D. and Alava, Juan Jose and Fair, Patricia A.},
Title = {{Modeling PCB-Bioaccumulation in the Bottlenose Dolphin (Tursiops
   truncatus): Estimating a Dietary Threshold Concentration}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2013}},
Volume = {{47}},
Number = {{21}},
Pages = {{12314-12324}},
Month = {{NOV 5}},
Abstract = {{An individually based (IB) model to predict PCB concentrations in the
   bottlenose dolphin population of Charleston, SC, USA, was developed with
   the aim to gain a better understanding of the bioaccumulation behavior
   and health risk of dietary PCBs across the population and their prey.
   PCB concentrations predicted in male and female bottlenose dolphin were
   in good agreement with observed tissue concentrations corroborating the
   reliability of the model performance and its utility in gaining a more
   complete view of risk. The modeled cumulative distribution of Sigma PCB
   concentrations for the population with a breakdown into juvenile, adult
   male, and female subclasses ranged from 3600 to 144,400 ng/g lipid with
   66\% to >80\% of the population exceeding the established threshold for
   adverse health effects of 17,000 ng/g lipid. The model estimated that a
   dietary PCB concentration not exceeding 5.1 ng/g wet wt would be
   required to reach a condition where 95\% of the population would have
   tissue levels below the health effect threshold. The IB model for PCBs
   in bottlenose dolphins provides a novel approach to estimating the
   maximum acceptable dietary concentration for PCBs, a central and
   important factor to protect these apex predators. The model also enables
   effective prediction of concentrations in dolphins from fish contaminant
   surveys which are logistically easier and less costly to collect.}},
DOI = {{10.1021/es403166b}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Alava, Juan Jose/A-7731-2015}},
ORCID-Numbers = {{Alava, Juan Jose/0000-0002-6312-7776}},
Unique-ID = {{ISI:000326711300055}},
}

@article{ ISI:000330377700010,
Author = {Cheah, Christine S. and Westenbroek, Ruth E. and Roden, William H. and
   Kalume, Franck and Oakley, John C. and Jansen, Laura A. and Catterall,
   William A.},
Title = {{Correlations in timing of sodium channel expression, epilepsy, and
   sudden death in Dravet syndrome}},
Journal = {{CHANNELS}},
Year = {{2013}},
Volume = {{7}},
Number = {{6}},
Pages = {{468-472}},
Month = {{NOV-DEC}},
Abstract = {{Dravet syndrome (DS) is an intractable genetic epilepsy caused by
   loss-of-function mutations in SCN1A, the gene encoding brain sodium
   channel Na(v)1.1. DS is associated with increased frequency of sudden
   unexpected death in humans and in a mouse genetic model of this disease.
   Here we correlate the time course of declining expression of the murine
   embryonic sodium channel Na(v)1.3 and the rise in expression of the
   adult sodium channel Na(v)1.1 with susceptibility to epileptic seizures
   and increased incidence of sudden death in DS mice. Parallel studies
   with unaffected human brain tissue demonstrate similar decline in
   Na(v)1.3 and increase in Na(v)1.1 with age. In light of these results,
   we introduce the hypothesis that the natural loss Na(v)1.3 channel
   expression in brain development, coupled with the failure of increase in
   functional Na(v)1.1 channels in DS, defines a tipping point that leads
   to disinhibition of neural circuits, intractable seizures,
   co-morbidities, and premature death in this disease.}},
DOI = {{10.4161/chan.26023}},
ISSN = {{1933-6950}},
EISSN = {{1933-6969}},
Unique-ID = {{ISI:000330377700010}},
}

@article{ ISI:000327166300086,
Author = {Oh, Min Hwan and Lee, Sang Min and Hong, Sun Hwa and Choi, Han Na and
   Lee, Eun Young},
Title = {{Monitoring of Lactobacillus sp inoculated in the reactor to evaluate the
   solubilization efficiency of primary sludge}},
Journal = {{INTERNATIONAL BIODETERIORATION \& BIODEGRADATION}},
Year = {{2013}},
Volume = {{85}},
Pages = {{603-607}},
Month = {{NOV}},
Abstract = {{A new approach for the solubilization of primary sludge using the
   fatty-acid-producing bacteria, Lactobacillus brevis, was investigated in
   a laboratory-scale solubilization reactor under anaerobic conditions.
   Compared with a non-inoculated experiment, the 10\%(v/v) inoculated
   experiment in an anaerobic bioreactor increased SCOD by more than 59\%,
   and led to 50.5\% COD solubilization as well as the production of mixed
   acids (acetic acid, lactic acid, propionic acid, butyric acid, valeric
   acid, etc.) after 4 days of reaction time. Denaturing gradient gel
   electrophoresis (DGGE) of PCR-amplified 16S rRNA fragments revealed that
   the inoculated L. brevis became the predominant population in the
   bacterial community during the anaerobic solubilization process. PCA
   analysis performed on the molecular weights from the DGGE band revealed
   that the microbial community structure of each sample was grouped by the
   presence of lactic acid. This suggests that bioaugmentation of the
   organism might be useful for enhancing the solubilization of primary
   sludge. L. brevis in the solubilization reactor was monitored using 16S
   rDNA-based quantitative real-time PCR assays. A specific probe for the
   detection of L. brevis was designed and no specific PCR products were
   amplified, as verified by the cycle threshold value. (C) 2013 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.ibiod.2013.03.018}},
ISSN = {{0964-8305}},
EISSN = {{1879-0208}},
Unique-ID = {{ISI:000327166300086}},
}

@article{ ISI:000326808900001,
Author = {Bei, Yuanzhe and Hong, Pengyu},
Title = {{A novel approach to minimize false discovery rate in genome-wide data
   analysis}},
Journal = {{BMC SYSTEMS BIOLOGY}},
Year = {{2013}},
Volume = {{7}},
Number = {{4}},
Month = {{OCT 23}},
Abstract = {{Background: High-throughput technologies, such as DNA microarray, have
   significantly advanced biological and biomedical research by enabling
   researchers to carry out genome-wide screens. One critical task in
   analyzing genome-wide datasets is to control the false discovery rate
   (FDR) so that the proportion of false positive features among those
   called significant is restrained. Recently a number of FDR control
   methods have been proposed and widely practiced, such as the
   Benjamini-Hochberg approach, the Storey approach and Significant
   Analysis of Microarrays (SAM).
   Methods: This paper presents a straight-forward yet powerful FDR control
   method termed miFDR, which aims to minimize FDR when calling a fixed
   number of significant features. We theoretically proved that the
   strategy used by miFDR is able to find the optimal number of significant
   features when the desired FDR is fixed.
   Results: We compared miFDR with the BH approach, the Storey approach and
   SAM on both simulated datasets and public DNA microarray datasets. The
   results demonstrated that miFDR outperforms others by identifying more
   significant features under the same FDR cut-offs. Literature search
   showed that many genes called only by miFDR are indeed relevant to the
   underlying biology of interest.
   Conclusions: FDR has been widely applied to analyzing high-throughput
   datasets allowed for rapid discoveries. Under the same FDR threshold,
   miFDR is capable to identify more significant features than its
   competitors at a compatible level of complexity. Therefore, it can
   potentially generate great impacts on biological and biomedical
   research.
   Availability: If interested, please contact the authors for getting
   miFDR.}},
DOI = {{10.1186/1752-0509-7-S4-S1}},
Article-Number = {{S1}},
ISSN = {{1752-0509}},
Unique-ID = {{ISI:000326808900001}},
}

@article{ ISI:000327432500060,
Author = {De Michele, C. and Salvadori, G. and Vezzoli, R. and Pecora, S.},
Title = {{Multivariate assessment of droughts: Frequency analysis and dynamic
   return period}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2013}},
Volume = {{49}},
Number = {{10}},
Pages = {{6985-6994}},
Month = {{OCT}},
Abstract = {{Droughts, like floods, are extreme expressions of the river flow
   dynamics. Here, droughts are intended as episodes during which the
   streamflow is below a given threshold, and are described as multivariate
   events characterized by two variables: average intensity and duration.
   In this work, we introduce the new concept of Dynamic Return Period,
   formulated using the theory of Copulas, and calculated via a Survival
   Kendall's approach. We show how it can be used (i) to monitor the
   temporal evolution of a drought event, and (ii) to perform real time
   assessment. In addition, a randomization strategy is introduced, in
   order to get rid of repeated measurements, which may adversely affect
   the statistical analysis of the available data, as well as the
   calculation of the return periods of interest: a practical example is
   shown, involving the fit of the drought duration distribution. The case
   study of the Po river basin (Northern Italy) is used as an illustration.}},
DOI = {{10.1002/wrcr.20551}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ResearcherID-Numbers = {{De Michele, Carlo/L-7503-2015
   }},
ORCID-Numbers = {{De Michele, Carlo/0000-0002-7098-4725
   VEZZOLI, Renata/0000-0002-2147-5529}},
Unique-ID = {{ISI:000327432500060}},
}

@article{ ISI:000326135800011,
Author = {Bruck, Hugh Alan and Yang, Minghui and Kostov, Yordan and Rasooly,
   Avraham},
Title = {{Electrical percolation based biosensors}},
Journal = {{METHODS}},
Year = {{2013}},
Volume = {{63}},
Number = {{3}},
Pages = {{282-289}},
Month = {{OCT}},
Abstract = {{A new approach to label free biosensing has been developed based on the
   principle of ``electrical percolation{''}. In electrical percolation,
   long-range electrical connectivity is formed in randomly oriented and
   distributed systems of discrete elements. By applying this principle to
   biological interactions, it is possible to measure biological components
   both directly and electronically. The main element for electrical
   percolation biosensor is the biological semiconductor (BSC) which is a
   multi-layer 3-D carbon nano-tube-antibody network. In the BSC, molecular
   interactions, such as binding of antigens to the antibodies, disrupt the
   network continuity causing increased resistance of the network. BSCs can
   be fabricated by immobilizing conducting elements, such as
   pre-functionalized single-walled carbon nanotubes (SWNTs)-antibody
   complex, directly onto a substrate, such as a Poly(methyl methacrylate)
   (PMMA) surface (also known as plexi-glass or Acrylic).
   BSCs have been demonstrated for direct (label-free) electronic
   measurements of antibody-antigen binding using SWNTs. If the
   concentration of the SWNT network is slightly above the electrical
   percolation threshold, then binding of a specific antigen to the
   pre-functionalized SWNT dramatically increases the electrical resistance
   due to changes in the tunneling between the SWNTs. Using
   anti-staphylococcal enterotoxin B (SEB) IgG as a ``gate{''} and SEB as
   an ``actuator{''}, it was demonstrated that the BSC was able to detect
   SEB at concentrations of 1 ng/ml. Based on this concept, an automated
   configuration for BSCs is described here that enables real time
   continuous detection. The new BSC configuration may permit assembly of
   multiple sensors on the same chip to create ``biological central
   processing units (CPUs){''} with multiple biological elements, capable
   of processing and sorting out information on multiple analytes
   simultaneously. Published by Elsevier Inc.}},
DOI = {{10.1016/j.ymeth.2013.08.031}},
ISSN = {{1046-2023}},
EISSN = {{1095-9130}},
ResearcherID-Numbers = {{bruck, hugh/I-1154-2018}},
ORCID-Numbers = {{bruck, hugh/0000-0002-0845-765X}},
Unique-ID = {{ISI:000326135800011}},
}

@article{ ISI:000325735500035,
Author = {Wu, Shuicai and Shen, Yanni and Zhou, Zhuhuang and Lin, Lan and Zeng,
   Yanjun and Gao, Xiaofeng},
Title = {{Research of fetal ECG extraction using wavelet analysis and adaptive
   filtering}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2013}},
Volume = {{43}},
Number = {{10}},
Pages = {{1622-1627}},
Month = {{OCT 1}},
Abstract = {{Extracting clean fetal electrocardiogram (ECG) signals is very important
   in fetal monitoring. In this paper, we proposed a new method for fetal
   ECG extraction based on wavelet analysis, the least mean square (LMS)
   adaptive filtering algorithm, and the spatially selective noise
   filtration (SSNF) algorithm. First, abdominal signals and thoracic
   signals were processed by stationary wavelet transform (SWT), and the
   wavelet coefficients at each scale were obtained. For each scale, the
   detail coefficients were processed by the LMS algorithm. The coefficient
   of the abdominal signal was taken as the original input of the LMS
   adaptive filtering system, and the coefficient of the thoracic signal as
   the reference input. Then, correlations of the processed wavelet
   coefficients were computed. The threshold was set and noise components
   were removed with the SSNF algorithm. Finally, the processed wavelet
   coefficients were reconstructed by inverse SWT to obtain fetal ECG.
   Twenty cases of simulated data and 12 cases of clinical data were used.
   Experimental results showed that the proposed method outperforms the LMS
   algorithm: (1) it shows improvement in case of superposition R-peaks of
   fetal ECG and maternal ECG; (2) noise disturbance is eliminated by
   incorporating the SSNF algorithm and the extracted waveform is more
   stable; and (3) the performance is proven quantitatively by SNR
   calculation. The results indicated that the proposed algorithm can be
   used for extracting fetal ECG from abdominal signals. (C) 2013 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiomed.2013.07.028}},
ISSN = {{0010-4825}},
EISSN = {{1879-0534}},
ORCID-Numbers = {{Zhou, Zhuhuang/0000-0003-0570-8473}},
Unique-ID = {{ISI:000325735500035}},
}

@article{ ISI:000325030900011,
Author = {Pellissier, Loic and Espindola, Anahi and Pradervand, Jean-Nicolas and
   Dubuis, Anne and Pottier, Julien and Ferrier, Simon and Guisan, Antoine},
Title = {{A probabilistic approach to niche-based community models for spatial
   forecasts of assemblage properties and their uncertainties}},
Journal = {{JOURNAL OF BIOGEOGRAPHY}},
Year = {{2013}},
Volume = {{40}},
Number = {{10}},
Pages = {{1939-1946}},
Month = {{OCT}},
Abstract = {{AimConservation strategies need predictions that capture spatial
   community composition and structure. Currently, the methods used to
   generate these predictive maps generally focus on deterministic
   processes and omit stochasticity and other uncertainty in model outputs.
   Here we present a novel approach to model the means and variance of
   assemblage properties.
   LocationThe western Swiss Alps.
   MethodsWe propose a new approach to processing probabilistic predictions
   derived from stacked species distribution models (S-SDMs) in order to
   predict and assess the uncertainty in predictions of community
   properties. We compare the utility of our novel approach with that of a
   traditional threshold-based approach. We used data sampled in 2009 and
   2010 from 192 sites in total for mountain butterfly communities spanning
   a large elevational gradient as a case study and evaluated the ability
   of our approach to model the species richness and phylogenetic diversity
   of communities within an ensemble forecasting framework.
   ResultsOur approach allowed mapping of the variability in species
   richness and phylogenetic diversity projections, in addition to the
   mean, for 78 butterfly species. S-SDMs reproduced the observed decrease
   in phylogenetic diversity and species richness with elevation, a
   consequence of environmental filtering. The prediction accuracy of
   community properties varied along environmental gradients: at low
   elevations, variability was higher for predictions of species richness
   while it was the opposite for phylogenetic diversity.
   Main conclusionsThe use of our probabilistic approach to process species
   distribution model outputs in order to reconstruct communities provides
   an improved picture of the range of possible assemblage realizations
   under similar environmental conditions given modelling uncertainty, and
   helps to inform managers of the usefulness of modelling results.}},
DOI = {{10.1111/jbi.12140}},
ISSN = {{0305-0270}},
EISSN = {{1365-2699}},
ResearcherID-Numbers = {{Guisan, Antoine/A-1057-2011
   Pellissier, Loic/J-2563-2015
   Ferrier, Simon/C-1490-2009}},
ORCID-Numbers = {{Guisan, Antoine/0000-0002-3998-4815
   Pellissier, Loic/0000-0002-2289-8259
   Ferrier, Simon/0000-0001-7884-2388}},
Unique-ID = {{ISI:000325030900011}},
}

@article{ ISI:000325459600009,
Author = {Chesters, Douglas and Yu, Fang and Cao, Huan-Xi and Dai, Qing-Yan and
   Wu, Qing-Tao and Shi, Weifeng and Zheng, Weimin and Zhu, Chao-Dong},
Title = {{Heuristic optimization for global species clustering of DNA sequence
   data from multiple loci}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2013}},
Volume = {{4}},
Number = {{10}},
Pages = {{961-970}},
Month = {{OCT}},
Abstract = {{Hierarchical clustering of molecular data is commonly used for
   estimation of species diversity in all forms of life. Parameters
   appropriate for species-level clustering are usually derived from
   reference data and applied for the delineation of sequences of unknown
   species membership, although it is not clear how this should be carried
   out in a multilocus scenario. We introduce a novel means of concurrent
   clustering parameter optimization and delineation for multilocus data. A
   simulated annealing heuristic search is performed, whereby clustering
   thresholds are independently varied for each locus, but optimized
   according to the recovery of expected taxonomic species globally over
   loci. For each iteration of the search, one or more loci are randomly
   selected and a different threshold is separately proposed to cluster
   each, then the loci are linked to form global species units. Where the
   set of thresholds group the reference (species labelled) data with high
   taxonomic congruence, they are adopted for clustering of the subject
   (nonlabelled) sequences into global molecular operational taxonomic
   units (global MOTU). Four mined test data sets composed of both
   reference and subject sequences are combined with a newly sequenced
   three gene Apoidea data set, and subject to the proposed method. Even
   optimizing four loci and thousands of sequences, the approach rapidly
   convergences on a set of parameters with maximal optimality score,
   although the method masks a high degree of incongruence, and does not
   always converge on a single set of thresholds. For example, of the 476
   Apoidea sequences, 70 global MOTU were inferred over the heuristic
   search, although the number of single gene MOTU were much lower for the
   28S RNA locus, and a range of equally optimal clustering thresholds were
   observed for the CytB gene. We demonstrate the approach as a scalable
   species delineation solution for heterogeneous data sets composed of
   incompletely and inconsistently labelled data from public DNA data
   bases, for newly sequenced multilocus data, or both. The delineation
   over a heuristic search of clustering parameters facilitates the
   estimation of species diversity in multilocus data, giving species
   estimates that take into account uncertainty regarding choice of
   clustering thresholds.}},
DOI = {{10.1111/2041-210X.12104}},
ISSN = {{2041-210X}},
EISSN = {{2041-2096}},
ResearcherID-Numbers = {{ZHU, Chao-Dong/E-5961-2011}},
ORCID-Numbers = {{ZHU, Chao-Dong/0000-0002-9347-3178}},
Unique-ID = {{ISI:000325459600009}},
}

@article{ ISI:000325009900004,
Author = {Roding, Magnus and Deschout, Hendrik and Martens, Thomas and Notelaers,
   Kristof and Hofkens, Johan and Ameloot, Marcel and Braeckmans, Kevin and
   Sarkka, Aila and Rudemo, Mats},
Title = {{Automatic Particle Detection in Microscopy Using Temporal Correlations}},
Journal = {{MICROSCOPY RESEARCH AND TECHNIQUE}},
Year = {{2013}},
Volume = {{76}},
Number = {{10}},
Pages = {{997-1006}},
Month = {{OCT}},
Abstract = {{One of the fundamental problems in the analysis of single particle
   tracking data is the detection of individual particle positions from
   microscopy images. Distinguishing true particles from noise with a
   minimum of false positives and false negatives is an important step that
   will have substantial impact on all further analysis of the data. A
   common approach is to obtain a plausible set of particles from a larger
   set of candidate particles by filtering using manually selected
   threshold values for intensity, size, shape, and other parameters
   describing a particle. This introduces subjectivity into the analysis
   and hinders reproducibility. In this paper, we introduce a method for
   automatic selection of these threshold values based on maximizing
   temporal correlations in particle count time series. We use Markov Chain
   Monte Carlo to find the threshold values corresponding to the maximum
   correlation, and we study several experimental data sets to assess the
   performance of the method in practice by comparing manually selected
   threshold values from several independent experts with automatically
   selected threshold values. We conclude that the method produces useful
   results, reducing subjectivity and the need for manual intervention, a
   great benefit being its easy integratability into many already existing
   particle detection algorithms. Microsc. Res. Tech., 76:997-1006, 2013.
   (c) 2013 Wiley Periodicals, Inc.}},
DOI = {{10.1002/jemt.22260}},
ISSN = {{1059-910X}},
ResearcherID-Numbers = {{Roding, Magnus/N-2169-2015
   }},
ORCID-Numbers = {{Roding, Magnus/0000-0002-5956-9934
   Hofkens, Johan/0000-0002-9101-0567}},
Unique-ID = {{ISI:000325009900004}},
}

@article{ ISI:000322688600003,
Author = {Bellingeri, Michele and Vincenzi, Simone},
Title = {{Robustness of empirical food webs with varying consumer's sensitivities
   to loss of resources}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2013}},
Volume = {{333}},
Pages = {{18-26}},
Month = {{SEP 21}},
Abstract = {{Food web responses to species loss have been mostly studied in binary
   food webs, thus without accounting for the amount of energy transferred
   in consumer resource interactions.
   We introduce an energetic criterion, called extinction threshold, for
   which a species goes secondarily extinct when a certain fraction of its
   incoming energy is lost. We study the robustness to random node loss of
   10 food webs based on empirically-derived weightings. We use different
   extinction scenarios (random removal and from most- to least-connected
   species), and we simulate 10(5) replicates for each extinction threshold
   to account for stochasticity of extinction dynamics.
   We quantified robustness on the basis of how many additional species
   (i.e. secondary extinctions) were lost after the direct removal of
   species (i.e. primary extinctions). For all food webs, the expected
   robustness linearly decreases with extinction threshold, although a
   large variance in robustness is observed. The sensitivity of robustness
   to variations in extinction threshold increases with food web species
   richness and quantitative unweighted link density, while we observed a
   nonlinear relationship when the predictor is food web connectance and no
   relationship with the proportion of autotrophs. (C) 2013 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.jtbi.2013.04.033}},
ISSN = {{0022-5193}},
ResearcherID-Numbers = {{Bellingeri, Michele/S-6169-2018}},
Unique-ID = {{ISI:000322688600003}},
}

@article{ ISI:000329285700009,
Author = {Boca, Simina M. and Bravo, Hector Ceorrada and Caffo, Brian and Leek,
   Jeffrey T. and Parmigiani, Giovanni},
Title = {{A Decision-Theory Approach to Interpretable Set Analysis for
   High-Dimensional Data}},
Journal = {{BIOMETRICS}},
Year = {{2013}},
Volume = {{69}},
Number = {{3}},
Pages = {{614-623}},
Month = {{SEP}},
Abstract = {{A key problem in high-dimensional significance analysis is to find
   pre-defined sets that show enrichment for a statistical signal of
   interest; the classic example is the enrichment of gene sets for
   differentially expressed genes. Here, we propose a new decision-theory
   approach to the analysis of gene sets which focuses on estimating the
   fraction of non-null variables in a set. We introduce the idea of atoms,
   non-overlapping sets based on the original pre-defined set annotations.
   Our approach focuses on finding the union of atoms that minimizes a
   weighted average of the number of false discoveries and missed
   discoveries. We introduce a new false discovery rate for sets, called
   the atomic false discovery rate (afdr), and prove that the optimal
   estimator in our decision-theory framework is to threshold the afdr.
   These results provide a coherent and interpretable framework for the
   analysis of sets that addresses the key issues of overlapping
   annotations and difficulty in interpreting p values in both competitive
   and self-contained tests. We illustrate our method and compare it to a
   popular existing method using simulated examples, as well as gene-set
   and brain ROI data analyses.}},
DOI = {{10.1111/biom.12060}},
ISSN = {{0006-341X}},
EISSN = {{1541-0420}},
ORCID-Numbers = {{Leek, Jeffrey/0000-0002-2873-2671
   Corrada Bravo, Hector/0000-0002-1255-4444
   Boca, Simina/0000-0002-1400-3398}},
Unique-ID = {{ISI:000329285700009}},
}

@article{ ISI:000326304800006,
Author = {Skjong, Morten and Naess, Arvid and Naess, Ole Erik Brandrud},
Title = {{Statistics of Extreme Sea Levels for Locations along the Norwegian Coast}},
Journal = {{JOURNAL OF COASTAL RESEARCH}},
Year = {{2013}},
Volume = {{29}},
Number = {{5}},
Pages = {{1029-1048}},
Month = {{SEP}},
Abstract = {{The focus of this article is the problem of estimating long
   return-period sea levels for locations along the Norwegian coast. For
   this purpose, a new method for estimating the extreme-value statistics
   associated with a recorded time series of data is applied. This method,
   briefly referred to as the Average Conditional Exceedance Rate or ACER
   method, is presented in some detail. The ACER method provides a
   statistical representation with error bounds of the actual extreme-value
   distribution given by the data and is applicable to nonstationary time
   series as well. The estimated return-level values obtained by the ACER
   method are compared with the corresponding values obtained by the annual
   maxima method, the peaks-over-threshold method, and the revised
   joint-probabilities method. Based on overall performance, the ACER
   method may be the preferred choice over the other three methods tested.}},
DOI = {{10.2112/JCOASTRES-D-12-00208.1}},
ISSN = {{0749-0208}},
EISSN = {{1551-5036}},
Unique-ID = {{ISI:000326304800006}},
}

@article{ ISI:000325992300004,
Author = {Wang, Xia and Liu, Shengqiang and Song, Xinyu},
Title = {{DYNAMICS OF A NON-AUTONOMOUS HIV-1 INFECTION MODEL WITH DELAYS}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMATHEMATICS}},
Year = {{2013}},
Volume = {{6}},
Number = {{5}},
Month = {{SEP}},
Abstract = {{In this paper, following a previous paper ({[}32] Permanence and
   extinction of a non-autonomous HIV-1 model with two time delays,
   preprint) on the permanence and extinction of a delayed non-autonomous
   HIV-1 within-host model, we introduce and investigate a delayed HIV-1
   model including maximum homeostatic proliferation rate of CD4(+) T-cells
   and varying coefficients. By applying the asymptotic analysis theory and
   oscillation theory, we show: (i) the system will be permanent when the
   threshold value R-{*} > 1, and for this case we also obtain the explicit
   estimate of the eventual lower bound of the HIV-1 virus load; (ii) the
   threshold value R{*} < 1 implies the extinction of the virus.
   Furthermore, we obtain that the threshold dynamics is in agreement with
   that of the corresponding autonomous system, which extends the classic
   results for the system with constant coefficients. Numerical simulations
   are also given to illustrate our main results, and in particular, some
   sensitivity test of R-{*} is established.}},
DOI = {{10.1142/S1793524513500307}},
Article-Number = {{1350030}},
ISSN = {{1793-5245}},
EISSN = {{1793-7159}},
Unique-ID = {{ISI:000325992300004}},
}

@article{ ISI:000324725700008,
Author = {Johnson, M. R. and Devillers, R. W. and Thomson, K. A.},
Title = {{A Generalized Sky-LOSA Method to Quantify Soot/Black Carbon Emission
   Rates in Atmospheric Plumes of Gas Flares}},
Journal = {{AEROSOL SCIENCE AND TECHNOLOGY}},
Year = {{2013}},
Volume = {{47}},
Number = {{9}},
Pages = {{1017-1029}},
Month = {{SEP 1}},
Abstract = {{A new generalized theory governing sky-LOSA measurements (line-of-sight
   attenuation measurements of sky-light) of soot mass flux in atmospheric
   plumes has been developed which enables accurate measurements in the
   presence of in-scattered light from the sky and sun. The new approach is
   quantitatively tested using field measurement data collected for a gas
   flare at a turbocompressor station in Mexico. Although the soot plume of
   the tested flare was on the threshold of visible to the naked eye, the
   sensitivity of the current hardware was more than sufficient to resolve
   the soot mass emission rate of 0.067g/s, with a quantified 95\%
   confidence interval of 0.050 to 0.090g/s. Results of a Monte Carlo
   simulation showed that soot optical property uncertainty was the major
   contributor to the overall measurement uncertainty. By contrast,
   correction of in-scattering via the generalized theory was a
   comparatively minor contributor, and was specifically insensitive to
   assumptions about the sky polarization state and intensity distribution.
   Given the prevalence of flaring and its implication as a potentially
   critical source of black carbon emissions, sky-LOSA is an essential new
   technology to directly quantify the impact of these globally distributed
   sources, for which comparable technologies do not exist. Copyright 2013
   American Association for Aerosol Research}},
DOI = {{10.1080/02786826.2013.809401}},
ISSN = {{0278-6826}},
ResearcherID-Numbers = {{Johnson, Matthew/F-2672-2010
   }},
ORCID-Numbers = {{Johnson, Matthew/0000-0002-3637-9919
   Cui, Xiangyu/0000-0001-7678-3249}},
Unique-ID = {{ISI:000324725700008}},
}

@article{ ISI:000323822000007,
Author = {Rao, Xiayu and Lai, Dejian and Huang, Xuelin},
Title = {{A New Method for Quantitative Real-Time Polymerase Chain Reaction Data
   Analysis}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2013}},
Volume = {{20}},
Number = {{9}},
Pages = {{703-711}},
Month = {{SEP 1}},
Abstract = {{Quantitative real-time polymerase chain reaction (qPCR) is a sensitive
   gene quantification method that has been extensively used in biological
   and biomedical fields. The currently used methods for PCR data analysis,
   including the threshold cycle method and linear and nonlinear
   model-fitting methods, all require subtracting background fluorescence.
   However, the removal of background fluorescence can hardly be accurate
   and therefore can distort results. We propose a new method, the
   taking-difference linear regression method, to overcome this limitation.
   Briefly, for each two consecutive PCR cycles, we subtract the
   fluorescence in the former cycle from that in the latter cycle,
   transforming the n cycle raw data into n-1 cycle data. Then, linear
   regression is applied to the natural logarithm of the transformed data.
   Finally, PCR amplification efficiencies and the initial DNA molecular
   numbers are calculated for each reaction. This taking-difference method
   avoids the error in subtracting an unknown background, and thus it is
   more accurate and reliable. This method is easy to perform, and this
   strategy can be extended to all current methods for PCR data analysis.}},
DOI = {{10.1089/cmb.2012.0279}},
ISSN = {{1066-5277}},
EISSN = {{1557-8666}},
Unique-ID = {{ISI:000323822000007}},
}

@article{ ISI:000323297500006,
Author = {Beaulieu, Jeremy M. and O'Meara, Brian C. and Donoghue, Michael J.},
Title = {{Identifying Hidden Rate Changes in the Evolution of a Binary
   Morphological Character: The Evolution of Plant Habit in Campanulid
   Angiosperms}},
Journal = {{SYSTEMATIC BIOLOGY}},
Year = {{2013}},
Volume = {{62}},
Number = {{5}},
Pages = {{725-737}},
Month = {{SEP}},
Abstract = {{The growth of phylogenetic trees in scope and in size is promising from
   the standpoint of understanding a wide variety of evolutionary patterns
   and processes. With trees comprised of larger, older, and globally
   distributed clades, it is likely that the lability of a binary character
   will differ significantly among lineages, which could lead to errors in
   estimating transition rates and the associated inference of ancestral
   states. Here we develop and implement a new method for identifying
   different rates of evolution in a binary character along different
   branches of a phylogeny. We illustrate this approach by exploring the
   evolution of growth habit in Campanulidae, a flowering plant clade
   containing some 35,000 species. The distribution of woody versus
   herbaceous species calls into question the use of traditional models of
   binary character evolution. The recognition and accommodation of changes
   in the rate of growth form evolution in different lineages demonstrates,
   for the first time, a robust picture of growth form evolution across a
   very large, very old, and very widespread flowering plant clade.}},
DOI = {{10.1093/sysbio/syt034}},
ISSN = {{1063-5157}},
ORCID-Numbers = {{O'Meara, Brian/0000-0002-0337-5997}},
Unique-ID = {{ISI:000323297500006}},
}

@article{ ISI:000322962300009,
Author = {Ren, Li and Xiang, Xin-Yi and Ni, Jian-Jun},
Title = {{Forecast Modeling of Monthly Runoff with Adaptive Neural Fuzzy Inference
   System and Wavelet Analysis}},
Journal = {{JOURNAL OF HYDROLOGIC ENGINEERING}},
Year = {{2013}},
Volume = {{18}},
Number = {{9}},
Pages = {{1133-1139}},
Month = {{SEP 1}},
Abstract = {{There is no real periodicity in the changes of hydrological systems.
   Changes in a hydrological system take place with different periodic
   variations from time to time. In this paper, a new method was utilized
   to predict monthly runoff with a wavelet analysis technique. Taking
   advantage of localized characteristics of wavelet transform and the
   approximation function of an adaptive neural fuzzy inference system
   (ANFIS), the combined approach of wavelet transform and ANFIS was used
   to predict monthly runoff. The ANFIS forecast model for monthly runoff
   was established based on wavelet analysis. To solve the problems related
   to the large amplitudes of intra- and interannual variation of monthly
   runoff, a resolving and reconstructing technique of wavelets was
   utilized to decompose runoff series into different information
   subspaces, by which decomposition signals with different frequencies
   were obtained. Based on the evaluation of simulated and measured values
   in Yichang Station, it was found that the percent of the pass of
   relative error was 100\% and the effect of prediction was acceptable.
   The certainty factor, , was 0.91 and the prediction level was A. By
   comparing the measured and predicted values, it was found that with this
   model, the trend of originals could be forecasted, but improvements are
   still needed. Because the new model is sensitive to sudden changes in
   rainfall, combined with the irregular monthly runoff variation, it is
   difficult to forecast runoff with this model, which should be improved
   in the future. (C) 2013 American Society of Civil Engineers.}},
DOI = {{10.1061/(ASCE)HE.1943-5584.0000514}},
ISSN = {{1084-0699}},
Unique-ID = {{ISI:000322962300009}},
}

@article{ ISI:000324248300008,
Author = {Chowdhury, Sharif and Kandhavelu, Meenakshisundaram and Yli-Harja, Olli
   and Ribeiro, Andre S.},
Title = {{Cell segmentation by multi-resolution analysis and maximum likelihood
   estimation (MAMLE)}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2013}},
Volume = {{14}},
Number = {{10}},
Month = {{AUG 12}},
Note = {{10th International Workshop on Computational Systems Biology, Tampere,
   FINLAND, JUN 10-12, 2013}},
Abstract = {{Background: Cell imaging is becoming an indispensable tool for cell and
   molecular biology research. However, most processes studied are
   stochastic in nature, and require the observation of many cells and
   events. Ideally, extraction of information from these images ought to
   rely on automatic methods. Here, we propose a novel segmentation method,
   MAMLE, for detecting cells within dense clusters.
   Methods: MAMLE executes cell segmentation in two stages. The first
   relies on state of the art filtering technique, edge detection in
   multi-resolution with morphological operator and threshold decomposition
   for adaptive thresholding. From this result, a correction procedure is
   applied that exploits maximum likelihood estimate as an objective
   function. Also, it acquires morphological features from the initial
   segmentation for constructing the likelihood parameter, after which the
   final segmentation is obtained.
   Conclusions: We performed an empirical evaluation that includes sample
   images from different imaging modalities and diverse cell types. The new
   method attained very high (above 90\%) cell segmentation accuracy in all
   cases. Finally, its accuracy was compared to several existing methods,
   and in all tests, MAMLE outperformed them in segmentation accuracy.}},
DOI = {{10.1186/1471-2105-14-S10-S8}},
Article-Number = {{S8}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Kandhavelu, Meenakshisundaram/L-1859-2016
   Ribeiro, Andre/G-4294-2014
   }},
ORCID-Numbers = {{Kandhavelu, Meenakshisundaram/0000-0002-4986-055X
   Ribeiro, Andre/0000-0002-7255-5211
   Yli-Harja, Olli/0000-0001-8581-4414}},
Unique-ID = {{ISI:000324248300008}},
}

@article{ ISI:000324491000011,
Author = {Odem-Davis, Katherine and Fleming, Thomas R.},
Title = {{Adjusting for Unknown Bias in Noninferiority Clinical Trials}},
Journal = {{STATISTICS IN BIOPHARMACEUTICAL RESEARCH}},
Year = {{2013}},
Volume = {{5}},
Number = {{3}},
Pages = {{248-258}},
Month = {{AUG}},
Abstract = {{Evaluation of noninferiority is based on ruling out a threshold for what
   would constitute unacceptable loss of efficacy of an experimental
   treatment relative to an active comparator ``Standard.{''} This
   threshold, the ``noninferiority margin,{''} is often based on
   preservation of a percentage of Standard's effect. To obtain an estimate
   of this effect to be used in the development of the ``noninferiority
   margin,{''} data are needed from earlier trials comparing Standard to
   Placebo if the noninferiority trial does not have a Placebo arm. This
   approach often provides a biased overestimate of Standard's true effect
   in the setting of the current noninferiority study. We describe two
   commonly used noninferiority margin methods that adjust for this bias,
   the two-confidence interval (95-95), and the Synthesis margins. However,
   the added `` variance inflation{''} adjustment made by 95-95 margin
   diminishes with increasing information from historical trial(s), and the
   Synthesis margin is based on a strong assumption that the relative bias
   is known. We introduce an alternative ``Bias-adjusted{''} margin
   addressing vulnerabilities of each by attenuating the estimate and by
   accounting for uncertainty in the true level of bias. Examples and
   asymptotic estimates of noninferiority hypothesis rejection rates in the
   proportional hazards setting are used to compare methods.}},
DOI = {{10.1080/19466315.2013.795910}},
ISSN = {{1946-6315}},
Unique-ID = {{ISI:000324491000011}},
}

@article{ ISI:000323161600022,
Author = {Hapca, Simona M. and Houston, Alasdair N. and Otten, Wilfred and Baveye,
   Philippe C.},
Title = {{New Local Thresholding Method for Soil Images by Minimizing Grayscale
   Intra-Class Variance}},
Journal = {{VADOSE ZONE JOURNAL}},
Year = {{2013}},
Volume = {{12}},
Number = {{3}},
Month = {{AUG}},
Abstract = {{Recent advances in imaging techniques offer the possibility of
   visualizing the three-dimensional structure of soils at very fine
   scales. To make use of such information, a thresholding process is
   commonly implemented to separate the image into solid particles and
   pores. Despite the multitude of thresholding algorithms available, their
   performance is being challenged by the complexity of the soil structure.
   Experience shows that, to improve thresholding performance, existing
   methods require significant input from a skilled operator, making the
   thresholding subjective. In this context, this article proposes a new,
   operator-independent thresholding technique based on the analysis of the
   intraclass grayscale variance. The method extends the well-established
   Otsu technique, by applying first a preclassification of the voxels
   corresponding to the solid phase. Then, a threshold value is determined
   through minimization of the intraclass variance of the unclassified
   voxels. The method was implemented globally, then locally for a range of
   window sizes, with the optimal window size selected as that for which
   the standardized grayscale variances of the two voxel populations are
   equal. Results on the three-dimensional soil images investigated suggest
   that the proposed method performs noticeably better than Otsu's method,
   and in particular is robust enough to variations in image contrast and
   soil structure. Tested on a synthetic image, the new method produces a
   misclassification of only 2\% of voxels, compared to 4.9\% with Otsu's
   method. These results suggest that the proposed method can be very
   useful in the analysis of images of a variety of heterogeneous media,
   including soils.}},
DOI = {{10.2136/vzj2012.0172}},
ISSN = {{1539-1663}},
ORCID-Numbers = {{otten, wilfred/0000-0002-3847-9825
   Baveye, Philippe/0000-0002-8432-6141}},
Unique-ID = {{ISI:000323161600022}},
}

@article{ ISI:000321088500019,
Author = {Zhang, Xuesong and Beeson, Peter and Link, Robert and Manowitz, David
   and Izaurralde, Roberto C. and Sadeghi, Ali and Thomson, Allison M. and
   Sahajpal, Ritvik and Srinivasan, Raghavan and Arnold, Jeffrey G.},
Title = {{Efficient multi-objective calibration of a computationally intensive
   hydrologic model with parallel computing software in Python}},
Journal = {{ENVIRONMENTAL MODELLING \& SOFTWARE}},
Year = {{2013}},
Volume = {{46}},
Pages = {{208-218}},
Month = {{AUG}},
Abstract = {{With enhanced data availability, distributed watershed models for large
   areas with high spatial and temporal resolution are increasingly used to
   understand water budgets and examine effects of human activities and
   climate change/variability on water resources. Developing parallel
   computing software to improve calibration efficiency has received
   growing attention of the watershed modeling community as it is very time
   demanding to run iteratively complex models for calibration. In this
   research, we introduce a Python-based parallel computing package,
   PP-SWAT, for efficient calibration of the Soil and Water Assessment Tool
   (SWAT) model. This software employs Python, MPI for Python (mpi4py) and
   OpenMPI to parallelize A Multi-method Genetically Adaptive
   Multi-objective Optimization Algorithm (AMALGAM), allowing for
   simultaneously addressing multiple objectives in calibrating SWAT. Test
   results on a Linux computer cluster showed that PP-SWAT can achieve a
   speedup of 45-109 depending on model complexity. Increasing the
   processor count beyond a certain threshold does not necessarily improve
   efficiency, because intensified resource competition may result in an
   I/O bottleneck. The efficiency achieved by PP-SWAT also makes it
   practical to implement multiple parameter adjustment schemes operating
   at different scales in affordable time, which helps provide SWAT users
   with a wider range of options of parameter sets to choose from for
   model(s) selection. PP-SWAT was not designed to address errors
   associated with other sources (e.g. model structure) and cautious
   supervision of its power should be exercised in order to attain
   physically meaningful calibration results. (C) 2013 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.envsoft.2013.03.013}},
ISSN = {{1364-8152}},
ResearcherID-Numbers = {{Srinivasan, R/D-3937-2009
   Thomson, Allison/B-1254-2010
   sahajpal, ritvik/N-4565-2013
   zhang, xuesong/B-7907-2009}},
ORCID-Numbers = {{Srinivasan, R/0000-0001-8375-6038
   sahajpal, ritvik/0000-0002-6418-289X
   }},
Unique-ID = {{ISI:000321088500019}},
}

@article{ ISI:000323613900001,
Author = {Meyer, Fernando and Kurtz, Stefan and Beckstette, Michael},
Title = {{Fast online and index-based algorithms for approximate search of RNA
   sequence-structure patterns}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2013}},
Volume = {{14}},
Month = {{JUL 17}},
Abstract = {{Background: It is well known that the search for homologous RNAs is more
   effective if both sequence and structure information is incorporated
   into the search. However, current tools for searching with RNA
   sequence-structure patterns cannot fully handle mutations occurring on
   both these levels or are simply not fast enough for searching large
   sequence databases because of the high computational costs of the
   underlying sequence-structure alignment problem.
   Results: We present new fast index-based and online algorithms for
   approximate matching of RNA sequence-structure patterns supporting a
   full set of edit operations on single bases and base pairs. Our methods
   efficiently compute semi-global alignments of structural RNA patterns
   and substrings of the target sequence whose costs satisfy a user-defined
   sequence-structure edit distance threshold. For this purpose, we
   introduce a new computing scheme to optimally reuse the entries of the
   required dynamic programming matrices for all substrings and combine it
   with a technique for avoiding the alignment computation of non-matching
   substrings. Our new index-based methods exploit suffix arrays
   preprocessed from the target database and achieve running times that are
   sublinear in the size of the searched sequences. To support the
   description of RNA molecules that fold into complex secondary structures
   with multiple ordered sequence-structure patterns, we use fast
   algorithms for the local or global chaining of approximate
   sequence-structure pattern matches. The chaining step removes spurious
   matches from the set of intermediate results, in particular of patterns
   with little specificity. In benchmark experiments on the Rfam database,
   our improved online algorithm is faster than the best previous method by
   up to factor 45. Our best new index-based algorithm achieves a speedup
   of factor 560.
   Conclusions: The presented methods achieve considerable speedups
   compared to the best previous method. This, together with the expected
   sublinear running time of the presented index-based algorithms, allows
   for the first time approximate matching of RNA sequence-structure
   patterns in large sequence databases. Beyond the algorithmic
   contributions, we provide with RaligNAtor a robust and well documented
   open-source software package implementing the algorithms presented in
   this manuscript. The RaligNAtor software is available at
   http://www.zbh.uni-hamburg.de/ralignator.}},
DOI = {{10.1186/1471-2105-14-226}},
Article-Number = {{226}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000323613900001}},
}

@article{ ISI:000318592800021,
Author = {Renoux, Agnes Y. and Zajdlik, Barry and Stephenson, Gladys L. and
   Moulins, L. Jacques},
Title = {{Risk-Based Management of Site Soils Contaminated with a Mixture of
   Hazardous Substances: Methodological Approach and Case Study}},
Journal = {{HUMAN AND ECOLOGICAL RISK ASSESSMENT}},
Year = {{2013}},
Volume = {{19}},
Number = {{4}},
Pages = {{1127-1146}},
Month = {{JUL 4}},
Abstract = {{A novel approach to improve the accuracy and to reduce the uncertainty
   associated with the assessment of ecotoxicological risks and the
   determination of remedial objectives was developed herein for a scenario
   involving multiple contaminants in soil. This approach used
   laboratory-derived site-specific toxicological data (i.e., obtained from
   toxicity testing using species in direct contact with soil such as
   plants and invertebrates) instead of the more traditional approach using
   generic toxicological benchmarks for corresponding groups of organisms.
   Inherent to this approach were the data exploration and reduction; the
   use of generalized linear models to integrate the data for stressors
   (site-specific chemical and edaphic characteristics with the potential
   for influencing toxicity) and effects (biological responses for multiple
   species and multiple endpoints); and the derivation of tools that could
   predict the level of impairment associated with any combination of
   metals' concentrations measured on site and compare it to a
   pre-specified acceptable threshold. A case study is presented whereby
   this method was applied to a large site contaminated with a mixture of
   metals. Ultimately, the distributions of predicted levels of risk for
   both soil invertebrates and plants were determined for the entire site
   and compared to those obtained using the traditional approach using
   benchmarks.}},
DOI = {{10.1080/10807039.2012.691825}},
ISSN = {{1080-7039}},
Unique-ID = {{ISI:000318592800021}},
}

@article{ ISI:000320468900005,
Author = {Li, Wenkai and Guo, Qinghua},
Title = {{How to assess the prediction accuracy of species presence-absence models
   without absence data?}},
Journal = {{ECOGRAPHY}},
Year = {{2013}},
Volume = {{36}},
Number = {{7}},
Pages = {{788-799}},
Month = {{JUL}},
Abstract = {{It is very common that only presence data are available in ecological
   niche modeling. However, most existing methods for evaluating the
   accuracy of presence-absence (binary) predictions of species require
   presence-absence data. The aim of this study is to present a new method
   for accuracy assessment that does not rely on absence data. Two new
   statistics Fpb and Fcpb were derived based on presence-background data.
   With generated six virtual species, we used DOMAIN, generalized linear
   modeling (GLM), and maximum entropy (MAXENT) to produce different
   species presence-absence predictions. To investigate the effectiveness
   of the new statistics in accuracy assessment, we used Fpb, Fcpb, the
   traditional F-measure (F), kappa coefficient, true skill statistic
   (TSS), area under the receiver operating characteristic curve (AUC), and
   the contrast validation index (CVI) to evaluate the accuracy of
   predictions, and the behaviors of these accuracy measures were compared.
   The effectiveness of Fpb for threshold selection and estimation of
   species prevalence was also investigated. Experimental results show that
   Fcpb is an estimate of F. The Pearson's correlation coefficient (COR)
   between Fcpb and F is 0.9882, with a root-mean-square error (RMSE) of
   0.0171. In general, Fpb, Fcpb, F, kappa coefficient, TSS, and CVI can
   sort models by the accuracy of binary prediction, but AUC is not
   appropriate to evaluate the accuracy of binary prediction. For DOMAIN,
   GLM, and MAXENT, finding the threshold by maximizing Fpb and by
   maximizing F result in similar accuracies. In addition, the estimation
   of species prevalence based on binary output with maximizing Fpb as the
   thresholding method is significantly more accurate than simply averaging
   the original continuous output. The best estimate of prevalence is
   provided by the binary output of MAXENT, with an RMSE of 0.0116.
   Finally, we conclude that the new method is promising in accuracy
   assessment, threshold selection, and estimation of species prevalence,
   all of which are important but challenging problems with presence-only
   data. Because it does not require absence data, the new method will have
   important applications in ecological niche modeling.}},
DOI = {{10.1111/j.1600-0587.2013.07585.x}},
ISSN = {{0906-7590}},
EISSN = {{1600-0587}},
Unique-ID = {{ISI:000320468900005}},
}

@article{ ISI:000320904500004,
Author = {Ide, Reiko and Oguma, Hiroyuki},
Title = {{A cost-effective monitoring method using digital time-lapse cameras for
   detecting temporal and spatial variations of snowmelt and vegetation
   phenology in alpine ecosystems}},
Journal = {{ECOLOGICAL INFORMATICS}},
Year = {{2013}},
Volume = {{16}},
Pages = {{25-34}},
Month = {{JUL}},
Abstract = {{Alpine ecosystems are particularly vulnerable to the effects of climate
   change. Although long-term and detailed monitoring is required to
   conserve alpine ecosystems, field surveillance and satellite remote
   sensing have difficulties in providing wide coverage or frequent
   observation in mountain areas. In this study, a new method for
   monitoring alpine ecosystems by digital cameras was developed in order
   to detect both snow-cover areas and vegetation phenology at the plant
   community or species level. We used images from cameras that have been
   installed at mountain lodges in the northern Japanese Alps (at
   elevations around 2350-3100 m). Red, green, and blue (RGB) digital
   numbers were derived from each pixel within the images. The snow-cover
   and snow-free pixels were statistically classified by analysis of
   variance of gray-level histograms. A flexible threshold was determined
   for each image to maximize the between-class variance. The temporal
   variations of the snowmelt rate showed site-specific characteristics and
   yearly variations. The snowmelt times reflected the local
   microtopography and differed among the habitats of various functional
   types of vegetation (i.e., evergreen dwarf pine, deciduous shrubs,
   evergreen Sasa, tall forbs, and snowbed plants). In addition, the
   vegetation phenology was quantified by using a vegetation index (green
   ratio) calculated from RGB digital numbers. An increase in the green
   ratio indicated the start of the growing period following snowmelt and a
   decrease indicated leaf senescence. By using pixel-based analysis of the
   temporal variations of the green ratio, local distributions of the start
   and end dates and length of the growing period were illustrated at the
   plant species level for the first time. The distribution of the start of
   the growing period strongly corresponded to the snowmelt gradient,
   whereas the end of the growing period was related to the vegetation
   type. Our results suggest that the length of the growing period mainly
   corresponded to the snowmelt gradient in relation to the local
   microtopography. Thus, commercially available digital time-lapse cameras
   enabled us to clarify the snow vegetation relationships and the growing
   period at high temporal and spatial resolutions. This monitoring method
   should greatly improve our understanding of alpine ecosystems and help
   to assess the influence of future climate change. (C) 2013 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.ecoinf.2013.04.003}},
ISSN = {{1574-9541}},
Unique-ID = {{ISI:000320904500004}},
}

@article{ ISI:000321337700001,
Author = {Stobbe, Miranda D. and Swertz, Morris A. and Thiele, Ines and Rengaw,
   Trebor and van Kampen, Antoine H. C. and Moerland, Perry D.},
Title = {{Consensus and conflict cards for metabolic pathway databases}},
Journal = {{BMC SYSTEMS BIOLOGY}},
Year = {{2013}},
Volume = {{7}},
Month = {{JUN 26}},
Abstract = {{Background: The metabolic network of H. sapiens and many other organisms
   is described in multiple pathway databases. The level of agreement
   between these descriptions, however, has proven to be low. We can use
   these different descriptions to our advantage by identifying conflicting
   information and combining their knowledge into a single, more accurate,
   and more complete description. This task is, however, far from trivial.
   Results: We introduce the concept of Consensus and Conflict Cards
   (C(2)Cards) to provide concise overviews of what the databases do or do
   not agree on. Each card is centered at a single gene, EC number or
   reaction. These three complementary perspectives make it possible to
   distinguish disagreements on the underlying biology of a metabolic
   process from differences that can be explained by different decisions on
   how and in what detail to represent knowledge. As a proof-of-concept, we
   implemented C(2)Cards(Human), as a web application
   http://www.molgenis.org/c2cards, covering five human pathway databases.
   Conclusions: C(2)Cards can contribute to ongoing reconciliation efforts
   by simplifying the identification of consensus and conflicts between
   pathway databases and lowering the threshold for experts to contribute.
   Several case studies illustrate the potential of the C(2)Cards in
   identifying disagreements on the underlying biology of a metabolic
   process. The overviews may also point out controversial biological
   knowledge that should be subject of further research. Finally, the
   examples provided emphasize the importance of manual curation and the
   need for a broad community involvement.}},
DOI = {{10.1186/1752-0509-7-50}},
Article-Number = {{50}},
ISSN = {{1752-0509}},
ORCID-Numbers = {{Moerland, Perry/0000-0002-2357-3659
   Thiele, Ines/0000-0002-8071-7110}},
Unique-ID = {{ISI:000321337700001}},
}

@article{ ISI:000318320700016,
Author = {Nayebifar, B. and Moghaddam, H. Abrishami},
Title = {{A novel method for retinal vessel tracking using particle filters}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2013}},
Volume = {{43}},
Number = {{5}},
Pages = {{541-548}},
Month = {{JUN 1}},
Abstract = {{Extraction of a proper map from the vessel paths in the retinal images
   is a prerequisite for many applications such as identification. In this
   paper, we present a new approach based on particle filtering to
   determine and locally track the vessel paths in retina. Particle filter
   needs to use an acceptable probability density function (PDF) describing
   the blood vessels which must be provided by the retinal image. For this
   purpose, the product of the green and blue channels of the RGB retinal
   images is considered and after a median filtering stage, it is used as a
   PDF for tracking procedure. Then a stage of optic disc localization is
   performed to localize the starting points around the optic disc. With a
   proper set of starting points, the iterative tracking procedure
   initiates. First, a uniform propagation of the particles on an annular
   ring around each point (including starting points or ones determined as
   central points in the previous iteration) is performed. The particle
   weights are evaluated and accordingly, each particle is decided to be
   inside or outside the vessel. The subsequent stage is to analyze the
   hypothetical vectors between a central point and each of the inside
   vessel particles to find ones located inside vessel. Afterwards, the
   particles are clustered using quality threshold clustering method.
   Finally, each cluster introduces a central point for pursuing the
   tracking procedure in the next iteration. The tracking proceeds towards
   a bifurcation or the end of the vessels. We introduced two criteria:
   automatic/manually tracked ratio (AMTR) and false/manually tracked ratio
   (FMTR) for evaluating the tracking results. Apart from the labeling
   accuracy, the average values of AMTR and FMTR were 0.7746 and 0.2091,
   respectively. The proposed method successfully deals with the
   bifurcations with robustness against noise and tracks the thin vessels.
   (C) 2013 Published by Elsevier Ltd.}},
DOI = {{10.1016/j.compbiomed.2013.01.016}},
ISSN = {{0010-4825}},
Unique-ID = {{ISI:000318320700016}},
}

@article{ ISI:000317943400002,
Author = {Gaiser, Thomas and Perkons, Ute and Kuepper, Paul Martin and Kautz, Timo
   and Uteau-Puschmann, Daniel and Ewert, Frank and Enders, Andreas and
   Krauss, Gunther},
Title = {{Modeling biopore effects on root growth and biomass production on soils
   with pronounced sub-soil clay accumulation}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2013}},
Volume = {{256}},
Pages = {{6-15}},
Month = {{MAY 10}},
Abstract = {{Soils with subsoil clay accumulation account for more than 20\% of the
   global land surface. These soils are characterized by vertical
   differences with respect to soil texture and increasing bulk density
   below the topsoil, which in turn affects root penetration into the
   subsoil. Biopores are preferential pathways for roots and assist in
   overcoming physical barriers like high density soil layers. An
   integration of these relationships into cropping systems models at the
   field scale is on-going. This paper presents a new approach to model the
   effect of biopores on root development in soils with clay accumulation
   at the plot scale. In this approach, the effect of biopores on root
   elongation rate depends on bulk density and on a biopore-root growth
   threshold (MPRT), which is the biopore volume at which the resistance of
   soil strength to root penetration is completely offset by the density of
   the biopores. The approach was integrated into a model solution of the
   model framework SIMPLACE (Scientific Impact assessment and Modeling
   PLatform for Advanced Crop and Ecosystem management). MPRT was
   parameterized for spring wheat using the inverse modeling approach based
   on root observations from a multi-factorial field experiment on a Haplic
   Luvisol. The observed biopore densities (>2 mm diameter) were between
   300 and 660 pores m(-2) (equivalent to a volumetric proportion of
   0.38-0.83\%) depending on the preceding crop. Observed soil bulk
   densities ranged between 1.31 and 1.62 g cm(-3). For spring wheat, the
   best fit between simulated and observed root densities in different
   layers was obtained with a MPRT of 0.023 m(3) m(-3) (equivalent to 2.3\%
   of soil volume). The mean simulated total above ground biomass was
   sensitive to MPRT and had the best agreement with observed values when a
   MPRT between 0.023 and 0.032 m(3) m(-3) was used in the simulations.
   Scenario simulations with the parameterized model at the same site
   demonstrate the importance of biopores for biomass production of
   short-cycle spring wheat when prolonged dry spells occur. The
   simulations allow a rough quantification of the biopore effects with
   respect to root elongation rate and biomass production at the plot scale
   with the potential to be extended to the field scale. (c) 2013 Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2013.02.016}},
ISSN = {{0304-3800}},
ResearcherID-Numbers = {{Kautz, Timo/N-7660-2013
   }},
ORCID-Numbers = {{Kautz, Timo/0000-0002-7906-8512
   Uteau, Daniel/0000-0003-1499-4344}},
Unique-ID = {{ISI:000317943400002}},
}

@article{ ISI:000318237100009,
Author = {Cohen, Arthur and Ma, Yingqiu and Sackrowitz, Harold B.},
Title = {{Individualized two-stage multiple testing procedures with corresponding
   interval estimates}},
Journal = {{BIOMETRICAL JOURNAL}},
Year = {{2013}},
Volume = {{55}},
Number = {{3}},
Pages = {{386-401}},
Month = {{MAY}},
Abstract = {{Multiple testing models have become an important part of statistical
   applications. Typically they can be presented as having M hypotheses
   each of which concerns an individual parameter. In addition to testing
   each of these hypotheses, there is often a desire to obtain interval
   estimates for the parameters. The use of stepwise procedures arises
   because single-step procedures are extremely conservative. Unfortunately
   research into the construction of useful, computationally feasible
   interval estimates corresponding to stepwise procedures has been slow.
   We present an alternative method of constructing multiple testing
   procedures (MTPs) that easily admits corresponding interval estimates.
   The new method places greater focus on each hypothesis separately while
   still using all the data. This method is particularly effective in the
   dependent case. Not only do these new MTPs perform as well as commonly
   used stepwise procedures but they also have a practical interval
   property not usually shared by stepwise procedures. That is, acceptance
   regions have desirable convexity properties. Furthermore, interval
   estimates associated with these tests are easily obtained. In addition,
   these intervals (i) are typically shorter than those based on the
   Bonferroni, Scheffe', Tukey or Dunnett method when they are applicable,
   (ii) are less likely to contain the null point falsely than other
   methods do, (iii) are informative, i.e. they are all finite in the
   two-sided case, unlike some constructed by other methods which often are
   infinite, (iv) have a form of the interval property.}},
DOI = {{10.1002/bimj.201200019}},
ISSN = {{0323-3847}},
EISSN = {{1521-4036}},
Unique-ID = {{ISI:000318237100009}},
}

@article{ ISI:000317999500020,
Author = {Akbari, Omar S. and Matzen, Kelly D. and Marshall, John M. and Huang,
   Haixia and Ward, Catherine M. and Hay, Bruce A.},
Title = {{A Synthetic Gene Drive System for Local, Reversible Modification and
   Suppression of Insect Populations}},
Journal = {{CURRENT BIOLOGY}},
Year = {{2013}},
Volume = {{23}},
Number = {{8}},
Pages = {{671-677}},
Month = {{APR 22}},
Abstract = {{Replacement of wild insect populations with genetically modified
   individuals unable to transmit disease provides a self-perpetuating
   method of disease prevention but requires a gene drive mechanism to
   spread these traits to high frequency {[}1-3]. Drive mechanisms
   requiring that transgenes exceed a threshold frequency in order to
   spread are attractive because they bring about local but not global
   replacement, and transgenes can be eliminated through dilution of the
   population with wild-type individuals {[}4-6]. These features are likely
   to be important in many social and regulatory contexts {[}7-10]. Here we
   describe the first creation of a synthetic threshold-dependent gene
   drive system, designated maternal-effect lethal underdominance (UDMEL),
   in which two maternally expressed toxins, located on separate
   chromosomes, are each linked with a zygotic antidote able to rescue
   maternal-effect lethality of the other toxin. We demonstrate
   threshold-dependent replacement in single- and two-locus configurations
   in Drosophila. Models suggest that transgene spread can often be limited
   to local environments. They also show that in a population in which
   single-locus UDMEL has been carried out, repeated release of wild-type
   males can result in population suppression, a novel method of genetic
   population manipulation.}},
DOI = {{10.1016/j.cub.2013.02.059}},
ISSN = {{0960-9822}},
ORCID-Numbers = {{Akbari, Omar/0000-0002-6853-9884}},
Unique-ID = {{ISI:000317999500020}},
}

@article{ ISI:000317949200013,
Author = {Kaliszewski, Miron and Trafny, Elzbieta A. and Lewandowski, Rafal and
   Wlodarski, Maksymilian and Bombalska, Aneta and Kopczynski, Krzysztof
   and Antos-Bielska, Malgorzata and Szpakowska, Malgorzata and Mlynczak,
   Jaroslaw and Mularczyk-Oliwa, Monika and Kwasny, Miroslaw},
Title = {{A new approach to UVAPS data analysis towards detection of biological
   aerosol}},
Journal = {{JOURNAL OF AEROSOL SCIENCE}},
Year = {{2013}},
Volume = {{58}},
Pages = {{148-157}},
Month = {{APR}},
Abstract = {{Laboratory and outdoor measurements of various bioaerosols, with UVAPS
   3314 biodetector, were performed. Two methods of analysis of the data
   recorded were compared. Our completely new approach with use of
   Hierarchical Cluster Analysis showed the possibility of increasing
   classification selectivity of the UVAPS. The laboratory experiments
   showed that bacteria, fungi and pollens form distinguishable clusters.
   Outdoor tests showed that discrimination between ambient air and the
   released bioaerosols was possible. All background samples were
   classified properly. Contrary to this, the commonly used method, based
   on percentage of fluorescent particles, did not produce distinct
   threshold between the samples examined: thus, many background
   measurements were identical to the dispersed aerosols. However,
   fluorescent fraction for ambient air was lower than for endospores or
   Luria-Bertani broth.
   We expect that our method of analysis will extend the UVAPS selectivity
   in differentiation of potentially hazardous bioaerosols. (C) 2013
   Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jaerosci.2013.01.007}},
ISSN = {{0021-8502}},
EISSN = {{1879-1964}},
ResearcherID-Numbers = {{Mlynczak, Jaroslaw/E-1725-2013
   Wlodarski, Maksymilian/X-3492-2018
   Antos-Bielska, Malgorzata/W-1736-2018
   Bombalska, Aneta/Q-2165-2018
   }},
ORCID-Numbers = {{Antos-Bielska, Malgorzata/0000-0002-5382-6995
   Bombalska, Aneta/0000-0003-0417-7340
   Mlynczak, Jaroslaw/0000-0002-0823-9302
   Kaliszewski, Miron/0000-0003-4062-3408
   Wlodarski, Maksymilian/0000-0001-7494-2859
   Kopczynski, Krzysztof/0000-0002-3319-3940
   Kwasny, Miroslaw/0000-0002-4585-1744
   Lewandowski, Rafal/0000-0002-7134-0809
   Trafny, Elzbieta/0000-0003-0318-1698}},
Unique-ID = {{ISI:000317949200013}},
}

@article{ ISI:000319729700001,
Author = {Bhar, Anirban and Haubrock, Martin and Mukhopadhyay, Anirban and Maulik,
   Ujjwal and Bandyopadhyay, Sanghamitra and Wingender, Edgar},
Title = {{Coexpression and coregulation analysis of time-series gene expression
   data in estrogen-induced breast cancer cell}},
Journal = {{ALGORITHMS FOR MOLECULAR BIOLOGY}},
Year = {{2013}},
Volume = {{8}},
Month = {{MAR 23}},
Abstract = {{Background: Estrogen is a chemical messenger that has an influence on
   many breast cancers as it helps cells to grow and divide. These cancers
   are often known as estrogen responsive cancers in which estrogen
   receptor occupies the surface of the cells. The successful treatment of
   breast cancers requires understanding gene expression, identifying of
   tumor markers, acquiring knowledge of cellular pathways, etc. In this
   paper we introduce our proposed triclustering algorithm delta-TRIMAX
   that aims to find genes that are coexpressed over subset of samples
   across a subset of time points. Here we introduce a novel mean-squared
   residue for such 3D dataset. Our proposed algorithm yields triclusters
   that have a mean-squared residue score below a threshold delta.
   Results: We have applied our algorithm on one simulated dataset and one
   real-life dataset. The real-life dataset is a time-series dataset in
   estrogen induced breast cancer cell line. To establish the biological
   significance of genes belonging to resultant triclusters we have
   performed gene ontology, KEGG pathway and transcription factor binding
   site enrichment analysis. Additionally, we represent each resultant
   tricluster by computing its eigengene and verify whether its eigengene
   is also differentially expressed at early, middle and late estrogen
   responsive stages. We also identified hub-genes for each resultant
   triclusters and verified whether the hub-genes are found to be
   associated with breast cancer. Through our analysis CCL2, CD47, NFIB,
   BRD4, HPGD, CSNK1E, NPC1L1, PTEN, PTPN2 and ADAM9 are identified as
   hub-genes which are already known to be associated with breast cancer.
   The other genes that have also been identified as hub-genes might be
   associated with breast cancer or estrogen responsive elements. The TFBS
   enrichment analysis also reveals that transcription factor POU2F1 binds
   to the promoter region of ESR1 that encodes estrogen receptor alpha.
   Transcription factor E2F1 binds to the promoter regions of coexpressed
   genes MCM7, ANAPC1 and WEE1.
   Conclusions: Thus our integrative approach provides insights into breast
   cancer prognosis.}},
DOI = {{10.1186/1748-7188-8-9}},
Article-Number = {{UNSP 9}},
ISSN = {{1748-7188}},
ORCID-Numbers = {{Bhar, Anirban/0000-0003-0425-561X
   Maulik, Ujjwal/0000-0003-1167-0774
   Wingender, Edgar/0000-0002-7729-8453}},
Unique-ID = {{ISI:000319729700001}},
}

@article{ ISI:000318488500001,
Author = {Marczyk, Michal and Jaksik, Roman and Polanski, Andrzej and Polanska,
   Joanna},
Title = {{Adaptive filtering of microarray gene expression data based on Gaussian
   mixture decomposition}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2013}},
Volume = {{14}},
Month = {{MAR 20}},
Abstract = {{Background: DNA microarrays are used for discovery of genes expressed
   differentially between various biological conditions. In microarray
   experiments the number of analyzed samples is often much lower than the
   number of genes (probe sets) which leads to many false discoveries.
   Multiple testing correction methods control the number of false
   discoveries but decrease the sensitivity of discovering differentially
   expressed genes. Concerning this problem, filtering methods for
   improving the power of detection of differentially expressed genes were
   proposed in earlier papers. These techniques are two-step procedures,
   where in the first step some pool of non-informative genes is removed
   and in the second step only the pool of the retained genes is used for
   searching for differentially expressed genes.
   Results: A very important parameter to choose is the proportion between
   the sizes of the pools of removed and retained genes. A new method,
   which we propose, allow to determine close to optimal threshold values
   for sample means and sample variances for gene filtering. The method is
   adaptive and based on the decomposition of the histogram of gene
   expression means or variances into mixture of Gaussian components.
   Conclusions: By performing analyses of several publicly available
   datasets and simulated datasets we demonstrate that our adaptive method
   increases sensitivity of finding differentially expressed genes compared
   to previous methods of filtering microarray data based on using fixed
   threshold values.}},
DOI = {{10.1186/1471-2105-14-101}},
Article-Number = {{101}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Polanska, Joanna/H-4513-2013
   Polanski, Andrzej/H-5323-2016
   }},
ORCID-Numbers = {{Polanska, Joanna/0000-0001-8004-9864
   Polanski, Andrzej/0000-0002-1793-9546
   Marczyk, Michal/0000-0003-2508-5736}},
Unique-ID = {{ISI:000318488500001}},
}

@article{ ISI:000316594000039,
Author = {Qian, Linbo and Chen, Baoliang and Hu, Dingfei},
Title = {{Effective Alleviation of Aluminum Phytotoxicity by Manure-Derived
   Biochar}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2013}},
Volume = {{47}},
Number = {{6}},
Pages = {{2737-2745}},
Month = {{MAR 19}},
Abstract = {{The alleviation of aluminum phytotoxicity to wheat plants in a
   hydroponic system through the amendment of biochar is investigated to
   explore the possibility of applying biochar in acidic soil amelioration.
   Biochar derived from cattle manure pyrolyzed at 400 degrees C (CM400)
   and the CM400 biochar washed with distilled-deionized water to remove
   alkalinity (WCM400) were prepared to determine the roles of the liming
   effect and adsorption during the alleviation of Al toxicity. Upon
   addition of 0.02\% (W/V) CM400 to the exposure solution, the inhibition
   of plant growth by Al was significantly reduced while the toxic
   threshold was extended from 3 to 95 mu mol/L Al3+. Due to the biochar
   liming effect, the aluminum species were converted to Al(OH)(2+) and
   Al(OH)(2)(+) monomers, which were strongly adsorbed by biochar;
   furthermore, the highly toxic Al3+ evolved to less toxic Al(OH)(3) and
   Al(OH)(4)(-) species. Adsorption of Al by the biochar is dominated by
   surface complexation of the carboxyl groups with Al(OH)(2+)/Al(OH)(2)(+)
   rather than through electrostatic attraction of Al3+ with negatively
   charged sites. Compared to the liming effect, the adsorption by biochar
   exhibited a sustainable effect on the alleviation of Al toxicity.
   Therefore, the biochar amendment appears to be a novel approach for
   aluminum detoxification in acidic soils.}},
DOI = {{10.1021/es3047872}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Chen, Baoliang/A-9275-2015}},
ORCID-Numbers = {{Chen, Baoliang/0000-0001-8196-081X}},
Unique-ID = {{ISI:000316594000039}},
}

@article{ ISI:000316184900001,
Author = {Mosen-Ansorena, David and Maria Aransay, Ana},
Title = {{Bivariate segmentation of SNP-array data for allele-specific copy number
   analysis in tumour samples}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2013}},
Volume = {{14}},
Month = {{MAR 5}},
Abstract = {{Background: SNP arrays output two signals that reflect the total genomic
   copy number (LRR) and the allelic ratio (BAF), which in combination
   allow the characterisation of allele-specific copy numbers (ASCNs).
   While methods based on hidden Markov models (HMMs) have been extended
   from array comparative genomic hybridisation (aCGH) to jointly handle
   the two signals, only one method based on change-point detection, ASCAT,
   performs bivariate segmentation.
   Results: In the present work, we introduce a generic framework for
   bivariate segmentation of SNP array data for ASCN analysis. For the
   matter, we discuss the characteristics of the typically applied BAF
   transformation and how they affect segmentation, introduce concepts of
   multivariate time series analysis that are of concern in this field and
   discuss the appropriate formulation of the problem. The framework is
   implemented in a method named CnaStruct, the bivariate form of the
   structural change model (SCM), which has been successfully applied to
   transcriptome mapping and aCGH.
   Conclusions: On a comprehensive synthetic dataset, we show that
   CnaStruct outperforms the segmentation of existing ASCN analysis
   methods. Furthermore, CnaStruct can be integrated into the workflows of
   several ASCN analysis tools in order to improve their performance,
   specially on tumour samples highly contaminated by normal cells.}},
DOI = {{10.1186/1471-2105-14-84}},
Article-Number = {{84}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Aransay, Ana/F-8086-2011}},
ORCID-Numbers = {{Aransay, Ana/0000-0002-8271-612X}},
Unique-ID = {{ISI:000316184900001}},
}

@article{ ISI:000322288100010,
Author = {Franco, Giuliano},
Title = {{Research Evaluation and Competition for Academic Positions in
   Occupational Medicine}},
Journal = {{ARCHIVES OF ENVIRONMENTAL \& OCCUPATIONAL HEALTH}},
Year = {{2013}},
Volume = {{68}},
Number = {{2}},
Pages = {{123-127}},
Month = {{MAR 1}},
Abstract = {{Citation analysis is widely used to evaluate the performance of
   individual researchers, journals, and universities. Its outcome plays a
   crucial role in the decision-making process of ranking applicants for an
   academic position. A number of indicators, including the h-index
   reflecting both scientific productivity and its relevance in medical
   fields, are available through the Web of Knowledge(SM) and Scopus (R).
   In the field of occupational medicine, the adoption of the h-index in
   assessing the value of core journals shows some advantages compared with
   traditional bibliometrics and may encourage researchers to submit their
   papers. Although evaluation of the overall individual performance for
   academic positions should assess several aspects, scientific performance
   is usually based on citation analysis indicators. Younger researchers
   should be aware of the new approach based on transparent threshold rules
   for career promotion and need to understand the new evaluation systems
   based on metrics.}},
DOI = {{10.1080/19338244.2011.639819}},
ISSN = {{1933-8244}},
Unique-ID = {{ISI:000322288100010}},
}

@article{ ISI:000321144400018,
Author = {Liu, Hui and Zhou, Shuigene and Guan, Jihong},
Title = {{Identifying Mammalian MicroRNA Targets Based on Supervised Distance
   Metric Learning}},
Journal = {{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}},
Year = {{2013}},
Volume = {{17}},
Number = {{2}},
Pages = {{427-435}},
Month = {{MAR}},
Abstract = {{MicroRNAs (miRNAs) have been emerged as a novel class of endogenous
   posttranscriptional regulators in a variety of animal and plant species.
   One challenge facing miRNA research is to accurately identify the target
   mRNAs, because of the very limited sequence complementarity between
   miRNAs and their target sites, and the scarcity of experimentally
   validated targets to guide accurate prediction. In this paper, we
   propose a new method called SuperMirTar that exploits supervised
   distance learning to predict miRNA targets. Specifically, we use the
   experimentally supported miRNA-mRNA pairs as a training set to learn a
   distance metric function that minimizes the distances between miRNAs and
   mRNAs with validated interactions, then use the learned function to
   calculate the distances of test miRNA-mRNA interactions, and those with
   smaller distances than a predefined threshold are regarded as true
   interactions. We carry out performance comparison between the proposed
   approach and seven existing methods on independent datasets; the results
   show that our method achieves superior performance and can effectively
   narrow the gap between the number of predicted miRNA targets and the
   number of experimentally validated ones.}},
DOI = {{10.1109/TITB.2012.2229286}},
ISSN = {{2168-2194}},
Unique-ID = {{ISI:000321144400018}},
}

@article{ ISI:000321144400026,
Author = {Abushakra, Ahmad and Faezipour, Miad},
Title = {{Acoustic Signal Classification of Breathing Movements to Virtually Aid
   Breath Regulation}},
Journal = {{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}},
Year = {{2013}},
Volume = {{17}},
Number = {{2}},
Pages = {{493-500}},
Month = {{MAR}},
Abstract = {{Monitoring breath and identifying breathing movements have settled
   importance in many biomedical research areas, especially in the
   treatment of those with breathing disorders, e. g., lung cancer
   patients. Moreover, virtual reality (VR) revolution and their
   implementations on ubiquitous hand-held devices have a lot of
   implications, which could be used as a simulation technology for healing
   purposes. In this paper, a novel method is proposed to detect and
   classify breathing movements. The overall VR framework is intended to
   encourage the subjects regulate their breath by classifying the
   breathing movements in real time. This paper focuses on a portion of the
   overall VR framework that deals with classifying the acoustic signal of
   respiration movements. We employ Mel-frequency cepstral coefficients
   (MFCCs) along with speech segmentation techniques using voice activity
   detection and linear thresholding to the acoustic signal of breath
   captured using a microphone to depict the differences between inhale and
   exhale in frequency domain. For every subject, 13 MFCCs of all voiced
   segments are computed and plotted. The inhale and exhale phases are
   differentiated using the sixth MFCC order, which carries important
   classification information. Experimental results on a number of
   individuals verify our proposed classification methodology.}},
DOI = {{10.1109/JBHI.2013.2244901}},
ISSN = {{2168-2194}},
ORCID-Numbers = {{Faezipour, Miad/0000-0003-2684-0887
   Abushakra, Dr. Ahmad/0000-0003-3367-7408}},
Unique-ID = {{ISI:000321144400026}},
}

@article{ ISI:000319397500020,
Author = {Wiedenmann, Joerg and D'Angelo, Cecilia and Smith, Edward G. and Hunt,
   Alan N. and Legiret, Francois-Eric and Postle, Anthony D. and
   Achterberg, Eric P.},
Title = {{Nutrient enrichment can increase the susceptibility of reef corals to
   bleaching}},
Journal = {{NATURE CLIMATE CHANGE}},
Year = {{2013}},
Volume = {{3}},
Number = {{2}},
Pages = {{160-164}},
Month = {{FEB}},
Abstract = {{Mass coral bleaching, resulting from the breakdown of coral-algal
   symbiosis has been identified as the most severe threat to coral reef
   survival on a global scale(1). Regionally, nutrient enrichment of reef
   waters is often associated with a significant loss of coral cover and
   diversity(2). Recently, increased dissolved inorganic nitrogen
   concentrations have been linked to a reduction of the temperature
   threshold of coral bleaching(3), a phenomenon for which no mechanistic
   explanation is available. Here we show that increased levels of
   dissolved inorganic nitrogen in combination with limited phosphate
   concentrations result in an increased susceptibility of corals to
   temperature- and light-induced bleaching. Mass spectrometric analyses of
   the algal lipidome revealed a marked accumulation of sulpholipids under
   these conditions. Together with increased phosphatase activities, this
   change indicates that the imbalanced supply of dissolved inorganic
   nitrogen results in phosphate starvation of the symbiotic algae. Based
   on these findings we introduce a conceptual model that links
   unfavourable ratios of dissolved inorganic nutrients in the water column
   with established mechanisms of coral bleaching. Notably, this model
   improves the understanding of the detrimental effects of coastal
   nutrient enrichment on coral reefs, which is urgently required to
   support knowledge-based management strategies to mitigate the effects of
   climate change.}},
DOI = {{10.1038/NCLIMATE1661}},
ISSN = {{1758-678X}},
ResearcherID-Numbers = {{Hunt, Alan/A-7432-2008
   Achterberg, Eric/C-5820-2009
   }},
ORCID-Numbers = {{Hunt, Alan/0000-0001-5938-2152
   Achterberg, Eric/0000-0002-3061-2767
   Postle, Anthony/0000-0001-7361-0756}},
Unique-ID = {{ISI:000319397500020}},
}

@article{ ISI:000315450100012,
Author = {Finocchietti, Sara and Takahashi, Ken and Okada, Kaoru and Watanabe,
   Yasuharu and Graven-Nielsen, Thomas and Mizumura, Kazue},
Title = {{Deformation and pressure propagation in deep tissue during mechanical
   painful pressure stimulation}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2013}},
Volume = {{51}},
Number = {{1-2}},
Pages = {{113-122}},
Month = {{FEB}},
Abstract = {{Manual palpation or pressure stimulation is often used for pain
   sensitivity assessment. The aim of the current study was to define a
   method for investigating the relation between pressure pain sensitivity
   and pressure propagation in soft or harder muscles. Three-dimensional
   finite-element computer-models were developed to simulate the tissue
   stress and strain distribution during pressure stimulation on the
   tibialis anterior and gastrocnemius muscles. Four cases were modelled
   representing females and males who were trained and untrained. The model
   geometry was based on MR images of the lower leg during pressure
   stimulation. Stress and strain were extracted from the models at
   pressure intensity levels equivalent to the pressure pain threshold. The
   principal strain peaked in the adipose tissue at 0.30 and 0.14 for
   stimulation on the gastrocnemius and tibialis anterior muscle,
   respectively. The principal strain in the muscle was higher for four
   models of the stimulation on the gastrocnemius muscle (0.22-0.30)
   compared with the four models of stimulation on the tibialis anterior
   muscle (0.11-0.14). Average pressure pain thresholds were significantly
   lower for the tibialis anterior compared with the gastrocnemius muscle
   (319 vs. 432 kPa) These data show different pressure propagation
   profiles in soft and hard muscle at the same pressure pain sensation
   level. This new approach is relevant as the clinical routine assesses
   all muscles equally. This results in a different exposure to pressure in
   relation to the muscle evaluated which may affect the outcome of the
   examination.}},
DOI = {{10.1007/s11517-012-0974-9}},
ISSN = {{0140-0118}},
ResearcherID-Numbers = {{TAKAHASHI, Ken/B-1622-2011
   }},
ORCID-Numbers = {{TAKAHASHI, Ken/0000-0002-7580-8691
   Graven-Nielsen, Thomas/0000-0002-7787-4860}},
Unique-ID = {{ISI:000315450100012}},
}

@article{ ISI:000314691500001,
Author = {Nilsson, Kenneth A. and Rains, Mark C. and Lewis, David B. and Trout,
   Kenneth E.},
Title = {{Hydrologic characterization of 56 geographically isolated wetlands in
   west-central Florida using a probabilistic method}},
Journal = {{WETLANDS ECOLOGY AND MANAGEMENT}},
Year = {{2013}},
Volume = {{21}},
Number = {{1}},
Pages = {{1-14}},
Month = {{FEB}},
Abstract = {{Wetlands are important hydrological elements of watersheds that
   influence water storage, surface water runoff, groundwater
   recharge/discharge processes, and evapotranspiration. Understanding the
   cumulative effect wetlands have on a watershed requires a good
   understanding of representative water-level fluctuations and storage
   characteristics associated with multiple wetlands across a region. We
   introduce a probabilistic approach based on frequency analysis of water
   levels in numerous geographically isolated wetlands across the mantled
   karst terrain of west-central Florida, in the Tampa Bay region. This
   approach estimates the probabilities, or percentage of time, that water
   levels in a wetland or upland groundwater wells are at or below a
   specific elevation. We applied this hydrologic characterization to 56
   wetlands in west-central Florida, and documented that standing water was
   present in the wetlands 61 \% of the time and that these wetlands were
   groundwater recharge zones at least 50 \% of the time over the 7 year
   study. Additionally, we demonstrated that various wetland types,
   classified according to vegetation community composition and structure,
   exhibit similar means, extremes and ranges in water-level behavior. We
   believe that this is the first paper to robustly quantify inundation
   frequency and recharge status in seasonally flooded wetlands at a
   regional scale. The analytical tool introduced in this manuscript could
   be used to detect, through changes in water-level frequency
   distribution, wetland hydrological response to different climatological
   or anthropogenic stressors. This tool is timely as changes in frequency
   distribution shape may provide early warnings of ecosystem regime
   change.}},
DOI = {{10.1007/s11273-012-9275-1}},
ISSN = {{0923-4861}},
ResearcherID-Numbers = {{Lewis, David/B-6313-2009}},
ORCID-Numbers = {{Lewis, David/0000-0002-8094-1577}},
Unique-ID = {{ISI:000314691500001}},
}

@article{ ISI:000314331200009,
Author = {Liu, Canran and White, Matt and Newell, Graeme and Griffioen, Peter},
Title = {{Species distribution modelling for conservation planning in Victoria,
   Australia}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2013}},
Volume = {{249}},
Number = {{SI}},
Pages = {{68-74}},
Month = {{JAN 24}},
Abstract = {{Detailed and reliable information about the spatial distribution of
   species provides critical information for effective conservation
   planning. In Victoria, Australia, this responsibility is held by the
   Department of Sustainability and Environment, which maintains and
   curates a database of site records for species: the Victorian
   Biodiversity Atlas. But the information is provided in point form, and
   therefore, it does not provide an adequate view of species distributions
   for all management purposes. By integrating known occurrences of species
   with environmental GIS data layers using a machine learning algorithm,
   random forest, we have built species distribution models for 523
   vertebrate fauna species across the whole state, providing predictive
   `maps' that are available for use in various conservation planning
   activities. In this paper, we introduce and discuss the methods we
   developed and implemented for producing these models. Specifically, 26
   explanatory variables were used for the modelling; three versions of
   models were built using different sets of explanatory variables;
   pseudo-absences were chosen by filtering random points with a profile
   model; model accuracy was assessed using several measures, especially
   lift curve-related ones; a threshold was selected for each model to
   transform continuous result to binary one by maximizing the sum of
   sensitivity and (pseudo) specificity, which was proved to be valid for
   presence-only data. (C) 2012 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2012.07.003}},
ISSN = {{0304-3800}},
Unique-ID = {{ISI:000314331200009}},
}

@article{ ISI:000317719900001,
Author = {Weber, Marc and Buceta, Javier},
Title = {{Dynamics of the quorum sensing switch: stochastic and non-stationary
   effects}},
Journal = {{BMC SYSTEMS BIOLOGY}},
Year = {{2013}},
Volume = {{7}},
Month = {{JAN 16}},
Abstract = {{Background: A wide range of bacteria species are known to communicate
   through the so called quorum sensing (QS) mechanism by means of which
   they produce a small molecule that can freely diffuse in the environment
   and in the cells. Upon reaching a threshold concentration, the
   signalling molecule activates the QS-controlled genes that promote
   phenotypic changes. This mechanism, for its simplicity, has become the
   model system for studying the emergence of a global response in
   prokaryotic cells. Yet, how cells precisely measure the signal
   concentration and act coordinately, despite the presence of fluctuations
   that unavoidably affects cell regulation and signalling, remains
   unclear.
   Results: We propose a model for the QS signalling mechanism in Vibrio
   fischeri based on the synthetic strains lux01 and lux02. Our approach
   takes into account the key regulatory interactions between LuxR and
   LuxI, the autoinducer transport, the cellular growth and the division
   dynamics. By using both deterministic and stochastic models, we analyze
   the response and dynamics at the single-cell level and compare them to
   the global response at the population level. Our results show how
   fluctuations interfere with the synchronization of the cell activation
   and lead to a bimodal phenotypic distribution. In this context, we
   introduce the concept of precision in order to characterize the
   reliability of the QS communication process in the colony. We show that
   increasing the noise in the expression of LuxR helps cells to get
   activated at lower autoinducer concentrations but, at the same time,
   slows down the global response. The precision of the QS switch under
   non-stationary conditions decreases with noise, while at steady-state it
   is independent of the noise value.
   Conclusions: Our in silico experiments show that the response of the
   LuxR/LuxI system depends on the interplay between non-stationary and
   stochastic effects and that the burst size of the
   transcription/translation noise at the level of LuxR controls the
   phenotypic variability of the population. These results, together with
   recent experimental evidences on LuxR regulation in wild-type species,
   suggest that bacteria have evolved mechanisms to regulate the intensity
   of those fluctuations.}},
DOI = {{10.1186/1752-0509-7-6}},
Article-Number = {{6}},
ISSN = {{1752-0509}},
ResearcherID-Numbers = {{Weber, Marc/A-3500-2013
   Buceta, Javier/E-3705-2010}},
ORCID-Numbers = {{Weber, Marc/0000-0001-7920-5655
   Buceta, Javier/0000-0003-1791-0011}},
Unique-ID = {{ISI:000317719900001}},
}

@article{ ISI:000334312200023,
Author = {Seghezzo, L. and Gatto D'Andrea, M. L. and Iribarnegaray, M. A. and
   Liberal, V. I. and Fleitas, A. and Bonifacio, J. L.},
Title = {{Improved risk assessment and risk reduction strategies in the Water
   Safety Plan (WSP) of Salta, Argentina}},
Journal = {{WATER SCIENCE AND TECHNOLOGY-WATER SUPPLY}},
Year = {{2013}},
Volume = {{13}},
Number = {{4}},
Pages = {{1080-1089}},
Abstract = {{The Water Safety Plan (WSP) for the city of Salta (Argentina) is
   presented and discussed. To develop this WSP, we used an adapted version
   of the methodology proposed by the World Health Organization (WHO). The
   new method included a preliminary weighting procedure to assess the
   relative importance of different parts of the system, and a more
   systematic estimation of the magnitude of control measures. These
   modifications allowed the definition of a variety of risk reduction
   strategies. The risk assessment step was performed during participatory
   workshops with members of the local water company. The Initial Risk for
   the entire system was 30.2\%, with variations among processes,
   subprocesses and components. More than 60\% of the hazardous situations
   identified require control measures to reduce the risk below an
   acceptable threshold. If all control measures were successfully
   implemented, the Final Risk could be lowered to 17.7\%. Methodological
   changes introduced allowed a more detailed analysis of the risks and can
   be an important improvement of the assessment procedure.}},
DOI = {{10.2166/ws.2013.087}},
ISSN = {{1606-9749}},
Unique-ID = {{ISI:000334312200023}},
}

@article{ ISI:000327266000001,
Author = {He, Peng and Wei, Biao and Feng, Peng and Chen, Mianyi and Mi, Deling},
Title = {{Material Discrimination Based on K-edge Characteristics}},
Journal = {{COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE}},
Year = {{2013}},
Abstract = {{Spectral/multienergy CT employing the state-of-the-art
   energy-discriminative photon-counting detector can identify absorption
   features in the multiple ranges of photon energies and has the potential
   to distinguish different materials based on K-edge characteristics.
   K-edge characteristics involve the sudden attenuation increase in the
   attenuation profile of a relatively high atomic number material. Hence,
   spectral CT can utilizematerial K-edge characteristics (sudden
   attenuation increase) to capture images in available energy bins
   (levels/windows) to distinguish different material components. In this
   paper, we propose an imaging model based on K-edge characteristics for
   maximum material discrimination with spectral CT. The wider the energy
   binwidth is, the lower the noise level is, but the poorer the
   reconstructed image contrast is. Here, we introduce the
   contrast-to-noise ratio (CNR) criterion to optimize the energy bin width
   after the K-edge jump for the maximum CNR. In the simulation, we analyze
   the reconstructed image quality in different energy bins and demonstrate
   that our proposed optimization approach can maximize CNR between target
   region and background region in reconstructed image.}},
DOI = {{10.1155/2013/308520}},
Article-Number = {{UNSP 308520}},
ISSN = {{1748-670X}},
EISSN = {{1748-6718}},
Unique-ID = {{ISI:000327266000001}},
}

@article{ ISI:000321142500021,
Author = {Rao, V. Sree Hari and Kumar, M. Naresh},
Title = {{Novel Approaches for Predicting Risk Factors of Atherosclerosis}},
Journal = {{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}},
Year = {{2013}},
Volume = {{17}},
Number = {{1}},
Pages = {{183-189}},
Month = {{JAN}},
Abstract = {{Coronary heart disease (CHD) caused by hardening of artery walls due to
   cholesterol known as atherosclerosis is responsible for large number of
   deaths worldwide. The disease progression is slow, asymptomatic, and may
   lead to sudden cardiac arrest, stroke, or myocardial infraction.
   Presently, imaging techniques are being employed to understand the
   molecular and metabolic activity of atherosclerotic plaques to estimate
   the risk. Though imaging methods are able to provide some information on
   plaque metabolism, they lack the required resolution and sensitivity for
   detection. In this paper, we consider the clinical observations and
   habits of individuals for predicting the risk factors of CHD. The
   identification of risk factors helps in stratifying patients for further
   intensive tests such as nuclear imaging or coronary angiography. We
   present a novel approach for predicting the risk factors of
   atherosclerosis with an in-built imputation algorithm and particle swarm
   optimization (PSO). We compare the performance of our methodology with
   other machine-learning techniques on STULONG dataset which is based on
   longitudinal study of middle-aged individuals lasting for 20 years. Our
   methodology powered by PSO search has identified physical inactivity as
   one of the risk factors for the onset of atherosclerosis in addition to
   other already known factors. The decision rules extracted by our
   methodology are able to predict the risk factors with an accuracy of
   99.73\% which are higher than the accuracies obtained by the application
   of the state-of-the-artmachine-learning techniques presently being
   employed in the identification of atherosclerosis risk studies.}},
DOI = {{10.1109/TITB.2012.2227271}},
ISSN = {{2168-2194}},
ResearcherID-Numbers = {{Mallenahalli, Naresh/F-7653-2013}},
ORCID-Numbers = {{Mallenahalli, Naresh/0000-0001-9124-4999}},
Unique-ID = {{ISI:000321142500021}},
}

@article{ ISI:000318275300018,
Author = {Fleischer, K. and Rebel, K. T. and van der Molen, M. K. and Erisman, J.
   W. and Wassen, M. J. and van Loon, E. E. and Montagnani, L. and Gough,
   C. M. and Herbst, M. and Janssens, I. A. and Gianelle, D. and Dolman, A.
   J.},
Title = {{The contribution of nitrogen deposition to the photosynthetic capacity
   of forests}},
Journal = {{GLOBAL BIOGEOCHEMICAL CYCLES}},
Year = {{2013}},
Volume = {{27}},
Number = {{1}},
Pages = {{187-199}},
Abstract = {{Global terrestrial carbon (C) sequestration has increased over the last
   few decades. The drivers of carbon sequestration, the geographical
   spread and magnitude of this sink are however hotly debated.
   Photosynthesis determines the total C uptake of terrestrial ecosystems
   and is a major flux of the global C balance. We contribute to the
   discussion on enhanced C sequestration by analyzing the influence of
   nitrogen (N) deposition on photosynthetic capacity (A(max)) of forest
   canopies. Eddy covariance measurements of net exchange of carbon provide
   estimates of gross primary production, from which A(max) is derived with
   a novel approach. Canopy A(max) is combined with modeled N deposition,
   environmental variables and stand characteristics to study the relative
   effects on A(max) for a unique global data set of 80 forest FLUXNET
   sites. Canopy A(max) relates positively to N deposition for evergreen
   needleleaf forests below an observed critical load of similar to 8 kgN
   ha(-1) yr(-1), with a slope of 2.0 +/- 0.4 (S.E.) mu mol CO2 m(-2) s(-1)
   per 1 kgN ha(-1) yr(-1). Above this threshold canopy A(max) levels off,
   exhibiting a saturating response in line with the N saturation
   hypothesis. Climate effects on canopy A(max) cannot be separated from
   the effect of N deposition due to considerable covariation. For
   deciduous broadleaf forests and forests in the temperate (-continental)
   climate zones, the analysis shows the N deposition effect to be either
   small or absent. Leaf area index and foliar N concentration are
   positively but weakly related to A(max). We conclude that flux tower
   measurements of C fluxes provide valuable data to study physiological
   processes at the canopy scale. Future efforts need to be directed toward
   standardizing measures N cycling and pools within C monitoring networks
   to gain a better understanding of C and N interactions, and to
   disentangle the role of climate and N deposition in forest ecosystems.}},
DOI = {{10.1002/gbc.20026}},
ISSN = {{0886-6236}},
ResearcherID-Numbers = {{Rebel, Karin/C-4750-2008
   Janssens, Ivan/P-1331-2014
   Montagnani, Leonardo/F-1837-2016
   Gianelle, Damiano/G-9437-2011
   Wassen, Martin/L-9228-2013
   }},
ORCID-Numbers = {{Janssens, Ivan/0000-0002-5705-1787
   Montagnani, Leonardo/0000-0003-2957-9071
   Gianelle, Damiano/0000-0001-7697-5793
   Wassen, Martin/0000-0002-9735-2103
   Fleischer, Katrin/0000-0002-9093-9526
   Dolman, A.J./0000-0003-0099-0457}},
Unique-ID = {{ISI:000318275300018}},
}

@article{ ISI:000316998300020,
Author = {Rieder, H. E. and Fiore, A. M. and Polvani, L. M. and Lamarque, J-F and
   Fang, Y.},
Title = {{Changes in the frequency and return level of high ozone pollution events
   over the eastern United States following emission controls}},
Journal = {{ENVIRONMENTAL RESEARCH LETTERS}},
Year = {{2013}},
Volume = {{8}},
Number = {{1}},
Month = {{JAN-MAR}},
Abstract = {{In order to quantify the impact of recent efforts to abate surface ozone
   (O-3) pollution, we analyze changes in the frequency and return level of
   summertime (JJA) high surface O-3 events over the eastern United States
   (US) from 1988-1998 to 1999-2009. We apply methods from extreme value
   theory (EVT) to maximum daily 8-hour average ozone (MDA8 O-3) observed
   by the Clean Air Status and Trends Network (CASTNet) and define O-3
   extremes as days on which MDA8 O-3 exceeds a threshold of 75 ppb (MDA8
   O-3 > 75). Over the eastern US, we find that the number of summer days
   with MDA8 O-3 > 75 declined on average by about a factor of two from
   1988-1998 to 1999-2009. The applied generalized Pareto distribution
   (GPD) fits the high tail of MDA8 O-3 much better than a Gaussian
   distribution and enables the derivation of probabilistic return levels
   (describing the probability of exceeding a value x within a time window
   T) for high O-3 pollution events. This new approach confirms the
   significant decline in both frequency and magnitude of high O-3
   pollution events over the eastern US during recent years reported in
   prior studies. Our analysis of 1-yr and 5-yr return levels at each
   station demonstrates the strong impact of changes in air quality
   regulations and subsequent control measures (e. g., the `NOx SIP Call'),
   as the 5-yr return levels of the period 1999-2009 correspond roughly to
   the 1-yr return levels of the earlier time period (1988-1998).
   Regionally, the return levels dropped between 1988-1998 and 1999-2009 by
   about 8 ppb in the Mid-Atlantic (MA) and Great Lakes (GL) regions, while
   the strongest decline, about 13 ppb, is observed in the Northeast (NE)
   region. Nearly all stations (21 out of 23) have 1-yr return levels well
   below 100 ppb and 5-yr return levels well below 110 ppb in 1999-2009.
   Decreases in eastern US O-3 pollution are largest after full
   implementation of the nitrogen oxide (NOx) reductions under the `NOx SIP
   Call'. We conclude that the application of EVT methods provides a useful
   approach for quantifying return levels of high O-3 pollution in
   probabilistic terms, which may help to guide long-term air quality
   planning.}},
DOI = {{10.1088/1748-9326/8/1/014012}},
Article-Number = {{014012}},
ISSN = {{1748-9326}},
ResearcherID-Numbers = {{Lamarque, Jean-Francois/L-2313-2014
   Fang, Yuanyuan/F-1308-2011
   }},
ORCID-Numbers = {{Lamarque, Jean-Francois/0000-0002-4225-5074
   Fang, Yuanyuan/0000-0001-7067-7103
   Rieder, Harald/0000-0003-2705-0801}},
Unique-ID = {{ISI:000316998300020}},
}

@article{ ISI:000313603500003,
Author = {Gendel, Youri and Lahav, Ori},
Title = {{A novel approach for ammonia removal from fresh-water recirculated
   aquaculture systems, comprising ion exchange and electrochemical
   regeneration}},
Journal = {{AQUACULTURAL ENGINEERING}},
Year = {{2013}},
Volume = {{52}},
Pages = {{27-38}},
Month = {{JAN}},
Abstract = {{A new physico-chemical process for ammonia removal from fresh-water
   recirculated aquaculture systems (RASs) is introduced. The method is
   based on separating NH4+ from RAS water through an ion-exchange resin,
   which is subsequently regenerated by simultaneous chemical desorption
   and indirect electrochemical ammonia oxidation. Approach advantages
   include (1) only slight temperature dependence and no dependence on
   bacterial predators and chemical toxins; (2) no startup period is
   required and the system can be switched on and off at will; and (3) the
   fish are grown in much lower bacterial concentration, making the
   potential for both disease and off-flavor, lower. A small pilot scale
   RAS was operated for 51 d for proving the concept. The system was
   stocked by 105 tilapia fish (initial weight 35.8 g). The fish, which
   were maintained at high TAN (total ammonia nitrogen) concentrations
   (10-23 mgN L-1) and fish density of up to 20 kg m(-3), grew at a rate
   identical to their established growth potential. NH3(aq) concentrations
   in the fish tank were maintained lower than the assumed toxicity
   threshold (0.1 mgN L-1) by operating the pond water at low pH (6.5-6.7).
   The low pH resulted in efficient CO2 air stripping, and low resultant
   CO2(aq) concentrations (<7 mg L-1). Due to efficient solids removal, no
   nitrification was observed in the fish tank and measured nitrite and
   nitrate concentrations were very low. The system was operated
   successfully, first at 10\% and then at 5\% daily makeup water exchange
   rate. The normalized operational costs, calculated based on data derived
   from the pilot operation, amounted to 28.7\$ cent per kg fish feed. The
   volume of the proposed process was calculated to be similar to 13 times
   smaller than that of a typical RAS biofilter. The results show the
   process to be highly feasible from both the operational and economical
   standpoints. (C) 2012 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.aquaeng.2012.07.005}},
ISSN = {{0144-8609}},
ResearcherID-Numbers = {{Lahav, Ori/E-4801-2011}},
ORCID-Numbers = {{Lahav, Ori/0000-0002-0654-352X}},
Unique-ID = {{ISI:000313603500003}},
}

@article{ ISI:000209207300009,
Author = {Picado-Muino, David and Borgelt, Christian and Berger, Denise and
   Gerstein, George and Gruen, Sonja},
Title = {{Finding neural assemblies with frequent item set mining}},
Journal = {{FRONTIERS IN NEUROINFORMATICS}},
Year = {{2013}},
Volume = {{7}},
Abstract = {{Cell assemblies, defined as groups of neurons exhibiting precise spike
   coordination, were proposed as a model of network processing in the
   cortex. Fortunately, in recent years considerable progress has been made
   in multi-electrode recordings, which enable recording massively parallel
   spike trains of hundred(s) of neurons simultaneously. However, due to
   the challenges inherent in multivariate approaches, most studies in
   favor of cortical cell assemblies still resorted to analyzing pairwise
   interactions. However, to recover the underlying correlation structures,
   higher-order correlations need to be identified directly. Inspired by
   the Accretion method proposed by Gerstein et al. (1978) we propose a new
   assembly detection method based on frequent item set mining (FIM). In
   contrast to Accretion, FIM searches effectively and without redundancy
   for individual spike patterns that exceed a given support threshold. We
   study different search methods, with which the space of potential cell
   assemblies may be explored, as well as different test statistics and
   subset conditions with which candidate assemblies may be assessed and
   filtered. It turns out that a core challenge of cell assembly detection
   is the problem of multiple testing, which causes a large number of false
   discoveries. Unfortunately, criteria that address individual candidate
   assemblies and try to assess them with statistical tests and/or subset
   conditions do not help much to tackle this problem. The core idea of our
   new method is that in order to cope with the multiple testing problem
   one has to shift the focus of statistical testing from specific
   assemblies (consisting of a specific set of neurons) to spike patterns
   of a certain size (i.e., with a certain number of neurons). This
   significantly reduces the number of necessary tests, thus alleviating
   the multiple testing problem. We demonstrate that our method is able to
   reliably suppress false discoveries, while it is still very sensitive in
   discovering synchronous activity. Since we exploit high-speed
   computational techniques from FIM for the tests, our method is also
   computationally efficient.}},
DOI = {{10.3389/fninf.2013.00009}},
Article-Number = {{UNSP 9}},
ISSN = {{1662-5196}},
ORCID-Numbers = {{Grun, Sonja/0000-0003-2829-2220}},
Unique-ID = {{ISI:000209207300009}},
}

@article{ ISI:000312392000001,
Author = {Han, Y. M. and Marlon, J. R. and Cao, J. J. and Jin, Z. D. and An, Z. S.},
Title = {{Holocene linkages between char, soot, biomass burning and climate from
   Lake Daihai, China}},
Journal = {{GLOBAL BIOGEOCHEMICAL CYCLES}},
Year = {{2012}},
Volume = {{26}},
Month = {{DEC 13}},
Abstract = {{Black or elemental carbon (EC), including soot and char, are byproducts
   of anthropogenic fossil-fuel and biomass burning, and also of wildfires.
   EC, and particularly soot, strongly affects atmospheric chemistry and
   physics and thus radiative forcing; it can also alter regional climate
   and precipitation. Pre-industrial variations in EC as well as its source
   areas and controls however, are poorly known. Here we use a
   lake-sediment EC record from China to reconstruct Holocene variations in
   soot (combustion emissions formed via gas-to-particle conversion
   processes) and char (combustion residues from pyrolysis) measured with a
   thermal/optical method. Comparisons with sedimentary charcoal records
   (i.e., particles measured microscopically), climate and population data
   are used to infer variations in biomass burning and its controls. During
   the Holocene, positive correlations are observed between EC and an
   independent index of regional biomass burning. Negative correlations are
   observed between EC and monsoon intensity, and tree cover inferred from
   arboreal pollen percentages. Abrupt declines in temperature are also
   linked with widespread declines in fire. Our results 1) confirm the
   robustness of a relatively new method for reconstructing variations in
   EC; 2) document variations in regional biomass burning; 3) support a
   strong climatic control of biomass burning throughout the Holocene; and
   4) indicate that char levels are higher today than at any time during
   the Holocene. Citation: Han, Y. M., J. R. Marlon, J. J. Cao, Z. D. Jin,
   and Z. S. An (2012), Holocene linkages between char, soot, biomass
   burning and climate from Lake Daihai, China, Global Biogeochem. Cycles,
   26, GB4017, doi:10.1029/2011GB004197.}},
DOI = {{10.1029/2011GB004197}},
Article-Number = {{GB4017}},
ISSN = {{0886-6236}},
EISSN = {{1944-9224}},
ResearcherID-Numbers = {{Jin, Zhangdong/I-8642-2014
   Cao, Junji/D-3259-2014
   AN, Zhisheng/F-8834-2012
   Han, Yongming/I-8824-2014
   }},
ORCID-Numbers = {{Cao, Junji/0000-0003-1000-7241
   an, zhi sheng/0000-0002-9538-9826}},
Unique-ID = {{ISI:000312392000001}},
}

@article{ ISI:000315237200001,
Author = {Ma Ronggui and Wang Weixing and Liu Sheng},
Title = {{Extracting roads based on Retinex and improved Canny operator with shape
   criteria in vague and unevenly illuminated aerial images}},
Journal = {{JOURNAL OF APPLIED REMOTE SENSING}},
Year = {{2012}},
Volume = {{6}},
Month = {{DEC 3}},
Abstract = {{An automatic road extraction method for vague aerial images is proposed
   in this paper. First, a high-resolution but low-contrast image is
   enhanced by using a Retinex-based algorithm. Then, the enhanced image is
   segmented with an improved Canny edge detection operator that can
   automatically threshold the image into a binary edge image.
   Subsequently, the linear and curved road segments are regulated by the
   Hough line transform and extracted based on several thresholds of road
   size and shapes, in which a number of morphological operators are used
   such as thinning (skeleton), junction detection, and endpoint detection.
   In experiments, a number of vague aerial images with bad uniformity are
   selected for testing. Similarity and discontinuation-based algorithms,
   such as Otsu thresholding, merge and split, edge detection-based
   algorithms, and the graph-based algorithm are compared with the new
   method. The experiment and comparison results show that the studied
   method can enhance vague, low-contrast, and unevenly illuminated color
   aerial road images; it can detect most road edges with fewer disturb
   elements and trace roads with good quality. The method in this study is
   promising. (c) 2012 Society of Photo-Optical Instrumentation Engineers
   (SPIE). {[}DOI: 10.1117/1.JRS.6.063610]}},
DOI = {{10.1117/1.JRS.6.063610}},
Article-Number = {{063610}},
ISSN = {{1931-3195}},
Unique-ID = {{ISI:000315237200001}},
}

@article{ ISI:000309745400008,
Author = {Laska, Eugene and Meisner, Morris and Wanderling, Joseph},
Title = {{A maximally selected test of symmetry about zero}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2012}},
Volume = {{31}},
Number = {{26}},
Pages = {{3178-3191}},
Month = {{NOV 20}},
Abstract = {{The problem of testing symmetry about zero has a long and rich history
   in the statistical literature. We introduce a new test that sequentially
   discards observations whose absolute value is below increasing
   thresholds defined by the data. McNemar's statistic is obtained at each
   threshold and the largest is used as the test statistic. We obtain the
   exact distribution of this maximally selected McNemar and provide tables
   of critical values and a program for computing p-values. Power is
   compared with the t-test, the Wilcoxon Signed Rank Test and the Sign
   Test. The new test, MM, is slightly less powerful than the t-test and
   Wilcoxon Signed Rank Test for symmetric normal distributions with
   nonzero medians and substantially more powerful than all three tests for
   asymmetric mixtures of normal random variables with or without zero
   medians. The motivation for this test derives from the need to appraise
   the safety profile of new medications. If pre and post safety measures
   are obtained, then under the null hypothesis, the variables are
   exchangeable and the distribution of their difference is symmetric about
   a zero median. Large prepost differences are the major concern of a
   safety assessment. The discarded small observations are not particularly
   relevant to safety and can reduce power to detect important asymmetry.
   The new test was utilized on data from an on-road driving study
   performed to determine if a hypnotic, a drug used to promote sleep, has
   next day residual effects. Copyright (c) 2012 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/sim.5384}},
ISSN = {{0277-6715}},
ORCID-Numbers = {{Laska, Eugene/0000-0001-6799-1361}},
Unique-ID = {{ISI:000309745400008}},
}

@article{ ISI:000311425000012,
Author = {Kumar, Pradeep and Choonara, Yahya E. and du Toit, Lisa C. and Modi,
   Girish and Naidoo, Dinesh and Pillay, Viness},
Title = {{Novel High-Viscosity Polyacrylamidated Chitosan for Neural Tissue
   Engineering: Fabrication of Anisotropic Neurodurable Scaffold via
   Molecular Disposition of Persulfate-Mediated Polymer Slicing and
   Complexation}},
Journal = {{INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES}},
Year = {{2012}},
Volume = {{13}},
Number = {{11}},
Pages = {{13966-13984}},
Month = {{NOV}},
Abstract = {{Macroporous polyacrylamide-grafted-chitosan scaffolds for neural tissue
   engineering were fabricated with varied synthetic and viscosity
   profiles. A novel approach and mechanism was utilized for polyacrylamide
   grafting onto chitosan using potassium persulfate (KPS) mediated
   degradation of both polymers under a thermally controlled environment.
   Commercially available high molecular mass polyacrylamide was used
   instead of the acrylamide monomer for graft copolymerization. This
   grafting strategy yielded an enhanced grafting efficiency (GE = 92\%),
   grafting ratio (GR = 263\%), intrinsic viscosity (IV = 5.231 dL/g) and
   viscometric average molecular mass (MW = 1.63 x 10(6) Da) compared with
   known acrylamide that has a GE = 83\%, GR = 178\%, IV = 3.901 dL/g and
   MW = 1.22 x 10(6) Da. Image processing analysis of SEM images of the
   newly grafted neurodurable scaffold was undertaken based on the
   polymer-pore threshold. Attenuated Total Reflectance-FTIR spectral
   analyses in conjugation with DSC were used for the characterization and
   comparison of the newly grafted copolymers. Static Lattice Atomistic
   Simulations were employed to investigate and elucidate the copolymeric
   assembly and reaction mechanism by exploring the spatial disposition of
   chitosan and polyacrylamide with respect to the reactional profile of
   potassium persulfate. Interestingly, potassium persulfate, a peroxide,
   was found to play a dual role initially degrading the
   polymers-{''}polymer slicing{''}-thereby initiating the formation of
   free radicals and subsequently leading to synthesis of the high
   molecular mass polyacrylamide-grafted-chitosan (PAAm-g-CHT)-{''}polymer
   complexation{''}. Furthermore, the applicability of the uniquely grafted
   scaffold for neural tissue engineering was evaluated via PC12 neuronal
   cell seeding. The novel PAAm-g-CHT exhibited superior neurocompatibility
   in terms of cell infiltration owing to the anisotropic porous
   architecture, high molecular mass mediated robustness, superior
   hydrophilicity as well as surface charge due to the acrylic chains.
   Additionally, these results suggested that the porous PAAm-g-CHT
   scaffold may act as a potential neural cell carrier.}},
DOI = {{10.3390/ijms131113966}},
ISSN = {{1422-0067}},
ResearcherID-Numbers = {{Choonara, Yahya/B-2955-2013
   Pillay, Viness/C-1569-2010
   Kumar, Pradeep/C-9424-2013}},
ORCID-Numbers = {{Choonara, Yahya/0000-0002-3889-1529
   Pillay, Viness/0000-0002-8119-3347
   Kumar, Pradeep/0000-0002-8640-4350}},
Unique-ID = {{ISI:000311425000012}},
}

@article{ ISI:000310834900017,
Author = {Sutherland, Chris and Elston, David A. and Lambin, Xavier},
Title = {{Multi-scale processes in metapopulations: contributions of stage
   structure, rescue effect, and correlated extinctions}},
Journal = {{ECOLOGY}},
Year = {{2012}},
Volume = {{93}},
Number = {{11}},
Pages = {{2465-2473}},
Month = {{NOV}},
Abstract = {{Metapopulations function and persist through a combination of processes
   acting at a variety of spatial scales. Although the contributions of
   stage structure, spatially correlated processes, and the rescue effect
   to metapopulation dynamics have been investigated in isolation, there is
   no empirical demonstration of all of these processes shaping dynamics in
   a single system. Dispersal and settlement differ according to the life
   stage involved; therefore, stage-specific population size may outperform
   total population size when predicting colonization-extinction dynamics.
   Synchrony in patch dynamics can lead to accelerated metapopulation
   extinction, although empirical evidence of the interplay between
   correlated colonization events and correlated extinctions is lacking.
   Likewise, few empirical examples exist that provide compelling evidence
   of migration acting to reduce extinction risk (the rescue effect). We
   parameterized a hierarchy of metapopulation models to investigate these
   predictions using a seven-year study of a naturally occurring water vole
   (Arvicola amphibius) metapopulation. Specifically, we demonstrated the
   importance of local stage structure in predicting both colonization and
   extinction events using juvenile and adult population sizes,
   respectively. Using a novel approach for quantifying correlation in
   extinction events, we compared the scale of synchrony in colonization
   and extinction. Strikingly, the scale of dispersal acting to synchronize
   colonization was an order of magnitude larger than that of correlated
   extinctions (halving distance of the effect: 12.40 km and 0.89 km,
   respectively). Additionally, we found compelling evidence for the
   existence of a nontrivial rescue effect. Here we provide a novel
   empirical demonstration of a variety of metapopulation processes
   operating at multiple spatial scales, further emphasizing the need to
   consider stage structure and local synchrony in the dynamics of
   spatially dependent, stage-structured (meta) populations.}},
DOI = {{10.1890/12-0172.1}},
ISSN = {{0012-9658}},
ResearcherID-Numbers = {{Sutherland, Chris/A-1737-2014
   Elston, David/H-8199-2013
   Lambin, Xavier/E-8284-2011
   }},
ORCID-Numbers = {{Lambin, Xavier/0000-0003-4643-2653
   Sutherland, Chris/0000-0003-2073-1751}},
Unique-ID = {{ISI:000310834900017}},
}

@article{ ISI:000310568800040,
Author = {Sukhorukov, Valerii M. and Dikov, Daniel and Reichert, Andreas S. and
   Meyer-Hermann, Michael},
Title = {{Emergence of the Mitochondrial Reticulum from Fission and Fusion
   Dynamics}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2012}},
Volume = {{8}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{Mitochondria form a dynamic tubular reticulum within eukaryotic cells.
   Currently, quantitative understanding of its morphological
   characteristics is largely absent, despite major progress in deciphering
   the molecular fission and fusion machineries shaping its structure. Here
   we address the principles of formation and the large-scale organization
   of the cell-wide network of mitochondria. On the basis of experimentally
   determined structural features we establish the tip-to-tip and
   tip-to-side fission and fusion events as dominant reactions in the
   motility of this organelle. Subsequently, we introduce a graph-based
   model of the chondriome able to encompass its inherent variability in a
   single framework. Using both mean-field deterministic and explicit
   stochastic mathematical methods we establish a relationship between the
   chondriome structural network characteristics and underlying kinetic
   rate parameters. The computational analysis indicates that mitochondrial
   networks exhibit a percolation threshold. Intrinsic morphological
   instability of the mitochondrial reticulum resulting from its vicinity
   to the percolation transition is proposed as a novel mechanism that can
   be utilized by cells for optimizing their functional competence via
   dynamic remodeling of the chondriome. The detailed size distribution of
   the network components predicted by the dynamic graph representation
   introduces a relationship between chondriome characteristics and cell
   function. It forms a basis for understanding the architecture of
   mitochondria as a cell-wide but inhomogeneous organelle. Analysis of the
   reticulum adaptive configuration offers a direct clarification for its
   impact on numerous physiological processes strongly dependent on
   mitochondrial dynamics and organization, such as efficiency of cellular
   metabolism, tissue differentiation and aging.}},
DOI = {{10.1371/journal.pcbi.1002745}},
Article-Number = {{e1002745}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
ResearcherID-Numbers = {{CEF, MC/B-4919-2018
   Reichert, Andreas/A-4090-2012}},
Unique-ID = {{ISI:000310568800040}},
}

@article{ ISI:000308876600010,
Author = {Di Virgilio, Giovanni and Laffan, Shawn W. and Ebach, Malte C.},
Title = {{Fine-scale quantification of floral and faunal breaks and their
   geographic correlates, with an example from south-eastern Australia}},
Journal = {{JOURNAL OF BIOGEOGRAPHY}},
Year = {{2012}},
Volume = {{39}},
Number = {{10}},
Pages = {{1862-1876}},
Month = {{OCT}},
Abstract = {{Aim We introduce a method to quantify shared breaks in aggregate biotic
   distributions and their relationships to geographic variables. The
   method is based on quantification of distributional taxic and abiotic
   data that can be applied over multiple spatial scales. We aim to show
   biogeographic breaks and varying transition zones at a fine level of
   detail (5-km resolution) and develop an approach to assess existing
   bioregionalization schemes. Location Global applicability, using an
   example from New South Wales in south-eastern Australia. Methods Moving
   window analyses, rotated in 15 degrees increments through 360 degrees,
   are used to assess the degree of anisotropic spatial turnover between
   sets of gridded cells containing georeferenced species observations.
   Patterns of biotic turnover are compared with equivalent analyses for
   elevation and lithology. Identified breaks are assessed against an
   existing bioregionalization scheme (Interim Biogeographic
   Regionalisation of Australia, IBRA). Results There was fine-scale
   concordance between turnover patterns and several IBRA bioregions.
   Breaks in turnover of flora and fauna corresponded with the boundaries
   of the Hunter Valley and Sydney Basin regions, particularly the boundary
   between the Brigalow Belt South and Sydney Basin. Low-turnover zones
   were quantified; prominent examples are the Sydney Cataract and Wyong
   bioregions. Turnover along many boundaries was gradational, confirming
   that mapped breaks are not abrupt. A previously unidentified break was
   identified in the South East Corner bioregion. Spatial turnover patterns
   were similar between biota and were reflected in mean correlation
   coefficients between turnover in each group: mammalsreptiles (r = 0.70,
   P << 0.01); mammalsflora (r = 0.56, P << 0.01); and reptilesflora (r =
   0.51, P << 0.01). Generally, patterns of abiotic turnover reflected
   biotic turnover, although mean turnover correlations were weaker than
   between biota. Main conclusions Using this method we were able to
   characterize taxic breaks and overlaps in detail and at a spatially fine
   resolution. For our study region, we confirm the overall integrity of
   the IBRA framework, but suggest that it may benefit from revision in
   some respects.}},
DOI = {{10.1111/j.1365-2699.2012.02739.x}},
ISSN = {{0305-0270}},
ResearcherID-Numbers = {{Laffan, Shawn/A-3761-2008
   }},
ORCID-Numbers = {{Laffan, Shawn/0000-0002-5996-0570
   Ebach, Malte/0000-0002-9594-9010}},
Unique-ID = {{ISI:000308876600010}},
}

@article{ ISI:000311247700033,
Author = {Chen, Gang and Wulder, Michael A. and White, Joanne C. and Hilker,
   Thomas and Coops, Nicholas C.},
Title = {{Lidar calibration and validation for geometric-optical modeling with
   Landsat imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2012}},
Volume = {{124}},
Pages = {{384-393}},
Month = {{SEP}},
Abstract = {{There is a paucity of detailed and timely forest inventory information
   available for Canada's large, remote northern boreal forests. The
   Canadian National Forest Inventory program has derived a limited set of
   attributes from a Landsat-based land cover product representing circa
   year 2000 conditions. Of the required inventory attributes, forest
   vertical structure (e.g., tree height) is critical for terrestrial
   biomass and carbon modeling and to date, is unavailable for these remote
   areas. In this study, we develop a large-area, fine-scale (25 m) mapping
   solution to estimate tree height (mean, dominant, and Lorey's height)
   across Canada's northern forests by integrating lidar data (representing
   0.27\% of the study area), and Landsat imagery (representing 100\% of
   the study area), using a geometric-optical modeling technique. First,
   spectral mixture analysis (SMA) was used to extract image endmembers and
   generate fraction images. Second, lidar data were used to calibrate the
   inverted geometric-optical model by adjusting the model's three key
   fractional inputs: sunlit crown, sunlit background, and shade fraction,
   based upon the SMA derived images. The heterogeneity of the study area,
   spanning 2.16 million ha, made it challenging to directly and accurately
   decompose mixed Landsat image pixels into the canopy and background
   fractions used for the Li-Strahler geometric-optical model inversion. As
   a result we developed a novel method to use the lidar plot data to
   facilitate the calculation of these fractions in an accurate and
   automated manner. The average estimation errors for mean, dominant, and
   Lorey's height were 4.9 m, 4.1 m, and 4.7 m, respectively when compared
   to the lidar data, with the best result achieved using dominant tree
   height, where the average error was 3.5 m for over 80\% of the forested
   area. Using this approach of optical remotely sensed data calibrated and
   validated with lidar height estimates, we generate and evaluate
   wall-to-wall estimates of tree height that can subsequently be used as
   inputs for biomass and carbon modeling. Crown Copyright (C) 2012
   Published by Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2012.05.026}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Coops, Nicholas/J-1543-2012
   Wulder, Michael/J-5597-2016
   }},
ORCID-Numbers = {{Coops, Nicholas/0000-0002-0151-9037
   Wulder, Michael/0000-0002-6942-1896
   Chen, Gang/0000-0002-7469-3650
   White, Joanne/0000-0003-4674-0373}},
Unique-ID = {{ISI:000311247700033}},
}

@article{ ISI:000307332300013,
Author = {Thoma, David P. and Irwin, Roy J. and Penoyer, Pete E.},
Title = {{Documenting measurement sensitivity and bias of field-measured
   parameters in water quality monitoring programs}},
Journal = {{ENVIRONMENTAL MONITORING AND ASSESSMENT}},
Year = {{2012}},
Volume = {{184}},
Number = {{9}},
Pages = {{5387-5398}},
Month = {{SEP}},
Abstract = {{Measurement sensitivity and bias quality control metrics are commonly
   reported for water-quality parameters measured in the laboratory. Less
   commonly recognized is that they should also be reported for
   field-measured parameters. Periodic evaluation helps document data
   quality and can help serve as early warning if there are problems with
   methods or techniques that could negatively affect ability to interpret
   threshold values and trends over time. This study focuses on traditional
   assessment of bias and introduces a new method for estimating
   measurement sensitivity of water-quality parameters measured monthly in
   the field. Alternative measurement sensitivity is a new data quality
   indicator used to demonstrate how quantifying sensitivity at the
   measurement level can improve understanding the uncertainty affecting
   each reported data value. That, in turn, can help interpret the meaning
   of results from many separate data points measured in the field. In this
   30-month study, pH and specific conductance consistently met, and
   dissolved oxygen did not always meet NPS and USGS quality control
   standards for bias. Evaluation of dissolved oxygen bias and sensitivity
   during the study provided impetus to improve calibration techniques that
   resulted in data that later met quality goals.}},
DOI = {{10.1007/s10661-011-2347-5}},
ISSN = {{0167-6369}},
Unique-ID = {{ISI:000307332300013}},
}

@article{ ISI:000307158600011,
Author = {Beaudouin, Remy and Zeman, Florence A. and Pery, Alexandre R. R.},
Title = {{Individual sensitivity distribution evaluation from survival data using
   a mechanistic model: Implications for ecotoxicological risk assessment}},
Journal = {{CHEMOSPHERE}},
Year = {{2012}},
Volume = {{89}},
Number = {{1}},
Pages = {{83-88}},
Month = {{SEP}},
Abstract = {{Two main alternatives are typically used to model mechanistically
   dose-survival relationship in ecotoxicity tests. Effects are related to
   a concentration of concern, for instance body concentration, and, to
   account for their differences relative to time-to-death, individuals
   have either different concentration thresholds for death ({''}individual
   tolerance approach{''}), or equal probability to die, with death
   occurring randomly ({''}stochastic death approach{''}). A general
   framework to unify both approaches has recently been proposed. We
   derived a model from this framework to analyse five datasets (daphnids
   exposed to selenium, guppies exposed to dieldrin and second, third and
   fourth instars chironomids exposed to copper), by extending the standard
   stochastic death approach. We showed the possibility to estimate
   properly the toxicity parameters together with inter-organisms
   differences of sensitivity for at least one of these parameters (here
   the threshold for effect). For the daphnids, there was no improvement of
   using the extended model, which confirms the expected low variability
   among genetically identical individuals. For all the other datasets, our
   model outperformed the standard approach without accounting for
   differences of sensitivity. We estimated coefficients of variations in
   the distribution of the logarithm of the threshold from 44\% to 4\% and
   showed, for chironomids, a decrease of inter-individual differences of
   sensitivity with the age of the larvae. All standard threshold estimates
   were close but above the medium value of the distribution in the new
   approach, which means that a concentration equal to the standard
   threshold would ultimately result in the death of more than half of the
   exposed organisms. A more relevant parameter, such as the concentration
   protecting 95\% of the population, would be 2-4 times inferior to the
   standard threshold. (C) 2012 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.chemosphere.2012.04.021}},
ISSN = {{0045-6535}},
ResearcherID-Numbers = {{Beaudouin, Remy/H-1491-2012
   Zeman, Florence/O-1135-2015
   INERIS, Ineris/B-5682-2009}},
ORCID-Numbers = {{Beaudouin, Remy/0000-0002-2855-1571
   }},
Unique-ID = {{ISI:000307158600011}},
}

@article{ ISI:000312101700009,
Author = {Castelli, Elena and Parasiewicz, Piotr and Rogers, Joseph N.},
Title = {{Use of Frequency and Duration Analysis for the Determination of Thermal
   Habitat Thresholds: Application for the Conservation of Alasmidonta
   heterodon in the Delaware River}},
Journal = {{JOURNAL OF ENVIRONMENTAL ENGINEERING-ASCE}},
Year = {{2012}},
Volume = {{138}},
Number = {{8}},
Pages = {{886-892}},
Month = {{AUG}},
Abstract = {{A novel approach for setting thermal habitat recommendations for an
   endangered aquatic species will be proposed in this paper. The method,
   uniform continuous above-threshold (UCAT) analysis, evaluates the
   duration and frequency of continuous events in which the temperature is
   higher than a specified value and identifies temperature levels that,
   because of their rare occurrence in the past, can be considered stressor
   thresholds. The UCAT analysis was applied to set thermal habitat
   recommendations for Alasmidonta heterodon, an endangered mussel species
   in the Upper Delaware River. It was found that a maximum daily water
   temperature of 26.5 degrees C lasting for more than 7 days is a rare
   event in the A. heterodon population centers in Upper Delaware River and
   consequently this condition should be avoided by adjusting already
   regulated cold-water releases from upstream reservoirs. Knowledge of
   temperature thresholds and their relationship with persistent low-flow
   periods provided further decision support for the development of flow
   management recommendations for species protection. The case study of A.
   heterodon in the Upper Delaware River demonstrates that in the absence
   of laboratory research on physiological temperature tolerance of a
   species, UCAT analysis provides an effective way to approximate habitat
   conditions that fall within thermal tolerances. Because of quick and
   easy preparation and analysis, this method may also have a broader
   application to habitat studies of other animal groups. DOI:
   10.1061/(ASCE)EE.1943-7870.0000520. (C) 2012 American Society of Civil
   Engineers.}},
DOI = {{10.1061/(ASCE)EE.1943-7870.0000520}},
ISSN = {{0733-9372}},
ORCID-Numbers = {{PARASIEWICZ, Piotr/0000-0003-1672-3759}},
Unique-ID = {{ISI:000312101700009}},
}

@article{ ISI:000327303000004,
Author = {Grace, James B. and Schoolmaster, Jr., Donald R. and Guntenspergen,
   Glenn R. and Little, Amanda M. and Mitchell, Brian R. and Miller,
   Kathryn M. and Schweiger, E. William},
Title = {{Guidelines for a graph-theoretic implementation of structural equation
   modeling}},
Journal = {{ECOSPHERE}},
Year = {{2012}},
Volume = {{3}},
Number = {{8}},
Month = {{AUG}},
Abstract = {{Structural equation modeling (SEM) is increasingly being chosen by
   researchers as a framework for gaining scientific insights from the
   quantitative analyses of data. New ideas and methods emerging from the
   study of causality, influences from the field of graphical modeling, and
   advances in statistics are expanding the rigor, capability, and even
   purpose of SEM. Guidelines for implementing the expanded capabilities of
   SEM are currently lacking. In this paper we describe new developments in
   SEM that we believe constitute a third-generation of the methodology.
   Most characteristic of this new approach is the generalization of the
   structural equation model as a causal graph. In this generalization,
   analyses are based on graph theoretic principles rather than analyses of
   matrices. Also, new devices such as metamodels and causal diagrams, as
   well as an increased emphasis on queries and probabilistic reasoning,
   are now included. Estimation under a graph theory framework permits the
   use of Bayesian or likelihood methods. The guidelines presented start
   from a declaration of the goals of the analysis. We then discuss how
   theory frames the modeling process, requirements for causal
   interpretation, model specification choices, selection of estimation
   method, model evaluation options, and use of queries, both to summarize
   retrospective results and for prospective analyses.
   The illustrative example presented involves monitoring data from
   wetlands on Mount Desert Island, home of Acadia National Park. Our
   presentation walks through the decision process involved in developing
   and evaluating models, as well as drawing inferences from the resulting
   prediction equations. In addition to evaluating hypotheses about the
   connections between human activities and biotic responses, we illustrate
   how the structural equation (SE) model can be queried to understand how
   interventions might take advantage of an environmental threshold to
   limit Typha invasions.
   The guidelines presented provide for an updated definition of the SEM
   process that subsumes the historical matrix approach under a
   graph-theory implementation. The implementation is also designed to
   permit complex specifications and to be compatible with various
   estimation methods. Finally, they are meant to foster the use of
   probabilistic reasoning in both retrospective and prospective
   considerations of the quantitative implications of the results.}},
DOI = {{10.1890/ES12-00048.1}},
Article-Number = {{UNSP 73}},
ISSN = {{2150-8925}},
ORCID-Numbers = {{Schoolmaster Jr., Donald/0000-0003-0910-4458}},
Unique-ID = {{ISI:000327303000004}},
}

@article{ ISI:000306067000005,
Author = {Inaba, Hisashi},
Title = {{On a new perspective of the basic reproduction number in heterogeneous
   environments}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2012}},
Volume = {{65}},
Number = {{2}},
Pages = {{309-348}},
Month = {{AUG}},
Abstract = {{Although its usefulness and possibility of the well-known definition of
   the basic reproduction number R (0) for structured populations by
   Diekmann, Heesterbeek and Metz (J Math Biol 28:365-382, 1990) (the DHM
   definition) have been widely recognized mainly in the context of
   epidemic models, originally it deals with population dynamics in a
   constant environment, so it cannot be applied to formulate the threshold
   principle for population growth in time-heterogeneous environments.
   Since the mid-1990s, several authors proposed some ideas to extend the
   definition of R (0) to the case of a periodic environment. In
   particular, the definition of R (0) in a periodic environment by BacaA
   << r and Guernaoui (J Math Biol 53:421-436, 2006) (the BG definition) is
   most important, because their definition of periodic R (0) can be
   interpreted as the asymptotic per generation growth rate, which is an
   essential feature of the DHM definition. In this paper, we introduce a
   new definition of R (0) based on the generation evolution operator
   (GEO), which has intuitively clear biological meaning and can be applied
   to structured populations in any heterogeneous environment. Using the
   generation evolution operator, we show that the DHM definition and the
   BG definition completely allow the generational interpretation and, in
   those two cases, the spectral radius of GEO equals the spectral radius
   of the next generation operator, so it gives the basic reproduction
   number. Hence the new definition is an extension of the DHM definition
   and the BG definition. Finally we prove a weak sign relation that if the
   average Malthusian parameter exists, it is nonnegative when R (0) > 1
   and it is nonpositive when R (0) < 1.}},
DOI = {{10.1007/s00285-011-0463-z}},
ISSN = {{0303-6812}},
EISSN = {{1432-1416}},
Unique-ID = {{ISI:000306067000005}},
}

@article{ ISI:000305687400010,
Author = {Kirschbaum, Dalia Bach and Adler, Robert and Hong, Yang and Kumar, Sujay
   and Peters-Lidard, Christa and Lerner-Lam, Arthur},
Title = {{Advances in landslide nowcasting: evaluation of a global and regional
   modeling approach}},
Journal = {{ENVIRONMENTAL EARTH SCIENCES}},
Year = {{2012}},
Volume = {{66}},
Number = {{6}},
Pages = {{1683-1696}},
Month = {{JUL}},
Abstract = {{The increasing availability of remotely sensed data offers a new
   opportunity to address landslide hazard assessment at larger spatial
   scales. A prototype global satellite-based landslide hazard algorithm
   has been developed to identify areas that may experience landslide
   activity. This system combines a calculation of static landslide
   susceptibility with satellite-derived rainfall estimates and uses a
   threshold approach to generate a set of `nowcasts' that classify
   potentially hazardous areas. A recent evaluation of this algorithm
   framework found that while this tool represents an important first step
   in larger-scale near real-time landslide hazard assessment efforts, it
   requires several modifications before it can be fully realized as an
   operational tool. This study draws upon a prior work's recommendations
   to develop a new approach for considering landslide susceptibility and
   hazard at the regional scale. This case study calculates a regional
   susceptibility map using remotely sensed and in situ information and a
   database of landslides triggered by Hurricane Mitch in 1998 over four
   countries in Central America. The susceptibility map is evaluated with a
   regional rainfall intensity-duration triggering threshold and results
   are compared with the global algorithm framework for the same event.
   Evaluation of this regional system suggests that this empirically based
   approach provides one plausible way to approach some of the data and
   resolution issues identified in the global assessment. The presented
   methodology is straightforward to implement, improves upon the global
   approach, and allows for results to be transferable between regions. The
   results also highlight several remaining challenges, including the
   empirical nature of the algorithm framework and adequate information for
   algorithm validation. Conclusions suggest that integrating additional
   triggering factors such as soil moisture may help to improve algorithm
   performance accuracy. The regional algorithm scenario represents an
   important step forward in advancing regional and global-scale landslide
   hazard assessment.}},
DOI = {{10.1007/s12665-011-0990-3}},
ISSN = {{1866-6280}},
ResearcherID-Numbers = {{Hong, Yang/D-5132-2009
   Kumar, Sujay/B-8142-2015
   Kirschbaum, Dalia/F-9596-2012
   Peters-Lidard, Christa/E-1429-2012}},
ORCID-Numbers = {{Hong, Yang/0000-0001-8720-242X
   Kumar, Sujay/0000-0001-8797-9482
   Kirschbaum, Dalia/0000-0001-5547-2839
   Peters-Lidard, Christa/0000-0003-1255-2876}},
Unique-ID = {{ISI:000305687400010}},
}

@article{ ISI:000311625900013,
Author = {Sethi, Suresh Andrew and Dalton, Mike},
Title = {{Risk Measures for Natural Resource Management: Description, Simulation
   Testing, and R Code With Fisheries Examples}},
Journal = {{JOURNAL OF FISH AND WILDLIFE MANAGEMENT}},
Year = {{2012}},
Volume = {{3}},
Number = {{1}},
Pages = {{150-157}},
Month = {{JUN}},
Abstract = {{Traditional measures that quantify variation in natural resource systems
   include both upside and downside deviations as contributing to
   variability, such as standard deviation or the coefficient of variation.
   Here we introduce three risk measures from investment theory, which
   quantify variability in natural resource systems by analyzing either
   upside or downside outcomes and typical or extreme outcomes separately:
   semideviation, conditional value-at-risk, and probability of ruin. Risk
   measures can be custom tailored to frame variability as a performance
   measure in terms directly meaningful to specific management objectives,
   such as presenting risk as harvest expected in an extreme bad year, or
   by characterizing risk as the probability of fishery escapement falling
   below a prescribed threshold. In this paper, we present formulae,
   empirical examples from commercial fisheries, and R code to calculate
   three risk measures. In addition, we evaluated risk measure performance
   with simulated data, and we found that risk measures can provide
   unbiased estimates at small sample sizes. By decomposing complex
   variability into quantitative metrics, we envision risk measures to be
   useful across a range of wildlife management scenarios, including policy
   decision analyses, comparative analyses across systems, and tracking the
   state of natural resource systems through time.}},
DOI = {{10.3996/122011-JFWM-072}},
ISSN = {{1944-687X}},
Unique-ID = {{ISI:000311625900013}},
}

@article{ ISI:000304537000002,
Author = {Peng, Yu and Leung, Henry C. M. and Yiu, S. M. and Chin, Francis Y. L.},
Title = {{IDBA-UD: a de novo assembler for single-cell and metagenomic sequencing
   data with highly uneven depth}},
Journal = {{BIOINFORMATICS}},
Year = {{2012}},
Volume = {{28}},
Number = {{11}},
Pages = {{1420-1428}},
Month = {{JUN 1}},
Abstract = {{Motivation: Next-generation sequencing allows us to sequence reads from
   a microbial environment using single-cell sequencing or metagenomic
   sequencing technologies. However, both technologies suffer from the
   problem that sequencing depth of different regions of a genome or
   genomes from different species are highly uneven. Most existing genome
   assemblers usually have an assumption that sequencing depths are even.
   These assemblers fail to construct correct long contigs.
   Results: We introduce the IDBA-UD algorithm that is based on the de
   Bruijn graph approach for assembling reads from single-cell sequencing
   or metagenomic sequencing technologies with uneven sequencing depths.
   Several non-trivial techniques have been employed to tackle the
   problems. Instead of using a simple threshold, we use multiple
   depthrelative thresholds to remove erroneous k-mers in both low-depth
   and high-depth regions. The technique of local assembly with paired-end
   information is used to solve the branch problem of low-depth short
   repeat regions. To speed up the process, an error correction step is
   conducted to correct reads of high-depth regions that can be aligned to
   highconfident contigs. Comparison of the performances of IDBA-UD and
   existing assemblers (Velvet, Velvet-SC, SOAPdenovo and Meta-IDBA) for
   different datasets, shows that IDBA-UD can reconstruct longer contigs
   with higher accuracy.}},
DOI = {{10.1093/bioinformatics/bts174}},
ISSN = {{1367-4803}},
Unique-ID = {{ISI:000304537000002}},
}

@article{ ISI:000301706300013,
Author = {Ashton, Nicholas N. and Taggart, Daniel S. and Stewart, Russell J.},
Title = {{Silk tape nanostructure and silk gland anatomy of trichoptera}},
Journal = {{BIOPOLYMERS}},
Year = {{2012}},
Volume = {{97}},
Number = {{6, SI}},
Pages = {{432-445}},
Month = {{JUN}},
Abstract = {{Caddisflys (order Trichoptera) construct elaborate protective shelters
   and food harvesting nets with underwater adhesive silk. The silk fiber
   resembles a nanostructured tape composed of thousands of nanofibrils
   (similar to similar to 120 nm) oriented with the major axis of the
   fiber, which in turn are composed of spherical subunits. Weaker lateral
   interactions between nanofibrils allow the fiber to conform to surface
   topography and increase contact area. Highly phosphorylated (pSX)4
   motifs in H-fibroin blocks of positively charged basic residues are
   conserved across all three suborders of Trichoptera. Electrostatic
   interactions between the oppositely charged motifs could drive
   liquidliquid phase separation of silk fiber precursors into a complex
   coacervates mesophase. Accessibility of phosphoserine to an
   anti-phosphoserine antibody is lower in the lumen of the silk gland
   storage region compared to the nascent fiber formed in the anterior
   conducting channel. The phosphorylated motifs may serve as a marker for
   the structural reorganization of the silk precursor mesophase into
   strongly refringent fibers. The structural change occurring at the
   transition into the conducting channel makes this region of special
   interest. Fiber formation from polyampholytic silk proteins in
   Trichoptera may suggest a new approach to create synthetic silk analogs
   from water-soluble precursors. (C) 2011 Wiley Periodicals, Inc.
   Biopolymers 97: 432445, 2012.}},
DOI = {{10.1002/bip.21720}},
ISSN = {{0006-3525}},
Unique-ID = {{ISI:000301706300013}},
}

@article{ ISI:000304053300006,
Author = {Pairo, Erola and Maynou, Joan and Marco, Santiago and Perera, Alexandre},
Title = {{A subspace method for the detection of transcription factor binding
   sites}},
Journal = {{BIOINFORMATICS}},
Year = {{2012}},
Volume = {{28}},
Number = {{10}},
Pages = {{1328-1335}},
Month = {{MAY 15}},
Abstract = {{Results: In this article, we introduce a novel motif finding method
   which constructs a subspace based on the covariance of numerical DNA
   sequences. When a candidate sequence is projected into the modeled
   subspace, a threshold in the Q-residuals confidence allows us to predict
   whether this sequence is a binding site. Using the TRANSFAC and JASPAR
   databases, we compared our Q-residuals detector with existing PSSM
   methods. In most of the studied TF binding sites, the Q-residuals
   detector performs significantly better and faster than MATCH and MAST.
   As compared with Motifscan, a method which takes into account
   interdependences, the performance of the Q-residuals detector is better
   when the number of available sequences is small.}},
DOI = {{10.1093/bioinformatics/bts147}},
ISSN = {{1367-4803}},
ResearcherID-Numbers = {{marco, santiago/A-6111-2009
   Perera Lluna, Alexandre/R-8546-2018}},
ORCID-Numbers = {{marco, santiago/0000-0003-2663-2965
   Perera Lluna, Alexandre/0000-0001-6427-851X}},
Unique-ID = {{ISI:000304053300006}},
}

@article{ ISI:000303811800003,
Author = {Terashi, Genki and Shibuya, Tetsuo and Takeda-Shitaka, Mayuko},
Title = {{LB3D: A Protein Three-Dimensional Substructure Search Program Based on
   the Lower Bound of a Root Mean Square Deviation Value}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2012}},
Volume = {{19}},
Number = {{5}},
Pages = {{493-503}},
Month = {{MAY}},
Abstract = {{Searching for protein structure-function relationships using
   three-dimensional (3D) structural coordinates represents a fundamental
   approach for determining the function of proteins with unknown
   functions. Since protein structure databases are rapidly growing in
   size, the development of a fast search method to find similar protein
   substructures by comparison of protein 3D structures is essential. In
   this article, we present a novel protein 3D structure search method to
   find all substructures with root mean square deviations (RMSDs) to the
   query structure that are lower than a given threshold value. Our new
   algorithm runs in O(m + N/m(0.5)) time, after O(N log N) preprocessing,
   where N is the database size and m is the query length. The new method
   is 1.8-41.6 times faster than the practically best known O(N) algorithm,
   according to computational experiments using a huge database (i.e., >
   20,000,000 C-alpha coordinates).}},
DOI = {{10.1089/cmb.2011.0230}},
ISSN = {{1066-5277}},
EISSN = {{1557-8666}},
Unique-ID = {{ISI:000303811800003}},
}

@article{ ISI:000303811800008,
Author = {Ibrahim, Maysson Al-Haj and Jassim, Sabah and Cawthorne, Michael Anthony
   and Langlands, Kenneth},
Title = {{A Topology-Based Score for Pathway Enrichment}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2012}},
Volume = {{19}},
Number = {{5}},
Pages = {{563-573}},
Month = {{MAY}},
Abstract = {{Investigators require intuitive tools to rationalize complex datasets
   generated by transcriptional profiling experiments. Pathway analysis
   methods, in which differentially expressed genes are mapped to databases
   of reference pathways to facilitate assessment of relative enrichment,
   lead investigators more effectively to biologically testable hypotheses.
   However, once a set of differentially expressed genes is isolated,
   pathway analysis approaches tend to ignore rich gene expression
   information and, moreover, do not exploit relationships between
   transcripts. In this article, we report the development of a new method
   in which both pathway topology and the magnitude of gene expression
   changes inform the scoring system, thereby providing a powerful filter
   in the enrichment of biologically relevant information. When four sample
   datasets were evaluated with this method, literature mining confirmed
   that those pathways germane to the physiological process under
   investigation were highlighted by our method relative to z-score
   overrepresentation calculations. Moreover, non-relevant processes were
   downgraded using the method described herein. The inclusion of
   expression and topological data in the calculation of a pathway
   regulation score (PRS) facilitated discrimination of key processes in
   real biological datasets. Specifically, by combining fold-change data
   for those transcripts exceeding a significance threshold, and by taking
   into account the potential for altered gene expression to impact upon
   downstream transcription, one may readily identify those pathways most
   relevant to pathophysiological processes.}},
DOI = {{10.1089/cmb.2011.0182}},
ISSN = {{1066-5277}},
Unique-ID = {{ISI:000303811800008}},
}

@article{ ISI:000303290500008,
Author = {Salat, Robert and Salat, Kinga},
Title = {{New approach to predicting proconvulsant activity with the use of
   Support Vector Regression}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2012}},
Volume = {{42}},
Number = {{5}},
Pages = {{575-581}},
Month = {{MAY}},
Abstract = {{Antiepileptic drugs are commonly used for many therapeutic indications,
   including epilepsy, neuropathic pain, bipolar disorder and anxiety.
   Accumulating data suggests that many of them may lower the seizure
   threshold in men. In the present paper we deal with the possibility of
   using Support Vector Regression (SVR) to forecast the proconvulsant
   activity of compounds exerting anticonvulsant activity in the
   electroconvulsive threshold test in mice. A new approach to forecast
   this drug-related toxic effect by means of the support vector machine
   (SVM) in the regression mode is discussed below. The efficacy of this
   mathematical method is compared to the results obtained in vivo. Since
   SVR investigates the anticonvulsant activity of the compounds more
   thoroughly than it is possible using animal models, this method seems to
   be a very helpful tool for predicting additional dose ranges at which
   maximum anticonvulsant activity without toxic effects is observed. Good
   generalizing properties of SVR allow to assess the therapeutic dose
   range and toxicity threshold. Noteworthy, this method is very
   interesting for ethical reasons as this mathematical model enables to
   limit the use of living animals during the anticonvulsant screening
   process. (C) 2012 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiomed.2012.02.001}},
ISSN = {{0010-4825}},
Unique-ID = {{ISI:000303290500008}},
}

@article{ ISI:000303440400011,
Author = {Bauer-Mehren, Anna and van Mullingen, Erik M. and Avillach, Paul and del
   Carmen Carrascosa, Maria and Garcia-Serna, Ricard and Pinero, Janet and
   Singh, Bharat and Lopes, Pedro and Oliveira, Jose L. and Diallo, Gayo
   and Helgee, Ernst Ahlberg and Boyer, Scott and Mestres, Jordi and Sanz,
   Ferran and Kors, Jan A. and Furlong, Laura I.},
Title = {{Automatic Filtering and Substantiation of Drug Safety Signals}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2012}},
Volume = {{8}},
Number = {{4}},
Month = {{APR}},
Abstract = {{Drug safety issues pose serious health threats to the population and
   constitute a major cause of mortality worldwide. Due to the prominent
   implications to both public health and the pharmaceutical industry, it
   is of great importance to unravel the molecular mechanisms by which an
   adverse drug reaction can be potentially elicited. These mechanisms can
   be investigated by placing the pharmaco-epidemiologically detected
   adverse drug reaction in an information-rich context and by exploiting
   all currently available biomedical knowledge to substantiate it. We
   present a computational framework for the biological annotation of
   potential adverse drug reactions. First, the proposed framework
   investigates previous evidences on the drug-event association in the
   context of biomedical literature (signal filtering). Then, it seeks to
   provide a biological explanation (signal substantiation) by exploring
   mechanistic connections that might explain why a drug produces a
   specific adverse reaction. The mechanistic connections include the
   activity of the drug, related compounds and drug metabolites on protein
   targets, the association of protein targets to clinical events, and the
   annotation of proteins (both protein targets and proteins associated
   with clinical events) to biological pathways. Hence, the workflows for
   signal filtering and substantiation integrate modules for literature and
   database mining, in silico drug-target profiling, and analyses based on
   gene-disease networks and biological pathways. Application examples of
   these workflows carried out on selected cases of drug safety signals are
   discussed. The methodology and workflows presented offer a novel
   approach to explore the molecular mechanisms underlying adverse drug
   reactions.}},
DOI = {{10.1371/journal.pcbi.1002457}},
Article-Number = {{e1002457}},
ISSN = {{1553-7358}},
ResearcherID-Numbers = {{Sanz, Ferran/B-3852-2009
   Gasull, Martina/A-6630-2013
   Avillach, Paul/H-8858-2012
   Pinero, Janet/O-6583-2016
   Mestres, Jordi/B-3673-2009
   Oliveira, Jose Luis/A-2223-2012
   Menezes Oliveira, Joselina Luzia/D-1416-2012
   }},
ORCID-Numbers = {{Sanz, Ferran/0000-0002-7534-7661
   Pinero, Janet/0000-0003-1244-7654
   Mestres, Jordi/0000-0002-5202-4501
   Oliveira, Jose Luis/0000-0002-6672-6176
   Menezes Oliveira, Joselina Luzia/0000-0002-3701-1323
   Furlong, Laura I./0000-0002-9383-528X
   Lopes, Pedro/0000-0001-5330-6562
   van Mulligen, Erik/0000-0003-1377-9386}},
Unique-ID = {{ISI:000303440400011}},
}

@article{ ISI:000302538500003,
Author = {Ouellette, Marie-Helene and Legendre, Pierre and Borcard, Daniel},
Title = {{Cascade multivariate regression tree: a novel approach for modelling
   nested explanatory sets}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2012}},
Volume = {{3}},
Number = {{2}},
Pages = {{234-244}},
Month = {{APR}},
Abstract = {{1. Ecological data analysis frequently calls for the assessment of the
   relationship between species composition and a set of explanatory
   variables of interest. The assessment may have to be pursued while
   taking into account the influence of another set of explanatory
   variables. The hypothetical nature and structure of the influence of an
   explanatory set on the effect of a distinct explanatory set guides the
   proper choice of modelling methodology for a combined explanatory
   assessment. 2. Here, we describe a framework where the relationship
   between the response data and a main set of explanatory variables is not
   linear. It may, for example, take the form of abrupt changes in the
   response following thresholds of the explanatory variables, or any other
   nonlinearizable relationship. The influence of a second set of
   explanatory variables is determined a posteriori, after the influence of
   the main explanatory set has been taken into account. This is useful
   when one of the sets is thought to have an effect that varies as a
   function of the other. 3. To achieve this type of assessment, we propose
   a cascade of multivariate regression trees ( CMRT). We decompose the
   total dispersion of a response matrix between two explanatory data sets
   in a nested manner. By handling each leaf ( group) resulting from the
   first-level multivariate regression tree ( MRT) analysis as separate
   independent data sets in following analyses, we can separate the
   explanatory power of the first partition from those of the subordinate
   partitions computed using a second explanatory set. A preliminary
   biological hypothesis will guide the choice of which set of explanatory
   variables should be used to compute the main partition. The method could
   be extended to more than two explanatory data sets whose effects on the
   response data are hierarchical. 4. Cascade of multivariate regression
   trees allows the users to impose a nested structure to their causal
   hypotheses in MRT analysis. To illustrate this new procedure, we use the
   well-known and readily available Doubs fish and oribatid mite data sets
   and provide the necessary R functions in a package available onCRAN(
   http:// cran. r-project. org).}},
DOI = {{10.1111/j.2041-210X.2011.00171.x}},
ISSN = {{2041-210X}},
ResearcherID-Numbers = {{Legendre, Pierre/C-8279-2013}},
ORCID-Numbers = {{Legendre, Pierre/0000-0002-3838-3305}},
Unique-ID = {{ISI:000302538500003}},
}

@article{ ISI:000301890100012,
Author = {Sample, Vedangi and DiPilato, Lisa M. and Yang, Jason H. and Ni, Qiang
   and Saucerman, Jeffrey J. and Zhang, Jin},
Title = {{Regulation of nuclear PKA revealed by spatiotemporal manipulation of
   cyclic AMP}},
Journal = {{NATURE CHEMICAL BIOLOGY}},
Year = {{2012}},
Volume = {{8}},
Number = {{4}},
Pages = {{375-382}},
Month = {{APR}},
Abstract = {{Understanding how specific cyclic AMP (cAMP) signals are organized and
   relayed to their effectors in different compartments of the cell to
   achieve functional specificity requires molecular tools that allow
   precise manipulation of cAMP in these compartments. Here we characterize
   a new method using bicarbonate-activatable and genetically targetable
   soluble adenylyl cyclase to control the location, kinetics and magnitude
   of the cAMP signal. Using this live-cell cAMP manipulation in
   conjunction with fluorescence imaging and mechanistic modeling, we
   uncovered the activation of a resident pool of protein kinase A (PKA)
   holoenzyme in the nuclei of HEK-293 cells, modifying the existing dogma
   of cAMP-PKA signaling in the nucleus. Furthermore, we show that
   phosphodiesterases and A-kinase anchoring proteins (AKAPs) are critical
   in shaping nuclear PKA responses. Collectively, our data suggest a new
   model in which AKAP-localized phosphodiesterases tune an activation
   threshold for nuclear PKA holoenzyme, thereby converting spatially
   distinct second messenger signals to temporally controlled nuclear
   kinase activity.}},
DOI = {{10.1038/NCHEMBIO.799}},
ISSN = {{1552-4450}},
EISSN = {{1552-4469}},
ORCID-Numbers = {{Saucerman, Jeffrey/0000-0001-9464-8374
   Yang, Jason/0000-0003-0921-4657}},
Unique-ID = {{ISI:000301890100012}},
}

@article{ ISI:000301177600006,
Author = {Inaba, Hisashi},
Title = {{THE MALTHUSIAN PARAMETER AND R-0 FOR HETEROGENEOUS POPULATIONS IN
   PERIODIC ENVIRONMENTS}},
Journal = {{MATHEMATICAL BIOSCIENCES AND ENGINEERING}},
Year = {{2012}},
Volume = {{9}},
Number = {{2}},
Pages = {{313-346}},
Month = {{APR}},
Abstract = {{Since the classical stable population theory in demography by Sharpe and
   Lotka, the sign relation sign(lambda(0)) - sign(R-0 - 1) between the
   basic reproduction number R-0 and the Malthusian parameter (the
   intrinsic rate of natural increase) lambda(0) has played a central role
   in population theory and its applications, because it connects
   individual's average reproductivity described by life cycle parameters
   to growth character of the whole population. Since R-0 is originally
   defined for linear population evolution process in a constant
   environment, it is an important extension if we could formulate the same
   kind of threshold principle for population growth in time-heterogeneous
   environments.
   Since the mid-1990s, several authors proposed some ideas to extend the
   definition of R-0 so that it can be applied to population dynamics in
   periodic environments. In particular, the definition of R-0 in a
   periodic environment by Bacaer and Guernaoui (J. Math. Biol. 53, 2006)
   is most important, because their definition of R-0 in a periodic
   environment can be interpreted as the asymptotic per generation growth
   rate, so from the generational point of view, it can be seen as a direct
   extension of the most successful definition of R-0 in a constant
   environment by Diekmann, Heesterbeek and Metz (J. Math. Biol. 28, 1990).
   In this paper, we propose a new approach to establish the sign relation
   between R-0 and the Malthusian parameter lambda(0) for linear structured
   population dynamics in a periodic environment. Our arguments depend on
   the uniform primitivity of positive evolutionary system, which leads the
   weak ergodicity and the existence of exponential solution in periodic
   environments. For typical finite and infinite dimensional linear
   population models, we prove that a positive exponential solution exists
   and the sign relation holds between the Malthusian parameter, which is
   defined as the exponent of the exponential solution, and R-0 given by
   the spectral radius of the next generation operator by Bacaer and
   Guernaoui's definition.}},
DOI = {{10.3934/mbe.2012.9.313}},
ISSN = {{1547-1063}},
EISSN = {{1551-0018}},
Unique-ID = {{ISI:000301177600006}},
}

@article{ ISI:000301443200008,
Author = {Leroy, Boris and Petillon, Julien and Gallon, Regis and Canard, Alain
   and Ysnel, Frederic},
Title = {{Improving occurrence-based rarity metrics in conservation studies by
   including multiple rarity cut-off points}},
Journal = {{INSECT CONSERVATION AND DIVERSITY}},
Year = {{2012}},
Volume = {{5}},
Number = {{2}},
Pages = {{159-168}},
Month = {{MAR}},
Abstract = {{. 1. This study aims to develop a new method for assigning rarity
   weights to species in evaluations of the relative rarity of arthropod
   assemblages in conservation/monitoring studies. 2. A flexible
   characteristic was included in the rarity weighting method by
   introducing the possibility of fitting the method to a rarity cut-off
   point defined as the threshold of occurrence below which species are
   considered as being rare. This allows calculation of a rarity metric
   (index of relative rarity IRR) with multiple rarity cut-off points. 3.
   The proposed weighting method was used and compared with three
   previously proposed methods in a theoretical analysis. IRR values were
   then calculated for spider assemblages of a National Nature Reserve in
   France. Two methods of rankings were proposed: a local ranking between
   sites of the Nature Reserve, and a regional ranking in comparison to a
   reference database. 4. The proposed weighting method consistently
   weighted species according to the chosen rarity cut-offs. Species
   weights were less biased toward common species and rare species weights
   were less dispersed than with previous methods. Assemblages were
   consistently ranked according to the rarity of spiders in each
   assemblage. The index showed different patterns of rarity in assemblages
   which could not be detected by previous rarity metrics. 5. This method
   provides an improved understanding of assemblage rarity patterns
   relative to previous methods and can be consistently applied to other
   arthropod taxa in other geographic area and/ or spatial scales.}},
DOI = {{10.1111/j.1752-4598.2011.00148.x}},
ISSN = {{1752-458X}},
ResearcherID-Numbers = {{Leroy, Boris/G-4076-2013
   }},
ORCID-Numbers = {{Leroy, Boris/0000-0002-7686-4302
   Petillon, Julien/0000-0002-7611-5133}},
Unique-ID = {{ISI:000301443200008}},
}

@article{ ISI:000301910800002,
Author = {Eberle, Claudia and Ament, Christoph},
Title = {{Identifiability and online estimation of diagnostic parameters with in
   the glucose insulin homeostasis}},
Journal = {{BIOSYSTEMS}},
Year = {{2012}},
Volume = {{107}},
Number = {{3}},
Pages = {{135-141}},
Month = {{MAR}},
Abstract = {{Today, diagnostic decisions about pre-diabetes or diabetes are made
   using static threshold rules for the measured plasma glucose. In order
   to develop an alternative diagnostic approach, dynamic models as the
   Minimal Model may be deployed. We present a novel method to analyze the
   identifiability of model parameters based on the interpretation of the
   empirical observability Gramian. This allows a unifying view of both,
   the observability of the system's states (with dynamics) and the
   identifiability of the system's parameters (without dynamics). We give
   an iterative algorithm, in order to find an optimized set of states and
   parameters to be estimated. For this set, estimation results using an
   Unscented Kalman Filter (UKF) are presented.
   Two parameters are of special interest for diagnostic purposes: the
   glucose effectiveness S-G characterizes the ability of plasma glucose
   clearance, and the insulin sensitivity S-l quantifies the impact from
   the plasma insulin to the interstitial insulin subsystem. Applying the
   identifiability analysis to the trajectories of the insulin glucose
   system during an intravenous glucose tolerance test (IVGTT) shows the
   following result: (1) if only plasma glucose G(t) is measured, plasma
   insulin 1(t) and S-G can be estimated, but not S-l. (2) If plasma
   insulin 1(t) is captured additionally, identifiability is improved
   significantly such that up to four model parameters can be estimated
   including S-l. (3) The situation of the first case can be improved, if a
   controlled external dosage of insulin is applied. Then, parameters of
   the insulin subsystem can be identified approximately from measurement
   of plasma glucose G(t) only. (C) 2011 Elsevier Ireland Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.biosystems.2011.11.003}},
ISSN = {{0303-2647}},
EISSN = {{1872-8324}},
ResearcherID-Numbers = {{Ament, Christoph/O-6076-2017
   Eberle, Claudia/J-8855-2017}},
ORCID-Numbers = {{Ament, Christoph/0000-0002-6396-4355
   Eberle, Claudia/0000-0001-7878-2020}},
Unique-ID = {{ISI:000301910800002}},
}

@article{ ISI:000297564600019,
Author = {Serramo Lopez, Luiz Carlos and de Aguiar Fracasso, Maria Paula and
   Mesquita, Daniel Oliveira and Torre Palma, Alexandre Ramlo and Riul,
   Pablo},
Title = {{The relationship between percentage of singletons and sampling effort: A
   new approach to reduce the bias of richness estimates}},
Journal = {{ECOLOGICAL INDICATORS}},
Year = {{2012}},
Volume = {{14}},
Number = {{1}},
Pages = {{164-169}},
Month = {{MAR}},
Abstract = {{Estimate the richness of a community with accuracy despite differences
   in sampling effort is a key aspect to monitoring high diverse
   ecosystems. We compiled a worldwide multitaxa database, comprising 185
   communities, in order to study the relationship between the percentage
   of species represented by one individual (singletons) and the intensity
   of sampling (number of individuals divided by the number of species
   sampled). The database was used to empirically adjust a correction
   factor to improve the performance of non-parametrical estimators under
   conditions of low sampling effort. The correction factor was tested on
   seven estimators (Chao1, Chao2, Jack1, Jack2, ACE, ICE and Bootstrap).
   The correction factor was able to reduce the bias of all estimators
   tested under conditions of undersampling, while converging to the
   original uncorrected values at higher intensities. Our findings led us
   to recommend the threshold of 20 individuals/species, or less than 21\%
   of singletons, as a minimum sampling effort to produce reliable richness
   estimates of high diverse ecosystems using corrected non-parametric
   estimators. This threshold rise for 50 individuals/species if
   non-corrected estimators are used which implies in an economy of 60\% of
   sampling effort if the correction factor is used. (C) 2011 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.ecolind.2011.07.012}},
ISSN = {{1470-160X}},
ResearcherID-Numbers = {{Lopez, Luiz/K-1739-2015
   Lopez, Luiz/H-9004-2012
   Riul, Pablo/H-5843-2012
   Fracasso, Maria/K-3551-2015
   Mesquita, Daniel/I-5007-2012}},
ORCID-Numbers = {{Riul, Pablo/0000-0003-4035-1975
   Fracasso, Maria/0000-0001-7344-4532
   Mesquita, Daniel/0000-0002-8174-6837}},
Unique-ID = {{ISI:000297564600019}},
}

@article{ ISI:000304802500027,
Author = {Sammartino, Stephane and Michel, Eric and Capowiez, Yvan},
Title = {{A Novel Method to Visualize and Characterize Preferential Flow in
   Undisturbed Soil Cores by Using Multislice Helical CT}},
Journal = {{VADOSE ZONE JOURNAL}},
Year = {{2012}},
Volume = {{11}},
Number = {{1}},
Month = {{FEB}},
Abstract = {{Our knowledge of preferential flow in heterogeneous unsaturated porous
   media such as soils is still limited. In this work, we introduced a
   novel method based ontime resolved three-dimensional images fast
   acquisitions using a four-slice helical computed tomography (CT). This
   method provides new quantitative information on preferential flow
   processes occurring in an unsaturated undisturbed soil. Twenty-four
   three-dimensional images were acquired every 3 min in transient flow
   regime and every 5 min in stationary flow regime during a specific
   simulated rainfall event hold inside the scanner gantry. The
   macroporosity thresholding was realized according to a linear
   combination of attenuation coefficients of the main soil phases
   ({''}mobile{''} air or water in macropores and saturated soil matrix)
   weighted by their volume fractions in voxels. Mobile water contained in
   the macropores and in the surrounding thinner pores (diffuse
   macroporosity) was identified using a specific thresholding method based
   on subtracted images. In spite of the experimental limitations-noise,
   artifacts, limited spatial and temporal resolutions that are discussed
   in the paper-the method allows identifying about 65\% of the
   infiltration water present in the core during the rainfall event.
   Although identifying water films on images is not possible when their
   volume is lower than the detection threshold, quantitative time-lapse
   slice-averaged information (fraction of active macropores and mean water
   filling of active macropores) can be obtained. We showed that even
   during a quite intense rainfall event, macropore flow was localized only
   into 30\% of the macropore network of the soil core; that is, macropores
   remained mostly unsaturated along the experiment.}},
DOI = {{10.2136/vzj2011.0100}},
ISSN = {{1539-1663}},
ResearcherID-Numbers = {{Sammartino, Stephane/L-1429-2013}},
ORCID-Numbers = {{Sammartino, Stephane/0000-0002-6541-3071}},
Unique-ID = {{ISI:000304802500027}},
}

@article{ ISI:000300729900007,
Author = {Lade, Steven J. and Gross, Thilo},
Title = {{Early Warning Signals for Critical Transitions: A Generalized Modeling
   Approach}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2012}},
Volume = {{8}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{Critical transitions are sudden, often irreversible, changes that can
   occur in a large variety of complex systems; signals that warn of
   critical transitions are therefore highly desirable. We propose a new
   method for early warning signals that integrates multiple sources of
   information and data about the system through the framework of a
   generalized model. We demonstrate our proposed approach through several
   examples, including a previously published fisheries model. We regard
   our method as complementary to existing early warning signals, taking an
   approach of intermediate complexity between model-free approaches and
   fully parameterized simulations. One potential advantage of our approach
   is that, under appropriate conditions, it may reduce the amount of time
   series data required for a robust early warning signal.}},
DOI = {{10.1371/journal.pcbi.1002360}},
Article-Number = {{e1002360}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
ResearcherID-Numbers = {{Lade, Steven/F-9496-2010
   Gross, Thilo/B-4444-2010
   }},
ORCID-Numbers = {{Gross, Thilo/0000-0002-1356-6690
   Lade, Steven/0000-0001-9719-9826}},
Unique-ID = {{ISI:000300729900007}},
}

@article{ ISI:000300926300008,
Author = {Tan, Chun Hua and He, Wei Xin and Meng, Hong Yun and Huang, Xu Guang},
Title = {{A new method based on fiber-optic sensing for the determination of
   deacetylation degree of chitosans}},
Journal = {{CARBOHYDRATE RESEARCH}},
Year = {{2012}},
Volume = {{348}},
Pages = {{64-68}},
Month = {{FEB 1}},
Abstract = {{A novel method for determining the degree of deacetylation of chitosan
   is described. This involves a two-abrupt-change of the refractive index
   of solutions. Principle of the method is that the sample reacts with
   excessive acid, and the excessive acid is measured by alkaline
   titration. The refractive index of an aqueous chitosan solution was
   monitored and recorded in the course of acid-base titration. This gives
   a titration curve having two inflexion points and the difference between
   the two points corresponds to the volume of base required to neutralize
   the ammonium groups. It was proved that flocculation did not interfere
   with the measurement. The method is found to be low-cost, precise, and
   easy to operate for industrial applications. The DD values of three
   chitosan samples obtained with this new method show good agreement with
   those yielded from H-1 NMR. Such a mechanism of refractive index
   monitoring should open up a new application in the field of
   chitosanolytic enzymes, such as chitosanase, that are important in
   bioprocesses. (C) 2011 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.carres.2011.04.047}},
ISSN = {{0008-6215}},
Unique-ID = {{ISI:000300926300008}},
}

@article{ ISI:000299243800011,
Author = {Balcan, Duygu and Vespignani, Alessandro},
Title = {{Invasion threshold in structured populations with recurrent mobility
   patterns}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2012}},
Volume = {{293}},
Pages = {{87-100}},
Month = {{JAN 21}},
Abstract = {{In this paper we develop a framework to analyze the behavior of
   contagion and spreading processes in complex subpopulation networks
   where individuals have memory of their subpopulation of origin. We
   introduce a metapopulation model in which subpopulations are connected
   through heterogeneous fluxes of individuals. The mobility process among
   communities takes into account the memory of residence of individuals
   and is incorporated with the classical susceptible-infectious-recovered
   epidemic model within each subpopulation. In order to gain analytical
   insight into the behavior of the system we use degree-block variables
   describing the heterogeneity of the subpopulation network and a
   time-scale separation technique for the dynamics of individuals. By
   considering the stochastic nature of the epidemic process we obtain the
   explicit expression of the global epidemic invasion threshold, below
   which the disease dies out before reaching a macroscopic fraction of the
   subpopulations. This threshold is not present in continuous
   deterministic diffusion models and explicitly depends on the disease
   parameters, the mobility rates, and the properties of the coupling
   matrices describing the mobility across subpopulations. The results
   presented here take a step further in offering insight into the
   fundamental mechanisms controlling the spreading of infectious diseases
   and other contagion processes across spatially structured communities.
   (C) 2011 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2011.10.010}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
ORCID-Numbers = {{Vespignani, Alessandro/0000-0003-3419-4205}},
Unique-ID = {{ISI:000299243800011}},
}

@article{ ISI:000306270100010,
Author = {Keller, Merlin and Lavielle, Marc},
Title = {{Random threshold for linear model selection, revisited}},
Journal = {{STATISTICS AND ITS INTERFACE}},
Year = {{2012}},
Volume = {{5}},
Number = {{2, SI}},
Pages = {{263-275}},
Abstract = {{In {[}11], a random thresholding method is introduced to select the
   significant, or non-null, mean terms among a collection of independent
   random variables, and applied to the problem of recovering the
   significant coefficients in non-ordered model selection. We introduce a
   simple modification which removes the dependency of the proposed
   estimator on a window parameter while maintaining its asymptotic
   properties. A simulation study suggests that both procedures compare
   favorably to standard thresholding approaches, such as multiple testing
   or model-based clustering, in terms of the binary classification risk.
   An application of the method to the problem of activation detection on
   functional magnetic resonance imaging (fMRI) data is discussed.}},
ISSN = {{1938-7989}},
Unique-ID = {{ISI:000306270100010}},
}

@article{ ISI:000305091700004,
Author = {Clement, Lieven and De Beuf, Kristof and Thas, Olivier and Vuylsteke,
   Marnik and Irizarry, Rafael A. and Crainiceanu, Ciprian M.},
Title = {{Fast Wavelet Based Functional Models for Transcriptome Analysis with
   Tiling Arrays}},
Journal = {{STATISTICAL APPLICATIONS IN GENETICS AND MOLECULAR BIOLOGY}},
Year = {{2012}},
Volume = {{11}},
Number = {{1}},
Abstract = {{For a better understanding of the biology of an organism, a complete
   description is needed of all regions of the genome that are actively
   transcribed. Tiling arrays are used for this purpose. They allow for the
   discovery of novel transcripts and the assessment of differential
   expression between two or more experimental conditions such as genotype,
   treatment, tissue, etc. In tiling array literature, many efforts are
   devoted to transcript discovery, whereas more recent developments also
   focus on differential expression. To our knowledge, however, no methods
   for tiling arrays have been described that can simultaneously assess
   transcript discovery and identify differentially expressed transcripts.
   In this paper, we adopt wavelet based functional models to the context
   of tiling arrays. The high dimensionality of the data triggered us to
   avoid inference based on Bayesian MCMC methods. Instead, we introduce a
   fast empirical Bayes method that provides adaptive regularization of the
   functional effects. A simulation study and a case study illustrate that
   our approach is well suited for the simultaneous assessment of
   transcript discovery and differential expression in tiling array
   studies, and that it outperforms methods that accomplish only one of
   these tasks.}},
DOI = {{10.2202/1544-6115.1726}},
Article-Number = {{4}},
ISSN = {{1544-6115}},
ResearcherID-Numbers = {{Clement, Lieven/G-5143-2012
   }},
ORCID-Numbers = {{clement, lieven/0000-0002-9050-4370}},
Unique-ID = {{ISI:000305091700004}},
}

@incollection{ ISI:000370802300007,
Author = {Huebner, Cynthia D. and Nowak, David J. and Pouyat, Richard V. and
   Bodine, Allison R.},
Editor = {{Laband, DN and Lockaby, BG and Zipperer, WC}},
Title = {{Nonnative Invasive Plants: Maintaining Biotic and Socioeconomic
   Integrity along the Urban-Rural-Natural Area Gradient}},
Booktitle = {{URBAN-RURAL INTERFACES: LINKING PEOPLE AND NATURE}},
Series = {{ACSESS Publications}},
Year = {{2012}},
Pages = {{71-98}},
Abstract = {{In this chapter, we evaluate nonnative invasive plant species of the
   urban-rural-natural area gradient in order to reduce negative impacts of
   invasive plants on native species and ecosystems. This evaluation
   includes addressing (i) the concept of urban areas as the primary source
   of invasive plant species and characteristics of urban nonnative plants,
   including their documented impacts on associated native plants and
   biodiversity along the urban-rural-natural area gradient, (ii) the most
   vulnerable land uses and potential barriers to invasion along the
   urban-rural-natural area gradient, and (iii) possible mitigation of
   invasions and urbanization using restoration or rehabilitation. Finally,
   we introduce three possible solutions: (i) use of spatially explicit
   land use planning and management that places invasion barriers between
   the urban core and the rural-natural area interfaces, (ii) increasing
   native and exotic species interactions within the urban core and
   rural-natural area interface, thereby increasing the number of pathogen
   and enemy interactions or the loss of novel weapons, and (iii) changing
   the horticultural trade and people's behavior, such that propagule
   pressure is kept below threshold levels required by growing invasive
   plant populations.}},
DOI = {{10.2136/2012.urban-rural.c5}},
ISSN = {{2165-9842}},
ISBN = {{978-0-89118-616-8; 978-0-89118-615-1}},
Unique-ID = {{ISI:000370802300007}},
}

@article{ ISI:000298311300022,
Author = {Dronova, Iryna and Gong, Peng and Wang, Lin},
Title = {{Object-based analysis and change detection of major wetland cover types
   and their classification uncertainty during the low water period at
   Poyang Lake, China}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2011}},
Volume = {{115}},
Number = {{12}},
Pages = {{3220-3236}},
Month = {{DEC 15}},
Abstract = {{Productive wetland systems at land-water interfaces that provide unique
   ecosystem services are challenging to study because of water dynamics,
   complex surface cover and constrained field access. We applied
   object-based image analysis and supervised classification to four 32-m
   Beijing-1 microsatellite images to examine broad-scale surface cover
   composition and its change during November 2007-March 2008 low water
   season at Poyang Lake, the largest freshwater lake-wetland system in
   China (>4000 km(2)). we proposed a novel method for semi-automated
   selection of training objects in this heterogeneous landscape using
   extreme values of spectral indices (SIs) estimated from satellite data.
   Dynamics of the major wetland cover types (Water, Mudflat, Vegetation
   and Sand) were investigated both as transitions among primary classes
   based on maximum membership value, and as changes in memberships to all
   classes even under no change in a primary class. Fuzzy classification
   accuracy was evaluated as match frequencies between classification
   outcome and a) the best reference candidate class (MAX function) and b)
   any acceptable reference class (RIGHT function). MAX-based accuracy was
   relatively high for Vegetation (>= 90\%), Water (>= 82\%), Mudflat (>=
   76\%) and the smallest-area Sand (>= 75\%) in all scenes; these scores
   improved with the RIGHT function to 87-100\%. Classification uncertainty
   assessed as the proportion of fuzzy object area within a class at a
   given fuzzy threshold value was the highest for all classes in November
   2007, and consistently higher for Mudflat than for other classes in all
   scenes. Vegetation was the dominant class in all scenes, occupying
   41.2-49.3\% of the study area. Object memberships to Vegetation mostly
   declined from November 2007 to February 2008 and increased substantially
   only in February-March 2008, possibly reflecting growing season
   conditions and grazing. Spatial extent of Water both declined and
   increased during the study period, reflecting precipitation and
   hydrological events. The ``fuzziest{''} Mudflat class was involved in
   major detected transitions among classes and declined in classification
   accuracy by March 2008, representing a key target for finer-scale
   research. Future work should introduce Vegetation sub-classes reflecting
   differences in phenology and alternative methods to discriminate Mudflat
   from other classes. Results can be used to guide field sampling and
   top-down landscape analyses in this wetland. (C) 2011 Elsevier Inc. All
   rights reserved.}},
DOI = {{10.1016/j.rse.2011.07.006}},
ISSN = {{0034-4257}},
ResearcherID-Numbers = {{Ma, Lei/I-4597-2014
   Gong, Peng/L-8184-2013}},
Unique-ID = {{ISI:000298311300022}},
}

@article{ ISI:000299650600006,
Author = {Zhang, Yuanyuan and Wang, Shudong and Yang, Meixi and Xu, Dashun and
   Meng, Dazhi},
Title = {{A NEW METHOD OF DETERMINING THRESHOLD OF GENE NETWORK BASED ON
   PHENOTYPES}},
Journal = {{JOURNAL OF BIOLOGICAL SYSTEMS}},
Year = {{2011}},
Volume = {{19}},
Number = {{4}},
Pages = {{607-616}},
Month = {{DEC}},
Abstract = {{With the rapid growth of microarray data, it has become a hot topic to
   reveal complex behaviors and functions of life system by studying the
   relationships among genes. In the process of reverse network modeling,
   the relationships with less relevance are generally not considered by
   determining a threshold when the relationships among genes are mined.
   However, there are no effective methods to determine the threshold up to
   now. It is worthwhile to note that the phenotypes of genetic diseases
   are generally regarded as external representation of the functions of
   genes. Therefore, two types of phenotype networks are constructed from
   gene and disease views, respectively, and through comparing these two
   types of phenotype networks, the threshold of gene network corresponding
   to a certain disease can be determined when their similarity reaches to
   maximum. Because the gene network is determined based on the
   relationships among phenotypes and phenotypes are external
   representation of the functions of genes, it is considered that
   relationships in the gene network may show functional relationships
   among genes in biological system. In this work, the thresholds 0.47 and
   0.48 of gene network are determined based on Parkinson disease
   phenotypes. Furthermore, the validity of these thresholds is verified by
   the specificity and susceptibility of phenotype networks. Also, through
   comparing the structural parameters of gene networks for normal and
   disease stage at different thresholds, significant difference between
   the two gene networks at threshold 0.47 or 0.48 is found. The
   significant difference of structural parameters further verifies the
   efficiency of this method.}},
DOI = {{10.1142/S0218339011004068}},
ISSN = {{0218-3390}},
EISSN = {{1793-6470}},
Unique-ID = {{ISI:000299650600006}},
}

@article{ ISI:000298090100006,
Author = {Wilson, Peter D.},
Title = {{Distance-based methods for the analysis of maps produced by species
   distribution models}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2011}},
Volume = {{2}},
Number = {{6}},
Pages = {{623-633}},
Month = {{DEC}},
Abstract = {{1. Species distribution models (SDMs) are now widely applied to
   determine the potential distributions of species in relation to
   environmental covariates. Many modelling tools are available, and large
   sets of maps may be produced easily.
   2. A wide range of methods have been developed for the comparison or
   analysis of raster images and SDM output maps including pairwise
   (two-sample) tests and overall measures of similarity such as
   correlation coefficients and distance measures.
   3. I present an analytical framework applying a distance-based approach
   to the ordination and analysis of maps produced by species distribution
   modelling tools. The method combines aspects of image analysis with
   distance-based statistical tests and allows ecologists to apply familiar
   forms of ordination and analysis to SDM output maps. A novel method of
   recombining elements of information extracted from distance-based map
   analysis is also presented.}},
DOI = {{10.1111/j.2041-210X.2011.00115.x}},
ISSN = {{2041-210X}},
Unique-ID = {{ISI:000298090100006}},
}

@article{ ISI:000297468600001,
Author = {Oldmeadow, Christopher and Riveros, Carlos and Holliday, Elizabeth G.
   and Scott, Rodney and Moscato, Pablo and Wang, Jie Jin and Mitchell,
   Paul and Buitendijk, Gabrielle H. S. and Vingerling, Johannes R. and
   Klaver, Caroline C. W. and Klein, Ronald and Attia, John},
Title = {{Sifting the wheat from the chaff: prioritizing GWAS results by
   identifying consistency across analytical methods}},
Journal = {{GENETIC EPIDEMIOLOGY}},
Year = {{2011}},
Volume = {{35}},
Number = {{8}},
Pages = {{745-754}},
Month = {{DEC}},
Abstract = {{The curse of multiple testing has led to the adoption of a stringent
   Bonferroni threshold for declaring genome-wide statistical significance
   for any one SNP as standard practice. Although justified in avoiding
   false positives, this conservative approach has the potential to miss
   true associations as most studies are drastically underpowered. As an
   alternative to increasing sample size, we compare results from a typical
   SNP-by-SNP analysis with three other methods that incorporate regional
   information in order to boost or dampen an otherwise noisy signal: the
   haplotype method (Schaid et al. {[}2002] Am J Hum Genet 70:425434), the
   gene-based method (Liu et al. {[}2010] Am J Hum Genet 87:139145), and a
   new method (interaction count) that uses genome-wide screening of
   pairwise SNP interactions. Using a modestly sized case-control study, we
   conduct a genome-wide association studies (GWAS) of age-related macular
   degeneration, and find striking agreement across all methods in regions
   of known associated variants. We also find strong evidence of novel
   associated variants in two regions (Chromosome 2p25 and Chromosome
   10p15) in which the individual SNP P-values are only suggestive, but
   where there are very high levels of agreement between all methods. We
   propose that consistency between different analysis methods may be an
   alternative to increasingly larger sample sizes in sifting true signals
   from noise in GWAS. Genet. Epidemiol. 2011. (C) 2011 Wiley Periodicals,
   Inc. 35:745-754, 2011}},
DOI = {{10.1002/gepi.20622}},
ISSN = {{0741-0395}},
ResearcherID-Numbers = {{Wang, Jie Jin/P-1499-2014
   Mitchell, Paul/P-1498-2014
   Attia, John/F-5376-2013
   Riveros, Carlos/F-3373-2012
   Riveros, Carlos/A-6880-2013
   Klaver, Caroline/A-2013-2016
   MOSCATO, PABLO/G-7668-2013
   }},
ORCID-Numbers = {{Wang, Jie Jin/0000-0001-9491-4898
   Attia, John/0000-0001-9800-1308
   Riveros, Carlos/0000-0002-2551-7618
   Oldmeadow, Christopher/0000-0001-6104-1322
   Moscato, Pablo/0000-0003-2570-5966
   Klein, Ronald/0000-0002-4428-6237
   Klaver, Caroline/0000-0002-2355-5258}},
Unique-ID = {{ISI:000297468600001}},
}

@article{ ISI:000208588900006,
Author = {Ghosh, Madhumala and Das, Dev Kumar and Ray, Ajoy K. and Chakraborty,
   Chandan},
Title = {{Development of Renyi's Entropy Based Fuzzy Divergence Measure for
   Leukocyte Segmentation}},
Journal = {{JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS}},
Year = {{2011}},
Volume = {{1}},
Number = {{4}},
Pages = {{334-340}},
Month = {{DEC}},
Abstract = {{This paper introduces a new approach towards the fuzzy extension of
   divergence measure using Renyi's exponential entropy measure. Divergence
   based threshold selection is a powerful technique for segmentation of an
   image. Here divergence measure is developed based on fuzzy extension of
   Renyi's entropy function. The result of our proposed method is compared
   mainly with fuzzy extension of Shannon entropy and hard Renyi's entropy
   based divergence measures respectively and the segmentation results are
   shown accordingly. The proposed measure is tested on various types of
   images viz, peripheral blood smears. But this methodology is compared
   with some well proposed methods also like Fuzzy C means, Otsu's method
   etc. In particular, fuzzy divergence using Renyi's entropy shows better
   segmentation (99.14\% of overall accuracy) of leukocyte nuclei from
   peripheral blood smears images leading to leukemia detection. The result
   shown in table1 produces fuzzy extension of Renyi's entropy based
   divergence approach comparatively better segmentation than the other
   methods.}},
DOI = {{10.1166/jmihi.2011.1052}},
ISSN = {{2156-7018}},
ORCID-Numbers = {{Das, Dev Kumar/0000-0002-5362-1324}},
Unique-ID = {{ISI:000208588900006}},
}

@article{ ISI:000285682200003,
Author = {Montero Lorenzo, J. M. and Garcia-Centeno, M. C. and Fernandez-Aviles,
   G.},
Title = {{A Threshold Autoregressive Asymmetric Stochastic Volatility Strategy to
   Alert of Violations of the Air Quality Standards}},
Journal = {{INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH}},
Year = {{2011}},
Volume = {{5}},
Number = {{1}},
Pages = {{23-32}},
Month = {{WIN}},
Abstract = {{Air quality is a topic of crucial importance, because air pollution is
   one of the most important pollution problems in the world. In
   particular, predicting or detecting a future extreme air pollution
   episode or predicting the violation of an air quality standard, is of
   crucial interest in the field of pollution control. There have been a
   variety of attempts to reach this purpose both from the perspective of
   the extreme value theory and the time series analysis, but as far as we
   know there is none successful strategy to alert of violations of the
   standards. This is why in this article we propose a new strategy, a
   threshold autoregressive asymmetric stochastic volatility strategy to
   alert of an immediate violation of the particulate matter quality
   standards, which take into account the different answer of the
   volatility to a positive or negative, but equal in magnitude, relative
   variation of the level of the pollutant in the previous period.
   Particulate matter is one of the still uncontrolled pollutants in big
   cities. This novel approach has been applied in Madrid City (Spain), the
   third-most populous municipality in the European Union, and it is able
   to predict a great percentage of violations of the standard.}},
ISSN = {{1735-6865}},
ResearcherID-Numbers = {{FERNANDEZ-AVILES, 2/S-6003-2017}},
ORCID-Numbers = {{FERNANDEZ-AVILES, 2/0000-0001-5934-1916}},
Unique-ID = {{ISI:000285682200003}},
}

@article{ ISI:000295316100008,
Author = {Galanzha, Ekaterina I. and Sarimollaoglu, Mustafa and Nedosekin, Dmitry
   A. and Keyrouz, Salah G. and Mehta, Jawahar L. and Zharov, Vladimir P.},
Title = {{In Vivo Flow Cytometry of Circulating Clots Using Negative Photothermal
   and Photoacoustic Contrasts}},
Journal = {{CYTOMETRY PART A}},
Year = {{2011}},
Volume = {{79A}},
Number = {{10, SI}},
Pages = {{814-824}},
Month = {{OCT}},
Abstract = {{Conventional photothermal (PT) and photoacousic (PA) imaging,
   spectroscopy, and cytometry are preferentially based on positive PT/PA
   effects, when signals are above background. Here, we introduce PT/PA
   technique based on detection of negative signals below background. Among
   various new applications, we propose label-free in vivo flow cytometry
   of circulating clots. No method has been developed for the early
   detection of clots of different compositions as a source of
   thromboembolism including ischemia at strokes and myocardial infarction.
   When a low-absorbing, platelet-rich clot passes a laser-irradiated
   vessel volume, a transient decrease in local absorption results in an
   ultrasharp negative PA hole in blood background. Using this phenomenon
   alone or in combination with positive contrasts, we demonstrated
   identification of white, red, and mixed clots on a mouse model of
   myocardial infarction and human blood. The concentration and size of
   clots were measured with threshold down to few clots in the entire
   circulation with size as low as 20 mu m. This multiparameter diagnostic
   platform using portable personal high-speed flow cytometer with negative
   dynamic contrast mode has potential to real-time defining risk factors
   for cardiovascular diseases, and for prognosis and prevention of stroke
   or use clot count as a marker of therapy efficacy. Possibility for
   label-free detection of platelets, leukocytes, tumor cells or targeting
   them by negative PA probes (e.g., nonabsorbing beads or bubbles) is also
   highlighted.}},
DOI = {{10.1002/cyto.a.21106}},
ISSN = {{1552-4922}},
EISSN = {{1552-4930}},
ResearcherID-Numbers = {{Nedosekin, Dmitry/C-3105-2008
   Zharov, Vladimir/J-9728-2013
   }},
ORCID-Numbers = {{Nedosekin, Dmitry/0000-0003-3761-7824
   Zharov, Vladimir/0000-0003-4773-0548}},
Unique-ID = {{ISI:000295316100008}},
}

@article{ ISI:000295033800005,
Author = {Benito, Blas and Lorite, Juan and Penas, Julio},
Title = {{Simulating potential effects of climatic warming on altitudinal patterns
   of key species in Mediterranean-alpine ecosystems}},
Journal = {{CLIMATIC CHANGE}},
Year = {{2011}},
Volume = {{108}},
Number = {{3}},
Pages = {{471-483}},
Month = {{OCT}},
Abstract = {{In this paper we study an isolated high-mountain (Sierra Nevada, SE
   Iberian Peninsula) to identify the potential trends in the
   habitat-suitability of five key species (i.e. species that domain a
   given vegetation type and drive the conditions for appearance of many
   other species) corresponding to four vegetation types occupying
   different altitudinal belts, that might result from a sudden climatic
   shift. We used topographical variables and downscaled climate warming
   simulations to build a high-resolution spatial database (10 m) according
   to four different climate warming scenarios for the twenty-first
   century. The spatial changes in the suitable habitat were simulated
   using a species distribution model, in order to analyze altitudinal
   shifts and potential habitat loss of the key species. Thus, the advance
   and receding fronts of known occurrence locations were computed by
   introducing a new concept named differential suitability, and potential
   patterns of substitution among the key species were established. The
   average mean temperature trend show an increase of 4.8A degrees C, which
   will induce the vertical shift of the suitable habitat for all the five
   key species considered at an average rate of 11.57 m/year. According to
   the simulations, the suitable habitat for the key species inhabiting the
   summit area, where most of the endemic and/or rare species are located,
   may disappear before the middle of the century. The other key species
   considered show moderate to drastic suitable habitat loss depending on
   the considered scenario. Climate warming should provoke a strong
   substitution dynamics between species, increasing spatial competition
   between both of them. In this study, we introduce the application of
   differential suitability concept into the analysis of potential impact
   of climate change, forest management and environmental monitoring, and
   discuss the limitations and uncertainties of these simulations.}},
DOI = {{10.1007/s10584-010-0015-3}},
ISSN = {{0165-0009}},
EISSN = {{1573-1480}},
ResearcherID-Numbers = {{Penas, Julio/N-4885-2014
   Benito, Blas M./D-4681-2014
   Lorite, Juan/F-4690-2011}},
ORCID-Numbers = {{Penas, Julio/0000-0001-6102-4610
   Benito, Blas M./0000-0001-5105-7232
   Lorite, Juan/0000-0003-4617-8069}},
Unique-ID = {{ISI:000295033800005}},
}

@article{ ISI:000294885700001,
Author = {Kimori, Yoshitaka and Katayama, Eisaku and Morone, Nobuhiro and Kodama,
   Takao},
Title = {{Fractal dimension analysis and mathematical morphology of structural
   changes in actin filaments imaged by electron microscopy}},
Journal = {{JOURNAL OF STRUCTURAL BIOLOGY}},
Year = {{2011}},
Volume = {{176}},
Number = {{1}},
Pages = {{1-8}},
Month = {{OCT}},
Abstract = {{In this work, we examined structural changes of actin filaments
   interacting with myosin visualized by quick freeze deep-etch replica
   electron microscopy (EM) by using a new method of image
   processing/analysis based on mathematical morphology.
   In order to quantify the degree of structural changes, two
   characteristic patterns were extracted from the EM images. One is the
   winding pattern of the filament shape (WP) reflecting flexibility of the
   filament, and the other is the surface pattern of the filament (SP)
   reflecting intra-molecular domain-mobility of actin monomers
   constituting the filament. EM images were processed by morphological
   filtering followed by box-counting to calculate the fractal dimensions
   for WP (D(WP)) and SP (D(SP)). The result indicates that D(WP) was
   larger than D(SP) irrespective of the state of the filament (myosin-free
   or bound) and that both parameters for myosin-bound filaments were
   significantly larger than those for myosin-free filaments. Overall, this
   work provides the first quantitative insight into how conformational
   disorder of actin monomers is correlated with the myosin-induced
   increase in flexibility of actin filaments along their length as
   suggested by earlier studies with different techniques. Our method is
   yet to be improved in details, but promising as a powerful tool for
   studying the structural change of protein molecules and their
   assemblies, which can potentially be applied to a wide range of
   biological and biomedical images. (C) 2011 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.jsb.2011.07.007}},
ISSN = {{1047-8477}},
ResearcherID-Numbers = {{Kodama, Takao/G-5504-2011}},
Unique-ID = {{ISI:000294885700001}},
}

@article{ ISI:000293544400008,
Author = {Cayuela, Luis and Galvez-Bravo, Lucia and Maria Carrascal, Luis and de
   Albuquerque, Fabio S.},
Title = {{Comments on Bartolino et al. (2011): limits of cumulative relative
   frequency distribution curves for hotspot identification}},
Journal = {{POPULATION ECOLOGY}},
Year = {{2011}},
Volume = {{53}},
Number = {{4}},
Pages = {{597-601}},
Month = {{OCT}},
Abstract = {{The recent paper by Bartolino et al. (Popul Ecol 53:351-359, 2011)
   presents a new method to objectively select hotspots using cumulative
   relative frequency distribution (CRFD) curves. This method is presented
   as being independent from the selection of any threshold and, therefore,
   less arbitrary than traditional approaches. We argue that this method,
   albeit mathematically sound, is based on likewise arbitrary decisions
   regarding threshold selection. Specifically, the use of the CRFD curve
   approach requires the occurrence of two criteria for the method to be
   applied correctly: the selection of a 45A degrees tangent to the curve,
   and the need to consider the highest relative value of the study
   parameter corresponding to a 45A degrees slope tangent to the curve.
   Using two case studies (dealing with species richness and abundance of a
   particular species), we demonstrate that these two criteria are really
   unrelated to the underlying causes that shape the spatial pattern of the
   phenomena under study, but rather related to sampling design and spatial
   scale; hence, one could likewise use different but valid criteria.
   Consequently, the CRFD curve approach is based on the selection of a
   pre-defined threshold that has little, if any, ecological justification,
   and that heavily influences the final hotspot selection. Therefore, we
   conclude that the CRFD curve approach itself is not necessarily better
   and more objective than any of the global methods typically used for
   hotspot identification. Indeed, mathematical and/or statistical
   approaches should not be viewed as a panacea to solve conservation
   problems, but rather used in combination with biological, practical,
   economic and social considerations.}},
DOI = {{10.1007/s10144-011-0272-7}},
ISSN = {{1438-3896}},
EISSN = {{1438-390X}},
ResearcherID-Numbers = {{Carrascal, Luis M./B-8381-2008
   Galvez-Bravo, Lucia/L-7478-2017
   Cayuela, Luis/K-7255-2015}},
ORCID-Numbers = {{Carrascal, Luis M./0000-0003-1288-5531
   Galvez-Bravo, Lucia/0000-0002-9684-9775
   Cayuela, Luis/0000-0003-3562-2662}},
Unique-ID = {{ISI:000293544400008}},
}

@article{ ISI:000294064700002,
Author = {Hwang, Han-Jeong and Kim, Kyung-Hwan and Jung, Young-Jin and Kim, Do-Won
   and Lee, Yong-Ho and Im, Chang-Hwan},
Title = {{An EEG-based real-time cortical functional connectivity imaging system}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2011}},
Volume = {{49}},
Number = {{9}},
Pages = {{985-995}},
Month = {{SEP}},
Abstract = {{In the present study, we introduce an EEG-based, real-time, cortical
   functional connectivity imaging system capable of monitoring and tracing
   dynamic changes in cortical functional connectivity between different
   regions of interest (ROIs) on the brain cortical surface. The proposed
   system is based on an EEG-based dynamic neuroimaging system, which is
   capable of monitoring spatiotemporal changes of cortical rhythmic
   activity at a specific frequency band by conducting real-time cortical
   source imaging. To verify the implemented system, we performed three
   test experiments in which we monitored temporal changes in cortical
   functional connectivity patterns in various frequency bands during
   structural face processing, finger movements, and working memory task.
   We also traced the changes in the number of connections between all
   possible pairs of ROIs whose correlations exceeded a predetermined
   threshold. The quantitative analysis results were consistent with those
   of previous off-line studies, thereby demonstrating the possibility of
   imaging cortical functional connectivity in real-time. We expect our
   system to be applicable to various potential applications, including
   real-time diagnosis of psychiatric diseases and EEG neurofeedback.}},
DOI = {{10.1007/s11517-011-0791-6}},
ISSN = {{0140-0118}},
ORCID-Numbers = {{Im, Chang-Hwan/0000-0003-3795-3318
   Jung, Young-Jin/0000-0003-0169-2865
   Kim, Kyung Hwan/0000-0001-9044-0437}},
Unique-ID = {{ISI:000294064700002}},
}

@article{ ISI:000293738600010,
Author = {Molanes-Lopez, Elisa M. and Leton, Emilio},
Title = {{Inference of the Youden index and associated threshold using empirical
   likelihood for quantiles}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2011}},
Volume = {{30}},
Number = {{19}},
Pages = {{2467-2480}},
Month = {{AUG 30}},
Abstract = {{The Youden index is a widely used measure in the framework of medical
   diagnostics, where the effectiveness of a biomarker (screening marker or
   predictor) for classifying a disease status is studied. When the
   biomarker is continuous, it is important to determine the threshold or
   cut-off point to be used in practice for the discrimination between
   diseased and healthy populations. We introduce two methods aimed at
   estimating the Youden index and its associated threshold. The first one
   is a modified version of a recent approach based on the delta method,
   and the second one is based on the adjusted empirical likelihood for
   quantiles in the setting of a two-sample problem. We also include CIs
   for both of them. In the simulation study, we compare both methods under
   different scenarios. Finally, a real example of prostatic cancer, well
   known in the literature, is analysed to provide the reader with a better
   understanding of the new methodology. Copyright (C) 2011 John Wiley \&
   Sons, Ltd.}},
DOI = {{10.1002/sim.4303}},
ISSN = {{0277-6715}},
ResearcherID-Numbers = {{Leton, Emilio/F-9363-2010
   Molanes-Lopez, Elisa M./K-4467-2014}},
ORCID-Numbers = {{Leton, Emilio/0000-0001-6908-9590
   Molanes-Lopez, Elisa M./0000-0003-3217-8551}},
Unique-ID = {{ISI:000293738600010}},
}

@article{ ISI:000298526600015,
Author = {Youssef, Mina and Scoglio, Caterina},
Title = {{An individual-based approach to SIR epidemics in contact networks}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2011}},
Volume = {{283}},
Number = {{1}},
Pages = {{136-144}},
Month = {{AUG 21}},
Abstract = {{Many approaches have recently been proposed to model the spread of
   epidemics on networks. For instance, the Susceptible/Infected/Recovered
   (SIR) compartmental model has successfully been applied to different
   types of diseases that spread out among humans and animals. When this
   model is applied on a contact network, the centrality characteristics of
   the network plays an important role in the spreading process. However,
   current approaches only consider an aggregate representation of the
   network structure, which can result in inaccurate analysis. In this
   paper, we propose a new individual-based SIR approach, which considers
   the whole description of the network structure. The individual-based
   approach is built on a continuous time Markov chain, and it is capable
   of evaluating the state probability for every individual in the network.
   Through mathematical analysis, we rigorously confirm the existence of an
   epidemic threshold below which an epidemic does not propagate in the
   network. We also show that the epidemic threshold is inversely
   proportional to the maximum eigenvalue of the network. Additionally, we
   study the role of the whole spectrum of the network, and determine the
   relationship between the maximum number of infected individuals and the
   set of eigenvalues and eigenvectors. To validate our approach, we
   analytically study the deviation with respect to the continuous time
   Markov chain model, and we show that the new approach is accurate for a
   large range of infection strength. Furthermore, we compare the new
   approach with the well-known heterogeneous mean field approach in the
   literature. Ultimately, we support our theoretical results through
   extensive numerical evaluations and Monte Carlo simulations. Published
   by Elsevier}},
DOI = {{10.1016/j.jtbi.2011.05.029}},
ISSN = {{0022-5193}},
Unique-ID = {{ISI:000298526600015}},
}

@article{ ISI:000295224600002,
Author = {Daniluk, Pawel and Lesyng, Bogdan},
Title = {{A novel method to compare protein structures using local descriptors}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2011}},
Volume = {{12}},
Month = {{AUG 17}},
Abstract = {{Background: Protein structure comparison is one of the most widely
   performed tasks in bioinformatics. However, currently used methods have
   problems with the so-called ``difficult similarities{''}, including
   considerable shifts and distortions of structure, sequential swaps and
   circular permutations. There is a demand for efficient and automated
   systems capable of overcoming these difficulties, which may lead to the
   discovery of previously unknown structural relationships.
   Results: We present a novel method for protein structure comparison
   based on the formalism of local descriptors of protein structure -
   DEscriptor Defined Alignment (DEDAL). Local similarities identified by
   pairs of similar descriptors are extended into global structural
   alignments. We demonstrate the method's capability by aligning
   structures in difficult benchmark sets: curated alignments in the
   SISYPHUS database, as well as SISY and RIPC sets, including
   non-sequential and non-rigid-body alignments. On the most difficult RIPC
   set of sequence alignment pairs the method achieves an accuracy of 77\%
   (the second best method tested achieves 60\% accuracy).
   Conclusions: DEDAL is fast enough to be used in whole proteome
   applications, and by lowering the threshold of detectable structure
   similarity it may shed additional light on molecular evolution
   processes. It is well suited to improving automatic classification of
   structure domains, helping analyze protein fold space, or to improving
   protein classification schemes. DEDAL is available online at
   http://bioexploratorium.pl/EP/DEDAL.}},
DOI = {{10.1186/1471-2105-12-344}},
Article-Number = {{344}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Daniluk, Pawel/C-1723-2013
   }},
ORCID-Numbers = {{Daniluk, Pawel/0000-0002-2316-6586}},
Unique-ID = {{ISI:000295224600002}},
}

@article{ ISI:000292235400018,
Author = {Frazier, A. E. and Wang, L.},
Title = {{Characterizing spatial patterns of invasive species using sub-pixel
   classifications}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2011}},
Volume = {{115}},
Number = {{8}},
Pages = {{1997-2007}},
Month = {{AUG 15}},
Abstract = {{Invasive species disrupt landscape patterns and compromise the
   functionality of ecosystem processes. Non-native saltcedar poses
   significant threats to native vegetation and groundwater resources in
   the southwestern U.S. and Mexico, and quantifying spatial and temporal
   distribution patterns is essential for monitoring its spread.
   Considerable research focuses on determining the accuracy of various
   remote sensing techniques for distinguishing saltcedar from native woody
   riparian vegetation through sub-pixel, or soft classifications. However,
   there is a lack of research quantifying spatial distribution patterns
   from these classifications, mainly because landscape metrics, which are
   commonly used to statistically assess these patterns, require bounded
   classes and cannot be applied directly to soft classifications. This
   study tests a new method for discretizing sub-pixel data to generate
   landscape metrics using a continuum of fractional cover thresholds. The
   developed approach transforms sub-pixel classifications into discrete
   maps compliant with metric terms and computes and interprets metric
   results in the context of the region to explain patterns in the extent,
   distribution, and connectivity of saltcedar in the Rio Grande basin.
   Results indicate that landscape metrics are sensitive to sub-pixel
   values and can vary greatly with fractional cover. Therefore spectral
   unmixing should be performed prior to metric calculations. Analysis of
   metric trends provides evidence that saltcedar has expanded away from
   the immediate riparian zones and is displacing native vegetation. This
   information, coupled with control management strategies, can be used to
   target remediation activities along the Rio Grande. (C) 2011 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2011.04.002}},
ISSN = {{0034-4257}},
ResearcherID-Numbers = {{wang, le/A-7236-2011}},
Unique-ID = {{ISI:000292235400018}},
}

@article{ ISI:000294105500010,
Author = {Lester, Rebecca E. and Fairweather, Peter G.},
Title = {{Ecosystem states: Creating a data-derived, ecosystem-scale ecological
   response model that is explicit in space and time}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2011}},
Volume = {{222}},
Number = {{15}},
Pages = {{2690-2703}},
Month = {{AUG 10}},
Abstract = {{Increasing difficulties associated with balancing consumptive demands
   for water and achieving ecological benefits in aquatic ecosystems
   provide opportunities for new ecosystem-scale ecological response models
   to assist managers. Using an Australian estuary as a case study, we
   developed a novel approach to create a data-derived state-and-transition
   model. The model identifies suites of co-occurring birds, fish, benthic
   invertebrates and aquatic macrophytes (as `states') and the changing
   physico-chemical conditions that are associated with each
   ('transitions'). The approach first used cluster analysis to identify
   sets of co-occurring biota. Differences in the physico-chemical data
   associated with each state were identified using classification trees,
   with the biotic distinctness of the resultant statistical model tested
   using analysis of similarities. The predictive capacity of the model was
   tested using new cases. Two models were created using different
   time-steps (annual and quarterly) and then combined to capture both
   longer-term trends and more-recent declines in ecological condition. We
   identified eight ecosystem states that were differentiated by a mix of
   water-quantity and water-quality variables. Each ecosystem state
   represented a distinct biotic assemblage under well-defined
   physico-chemical conditions. Two `basins of attraction' were identified,
   with four tidally-influenced states, and another four independent of
   tidal influence. Within each basin, states described a continuum of
   relative health, manifest through declining taxonomic diversity and
   abundances. The main threshold determining relative health was whether
   freshwater flows had occurred in the region during the previous 339
   days. Canonical analyses of principal coordinates tested the predictive
   capacity of the model and demonstrated that the variance in the
   environmental data set was well captured (87\%) with 52\% of the
   variance in the biological data set also captured. The latter increased
   to >80\% when long- and short-term biological data were analysed
   separately, indicating that the model described the available data for
   the Coorong well. This approach thus created a data-derived,
   multivariate model, where neither states nor transitions were determined
   a priori. The approach did not over-fit the data, was robust to patchy
   or missing data, the choice of initial clustering technique and random
   errors in the biological data set, and was well-received by local
   natural resource managers. However, the model did not capture causal
   relationships and requires additional testing, particularly during
   future episodes of ecological recovery. The approach shows significant
   promise for simplifying management definitions of ecological condition
   and, via scenario analyses, can be used to assist in manager
   decision-making of large, complex aquatic ecosystems in the future. (C)
   2011 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2011.05.009}},
ISSN = {{0304-3800}},
ResearcherID-Numbers = {{Lester, Rebecca/H-3907-2011}},
ORCID-Numbers = {{Lester, Rebecca/0000-0003-2682-6495}},
Unique-ID = {{ISI:000294105500010}},
}

@article{ ISI:000293224200019,
Author = {Nelson, Andrew D. and Lamb, Jonathan C. and Kobrossly, Pierre S. and
   Shippen, Dorothy E.},
Title = {{Parameters Affecting Telomere-Mediated Chromosomal Truncation in
   Arabidopsis}},
Journal = {{PLANT CELL}},
Year = {{2011}},
Volume = {{23}},
Number = {{6}},
Pages = {{2263-2272}},
Month = {{JUN}},
Abstract = {{Conversion of a double-strand break into a telomere is a dangerous,
   potentially lethal event. However, little is known about the mechanism
   and control of de novo telomere formation (DNTF). DNTF can be instigated
   by the insertion of a telomere repeat array (TRA) into the host genome,
   which seeds the formation of a new telomere, resulting in chromosome
   truncation. Such events are rare and concentrated at chromosome ends.
   Here, we introduce tetraploid Arabidopsis thaliana as a robust genetic
   model for DNTF. Transformation of a 2.6-kb TRA into tetraploid plants
   resulted in a DNTF efficiency of 56\%, fivefold higher than in diploid
   plants and 50-fold higher than in human cells. DNTF events were
   recovered across the entire genome, indicating that genetic redundancy
   facilitates recovery of DNTF events. Although TRAs as short as 100 bp
   seeded new telomeres, these tracts were unstable unless they were
   extended above a 1-kb size threshold. Unexpectedly, DNTF efficiency
   increased in plants lacking telomerase, and DNTF rates were lower in
   plants null for Ku70 or Lig4, components of the nonhomologous
   end-joining repair pathway. We conclude that multiple competing pathways
   modulate DNTF, and that tetraploid Arabidopsis will be a powerful model
   for elucidating the molecular details of these processes.}},
DOI = {{10.1105/tpc.111.086017}},
ISSN = {{1040-4651}},
ORCID-Numbers = {{Nelson, Andrew/0000-0001-9896-1739}},
Unique-ID = {{ISI:000293224200019}},
}

@article{ ISI:000292633900015,
Author = {Chazdon, Robin L. and Chao, Anne and Colwell, Robert K. and Lin,
   Shang-Yi and Norden, Natalia and Letcher, Susan G. and Clark, David B.
   and Finegan, Bryan and Arroyo, J. Pablo},
Title = {{A novel statistical method for classifying habitat generalists and
   specialists}},
Journal = {{ECOLOGY}},
Year = {{2011}},
Volume = {{92}},
Number = {{6}},
Pages = {{1332-1343}},
Month = {{JUN}},
Abstract = {{We develop a novel statistical approach for classifying generalists and
   specialists in two distinct habitats. Using a multinomial model based on
   estimated species relative abundance in two habitats, our method
   minimizes bias due to differences in sampling intensities between two
   habitat types as well as bias due to insufficient sampling within each
   habitat. The method permits a robust statistical classification of
   habitat specialists and generalists, without excluding rare species a
   priori. Based on a user-defined specialization threshold, the model
   classifies species into one of four groups: (1) generalist; (2) habitat
   A specialist; (3) habitat B specialist; and (4) too rare to classify
   with confidence. We illustrate our multinomial classification method
   using two contrasting data sets: (1) bird abundance in woodland and
   heath habitats in southeastern Australia and (2) tree abundance in
   secondgrowth (SG) and old-growth (OG) rain forests in the Caribbean
   lowlands of northeastern Costa Rica. We evaluate the multinomial model
   in detail for the tree data set. Our results for birds were highly
   concordant with a previous nonstatistical classification, but our method
   classified a higher fraction (57.7\%) of bird species with statistical
   confidence. Based on a conservative specialization threshold and
   adjustment for multiple comparisons, 64.4\% of tree species in the full
   sample were too rare to classify with confidence. Among the species
   classified, OG specialists constituted the largest class (40.6\%),
   followed by generalist tree species (36.7\%) and SG specialists
   (22.7\%). The multinomial model was more sensitive than indicator value
   analysis or abundance-based phi coefficient indices in detecting habitat
   specialists and also detects generalists statistically. Classification
   of specialists and generalists based on rarefied subsamples was highly
   consistent with classification based on the full sample, even for
   sampling percentages as low as 20\%. Major advantages of the new method
   are (1) its ability to distinguish habitat generalists (species with no
   significant habitat affinity) from species that are simply too rare to
   classify and (2) applicability to a single representative sample or a
   single pooled set of representative samples from each of two habitat
   types. The method as currently developed can be applied to no more than
   two habitats at a time.}},
DOI = {{10.1890/10-1345.1}},
ISSN = {{0012-9658}},
ResearcherID-Numbers = {{Colwell, Robert/C-7276-2015
   publist, CMEC/C-3010-2012
   publicationpage, cmec/B-4405-2017
   zhang, yaqun/J-8478-2014
   }},
ORCID-Numbers = {{Letcher, Susan/0000-0002-9475-7674
   Chazdon, Robin/0000-0002-7349-5687}},
Unique-ID = {{ISI:000292633900015}},
}

@article{ ISI:000291712900013,
Author = {Nicholls, K. H. and Hoyle, J. A. and Johannsson, O. E. and Dermott, R.},
Title = {{A biological regime shift in the Bay of Quinte ecosystem (Lake Ontario)
   associated with the establishment of invasive dreissenid mussels}},
Journal = {{JOURNAL OF GREAT LAKES RESEARCH}},
Year = {{2011}},
Volume = {{37}},
Number = {{2}},
Pages = {{310-317}},
Month = {{JUN}},
Abstract = {{Thirty-two biological variables (taxonomic and/or functional groups)
   representing the four major communities, phytoplankton, zooplankton,
   benthos and fish, characterizing the upper Bay of Quinte (Lake Ontario)
   ecosystem, were assembled for the 27-year period, 1982-2008. Coincident
   regime shifts were detected in phytoplankton, benthos, and fish in 1995,
   which was just after invasive zebra mussels (Dreissena spp.) became
   established in the bay in 1993-1994. Two independent methods were used
   to detect these shifts: 1) principal components analysis followed by a
   Regime Shift Detector test for a change point in the running mean of the
   first principal component scores and (2) measurements of significant
   difference between pre- and post-Dreissena ecosystem structure based on
   measures of Bray-Curtis community similarity. Although a statistically
   significant shift was not detected in the zooplankton community by
   itself, zooplankton variables were instrumental in the overall ecosystem
   shift, determined for the combined four communities. All 32 variables
   were ranked for their individual contribution to the difference between
   the pre- and post-Dreissena ecosystem structures. The resolution of two
   distinct ecosystem structures, pre- and post-Dreissena, was greatly
   improved after employing a novel method of variable optimization that
   involved a selective and sequential removal of variables contributing
   least to the statistical difference between pre- and post-Dreissena
   ecosystem structures. The resultant 20-variable subset defined a 1995
   ecosystem regime shift at very high level of statistical confidence
   (ANOSIM-R=0.970). (C) 2010 International Association for Great Lakes
   Research. Published by Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.jglr.2010.12.004}},
ISSN = {{0380-1330}},
Unique-ID = {{ISI:000291712900013}},
}

@article{ ISI:000291063300013,
Author = {Samia, Noelle I. and Chan, Kung-Sik},
Title = {{Maximum likelihood estimation of a generalized threshold stochastic
   regression model}},
Journal = {{BIOMETRIKA}},
Year = {{2011}},
Volume = {{98}},
Number = {{2}},
Pages = {{433-448}},
Month = {{JUN}},
Abstract = {{There is hardly any literature on modelling nonlinear dynamic relations
   involving nonnormal time series data. This is a serious lacuna because
   nonnormal data are far more abundant than normal ones, for example, time
   series of counts and positive time series. While there are various forms
   of nonlinearities, the class of piecewise-linear models is particularly
   appealing for its relative ease of tractability and interpretation. We
   propose to study the generalized threshold model which specifies that
   the conditional probability distribution of the response variable
   belongs to an exponential family, and the conditional mean response is
   linked to some piecewise-linear stochastic regression function. We
   introduce a likelihood-based estimation scheme, and the consistency and
   limiting distribution of the maximum likelihood estimator are derived.
   We illustrate the proposed approach with an analysis of a hare abundance
   time series, which gives new insights on how phase-dependent
   predator-prey-climate interactions shaped the ten-year hare population
   cycle. A simulation study is conducted to examine the finite-sample
   performance of the proposed estimation method.}},
DOI = {{10.1093/biomet/asr008}},
ISSN = {{0006-3444}},
Unique-ID = {{ISI:000291063300013}},
}

@article{ ISI:000290735100010,
Author = {Reinert, Anja and Mittag, Anja and Reinert, Tilo and Tarnok, Attila and
   Arendt, Thomas and Morawski, Markus},
Title = {{On the Quantification of Intracellular Proteins in
   Multifluorescence-Labeled Rat Brain Slices Using Slide-Based Cytometry}},
Journal = {{CYTOMETRY PART A}},
Year = {{2011}},
Volume = {{79A}},
Number = {{6}},
Pages = {{485-491}},
Month = {{JUN}},
Abstract = {{In several brain regions, a subpopulation of neurons exists being
   characterized by the expression of a peculiar form of extracellular
   matrix, a so-called perineuronal net (PNN). We have previously shown
   that the PNN can bind large amounts of iron due to its polyanionic
   charge. Because free iron can generate reactive oxygen species thus
   being potentially toxic, the PNN may have a protective function by
   ``scavenging'' this free iron. Because of this ability, we have
   hypothesized that PNN-related neurons have an altered iron-specific
   metabolism.
   Thus, to compare the intracellular concentrations of iron containing
   proteins, specifically, the iron storage protein ferritin H between
   neurons with and without a PNN, we have used slide-based cytometry with
   image-based threshold-boundary cell detection on brain sections. In
   tissue sections, the integrity of the extracellular matrix, especially
   the characteristic PNNs, is preserved, which is necessary for the
   identification of the two neuronal subpopulations. A multilabeling
   approach was chosen to select neurons (neuronal marker NeuN), to
   classify the neurons according to their subtype (matrix marker Wisteria
   floribunda agglutinin), and to quantify the protein concentration
   (protein marker).
   Using this novel method, we were able to detect a relative difference in
   protein concentration as low as 12\% between the two subpopulations of
   neurons in the neuronal population of the rat parietal cortex. (C) 2011
   International Society for Advancement of Cytometry}},
DOI = {{10.1002/cyto.a.21047}},
ISSN = {{1552-4922}},
ResearcherID-Numbers = {{Arendt, Thomas/C-8747-2012
   Reinert, Tilo/H-2453-2013
   Morawski, Markus/D-4877-2011}},
ORCID-Numbers = {{Morawski, Markus/0000-0002-3817-5186}},
Unique-ID = {{ISI:000290735100010}},
}

@article{ ISI:000292129700001,
Author = {Chen, Wen Han and Sun, Ping Ping and Lu, Yang and Guo, William W. and
   Huang, Yan Xin and Ma, Zhi Qiang},
Title = {{MimoPro: a more efficient Web-based tool for epitope prediction using
   phage display libraries}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2011}},
Volume = {{12}},
Month = {{MAY 25}},
Abstract = {{Background: A B-cell epitope is a group of residues on the surface of an
   antigen which stimulates humoral responses. Locating these epitopes on
   antigens is important for the purpose of effective vaccine design. In
   recent years, mapping affinity-selected peptides screened from a random
   phage display library to the native epitope has become popular in
   epitope prediction. These peptides, also known as mimotopes, share the
   similar structure and function with the corresponding native epitopes.
   Great effort has been made in using this similarity between such
   mimotopes and native epitopes in prediction, which has resulted in
   better outcomes than statistics-based methods can. However, it cannot
   maintain a high degree of satisfaction in various circumstances.
   Results: In this study, we propose a new method that maps a group of
   mimotopes back to a source antigen so as to locate the interacting
   epitope on the antigen. The core of this method is a searching algorithm
   that is incorporated with both dynamic programming (DP) and branch and
   bound (BB) optimization and operated on a series of overlapping patches
   on the surface of a protein. These patches are then transformed to a
   number of graphs using an adaptable distance threshold (ADT) regulated
   by an appropriate compactness factor (CF), a novel parameter proposed in
   this study. Compared with both Pep-3D-Search and PepSurf, two leading
   graph-based search tools, on average from the results of 18 test cases,
   MimoPro, the Web-based implementation of our proposed method, performed
   better in sensitivity, precision, and Matthews correlation coefficient
   (MCC) than both did in epitope prediction. In addition, MimoPro is
   significantly faster than both Pep-3D-Search and PepSurf in processing.
   Conclusions: Our search algorithm designed for processing well
   constructed graphs using an ADT regulated by CF is more sensitive and
   significantly faster than other graph-based approaches in epitope
   prediction. MimoPro is a viable alternative to both PepSurf and
   Pep-3D-Search for epitope prediction in the same kind, and freely
   accessible through the MimoPro server located at
   http://informatics.nenu.edu.cn/MimoPro.}},
DOI = {{10.1186/1471-2105-12-199}},
Article-Number = {{199}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000292129700001}},
}

@article{ ISI:000290622300003,
Author = {de Barros, Felipe P. J. and Bolster, Diogo and Sanchez-Vila, Xavier and
   Nowak, Wolfgang},
Title = {{A divide and conquer approach to cope with uncertainty, human health
   risk, and decision making in contaminant hydrology}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2011}},
Volume = {{47}},
Month = {{MAY 10}},
Abstract = {{Assessing health risk in hydrological systems is an interdisciplinary
   field. It relies on the expertise in the fields of hydrology and public
   health and needs powerful translation concepts to provide decision
   support and policy making. Reliable health risk estimates need to
   account for the uncertainties and variabilities present in hydrological,
   physiological, and human behavioral parameters. Despite significant
   theoretical advancements in stochastic hydrology, there is still a dire
   need to further propagate these concepts to practical problems and to
   society in general. Following a recent line of work, we use fault trees
   to address the task of probabilistic risk analysis and to support
   related decision and management problems. Fault trees allow us to
   decompose the assessment of health risk into individual manageable
   modules, thus tackling a complex system by a structural divide and
   conquer approach. The complexity within each module can be chosen
   individually according to data availability, parsimony, relative
   importance, and stage of analysis. Three differences are highlighted in
   this paper when compared to previous works: (1) The fault tree proposed
   here accounts for the uncertainty in both hydrological and health
   components, (2) system failure within the fault tree is defined in terms
   of risk being above a threshold value, whereas previous studies that
   used fault trees used auxiliary events such as exceedance of critical
   concentration levels, and (3) we introduce a new form of stochastic
   fault tree that allows us to weaken the assumption of independent
   subsystems that is required by a classical fault tree approach. We
   illustrate our concept in a simple groundwater-related setting.}},
DOI = {{10.1029/2010WR009954}},
Article-Number = {{W05508}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
ResearcherID-Numbers = {{Bolster, Diogo/D-9667-2011
   de Barros, Felipe/M-6542-2013
   }},
ORCID-Numbers = {{Bolster, Diogo/0000-0003-3960-4090
   Sanchez-Vila, Xavier/0000-0002-1234-9897}},
Unique-ID = {{ISI:000290622300003}},
}

@article{ ISI:000291119500017,
Author = {Madsen, Anni T. and Murray, Andrew S. and Jain, Mayank and Andersen,
   Thorbjorn J. and Pejrup, Morten},
Title = {{A new method for measuring bioturbation rates in sandy tidal flat
   sediments based on luminescence dating}},
Journal = {{ESTUARINE COASTAL AND SHELF SCIENCE}},
Year = {{2011}},
Volume = {{92}},
Number = {{3}},
Pages = {{464-471}},
Month = {{MAY 1}},
Abstract = {{The rates of post-depositional mixing by bioturbation have been
   investigated using Optically Stimulated Luminescence (OSL) dating in two
   sediment cores (BAL2 and BAL5), retrieved from a sandy tidal flat in the
   Danish part of the Wadden Sea. A high-resolution chronology, consisting
   of thirty-six OSL ages with ages ranging between 2 +/- 4 and 410 +/- 20
   years, is presented. Slices of sediment (1-2 cm thick) have been dated
   at least every 5 cm. and from these data mixing depths of 20 cm and 22
   cm (BAL5 and BAL2, respectively) are readily identified. Below the
   mixing zone there is a significant decrease in the apparent
   sedimentation rate at BAL5 (from 6.4 to 0.3 mm a(-1)) and an abrupt
   increase in OSL age at the other site, BAL2 (from similar to 19
   years-similar to 290 years). Apparent sedimentation rates in the upper
   similar to 20 cm are more rapid than at greater depths, due to the age
   underestimation arising from the mixing processes. The significant
   change in sedimentation rates at BAL5 may indicate an offset in OSL age
   of up to 620 years. This paper uses a simple conceptual model for
   vertical mixing in which all the sediment excreted at the surface by
   lugworms is assumed to be completely reset before being re-incorporated
   in the sediment column. Using this approach, we conclude that the upward
   transport rate due to lugworms at BAL5 is 5-6 mm a(-1), and that the net
   sedimentation rate is similar to 0.3 mm a(-1), whereas at BAL2 the
   upward transport rate is currently similar to 16 mm a(-1) and that the
   site is probably eroding. The situation below the mixing depth (22 cm)
   is however unclear because of a marked 300 year hiatus. This is the
   first time that OSL has been used to determine bioturbation rates and
   our data clearly show the potential of such measurements to provide
   retrospective field estimates of this important environmental parameter.
   (C) 2011 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ecss.2011.02.004}},
ISSN = {{0272-7714}},
EISSN = {{1096-0015}},
ResearcherID-Numbers = {{Murray, Andrew/A-4388-2012
   Andersen, Thorbjorn/N-7560-2014
   }},
ORCID-Numbers = {{Andersen, Thorbjorn/0000-0001-5032-9945
   Jain, Mayank/0000-0001-8290-7324
   Pejrup, Morten/0000-0002-4662-5132
   Murray, Andrew/0000-0001-5559-1862
   Jain, Mayank/0000-0002-8942-7566}},
Unique-ID = {{ISI:000291119500017}},
}

@article{ ISI:000290095400011,
Author = {Franke, John E. and Yakubu, Abdul-Aziz},
Title = {{PERIODICALLY FORCED DISCRETE-TIME SIS EPIDEMIC MODEL WITH DISEASE
   INDUCED MORTALITY}},
Journal = {{MATHEMATICAL BIOSCIENCES AND ENGINEERING}},
Year = {{2011}},
Volume = {{8}},
Number = {{2, SI}},
Pages = {{385-408}},
Month = {{APR}},
Abstract = {{We use a periodically forced SIS epidemic model with disease induced
   mortality to study the combined effects of seasonal trends and death on
   the extinction and persistence of discretely reproducing populations. We
   introduce the epidemic threshold parameter, R-0, for predicting disease
   dynamics in periodic environments. Typically, R-0 < 1 implies disease
   extinction. However, in the presence of disease induced mortality, we
   extend the results of Franke and Yakubu to periodic environments and
   show that a small number of infectives can drive an otherwise persistent
   population with R-0 > 1 to extinction. Furthermore, we obtain conditions
   for the persistence of the total population. In addition, we use the
   Beverton-Holt recruitment function to show that the infective population
   exhibits period-doubling bifurcations route to chaos where the
   disease-free susceptible population lives on a 2-cycle (non-chaotic)
   attractor.}},
DOI = {{10.3934/mbe.2011.8.385}},
ISSN = {{1547-1063}},
Unique-ID = {{ISI:000290095400011}},
}

@article{ ISI:000289280300001,
Author = {Grossi, Roberto and Pietracaprina, Andrea and Pisanti, Nadia and Pucci,
   Geppino and Upfal, Eli and Vandin, Fabio},
Title = {{MADMX: A Strategy for Maximal Dense Motif Extraction}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2011}},
Volume = {{18}},
Number = {{4}},
Pages = {{535-545}},
Month = {{APR}},
Abstract = {{We develop, analyze, and experiment with a new tool, called madmx, which
   extracts frequent motifs from biological sequences. We introduce the
   notion of density to single out the ``significant{''} motifs. The
   density is a simple and flexible measure for bounding the number of
   don't cares in a motif, defined as the fraction of solid (i.e.,
   different from don't care) characters in the motif. A maximal dense
   motif has density above a certain threshold, and any further
   specialization of a don't care symbol in it or any extension of its
   boundaries decreases its number of occurrences in the input sequence. By
   extracting only maximal dense motifs, madmx reduces the output size and
   improves performance, while enhancing the quality of the discoveries.
   The efficiency of our approach relies on a newly defined combining
   operation, dubbed fusion, which allows for the construction of maximal
   dense motifs in a bottom-up fashion, while avoiding the generation of
   nonmaximal ones. We provide experimental evidence of the efficiency and
   the quality of the motifs returned by madmx.}},
DOI = {{10.1089/cmb.2010.0177}},
ISSN = {{1066-5277}},
EISSN = {{1557-8666}},
ORCID-Numbers = {{PISANTI, NADIA/0000-0003-3915-7665
   Upfal, Eli/0000-0002-9321-9460}},
Unique-ID = {{ISI:000289280300001}},
}

@article{ ISI:000289162000006,
Author = {Kifer, I. and Nussinov, R. and Wolfson, H. J.},
Title = {{GOSSIP: a method for fast and accurate global alignment of protein
   structures}},
Journal = {{BIOINFORMATICS}},
Year = {{2011}},
Volume = {{27}},
Number = {{7}},
Pages = {{925-932}},
Month = {{APR 1}},
Abstract = {{Motivation: The database of known protein structures (PDB) is increasing
   rapidly. This results in a growing need for methods that can cope with
   the vast amount of structural data. To analyze the accumulating data, it
   is important to have a fast tool for identifying similar structures and
   clustering them by structural resemblance. Several excellent tools have
   been developed for the comparison of protein structures. These usually
   address the task of local structure alignment, an important yet
   computationally intensive problem due to its complexity. It is difficult
   to use such tools for comparing a large number of structures to each
   other at a reasonable time.
   Results: Here we present GOSSIP, a novel method for a global
   all-against-all alignment of any set of protein structures. The method
   detects similarities between structures down to a certain cutoff (a
   parameter of the program), hence allowing it to detect similar
   structures at a much higher speed than local structure alignment
   methods. GOSSIP compares many structures in times which are several
   orders of magnitude faster than well-known available structure alignment
   servers, and it is also faster than a database scanning method. We
   evaluate GOSSIP both on a dataset of short structural fragments and on
   two large sequence-diverse structural benchmarks. Our conclusions are
   that for a threshold of 0.6 and above, the speed of GOSSIP is obtained
   with no compromise of the accuracy of the alignments or of the number of
   detected global similarities.}},
DOI = {{10.1093/bioinformatics/btr044}},
ISSN = {{1367-4803}},
Unique-ID = {{ISI:000289162000006}},
}

@article{ ISI:000287612100006,
Author = {Kuhnert, Ronny and Hecker, Hartmut and Poethko-Mueller, Christina and
   Schlaud, Martin and Vennemann, Mechtild and Whitaker, Heather J. and
   Farrington, C. Paddy},
Title = {{A modified self-controlled case series method to examine association
   between multidose vaccinations and death}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2011}},
Volume = {{30}},
Number = {{6}},
Pages = {{666-677}},
Month = {{MAR 15}},
Abstract = {{The self-controlled case series method (SCCS) was developed to analyze
   the association between a time-varying exposure and an outcome event. We
   consider penta-or hexavalent vaccination as the exposure and unexplained
   sudden unexpected death (uSUD) as the event. The special situation of
   multiple exposures and a terminal event requires adaptation of the
   standard SCCS method. This paper proposes a new adaptation, in which
   observation periods are truncated according to the vaccination schedule.
   The new method exploits known minimum spacings between successive
   vaccine doses. Its advantage is that it is very much simpler to apply
   than the method for censored, perturbed or curtailed post-event
   exposures recently introduced. This paper presents a comparison of these
   two SCCS methods by simulation studies and an application to a real data
   set. In the simulation studies, the age distribution and the assumed
   vaccination schedule were based on real data. Only small differences
   between the two SCCS methods were observed, although 50 per cent of
   cases could not be included in the analysis with the SCCS method with
   truncated observation periods. By means of a study including 300 uSUD, a
   16-fold risk increase after the 4th dose could be detected with a power
   of at least 90 per cent. A general 2-fold risk increase after
   vaccination could be detected with a power of 80 per cent. Reanalysis of
   data from cases of the German case-control study on sudden infant death
   (GeSID) resulted in slightly higher point estimates using the SCCS
   methods than the odds ratio obtained by the case-control analysis.
   Copyright (C) 2010 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/sim.4120}},
ISSN = {{0277-6715}},
ResearcherID-Numbers = {{Whitaker, Heather/O-4995-2017
   Vennemann, Mechtild/D-2169-2012
   }},
ORCID-Numbers = {{Whitaker, Heather/0000-0001-5833-1863
   Farrington, Conor Patrick/0000-0002-7148-2612}},
Unique-ID = {{ISI:000287612100006}},
}

@article{ ISI:000288156000021,
Author = {Paten, Benedict and Diekhans, Mark and Earl, Dent and St John, John and
   Ma, Jian and Suh, Bernard and Haussler, David},
Title = {{Cactus Graphs for Genome Comparisons}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2011}},
Volume = {{18}},
Number = {{3}},
Pages = {{469-481}},
Month = {{MAR}},
Abstract = {{We introduce a data structure, analysis, and visualization scheme called
   a cactus graph for comparing sets of related genomes. In common with
   multi-break point graphs and A-Bruijn graphs, cactus graphs can
   represent duplications and general genomic rearrangements, but
   additionally, they naturally decompose the common substructures in a set
   of related genomes into a hierarchy of chains that can be visualized as
   two-dimensional multiple alignments and nets that can be visualized in
   circular genome plots. Supplementary Material is available at
   www.liebertonline.com/cmb.}},
DOI = {{10.1089/cmb.2010.0252}},
ISSN = {{1066-5277}},
ORCID-Numbers = {{/0000-0002-4202-5834}},
Unique-ID = {{ISI:000288156000021}},
}

@article{ ISI:000287759000020,
Author = {Li, Guodong and Li, Wai Keung},
Title = {{Testing a linear time series model against its threshold extension}},
Journal = {{BIOMETRIKA}},
Year = {{2011}},
Volume = {{98}},
Number = {{1}},
Pages = {{243-250}},
Month = {{MAR}},
Abstract = {{This paper derives the asymptotic null distribution of a quasilikelihood
   ratio test statistic for an autoregressive moving average model against
   its threshold extension. The null hypothesis is that of no threshold,
   and the error term could be dependent. The asymptotic distribution is
   rather complicated, and all existing methods for approximating a
   distribution in the related literature fail to work. Hence, a novel
   bootstrap approximation based on stochastic permutation is proposed in
   this paper. Besides being robust to the assumptions on the error term,
   our method enjoys more flexibility and needs less computation when
   compared with methods currently used in the literature. Monte Carlo
   experiments give further support to the new approach, and an
   illustration is reported.}},
DOI = {{10.1093/biomet/asq074}},
ISSN = {{0006-3444}},
ResearcherID-Numbers = {{Li, Wai/I-7954-2015
   Li, Guodong/A-2741-2010}},
ORCID-Numbers = {{Li, Guodong/0000-0003-3137-8471}},
Unique-ID = {{ISI:000287759000020}},
}

@article{ ISI:000287620000015,
Author = {Ramos-Jiliberto, Rodrigo and Valdovinos, Fernanda S. and Arias, Jonathan
   and Alcaraz, Carles and Garcia-Berthou, Emili},
Title = {{A network-based approach to the analysis of ontogenetic diet shifts: An
   example with an endangered, small-sized}},
Journal = {{ECOLOGICAL COMPLEXITY}},
Year = {{2011}},
Volume = {{8}},
Number = {{1}},
Pages = {{123-129}},
Month = {{MAR}},
Abstract = {{Many organisms exhibit ontogenetic shifts in their diet and habitat use,
   which often exert a large influence on the structure and expected
   dynamics of food webs and ecological communities. Nevertheless, reliable
   methods for detecting these niche shifts from consumption data are
   limited. In this study we present a new approach for the detection and
   analysis of ontogenetic diet shifts, based on complex network theory. As
   a case study, we apply these methods to the endangered, small fish
   Aphanius iberus. The stage-structured consumer population and its set of
   consumed prey are represented as an unweighted bipartite network. A
   statistical evaluation of the resulting network structure permits to
   uncover empirical patterns of ontogenetic diet shifts. We test for
   changes in niche breadth, as well as nestedness and diet modularity
   along ontogeny. These tests were carried out on the subnetworks
   describing consumption, positive electivity, and negative electivity on
   prey items. The statistical significance was established by means of
   null model analyses. Our analyses reveal a nested diet, along with a
   gradual decrease in diet breadth and a modular structure (i.e. abrupt
   changes) of elected preys along the ontogeny of A. iberus. The detection
   of network structure by means of the use of tools from complex network
   theory is shown to be a promising method for studying ontogenetic niche
   shifts. (C) 2010 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecocom.2010.11.005}},
ISSN = {{1476-945X}},
EISSN = {{1476-9840}},
ResearcherID-Numbers = {{Garcia-Berthou, Emili/A-5392-2008
   Alcaraz, Carles/D-1930-2009
   Ramos-Jiliberto, Rodrigo/F-3160-2013}},
ORCID-Numbers = {{Garcia-Berthou, Emili/0000-0001-8412-741X
   Alcaraz, Carles/0000-0002-2147-4796
   }},
Unique-ID = {{ISI:000287620000015}},
}

@article{ ISI:000288344000039,
Author = {Mogireddy, K. and Devabhaktuni, V. and Kumar, A. and Aggarwal, P. and
   Bhattacharya, P.},
Title = {{A new approach to simulate characterization of particulate matter
   employing support vector machines}},
Journal = {{JOURNAL OF HAZARDOUS MATERIALS}},
Year = {{2011}},
Volume = {{186}},
Number = {{2-3}},
Pages = {{1254-1262}},
Month = {{FEB 28}},
Abstract = {{This paper, for the first time, applies the support vector machines
   (SVMs) paradigm to identify the optimal segmentation algorithm for
   physical characterization of particulate matter. Size of the particles
   is an essential component of physical characterization as larger
   particles get filtered through nose and throat while smaller particles
   have detrimental effect on human health. Typical particulate
   characterization processes involve image reading, preprocessing,
   segmentation, feature extraction, and representation. Of these various
   steps, knowledge based selection of optimal image segmentation algorithm
   (from existing segmentation algorithms) is the key for accurately
   analyzing the captured images of fine particulate matter. Motivated by
   the emerging machine-learning concepts, we present a new framework for
   automating the selection of optimal image segmentation algorithm
   employing SVMs trained and validated with image feature data. Results
   show that the SVM method accurately predicts the best segmentation
   algorithm. As well, an image processing algorithm based on Sobel edge
   detection is developed and illustrated. (C) 2010 Elsevier B.V. All
   rights reserved.}},
DOI = {{10.1016/j.jhazmat.2010.11.129}},
ISSN = {{0304-3894}},
EISSN = {{1873-3336}},
Unique-ID = {{ISI:000288344000039}},
}

@article{ ISI:000287106200002,
Author = {Sivaganesan, S. and Laud, Purushottam W. and Mueller, Peter},
Title = {{A Bayesian subgroup analysis with a zero-enriched Polya Urn scheme}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2011}},
Volume = {{30}},
Number = {{4}},
Pages = {{312-323}},
Month = {{FEB 20}},
Abstract = {{We introduce a new approach to inference for subgroups in clinical
   trials. We use Bayesian model selection, and a threshold on posterior
   model probabilities to identify subgroup effects for reporting. For each
   covariate of interest, we define a separate class of models, and use the
   posterior probability associated with each model and the threshold to
   determine the existence of a subgroup effect. As usual in Bayesian
   clinical trial design we compute frequentist operating characteristics,
   and achieve the desired error probabilities by choosing an appropriate
   threshold(s) for the posterior probabilities. Copyright (C) 2010 John
   Wiley \& Sons, Ltd.}},
DOI = {{10.1002/sim.4108}},
ISSN = {{0277-6715}},
Unique-ID = {{ISI:000287106200002}},
}

@article{ ISI:000290221000053,
Author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S.},
Title = {{Repeat-aware modeling and correction of short read errors}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2011}},
Volume = {{12}},
Number = {{1}},
Month = {{FEB 15}},
Note = {{9th Asia Pacific Bioinformatics Conference (APBC), Incheon, SOUTH KOREA,
   JAN 11-14, 2011}},
Abstract = {{Background: High-throughput short read sequencing is revolutionizing
   genomics and systems biology research by enabling cost-effective deep
   coverage sequencing of genomes and transcriptomes. Error detection and
   correction are crucial to many short read sequencing applications
   including de novo genome sequencing, genome resequencing, and digital
   gene expression analysis. Short read error detection is typically
   carried out by counting the observed frequencies of kmers in reads and
   validating those with frequencies exceeding a threshold. In case of
   genomes with high repeat content, an erroneous kmer may be frequently
   observed if it has few nucleotide differences with valid kmers with
   multiple occurrences in the genome. Error detection and correction were
   mostly applied to genomes with low repeat content and this remains a
   challenging problem for genomes with high repeat content.
   Results: We develop a statistical model and a computational method for
   error detection and correction in the presence of genomic repeats. We
   propose a method to infer genomic frequencies of kmers from their
   observed frequencies by analyzing the misread relationships among
   observed kmers. We also propose a method to estimate the threshold
   useful for validating kmers whose estimated genomic frequency exceeds
   the threshold. We demonstrate that superior error detection is achieved
   using these methods. Furthermore, we break away from the common
   assumption of uniformly distributed errors within a read, and provide a
   framework to model position-dependent error occurrence frequencies
   common to many short read platforms. Lastly, we achieve better error
   correction in genomes with high repeat content. Availability: The
   software is implemented in C++ and is freely available under GNU GPL3
   license and Boost Software V1.0 license at
   ``http://aluru-sun.ece.iastate.edu/doku.php?id=redeem{''}.
   Conclusions: We introduce a statistical framework to model sequencing
   errors in next-generation reads, which led to promising results in
   detecting and correcting errors for genomes with high repeat content.}},
DOI = {{10.1186/1471-2105-12-S1-S52}},
Article-Number = {{S52}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Yang, Xiao/I-7329-2013
   }},
ORCID-Numbers = {{Dorman, Karin/0000-0003-3650-0018}},
Unique-ID = {{ISI:000290221000053}},
}

@article{ ISI:000288098700015,
Author = {Phillis, Corey C. and Ostrach, David J. and Ingram, B. Lynn and Weber,
   Peter K.},
Title = {{Evaluating otolith Sr/Ca as a tool for reconstructing estuarine habitat
   use}},
Journal = {{CANADIAN JOURNAL OF FISHERIES AND AQUATIC SCIENCES}},
Year = {{2011}},
Volume = {{68}},
Number = {{2}},
Pages = {{360-373}},
Month = {{FEB}},
Abstract = {{There is no standard method to determine the applicability of otolith
   Sr/Ca ratio to reconstructing estuary use. We have developed a novel
   method to determine the response of otolith Sr/Ca to changes in water
   Sr/Ca and salinity in San Francisco Estuary ( California, USA). We
   perform correlated, spatially resolved Sr/Ca and Sr isotope measurements
   using otoliths from adult striped bass ( Morone saxatilis) in the San
   Francisco Estuary to estimate the otolith-water Sr/Ca partition
   coefficient (D(Sr) = 0.305 +/- 0.009). DSr did not vary significantly
   with salinity, and therefore the salinity-otolith Sr/Ca model was
   constructed by substituting the partition coefficient into the nonlinear
   salinity-water Sr/Ca mixing model for the system. The model demonstrates
   that the primary factor controlling the response of Sr/Ca to salinity is
   the Ca concentration in the freshwater source flowing into the estuary.
   A concentration of 60 ppm Ca is an approximate threshold below which
   estuary Sr/Ca increases rapidly to near the marine Sr/Ca at low
   salinities (5 parts per thousand-15 parts per thousand), thereby
   providing sharp delineation of estuary entrance, but little to no
   discrimination among higher salinity habitats. Our approach provides a
   general framework for assessing the potential utility of Sr/Ca in
   estuarine systems and specifically for the San Francisco Estuary.}},
DOI = {{10.1139/F10-152}},
ISSN = {{0706-652X}},
Unique-ID = {{ISI:000288098700015}},
}

@article{ ISI:000287470800009,
Author = {Bodvard, Kristofer and Wrangborg, David and Tapani, Sofia and Logg,
   Katarina and Sliwa, Piotr and Blomberg, Anders and Kvarnstrom, Mats and
   Kall, Mikael},
Title = {{Continuous light exposure causes cumulative stress that affects the
   localization oscillation dynamics of the transcription factor Msn2p}},
Journal = {{BIOCHIMICA ET BIOPHYSICA ACTA-MOLECULAR CELL RESEARCH}},
Year = {{2011}},
Volume = {{1813}},
Number = {{2}},
Pages = {{358-366}},
Month = {{FEB}},
Abstract = {{Light exposure is a potentially powerful stress factor during in vivo
   optical microscopy studies. In yeast, the general transcription factor
   Msn2p translocates from the cytoplasm to the nucleus in response to
   illumination. However, previous time-lapse fluorescence microscopy
   studies of Msn2p have utilized a variety of discrete exposure settings,
   which makes it difficult to correlate stress levels and illumination
   parameters. We here investigate how continuous illumination with blue
   light, corresponding to GFP excitation wavelengths, affects the
   localization pattern of Msn2p-GFP in budding yeast. The localization
   pattern was analyzed using a novel approach that combines wavelet
   decomposition and change point analysis. It was found that the Msn2p
   nucleocytoplasmic localization trajectories for individual cells exhibit
   up to three distinct and successive states; i) Msn2p localizes to the
   cytoplasm; ii) Msn2p rapidly shuttles between the cytoplasm and the
   nucleus; iii) Msn2p localizes to the nucleus. Many cells pass through
   all states consecutively at high light intensities, while at lower light
   intensities most cells only reach states i) or ii). This behaviour
   strongly indicates that continuous light exposure gradually increases
   the stress level over time, presumably through continuous accumulation
   of toxic photoproducts, thereby forcing the cell through a bistable
   region corresponding to nucleocytoplasmic oscillations. We also show
   that the localization patterns are dependent on protein kinase A (PKA)
   activity, i.e. yeast cells with constantly low PKA activity showed a
   stronger stress response. In particular, the nucleocytoplasmic
   oscillation frequency was found to be significantly higher for cells
   with low PKA activity for all light intensities. (C) 2010 Elsevier B.V.
   All rights reserved.}},
DOI = {{10.1016/j.bbamcr.2010.12.004}},
ISSN = {{0167-4889}},
EISSN = {{0006-3002}},
ResearcherID-Numbers = {{Kall, Mikael/A-6753-2012
   Kall, Mikael/A-2732-2008}},
ORCID-Numbers = {{Kall, Mikael/0000-0002-1163-0345
   }},
Unique-ID = {{ISI:000287470800009}},
}

@incollection{ ISI:000291368200004,
Author = {Horvath, Steve},
Book-Author = {{Horvath, S}},
Title = {{Adjacency Functions and Their Topological Effects}},
Booktitle = {{WEIGHTED NETWORK ANALYSIS: APPLICATIONS IN GENOMICS AND SYSTEMS BIOLOGY}},
Year = {{2011}},
Pages = {{77-89}},
Abstract = {{An adjacency function AF can be used to transform a given (original)
   network into a new network, i.e., A(original) is transformed into A = AF
   (A(original)). We introduce several useful adjacency functions and show
   how they can be used to construct networks, e.g., the power adjacency
   function AF(power)(), which raises each adjacency to a fixed power beta,
   can be used to construct weighted correlation networks. In general,
   adjacency functions depend on parameter values, e.g., AF(power) depends
   on the power beta. The choice of these parameter values can be informed
   by topological criteria based on network concepts. For example, we
   describe the scale-free topology criterion that considers the effect of
   the AF parameters on the scale-free topology fitting index. The power
   adjacency function can also be used to calibrate one network with
   respect to another network, which is often a prerequisite for
   differential network analysis or for consensus module detection. If an
   adjacency function (e.g., AF(power)) is defined via a monotonically
   increasing real-valued function, then it is called rank-preserving since
   the ranking of the original adjacencies equals that of the transformed
   adjacencies. The terminology of ``rank-equivalence{''} and
   ``threshold-equivalence{''} between adjacency functions, networks, and
   network construction methods allows us (a) to provide a general
   definition of a correlation network and (b) to study the relationships
   between different network construction methods.}},
DOI = {{10.1007/978-1-4419-8819-5\_4}},
ISBN = {{978-1-4419-8818-8}},
Unique-ID = {{ISI:000291368200004}},
}

@article{ ISI:000288681800004,
Author = {Kornak, John and Lu, Ying},
Title = {{Bayesian decision analysis for choosing between diagnostic/prognostic
   prediction procedures}},
Journal = {{STATISTICS AND ITS INTERFACE}},
Year = {{2011}},
Volume = {{4}},
Number = {{1}},
Pages = {{27-36}},
Abstract = {{New diagnostic procedures and prognostic markers are contnually being
   developed for a wide range of medical complaints. Medical institutions
   are therefore regularly faced with the decision as to whether to replace
   an existing procedure with a new one. The decision to adopt a new method
   is primarily based on diagnostic/predictive accuracy and
   cost-effectiveness, but this trade-off is not usually considered in a
   formal decision-theoretic way. The decision process for diagnostic
   procedures is complicated by the fact that diagnostic decisions are
   typically based on thresholding one or more continuous variables.
   Therefore, a formal decision process should account for uncertainty in
   the optimal threshold value for each diagnostic procedure. We here
   propose a Bayesian decision approach based on maximizing expected
   utility (incorporating accuracy and costs) with respect to diagnostic
   procedure and threshold level simultaneously. The Bayesian decision
   approach is illustrated via an application comparing the utility of
   different bone mineral density (BMD) measurements for determining the
   need for preventative treatment of osteoporotic hip fracture in elderly
   patients.}},
ISSN = {{1938-7989}},
EISSN = {{1938-7997}},
Unique-ID = {{ISI:000288681800004}},
}

@article{ ISI:000283400100007,
Author = {Kennedy, Robert E. and Yang, Zhigiang and Cohen, Warren B.},
Title = {{Detecting trends in forest disturbance and recovery using yearly Landsat
   time series: 1. LandTrendr - Temporal segmentation algorithms}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2010}},
Volume = {{114}},
Number = {{12}},
Pages = {{2897-2910}},
Month = {{DEC 15}},
Abstract = {{We introduce and test LandTrendr (Landsat-based detection of Trends in
   Disturbance and Recovery), a new approach to extract spectral
   trajectories of land surface change from yearly Landsat time-series
   stacks (LTS). The method brings together two themes in time-series
   analysis of LTS: capture of short-duration events and smoothing of
   long-term trends. Our strategy is founded on the recognition that change
   is not simply a contrast between conditions at two points in time, but
   rather a continual process operating at both fast and slow rates on
   landscapes. This concept requires both new algorithms to extract change
   and new interpretation tools to validate those algorithms. The challenge
   is to resolve salient features of the time series while eliminating
   noise introduced by ephemeral changes in illumination, phenology,
   atmospheric condition, and geometric registration. In the LandTrendr
   approach, we use relative radiometric normalization and simple cloud
   screening rules to create on-the-fly mosaics of multiple images per
   year, and extract temporal trajectories of spectral data on a
   pixel-by-pixel basis. We then apply temporal segmentation strategies
   with both regression-based and point-to-point fitting of spectral
   indices as a function of time, allowing capture of both slowly-evolving
   processes, such as regrowth, and abrupt events, such as forest harvest.
   Because any temporal trajectory pattern is allowable, we use control
   parameters and threshold-based filtering to reduce the role of false
   positive detections. No suitable reference data are available to assess
   the role of these control parameters or to test overall algorithm
   performance. Therefore, we also developed a companion interpretation
   approach founded on the same conceptual framework of capturing both long
   and short-duration processes, and developed a software tool to apply
   this concept to expert interpretation and segmentation of spectral
   trajectories (TimeSync, described in a companion paper by Cohen et al.,
   2010). These data were used as a truth set against which to evaluate the
   behavior of the LandTrendr algorithms applied to three spectral indices.
   We applied the LandTrendr algorithms to several hundred points across
   western Oregon and Washington (USA). Because of the diversity of
   potential outputs from the LTS data, we evaluated algorithm performance
   against summary metrics for disturbance, recovery, and stability, both
   for capture of events and longer-duration processes. Despite the
   apparent complexity of parameters, our results suggest a simple grouping
   of parameters along a single axis that balances the detection of abrupt
   events with capture of long-duration trends. Overall algorithm
   performance was good, capturing a wide range of disturbance and recovery
   phenomena, even when evaluated against a truth set that contained new
   targets (recovery and stability) with much subtler thresholds of change
   than available from prior validation datasets. Temporal segmentation of
   the archive appears to be a feasible and robust means of increasing
   information extraction from the Luldsat archive. (C) 2010 Elsevier Inc.
   All rights reserved.}},
DOI = {{10.1016/j.rse.2010.07.008}},
ISSN = {{0034-4257}},
Unique-ID = {{ISI:000283400100007}},
}

@article{ ISI:000285931900006,
Author = {Sandoval, M. L. and Rodriguez-Ferran, A.},
Title = {{A multiplicative Schwarz method with active subdomains for transient
   convection-diffusion problems}},
Journal = {{INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING}},
Year = {{2010}},
Volume = {{26}},
Number = {{12}},
Pages = {{1573-1585}},
Month = {{DEC}},
Abstract = {{An efficient algorithm to find the solution of transient
   convection-diffusion problems with dominant convection is presented. The
   main idea is to follow the solution front and activate those subdomains
   where the solution satisfies a given threshold value. We call this novel
   method `the multiplicative Schwarz method with active subdomains', and
   it is motivated by the solution of a problem from activated-carbon
   filters used in the automotive industry to reduce emissions. Numerical
   experiments show that this method is more efficient than the
   preconditioned conjugate gradient method with an incomplete Cholesky
   factorization. Copyright (C) 2009 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/cnm.1239}},
ISSN = {{2040-7939}},
ResearcherID-Numbers = {{Rodriguez-Ferran, Antonio/D-1623-2012
   }},
ORCID-Numbers = {{Rodriguez-Ferran, Antonio/0000-0002-9680-6046}},
Unique-ID = {{ISI:000285931900006}},
}

@article{ ISI:000288914700004,
Author = {Gutzwiller, Kevin J. and Barrow, Jr., Wylie C. and White, Joseph D. and
   Johnson-Randall, Lori and Cade, Brian S. and Zygo, Lisa M.},
Title = {{Assessing conservation relevance of organism-environment relations using
   predicted changes in response variables}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2010}},
Volume = {{1}},
Number = {{4}},
Pages = {{351-358}},
Month = {{DEC}},
Abstract = {{1. Organism-environment models are used widely in conservation. The
   degree to which they are useful for informing conservation decisions -
   the conservation relevance of these relations - is important because
   lack of relevance may lead to misapplication of scarce conservation
   resources or failure to resolve important conservation dilemmas. Even
   when models perform well based on model fit and predictive ability,
   conservation relevance of associations may not be clear without also
   knowing the magnitude and variability of predicted changes in response
   variables.
   2. We introduce a method for evaluating the conservation relevance of
   organism-environment relations that employs confidence intervals for
   predicted changes in response variables. The confidence intervals are
   compared to a preselected magnitude of change that marks a threshold
   (trigger) for conservation action. To demonstrate the approach, we used
   a case study from the Chihuahuan Desert involving relations between
   avian richness and broad-scale patterns of shrubland. We considered
   relations for three winters and two spatial extents (1- and 2-km-radius
   areas) and compared predicted changes in richness to three thresholds
   (10\%, 20\% and 30\% change). For each threshold, we examined 48
   relations.
   3. The method identified seven, four and zero conservation-relevant
   changes in mean richness for the 10\%, 20\% and 30\% thresholds
   respectively. These changes were associated with major (20\%) changes in
   shrubland cover, mean patch size, the coefficient of variation for patch
   size, or edge density but not with major changes in shrubland patch
   density. The relative rarity of conservation-relevant changes indicated
   that, overall, the relations had little practical value for informing
   conservation decisions about avian richness.
   4. The approach we illustrate is appropriate for various response and
   predictor variables measured at any temporal or spatial scale. The
   method is broadly applicable across ecological environments,
   conservation objectives, types of statistical predictive models and
   levels of biological organization. By focusing on magnitudes of change
   that have practical significance, and by using the span of confidence
   intervals to incorporate uncertainty of predicted changes, the method
   can be used to help improve the effectiveness of conservation efforts.}},
DOI = {{10.1111/j.2041-210X.2010.00042.x}},
ISSN = {{2041-210X}},
Unique-ID = {{ISI:000288914700004}},
}

@article{ ISI:000288452000019,
Author = {Zhang, Jiahua and Kong, Weijia and Yang, Zhongle},
Title = {{Identification of a Novel Dynamic Red Blindness in Human by
   Event-related Brain Potentials}},
Journal = {{JOURNAL OF HUAZHONG UNIVERSITY OF SCIENCE AND TECHNOLOGY-MEDICAL
   SCIENCES}},
Year = {{2010}},
Volume = {{30}},
Number = {{6}},
Pages = {{786-791}},
Month = {{DEC}},
Abstract = {{Dynamic color is an important carrier that takes information in some
   special occupations. However, up to the present, there are no available
   and objective tests to evaluate dynamic color processing. To investigate
   the characteristics of dynamic color processing, we adopted two patterns
   of visual stimulus called ``onset-offset{''} which reflected static
   color stimuli and ``sustained moving{''} without abrupt mode which
   reflected dynamic color stimuli to evoke event-related brain potentials
   (ERPs) in primary color amblyopia patients (abnormal group) and subjects
   with normal color recognition ability (normal group). ERPs were recorded
   by Neuroscan system. The results showed that in the normal group, ERPs
   in response to the dynamic red stimulus showed frontal positive
   amplitudes with a latency of about 180 ms, a negative peak at about 240
   ms and a peak latency of the late positive potential (LPP) in a time
   window between 290 and 580 ms. In the abnormal group, ERPs in response
   to the dynamic red stimulus were fully lost and characterized by
   vanished amplitudes between 0 and 800 ms. No significant difference was
   noted in ERPs in response to the dynamic green and blue stimulus between
   the two groups (P > 0.05). ERPs of the two groups in response to the
   static red, green and blue stimulus were not much different, showing a
   transient negative peak at about 170 ms and a peak latency of LPP in a
   time window between 350 and 650 ms. Our results first revealed that some
   subjects who were not identified as color blindness under static color
   recognition could not completely apperceive a sort of dynamic red
   stimulus by ERPs, which was called ``dynamic red blindness{''}.
   Furthermore, these results also indicated that low-frequency ERPs
   induced by ``sustained moving{''} may be a good and new method to test
   dynamic color perception competence.}},
DOI = {{10.1007/s11596-010-0659-2}},
ISSN = {{1672-0733}},
Unique-ID = {{ISI:000288452000019}},
}

@article{ ISI:000283686000007,
Author = {Laurin, Michel},
Title = {{Assessment of the Relative Merits of a Few Methods to Detect
   Evolutionary Trends}},
Journal = {{SYSTEMATIC BIOLOGY}},
Year = {{2010}},
Volume = {{59}},
Number = {{6}},
Pages = {{689-704}},
Month = {{DEC}},
Abstract = {{Some of the most basic questions about the history of life concern
   evolutionary trends. These include determining whether or not metazoans
   have become more complex over time, whether or not body size tends to
   increase over time (the Cope-Deperet rule), or whether or not brain size
   has increased over time in various taxa, such as mammals and birds.
   Despite the proliferation of studies on such topics, assessment of the
   reliability of results in this field is hampered by the variability of
   techniques used and the lack of statistical validation of these methods.
   To solve this problem, simulations are performed using a variety of
   evolutionary models (gradual Brownian motion, speciational Brownian
   motion, and Ornstein-Uhlenbeck), with or without a drift of variable
   amplitude, with variable variance of tips, and with bounds placed close
   or far from the starting values and final means of simulated characters.
   These are used to assess the relative merits (power, Type I error rate,
   bias, and mean absolute value of error on slope estimate) of several
   statistical methods that have recently been used to assess the presence
   of evolutionary trends in comparative data. Results show widely
   divergent performance of the methods. The simple, nonphylogenetic
   regression (SR) and variance partitioning using phylogenetic eigenvector
   regression (PVR) with a broken stick selection procedure have greatly
   inflated Type I error rate (0.123-0.180 at a 0.05 threshold), which
   invalidates their use in this context. However, they have the greatest
   power. Most variants of Felsenstein's independent contrasts (FIC; five
   of which are presented) have adequate Type I error rate, although two
   have a slightly inflated Type I error rate with at least one of the two
   reference trees (0.064-0.090 error rate at a 0.05 threshold). The power
   of all contrast-based methods is always much lower than that of SR and
   PVR, except under Brownian motion with a strong trend and distant
   bounds. Mean absolute value of error on slope of all FIC methods is
   slightly higher than that of phylogenetic generalized least squares
   (PGLS), SR, and PVR. PGLS performs well, with low Type I error rate, low
   error on regression coefficient, and power comparable with some FIC
   methods. Four variants of skewness analysis are examined, and a new
   method to assess significance of results is presented. However, all have
   consistently low power, except in rare combinations of trees, trend
   strength, and distance between final means and bounds. Globally, the
   results clearly show that FIC-based methods and PGLS are globally better
   than nonphylogenetic methods and variance partitioning with PVR. FIC
   methods and PGLS are sensitive to the model of evolution (and, hence, to
   branch length errors). Our results suggest that regressing raw character
   contrasts against raw geological age contrasts yields a good combination
   of power and Type I error rate. New software to facilitate batch
   analysis is presented.}},
DOI = {{10.1093/sysbio/syq059}},
ISSN = {{1063-5157}},
ResearcherID-Numbers = {{Laurin, Michel/B-7884-2008}},
ORCID-Numbers = {{Laurin, Michel/0000-0003-2974-9835}},
Unique-ID = {{ISI:000283686000007}},
}

@article{ ISI:000285333100001,
Author = {Szabo, Adrienn and Novak, Adam and Miklos, Istvan and Hein, Jotun},
Title = {{Reticular alignment: A progressive corner-cutting method for multiple
   sequence alignment}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2010}},
Volume = {{11}},
Month = {{NOV 23}},
Abstract = {{Background: In this paper, we introduce a progressive corner cutting
   method called Reticular Alignment for multiple sequence alignment.
   Unlike previous corner-cutting methods, our approach does not define a
   compact part of the dynamic programming table. Instead, it defines a set
   of optimal and suboptimal alignments at each step during the progressive
   alignment. The set of alignments are represented with a network to store
   them and use them during the progressive alignment in an efficient way.
   The program contains a threshold parameter on which the size of the
   network depends. The larger the threshold parameter and thus the
   network, the deeper the search in the alignment space for better scored
   alignments.
   Results: We implemented the program in the Java programming language,
   and tested it on the BAliBASE database. Reticular Alignment can
   outperform ClustalW even if a very simple scoring scheme (BLOSUM62 and
   affine gap penalty) is implemented and merely the threshold value is
   increased. However, this set-up is not sufficient for outperforming
   other cutting-edge alignment methods. On the other hand, the reticular
   alignment search strategy together with sophisticated scoring schemes
   (for example, differentiating gap penalties for hydrophobic and
   hydrophylic amino acids) overcome FSA and in some accuracy measurement,
   even MAFFT. The program is available from http://phylogeny-cafe. elte.
   hu/RetAlign/
   Conclusions: Reticular alignment is an efficient search strategy for
   finding accurate multiple alignments. The highest accuracy achieved when
   this searching strategy is combined with sophisticated scoring schemes.}},
DOI = {{10.1186/1471-2105-11-570}},
Article-Number = {{570}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000285333100001}},
}

@article{ ISI:000286069100001,
Author = {Chen, Liang-Chien and Lin, Li-Jer},
Title = {{Detection of building changes from aerial images and light detection and
   ranging (LIDAR) data}},
Journal = {{JOURNAL OF APPLIED REMOTE SENSING}},
Year = {{2010}},
Volume = {{4}},
Month = {{NOV 19}},
Abstract = {{Building models are built to provide three-dimensional (3-D) spatial
   information, which is needed in a variety of applications including city
   planning, construction management, location-based services of urban
   infrastructures, and the like. However, 3-D building models have to be
   updated on a timely manner to meet the changing demand. Rather than
   reconstructing building models for the entire area, it would be more
   convenient and effective to only update parts of the areas where there
   were changes. This paper aims at developing a new method, namely
   double-threshold strategy, to find such changes within 3-D building
   models in the region of interest with the aid of light detection and
   ranging (LIDAR) data. The proposed modeling scheme comprises three
   steps, namely, data pre-processing, change detection in building areas,
   and validation. In the first step for data pre-processing, data
   registration was carried out based on multi-source data. The second step
   for data preprocessing requires using the triangulation of an irregular
   network of data points collected by Light Detection And Ranging (LIDAR),
   focusing on those locations containing walls or other above-ground
   objects that were ever removed. Then, change detection in the building
   models can be made possible for finding differences in height by
   comparing the LIDAR point measurements and the estimates of the building
   models. The results may be further refined using spectral and feature
   information collected from aerial imagery. A double-threshold strategy
   was applied to cope with the highly sensitive thresholding often
   encountered when using the rule-based approach. Finally, ground truth
   data were used for model validation. Research findings clearly indicate
   that the double-threshold strategy improves the overall accuracy from
   93.1\% to 95.9\%.}},
DOI = {{10.1117/1.3525560}},
Article-Number = {{041870}},
ISSN = {{1931-3195}},
Unique-ID = {{ISI:000286069100001}},
}

@article{ ISI:000282705900007,
Author = {Zhang, Jianlei and Zhang, Chunyan and Chu, Tianguang},
Title = {{Cooperation enhanced by the `survival of the fittest' rule in prisoner's
   dilemma games on complex networks}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2010}},
Volume = {{267}},
Number = {{1}},
Pages = {{41-47}},
Month = {{NOV 7}},
Abstract = {{Prevalence of cooperation within groups of selfish individuals is
   puzzling in that it contradicts with the basic premise of natural
   selection, whereby we introduce a model of strategy evolution taking
   place on evolving networks based on Darwinian `survival of the fittest'
   rule. In the present work, players whose payoffs are below a certain
   threshold will be deleted and the same number of new nodes will be added
   to the network to maintain the constant system size. Furthermore, the
   networking effect is also studied via implementing simulations on four
   typical network structures. Numerical results show that cooperators can
   obtain the biggest boost if the elimination threshold is fine-tuned.
   Notably, this coevolutionary rule drives the initial networks to evolve
   into statistically stationary states with a broad-scale degree
   distribution. Our results may provide many more insights for
   understanding the coevolution of strategy and network topology under the
   mechanism of nature selection whereby superior individuals will prosper
   and inferior ones be eliminated. (C) 2010 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.jtbi.2010.08.011}},
ISSN = {{0022-5193}},
ResearcherID-Numbers = {{Chu, Tianguang/C-3521-2012
   }},
ORCID-Numbers = {{Zhang, Jianlei/0000-0002-9932-9265}},
Unique-ID = {{ISI:000282705900007}},
}

@article{ ISI:000282862800012,
Author = {Stockdale, Anthony and Tipping, Edward and Lofts, Stephen and Ormerod,
   Stephen J. and Clements, William H. and Blust, Ronny},
Title = {{Toxicity of proton-metal mixtures in the field: Linking stream
   macroinvertebrate species diversity to chemical speciation and
   bioavailability}},
Journal = {{AQUATIC TOXICOLOGY}},
Year = {{2010}},
Volume = {{100}},
Number = {{1}},
Pages = {{112-119}},
Month = {{OCT 1}},
Abstract = {{Understanding metal and proton toxicity under field conditions requires
   consideration of the complex nature of chemicals in mixtures. Here, we
   demonstrate a novel method that relates streamwater concentrations of
   cationic metallic species and protons to a field ecological index of
   biodiversity. The model WHAM-F-TOX postulates that cation binding sites
   of aquatic macroinvertebrates can be represented by the functional
   groups of natural organic matter (humic acid), as described by the
   Windermere Humic Aqueous Model (WHAM6), and supporting field evidence is
   presented. We define a toxicity function (F-TOX) by summing the
   products: (amount of invertebrate-bound cation) x (cation-specific
   toxicity coefficient, alpha(i)). Species richness data for
   Ephemeroptera, Plecoptera and Trichoptera (EPT), are then described with
   a lower threshold of F-TOX, below which all organisms are present and
   toxic effects are absent, and an upper threshold above which organisms
   are absent. Between the thresholds the number of species declines
   linearly with F-TOX. We parameterised the model with chemistry and EPT
   data for low-order streamwaters affected by acid deposition and/or
   abandoned mines, representing a total of 412 sites across three
   continents. The fitting made use of quantile regression, to take into
   account reduced species richness caused by (unknown) factors other than
   cation toxicity. Parameters were derived for the four most common or
   abundant cations, with values of alpha(i) following the sequence
   (increasing toxicity) H+ < Al < Zn < Cu. For waters affected mainly by
   H+ and Al, F-TOX shows a steady decline with increasing pH, crossing the
   lower threshold near to pH 7. Competition effects among cations mean
   that toxicity due to Cu and Zn is rare at lower pH values, and occurs
   mostly between pH 6 and 8. (c) 2010 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.aquatox.2010.07.018}},
ISSN = {{0166-445X}},
ResearcherID-Numbers = {{Lofts, Stephen/K-1849-2012
   Stockdale, Anthony/A-4465-2009
   Tipping, Edward/I-6309-2012
   Ormerod, Steve/A-4326-2010
   Clements, William/N-2686-2016}},
ORCID-Numbers = {{Lofts, Stephen/0000-0002-3627-851X
   Stockdale, Anthony/0000-0002-1603-0103
   Tipping, Edward/0000-0001-6618-6512
   Ormerod, Steve/0000-0002-8174-302X
   }},
Unique-ID = {{ISI:000282862800012}},
}

@article{ ISI:000281127800011,
Author = {Nah, Kyeongah and Kim, Yongkuk and Lee, Jung Min},
Title = {{The dilution effect of the domestic animal population on the
   transmission of P. vivax malaria}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2010}},
Volume = {{266}},
Number = {{2}},
Pages = {{299-306}},
Month = {{SEP 21}},
Abstract = {{The diversion of disease carrying insect from humans to animals may
   reduce transmission of diseases such as malaria. The use of animals to
   mitigate mosquito bites on human is called `zooprophylaxis'. We
   introduce a mathematical model for Plasmodium vivax malaria transmission
   with two bloodmeal hosts (humans and domestic animals) to study the
   effect of zooprophylaxis. After computing the basic reproduction number
   from the proposed model, we explore how perturbations in the parameters,
   sensitive to the effects of control measures, affect its value.
   Zooprophylaxis is shown to determine whether a basic reproduction
   becomes bigger than an outbreak threshold value or not. Sensitivity
   analysis shows that increasing the relative animal population size works
   better in P. vivax malaria control than decreasing the mosquito
   population when the relative animal population size is larger than a
   threshold value. (C) 2010 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2010.06.032}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
ORCID-Numbers = {{Nah, Kyeongah/0000-0002-7708-1019}},
Unique-ID = {{ISI:000281127800011}},
}

@article{ ISI:000282071800019,
Author = {Lawn, Philip and Clarke, Matthew},
Title = {{The end of economic growth? A contracting threshold hypothesis}},
Journal = {{ECOLOGICAL ECONOMICS}},
Year = {{2010}},
Volume = {{69}},
Number = {{11}},
Pages = {{2213-2223}},
Month = {{SEP 15}},
Abstract = {{This paper argues that GDP growth in both developed and developing
   countries has associated costs that can outweigh the benefits and thus
   reduce sustainable well-being. This conclusion is based upon the
   findings of empirical applications of the Genuine Progress Indicator
   (GPI) to a range of countries in the Asia-Pacific region. The studies
   conducted on seven Asia-Pacific countries indicate that, in the case of
   five of the seven nations, more recent GDP growth has reduced the
   sustainable well-being experienced by the average citizen residing
   within them. Moreover, the threshold point at which the costs of GDP
   growth outweigh the benefits appears to be contracting (i.e., occurring
   at a much lower per capita level of GDP). This paper therefore
   introduces a new contracting threshold hypothesis: as the economies of
   the Asia-Pacific region and the world collectively expand in a
   globalised economic environment, there is a contraction over time in the
   threshold level of per capita GDP. As a consequence, the threshold point
   confronting growth late-comers (i.e., developing countries) occurs at a
   much lower level of sustainable welfare than what wealthy nations
   currently enjoy. The consequences of this for developing countries are
   clearly significant and require a new approach to economic development.
   (C) 2010 Published by Elsevier B.V.}},
DOI = {{10.1016/j.ecolecon.2010.06.007}},
ISSN = {{0921-8009}},
Unique-ID = {{ISI:000282071800019}},
}

@article{ ISI:000279495200012,
Author = {Shu, Yuanming and Li, Jonathan and Yousif, Hamad and Gomes, Gary},
Title = {{Dark-spot detection from SAR intensity imagery with spatial density
   thresholding for oil-spill monitoring}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2010}},
Volume = {{114}},
Number = {{9}},
Pages = {{2026-2035}},
Month = {{SEP 15}},
Abstract = {{Dark-spot detection is a critical and fundamental step in marine
   oil-spill detection and monitoring. In this paper, a novel approach for
   automated dark-spot detection using synthetic aperture radar (SAR)
   intensity imagery is presented The key to the approach is making use of
   a spatial density feature to differentiate between dark spots and the
   background. A detection window is passed through the entire SAR image.
   First, intensity threshold segmentation is applied to each window.
   Pixels with intensities below the threshold are regarded as potential
   dark-spot pixels while the others are potential background pixels.
   Second, the density of potential background pixels is estimated using
   kernel density estimation within each window Pixels with densities below
   a certain threshold are the real dark-spot pixels. Third, an area
   threshold and a contrast threshold are used to eliminate any remaining
   false targets. In the last step, the individual detection results are
   mosaicked to produce the final result. The proposed approach was tested
   on 60 RADARSAT-1 ScanSAR intensity Images which contain verified
   oil-spill anomalies. The same parameters were used in all tests For the
   overall dataset, the average of commission error, omission error, and
   average difference were 70\%, 6.1\%, and 04 pixels, respectively The
   average number of false alarms was 0 5 per unit image and the average
   computational time for a detection window was 1.2 s using a PC-based
   MATLAB platform. Our experimental results demonstrate that the proposed
   approach is fast, robust and effective. (C) 2010 Elsevier Inc. All
   rights reserved.}},
DOI = {{10.1016/j.rse.2010.04.009}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Hashim, Mazlan/J-7291-2012
   chen, zhu/K-5923-2013}},
ORCID-Numbers = {{Hashim, Mazlan/0000-0001-8284-3332
   }},
Unique-ID = {{ISI:000279495200012}},
}

@article{ ISI:000281714100045,
Author = {Wohlers, Inken and Domingues, Francisco S. and Klau, Gunnar W.},
Title = {{Towards optimal alignment of protein structure distance matrices}},
Journal = {{BIOINFORMATICS}},
Year = {{2010}},
Volume = {{26}},
Number = {{18}},
Pages = {{2273-2280}},
Month = {{SEP}},
Abstract = {{Motivation: Structural alignments of proteins are important for
   identification of structural similarities, homology detection and
   functional annotation. The structural alignment problem is well studied
   and computationally difficult. Many different scoring schemes for
   structural similarity as well as many algorithms for finding
   high-scoring alignments have been proposed. Algorithms using contact map
   overlap (CMO) as scoring function are currently the only practical
   algorithms able to compute provably optimal alignments.
   Results: We propose a new mathematical model for the alignment of
   inter-residue distance matrices, building upon previous work on maximum
   CMO. Our model includes all elements needed to emulate various scoring
   schemes for the alignment of protein distance matrices. The algorithm
   that we use to compute alignments is practical only for sparse distance
   matrices. Therefore, we propose a more effective scoring function, which
   uses a distance threshold and only positive structural scores. We show
   that even under these restrictions our approach is in terms of alignment
   accuracy competitive with state-of-the-art structural alignment
   algorithms, whereas it additionally either proves the optimality of an
   alignment or returns bounds on the optimal score. Our novel method is
   freely available and constitutes an important promising step towards
   truly provably optimal structural alignments of proteins.}},
DOI = {{10.1093/bioinformatics/btq420}},
ISSN = {{1367-4803}},
ResearcherID-Numbers = {{Klau, Gunnar W./A-6934-2011
   }},
ORCID-Numbers = {{Klau, Gunnar W./0000-0002-6340-0090
   Domingues, Francisco/0000-0003-4567-4562}},
Unique-ID = {{ISI:000281714100045}},
}

@article{ ISI:000281125400006,
Author = {Saether, Bernt-Erik and Engen, Steinar and Odden, John and Linnell, John
   D. C. and Grotan, Vidar and Andren, Henrik},
Title = {{Sustainable harvest strategies for age-structured Eurasian lynx
   populations: The use of reproductive value}},
Journal = {{BIOLOGICAL CONSERVATION}},
Year = {{2010}},
Volume = {{143}},
Number = {{9}},
Pages = {{1970-1979}},
Month = {{SEP}},
Abstract = {{Eurasian lynx in Scandinavia are subject to regular harvest and lethal
   control to reduce depredation on domestic livestock and semi-domestic
   reindeer. Here we introduce the use of total reproductive value to model
   the effects of current harvest on population dynamics and to propose
   sustainable harvest strategies for lynx. Demographic stochasticity
   strongly influences lynx population dynamics. Analyses of the number of
   lynx shot in relation to the number of family groups registered in
   annual censuses showed proportional harvest in large parts of Norway
   because the quotas were higher at larger population sizes. In other
   areas of Norway the number of lynx shot was independent of population
   size. The analyses of the model showed that a pure proportional harvest
   strategy may lead to rapid extinction of lynx populations. In contrast,
   applying a threshold or proportional threshold harvest strategy in which
   no harvest occurs below a given threshold can result in the maintenance
   of viable populations. Thus, this study shows that harvest without any
   lower threshold for stopping harvest will result in rapid extinction of
   lynx populations. Accordingly, lynx harvest is not likely to be
   sustainable if the illegal killing of animals is not controlled because
   poaching can result in a de facto proportional harvest even at very
   small population sizes. Under the influence of the large demographic
   stochasticity in lynx populations this harvest would result in short
   expected times to extinction. This gives an empirical demonstration that
   a correct choice of harvest strategy is essential for maintenance of
   viable populations of harvested species. Our analyses illustrate that
   parameters determining the viability of small populations can be
   estimated from individual-based demographic data from a sample of
   individuals without using time series of fluctuations in population
   size, which facilitates quantitative analyses of how harvest or removal
   of individuals, e.g. for captive breeding or translocations, affect the
   expected lifetime of populations. (C) 2010 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.biocon.2010.04.048}},
ISSN = {{0006-3207}},
EISSN = {{1873-2917}},
ResearcherID-Numbers = {{Grotan, Vidar/A-4056-2008
   }},
ORCID-Numbers = {{Grotan, Vidar/0000-0003-1222-0724
   Andren, Henrik/0000-0002-5616-2426
   Linnell, John D C/0000-0002-8370-5633}},
Unique-ID = {{ISI:000281125400006}},
}

@article{ ISI:000279814200004,
Author = {Cui Baoshan and Hua Yanyan and Wang Chongfang and Liao Xiaolin and Tan
   Xuejie and Tao Wendong},
Title = {{Estimation of Ecological Water Requirements Based on Habitat Response to
   Water Level in Huanghe River Delta, China}},
Journal = {{CHINESE GEOGRAPHICAL SCIENCE}},
Year = {{2010}},
Volume = {{20}},
Number = {{4}},
Pages = {{318-329}},
Month = {{AUG}},
Abstract = {{In recent years, wetland ecological water requirements (EWRs) have been
   estimated by using hydrological and functional approaches, but those
   approaches have not yet been integrated for a whole ecosystem. This
   paper presents a new method for calculating wetland EWRs, which is based
   on the response of habitats to water level, and determines water level
   threshold through the functional integrity of habitats. Results show
   that in the Huanghe (Yellow) River Delta water levels between 5.0 m and
   5.5 m are required to maintain the functional integrity of the wetland
   at a value higher than 0.7. One of the dominant plants in the delta,
   Phragmites australis, tolerates water level fluctuation of about +/-
   0.25 m without the change in wetland functional integrity. The minimum,
   optimum and maximum EWRs for the Huanghe River Delta are 9.42 x 10(6)
   m(3), 15.56 x 10(6) m(3) and 24.12 x 10(6) m(3) with water levels of 5.0
   m, 5.2 m and 5.5 m, corresponding to functional integrity indices of
   0.70, 0.84 and 0.72, respectively. A wetland restoration program has
   been performed, which aims to meet these EWRs in attempt to recover from
   losses of up to 98\% in the delta's former wetland area.}},
DOI = {{10.1007/s11769-010-0404-6}},
ISSN = {{1002-0063}},
ResearcherID-Numbers = {{Liao, xiaolin/H-3029-2013
   }},
ORCID-Numbers = {{Tao, Wendong/0000-0003-3893-1313}},
Unique-ID = {{ISI:000279814200004}},
}

@article{ ISI:000281027400007,
Author = {Ma, Jin and Zhang, Cuntai and Huang, Shen and Wang, Guoqiang and Quan,
   Xiaoqing},
Title = {{Use of rats mesenchymal stem cells modified with mHCN2 gene to create
   biologic pacemakers}},
Journal = {{JOURNAL OF HUAZHONG UNIVERSITY OF SCIENCE AND TECHNOLOGY-MEDICAL
   SCIENCES}},
Year = {{2010}},
Volume = {{30}},
Number = {{4}},
Pages = {{447-452}},
Month = {{AUG}},
Abstract = {{The possibility of rats mesenchymal stem cells (MSCs) modified with
   murine hyperpolarization-activated cyclic nucleotide-gated 2 (mHCN2)
   gene as biological pacemakers in vitro was studied. The cultured MSCs
   were transfected with pIRES2-EGFP plasmid carrying enhanced green
   fluorescent protein (EGFP) gene and mHCN2 gene. The identification using
   restriction enzyme and sequencing indicated that the mHCN2 gene was
   inserted to the pIRES2-EGFP. Green fluorescence could be seen in MSCs
   after transfection for 24-48 h. The expression of mHCN2 mRNA and protein
   in the transfected cells was detected by RT-PCR and Western blot, and
   the quantity of mHCN2 mRNA and protein expression in transfected MSCs
   was 5.31 times and 7.55 times higher than that of the non-transfected
   MSCs respectively (P < 0.05, P < 0.05). I(HCN2) was recorded by
   whole-cell patch clamp method. The effect of Cs(+), a specific blocker
   of pacemaker current, was measured after perfusion by patch clamp. The
   results of inward current indicated that there was no inward current
   recording in non-transfected MSCs and a large voltage-dependent inward
   and Cs(+)-sensitive current activated on hyperpolarizations presented in
   the transfected MSCs. I(HCN2) was fully activated around -140 mV with an
   activation threshold of -60 mV. The midpoint (V(50)) was -95.1 +/- 0.9
   mV (n=9). The study demonstrates that mHCN2 mRNA and protein can be
   expressed and the currents of HCN2 channels can be detected in
   genetically modified MSCs. The gene-modified MSCs present a novel method
   for pacemaker genes into the heart or other electrical syncytia.}},
DOI = {{10.1007/s11596-010-0447-z}},
ISSN = {{1672-0733}},
Unique-ID = {{ISI:000281027400007}},
}

@article{ ISI:000279474400005,
Author = {Carroll, Hyrum D. and Kann, Maricel G. and Sheetlin, Sergey L. and
   Spouge, John L.},
Title = {{Threshold Average Precision (TAP-k): a measure of retrieval designed for
   bioinformatics}},
Journal = {{BIOINFORMATICS}},
Year = {{2010}},
Volume = {{26}},
Number = {{14}},
Pages = {{1708-1713}},
Month = {{JUL 15}},
Abstract = {{Motivation: Since database retrieval is a fundamental operation, the
   measurement of retrieval efficacy is critical to progress in
   bioinformatics. This article points out some issues with current methods
   of measuring retrieval efficacy and suggests some improvements. In
   particular, many studies have used the pooled receiver operating
   characteristic for n irrelevant records (ROCn) score, the area under the
   ROC curve (AUC) of a `pooled' ROC curve, truncated at n irrelevant
   records. Unfortunately, the pooled ROCn score does not faithfully
   reflect actual usage of retrieval algorithms. Additionally, a pooled
   ROCn score can be very sensitive to retrieval results from as little as
   a single query.
   Methods: To replace the pooled ROCn score, we propose the Threshold
   Average Precision (TAP-k), a measure closely related to the well-known
   average precision in information retrieval, but reflecting the usage of
   E-values in bioinformatics. Furthermore, in addition to conditions
   previously given in the literature, we introduce three new criteria that
   an ideal measure of retrieval efficacy should satisfy.
   Results: PSI-BLAST, GLOBAL, HMMER and RPS-BLAST provided examples of
   using the TAP-k and pooled ROCn scores to evaluate sequence retrieval
   algorithms. In particular, compelling examples using real data highlight
   the drawbacks of the pooled ROCn score, showing that it can produce
   evaluations skewing far from intuitive expectations. In contrast, the
   TAP-k satisfies most of the criteria desired in an ideal measure of
   retrieval efficacy.}},
DOI = {{10.1093/bioinformatics/btq270}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Kann, Maricel/E-5701-2012}},
Unique-ID = {{ISI:000279474400005}},
}

@article{ ISI:000278689300019,
Author = {Benelli, Matteo and Marseglia, Giuseppina and Nannetti, Genni and
   Paravidino, Roberta and Zara, Federico and Bricarelli, Franca Dagna and
   Torricelli, Francesca and Magi, Alberto},
Title = {{A very fast and accurate method for calling aberrations in array-CGH
   data}},
Journal = {{BIOSTATISTICS}},
Year = {{2010}},
Volume = {{11}},
Number = {{3}},
Pages = {{515-518}},
Month = {{JUL}},
Abstract = {{Array comparative genomic hybridization (aCGH) is a microarray
   technology that allows one to detect and map genomic alterations. The
   standard workflow of the aCGH data analysis consists of 2 steps:
   detecting the boundaries of the regions of changed copy number by means
   of a segmentation algorithm (break point identification) and then
   labeling each region as loss, neutral, or gain with a probabilistic
   framework (calling procedure). In this paper, we introduce a novel
   calling procedure based on a mixture of truncated normal distributions,
   named FastCall, that aims to give aberration probabilities to segmented
   aCGH data in a very fast and accurate way. Both on synthetic and real
   aCGH data, FastCall obtains excellent performances in terms of
   classification accuracy and running time.}},
DOI = {{10.1093/biostatistics/kxq008}},
ISSN = {{1465-4644}},
ResearcherID-Numbers = {{Benelli, Matteo/I-2497-2018
   Magi, Alberto/M-6773-2015
   }},
ORCID-Numbers = {{Benelli, Matteo/0000-0003-1227-356X
   Magi, Alberto/0000-0001-7393-4283
   Zara, Federico/0000-0001-9744-5222}},
Unique-ID = {{ISI:000278689300019}},
}

@article{ ISI:000277789800009,
Author = {Bendix, Laila and Horn, Peer Bendix and Jensen, Uffe Birk and Rubelj,
   Ivica and Kolvraa, Steen},
Title = {{The load of short telomeres, estimated by a new method, Universal STELA,
   correlates with number of senescent cells}},
Journal = {{AGING CELL}},
Year = {{2010}},
Volume = {{9}},
Number = {{3}},
Pages = {{383-397}},
Month = {{JUN}},
Abstract = {{P>Short telomeres are thought to trigger senescence, most likely through
   a single - or a group of few - critically shortened telomeres. Such
   short telomeres are thought to result from a combination of gradual
   linear shortening resulting from the end replication problem, reflecting
   the division history of the cell, superimposed by a more stochastic
   mechanism, suddenly causing a significant shortening of a single
   telomere. Previously, studies that have tried to explore the role of
   critically shortened telomeres have been hampered by methodological
   problems. With the method presented here, Universal STELA, we have a
   tool that can directly investigate the relationship between senescence
   and the load of short telomeres. The method is a variant of the
   chromosome-specific STELA method but has the advantage that it can
   demonstrate short telomeres regardless of chromosome. With Universal
   STELA, we find a strong correlation between the load of short telomeres
   and cellular senescence. Further we show that the load of short
   telomeres is higher in senescent cells compared to proliferating cells
   at the same passage, offering an explanation of premature cell
   senescence. This new method, Universal STELA, offers some advantages
   compared to existing methods and can be used to explore many of the
   unanswered questions in telomere biology including the role that
   telomeres play in cancer and aging.}},
DOI = {{10.1111/j.1474-9726.2010.00568.x}},
ISSN = {{1474-9718}},
ORCID-Numbers = {{Jensen, Uffe Birk/0000-0002-6205-6355}},
Unique-ID = {{ISI:000277789800009}},
}

@article{ ISI:000277985500003,
Author = {Lees, William D. and Moss, David S. and Shepherd, Adrian J.},
Title = {{A computational analysis of the antigenic properties of haemagglutinin
   in influenza A H3N2}},
Journal = {{BIOINFORMATICS}},
Year = {{2010}},
Volume = {{26}},
Number = {{11}},
Pages = {{1403-1408}},
Month = {{JUN 1}},
Abstract = {{Motivation: Modelling antigenic shift in influenza A H3N2 can help to
   predict the efficiency of vaccines. The virus is known to exhibit sudden
   jumps in antigenic distance, and prediction of such novel strains from
   amino acid sequence differences remains a challenge.
   Results: From analysis of 6624 amino acid sequences of wildtype H3, we
   propose updates to the frequently referenced list of 131 amino acids
   located at or near the five identified antibody binding regions in
   haemagglutinin ( HA). We introduce a class of predictive models based on
   the analysis of amino acid changes in these binding regions, and extend
   the principle to changes in HA1 as a whole by dividing the molecule into
   regional bands.
   Our results show that a range of simple models based on banded changes
   give better predictive performance than models based on the established
   five canonical regions and can identify a higher proportion of vaccine
   escape candidates among novel strains than a current state-of-the-art
   model.}},
DOI = {{10.1093/bioinformatics/btq160}},
ISSN = {{1367-4803}},
ORCID-Numbers = {{Shepherd, Adrian/0000-0003-0194-8613}},
Unique-ID = {{ISI:000277985500003}},
}

@article{ ISI:000278964200032,
Author = {Demirkale, Cumhur Yusuf and Nettleton, Dan and Maiti, Tapabrata},
Title = {{Linear Mixed Model Selection for False Discovery Rate Control in
   Microarray Data Analysis}},
Journal = {{BIOMETRICS}},
Year = {{2010}},
Volume = {{66}},
Number = {{2}},
Pages = {{621-629}},
Month = {{JUN}},
Abstract = {{In a microarray experiment, one experimental design is used to obtain
   expression measures for all genes. One popular analysis method involves
   fitting the same linear mixed model for each gene, obtaining
   gene-specific p-values for tests of interest involving fixed effects,
   and then choosing a threshold for significance that is intended to
   control false discovery rate (FDR) at a desired level. When one or more
   random factors have zero variance components for some genes, the
   standard practice of fitting the same full linear mixed model for all
   genes can result in failure to control FDR. We propose;I new method that
   combines results from the fit of full and selected linear mixed models
   to identify differentially expressed genes and provide FDR. control at
   target levels when the true underlying random effects structure varies
   across genes.}},
DOI = {{10.1111/.1.1541-0420.2009.01286.x}},
ISSN = {{0006-341X}},
ORCID-Numbers = {{Nettleton, Dan/0000-0002-6045-1036}},
Unique-ID = {{ISI:000278964200032}},
}

@article{ ISI:000277200400002,
Author = {van Ruijven, Bas and van der Sluijs, Jeroen P. and van Vuuren, Detlef P.
   and Janssen, Peter and Heuberger, Peter S. C. and de Vries, Bert},
Title = {{Uncertainty from Model Calibration: Applying a New Method to Transport
   Energy Demand Modelling}},
Journal = {{ENVIRONMENTAL MODELING \& ASSESSMENT}},
Year = {{2010}},
Volume = {{15}},
Number = {{3}},
Pages = {{175-188}},
Month = {{JUN}},
Abstract = {{Uncertainties in energy demand modelling originate from both limited
   understanding of the real-world system and a lack of data for model
   development, calibration and validation. These uncertainties allow for
   the development of different models, but also leave room for different
   calibrations of a single model. Here, an automated model calibration
   procedure was developed and tested for transport sector energy use
   modelling in the TIMER 2.0 global energy model. This model describes
   energy use on the basis of activity levels, structural change and
   autonomous and price-induced energy efficiency improvements. We found
   that the model could reasonably reproduce historic data under different
   sets of parameter values, leading to different projections of future
   energy demand levels. Projected energy use for 2030 shows a range of
   44-95\% around the best-fit projection. Two different model
   interpretations of the past can generally be distinguished: (1) high
   useful energy intensity and major energy efficiency improvements or (2)
   low useful energy intensity and little efficiency improvement.
   Generally, the first lead to higher future energy demand levels than the
   second, but model and insights do not provide decisive arguments to
   attribute a higher likelihood to one of the alternatives.}},
DOI = {{10.1007/s10666-009-9200-z}},
ISSN = {{1420-2026}},
EISSN = {{1573-2967}},
ResearcherID-Numbers = {{van Ruijven, Bas/G-8106-2011
   van der Sluijs, Jeroen/B-6302-2008
   }},
ORCID-Numbers = {{van Ruijven, Bas/0000-0003-1232-5892
   van der Sluijs, Jeroen/0000-0002-1346-5953
   van Vuuren, Detlef/0000-0003-0398-2831}},
Unique-ID = {{ISI:000277200400002}},
}

@article{ ISI:000277302300012,
Author = {Danielson, Matthew L. and Lill, Markus A.},
Title = {{New computational method for prediction of interacting protein loop
   regions}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS}},
Year = {{2010}},
Volume = {{78}},
Number = {{7}},
Pages = {{1748-1759}},
Month = {{MAY 15}},
Abstract = {{Flexible loop regions of proteins play a crucial role in many biological
   functions such as protein ligand recognition, enzymatic catalysis, and
   protein protein association. To date, most computational methods that
   predict the conformational states of loops only focus on individual loop
   regions. However, loop regions are often spatially in close proximity to
   one another and their mutual interactions stabilize their conformations.
   We have developed a new method, titled CorLps, capable of simultaneously
   predicting such interacting loop regions. First, an ensemble of
   individual loop conformations is generated for each loop region. The
   members of the individual ensembles are combined and are accepted or
   rejected based on a steric clash filter. After a subsequent side-chain
   optimization step, the resulting conformations of the interacting loops
   are ranked by the statistical scoring function DFIRE that originated
   from protein structure prediction. Our results show that predicting
   interacting loops with CorLps is superior to sequential prediction of
   the two interacting loop regions, and our method is comparable in
   accuracy to single loop predictions. Furthermore, improved predictive
   accuracy of the top-ranked solution is achieved for 12-residue length
   loop regions by diversifying the initial pool of individual loop
   conformations using a quality threshold clustering algorithm.}},
DOI = {{10.1002/prot.22690}},
ISSN = {{0887-3585}},
EISSN = {{1097-0134}},
ResearcherID-Numbers = {{Lill, Markus/E-5118-2017}},
ORCID-Numbers = {{Lill, Markus/0000-0003-3023-5188}},
Unique-ID = {{ISI:000277302300012}},
}

@article{ ISI:000276652600006,
Author = {Riveros, Andre J. and Gronenberg, Wulfila},
Title = {{Sensory allometry, foraging task specialization and resource
   exploitation in honeybees}},
Journal = {{BEHAVIORAL ECOLOGY AND SOCIOBIOLOGY}},
Year = {{2010}},
Volume = {{64}},
Number = {{6}},
Pages = {{955-966}},
Month = {{MAY}},
Abstract = {{Insect societies are important models for evolutionary biology and
   sociobiology. The complexity of some eusocial insect societies appears
   to arise from self-organized task allocation and group cohesion. One of
   the best-supported models explaining self-organized task allocation in
   social insects is the response threshold model, which predicts
   specialization due to inter-individual variability in sensitivity to
   task-associated stimuli. The model explains foraging task specialization
   among honeybee workers, but the factors underlying the differences in
   individual sensitivity remain elusive. Here, we propose that in
   honeybees, sensory sensitivity correlates with individual differences in
   the number of sensory structures, as it does in solitary species.
   Examining European and Africanized honeybees, we introduce and test the
   hypothesis that body size and/or sensory allometry is associated with
   foraging task preferences and resource exploitation. We focus on common
   morphological measures and on the size and number of structures
   associated with olfactory sensitivity. We show that the number of
   olfactory sensilla is greater in pollen and water foragers, which are
   known to exhibit higher sensory sensitivity, compared to nectar
   foragers. These differences are independent of the distribution of size
   within a colony. Our data also suggest that body mass and number of
   olfactory sensilla correlate with the concentration of nectar gathered
   by workers, and with the size of pollen loads they carry. We conclude
   that sensory allometry, but not necessarily body size, is associated
   with resource exploitation in honeybees and that the differences in
   number of sensilla may underlie the observed differences in sensitivity
   between bees specialized on water, pollen and nectar collection.}},
DOI = {{10.1007/s00265-010-0911-6}},
ISSN = {{0340-5443}},
ORCID-Numbers = {{Riveros, Andre J./0000-0001-7928-1885}},
Unique-ID = {{ISI:000276652600006}},
}

@article{ ISI:000278169400008,
Author = {Hatta, Ichiro and Nakazawa, Hiromitsu and Obata, Yasuko and Ohta, Noboru
   and Inoue, Katsuaki and Yagi, Naoto},
Title = {{Novel method to observe subtle structural modulation of stratum corneum
   on applying chemical agents}},
Journal = {{CHEMISTRY AND PHYSICS OF LIPIDS}},
Year = {{2010}},
Volume = {{163}},
Number = {{4-5}},
Pages = {{381-389}},
Month = {{MAY}},
Abstract = {{In the development of functional chemicals such as percutaneous
   penetration enhancers and cosmetics, the structural evidence at the
   molecular level in stratum corneum (SC) is highly desirable. We
   developed a method to observe a minute structural change of
   intercellular lipid matrix and corneocytes on applying the chemicals to
   the SC using synchrotron X-ray diffraction technique. The performance of
   the present method was demonstrated by applying typical chemicals,
   chloroform/methanol mixture, hydrophilic ethanol and hydrophobic
   d-limonene. From the small- and wide-angle X-ray diffraction we obtained
   the following results: on applying chloroform/methanol mixture the
   intercellular lipids were extracted markedly, on applying ethanol the
   intercellular lipid structure was slightly disrupted, ethanol molecules
   were taken into the corneocytes and in addition the pools of ethanol
   seem to be formed in the hydrophilic region of the intercellular lipid
   matrix in the SC, and on applying d-limonene the repeat distance of the
   long lamellar structure increased by incorporating d-limonene molecules,
   the intercellular lipid structure was slightly disrupted, and the pools
   of d-limonene were formed in the hydrophobic region of the intercellular
   lipid matrix in the SC. (C) 2010 Elsevier Ireland Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.chemphyslip.2010.02.005}},
ISSN = {{0009-3084}},
ORCID-Numbers = {{Nakazawa, Hiromitsu/0000-0001-8694-4579}},
Unique-ID = {{ISI:000278169400008}},
}

@article{ ISI:000274982700018,
Author = {Cheng, T. and Rivard, B. and Sanchez-Azofeifa, G. A. and Feng, J. and
   Calvo-Polanco, M.},
Title = {{Continuous wavelet analysis for the detection of green attack damage due
   to mountain pine beetle infestation}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2010}},
Volume = {{114}},
Number = {{4}},
Pages = {{899-910}},
Month = {{APR 15}},
Abstract = {{Mountain pine beetle (Dendroctonus ponderosae Hopkins) is the most
   destructive insect infesting mature pine forests in North America and
   has devastated millions of hectares of forest in western Canada. Past
   studies have demonstrated the use of multispectral imagery for remote
   identification and mapping of visible or red attack damage in forests.
   This study aims to detect pre-visual or green attack damage in lodgepole
   pine needles by means of hyperspectral measurements, particularly via
   continuous wavelet analysis. Field measurements of lodgepole pine stands
   were conducted at two sites located northwest of Edmonton, Alberta,
   Canada. In June and August of 2007, reflectance spectra (350-2500 nm)
   were collected for 16 pairs of trees. Each of the 16 tree pairs included
   one control tree (healthy), and one stressed tree (girdled to simulate
   the effects of beetle infestation). In addition, during the period of
   June through October 2008, spectra were collected from 15 pairs of
   control- and beetle-infested trees. Spectra derived from these 31 tree
   pairs were subjected to a continuous wavelet transform, generating a
   scalogram that compiles the wavelet power as a function of wavelength
   location and scale of decomposition. Linear relationships were then
   explored between the wavelet scalograms and chemical properties or class
   labels (control and non-control) of the sample populations in order to
   isolate the most useful distinguishing spectral features that related to
   infested or girdled trees vs. control trees.
   A deficit in water content is observed in infested trees while an
   additional deficit in chlorophyll content is seen for girdled trees. The
   measurable water deficit of infested and girdled tree samples was
   detectable from the wavelet analysis of the reflectance spectra
   providing a novel method for the detection of green attack. The spectral
   features distinguishing control and infested trees are predominantly
   located between 950 and 1390 nm from scales 1 to 8. Of those, five
   features between 1318 to 1322 nm at scale 7 are consistently found in
   the July and August 2008 datasets. These features are located at longer
   wavelengths than those investigated in previous studies (below 1100 nm)
   and provide new insights into the potential remote detection of green
   attack. Spectral features that distinguish control and girdled trees
   were mostly observed between 1550 and 2370 nm from scales 1 to 5. The
   differing response of girdled and infested trees appears to indicate
   that the girdling process does not provide a perfect simulation of the
   effects caused by beetle infestation.
   It remains to be determined if the location of the 1318-1322 nm
   features, near the edge of a strong atmospheric water absorption band,
   will be sufficiently separable for use in airborne detection of green
   attack. A plot comparing needle water content and wavelet power at 1320
   nm reveals considerable overlap between data derived from both infested
   and control samples, though the groups are statistically separable. This
   obstacle may preclude a high accuracy separation of healthy and infected
   single individuals, but establishing threshold identification levels may
   provide an economical, efficient and expeditious method for
   discriminating between healthy and infested tree populations. (C) 2009
   Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2009.12.005}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{Cheng, Tao/B-4807-2010
   Cheng, T. C. E./D-5688-2015
   }},
ORCID-Numbers = {{Cheng, Tao/0000-0002-4184-0730
   Cheng, T. C. E./0000-0001-5127-6419
   /0000-0002-0813-0921
   Sanchez-Azofeifa, Arturo/0000-0001-7768-6600}},
Unique-ID = {{ISI:000274982700018}},
}

@article{ ISI:000274809500008,
Author = {Tichy, Lubomir and Chytry, Milan and Hajek, Michal and Talbot, Stephen
   S. and Botta-Dukat, Zoltan},
Title = {{OptimClass: Using species-to-cluster fidelity to determine the optimal
   partition in classification of ecological communities}},
Journal = {{JOURNAL OF VEGETATION SCIENCE}},
Year = {{2010}},
Volume = {{21}},
Number = {{2}},
Pages = {{287-299}},
Month = {{APR}},
Abstract = {{Question
   Community ecologists are often confronted with multiple possible
   partitions of a single set of records of species composition and/or
   abundances from several sites. Different methods of numerical
   classification produce different results, and the question is which of
   them, and how many clusters, should be selected for interpretation. We
   demonstrate a new method for identifying the optimal partition from a
   series of partitions of the same set of sites, based on number of
   species with high fidelity to clusters in a partition (faithful
   species).
   Methods
   The new method, OptimClass, has two variants. OptimClass 1 searches the
   partition with the maximum number of faithful species across all
   clusters, while OptimClass 2 searches the partition with the maximum
   number of clusters that contain at least a preselected minimum number of
   faithful species. Faithful species are determined based on the P value
   of the Fisher's exact test, as a measure of fidelity. OptimClass was
   tested on three vegetation datasets that varied in species richness and
   internal heterogeneity, using several classification algorithms,
   resemblance measures and cover transformations.
   Results
   Results from both variants of OptimClass depended on the preselected
   threshold P value for faithful species: higher P gave higher probability
   that a partition with more clusters was selected as optimal. Good
   partitions, in terms of OptimClass criteria, involved flexible beta
   clustering, and also ordinal clustering. Good partitions were also
   obtained with TWINSPAN when the required number of clusters was small,
   or UPGMA when the required number of clusters was large. Poor partitions
   usually resulted from classifications that used resemblance measures and
   cover transformations emphasizing differences in species cover; this is
   not unexpected because OptimClass uses a presence/absence-based fidelity
   measure.
   Conclusions
   If the aim of a classification is to obtain clusters rich in faithful
   species, which can be subsequently used as diagnostic species for
   identification of community types, OptimClass is a suitable method for
   simultaneous choice of the optimal classification algorithm and optimal
   number of clusters. It can be computed in the JUICE program.}},
DOI = {{10.1111/j.1654-1103.2009.01143.x}},
ISSN = {{1100-9233}},
ResearcherID-Numbers = {{Botta-Dukat, Zoltan/B-2911-2015
   Chytry, Milan/J-4954-2012
   Botta Dukat, Zoltan/E-3469-2010
   Hajek, Michal/H-1648-2014
   }},
ORCID-Numbers = {{Botta-Dukat, Zoltan/0000-0002-9544-3474
   Chytry, Milan/0000-0002-8122-3075
   Tichy, Lubomir/0000-0001-8400-7741}},
Unique-ID = {{ISI:000274809500008}},
}

@article{ ISI:000277738200011,
Author = {Sund, Patrik and Bath, Magnus and Mansson, Lars Gunnar},
Title = {{INVESTIGATION OF THE EFFECT OF AMBIENT LIGHTING ON CONTRAST SENSITIVITY
   USING A NOVEL METHOD FOR CONDUCTING VISUAL RESEARCH ON LCDS}},
Journal = {{RADIATION PROTECTION DOSIMETRY}},
Year = {{2010}},
Volume = {{139}},
Number = {{1-3}},
Pages = {{62-70}},
Month = {{APR-MAY}},
Abstract = {{The DICOM part 14 greyscale standard display function provides one way
   of harmonising image appearance under different monitor luminance
   settings. This function is based on ideal observer conditions, where the
   eye is always adapted to the target luminance and thereby also at peak
   contrast sensitivity. Clinical workstations are, however, often
   submitted to variations in ambient light due to a sub-optimal reading
   room light environment. Also, clinical images are inhomogeneous and
   low-contrast patterns must be detected even at luminance levels that
   differ from the eye adaptation level. All deviations from ideal
   luminance conditions cause the observer to detect patterns with reduced
   eye sensitivity but the magnitude of this reduction is unclear. A method
   is presented to display well-defined sinusoidal low-contrast test
   patterns on an liquid crystal display. The observers were exposed to
   light from three different areas: (i) the test pattern covering
   approximately 28 3 28; (ii) the remaining of the display surface and
   (iii) ambient light from outside the display area covering most of the
   observers' field of view. By adjusting the luminance from each of these
   three areas, the observers' ability to detect low-contrast patterns
   under suboptimal viewing conditions was studied. Ambient light from
   outside the display area has a moderate effect on the contrast
   threshold, except for the combination of high ambient light and dark
   objects, where the contrast threshold increased considerably.}},
DOI = {{10.1093/rpd/ncq067}},
ISSN = {{0144-8420}},
ORCID-Numbers = {{Bath, Magnus/0000-0003-4004-2603}},
Unique-ID = {{ISI:000277738200011}},
}

@article{ ISI:000275243500001,
Author = {Cammer, Stephen and Carter, Jr., Charles W.},
Title = {{Six Rossmannoid folds, including the Class I aminoacyl-tRNA synthetases,
   share a partial core with the anti-codon-binding domain of a Class II
   aminoacyl-tRNA synthetase}},
Journal = {{BIOINFORMATICS}},
Year = {{2010}},
Volume = {{26}},
Number = {{6}},
Pages = {{709-714}},
Month = {{MAR 15}},
Abstract = {{Motivation: Similarities in core residue packing provide evidence for
   divergence or convergence not reported using other methods.
   Results: We apply a new method for rapid structure comparison based on
   Simplicial Neighborhood Analysis of Protein Packing (SNAPP) to the
   diverse structural classification of proteins (SCOP) alpha/beta-class of
   protein folds. The procedure identifies inter-residue packing motifs
   shared by protein pairs from different folds. A threshold of 0.67
   angstrom RMSD for all atoms of corresponding residues ensures inclusion
   of only highly significant similarities comparable with those observed
   for identical catalytic residues in homologues. Many tertiary packing
   motifs are shared among the three classical Rossmannoid folds, as well
   as thousands of other motifs that occur in at least two distinct folds.
   Merging of neighboring packing motifs facilitated recognition of larger,
   recurrent substructures or cores. The anti-codon-binding domain of an
   archeal aminoacyl-tRNA synthetase (aaRS) was discovered to possess a
   packed core in which eight identical amino acid residues are within 0.55
   angstrom RMSD of the comparable structure in the FixJ receiver, a member
   of the Rossmannoid family that also includes the CheY signaling protein
   and flavodoxin-like proteins. Further investigation identified close
   variants of this core in five other Rossmannoid folds, including a
   functionally relevant core in Class la aminoacyl-tRNA synthetases.
   Although it is possible that the two essentially identical cores in the
   ProRS anti-codon-binding domain and the FixJ receiver converged to the
   same structure, the consensus core obtained from the structural and
   sequence alignments suggests that all the implicated protein folds
   descended from a simpler ancestral protein in which this core provided
   nucleotide binding and proto-allosteric functions.}},
DOI = {{10.1093/bioinformatics/btq039}},
ISSN = {{1367-4803}},
Unique-ID = {{ISI:000275243500001}},
}

@article{ ISI:000274799400009,
Author = {Nevai, Andrew L. and Passino, Kevin M. and Srinivasan, Parthasarathy},
Title = {{Stability of choice in the honey bee nest-site selection process}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2010}},
Volume = {{263}},
Number = {{1}},
Pages = {{93-107}},
Month = {{MAR 7}},
Abstract = {{We introduce a pair of compartment models for the honey bee nest-site
   selection process that lend themselves to analytic methods. The first
   model represents a swarm of bees deciding whether a site is viable, and
   the second characterizes its ability to select between two viable sites.
   We find that the one-site assessment process has two equilibrium states:
   a disinterested equilibrium (DE) in which the bees show no interest in
   the site and an interested equilibrium (IE) in which bees show interest.
   In analogy with epidemic models, we define basic and absolute
   recruitment numbers (R(0) and B(0)) as measures of the swarm's
   sensitivity to dancing by a single bee. If R(0) is less than one then
   the DE is locally stable, and if B(0) is less than one then it is
   globally stable. If R(0) is greater than one then the DE is unstable and
   the IE is stable under realistic conditions. In addition, there exists a
   critical site quality threshold Q{*} above which the site can attract
   some interest (at equilibrium) and below which it cannot. We also find
   the existence of a second critical site quality threshold Q{*}{*} above
   which the site can attract a quorum (at equilibrium) and below which it
   cannot. The two-site discrimination process, in which we examine a
   swarm's ability to simultaneously consider two sites differing in both
   site quality and discovery time, has a stable DE if and only if both
   sites' individual basic recruitment numbers are less than one. Numerical
   experiments are performed to study the influences of site quality on
   quorum time and the outcome of competition between a lower quality site
   discovered first and a higher quality site discovered second. (C) 2009
   Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2009.11.006}},
ISSN = {{0022-5193}},
Unique-ID = {{ISI:000274799400009}},
}

@article{ ISI:000274973800003,
Author = {Demir-Kavuk, Ozgur and Riedesel, Henning and Knapp, Ernst-Walter},
Title = {{Exploring classification strategies with the CoEPrA 2006 contest}},
Journal = {{BIOINFORMATICS}},
Year = {{2010}},
Volume = {{26}},
Number = {{5}},
Pages = {{603-609}},
Month = {{MAR 1}},
Abstract = {{Motivation: In silico methods to classify compounds as potential drugs
   that bind to a specific target become increasingly important for drug
   design. To build classification devices training sets of drugs with
   known activities are needed. For many such classification problems, not
   only qualitative but also quantitative information of a specific
   property (e.g. binding affinity) is available. The latter can be used to
   build a regression scheme to predict this property for new compounds.
   Predicting a compound property explicitly is generally more difficult
   than classifying that the property lies below or above a given threshold
   value. Hence, an indirect classification that is based on regression may
   lead to poorer results than a direct classification scheme. In fact,
   initially researchers are only interested to classify compounds as
   potential drugs. The activities of these compounds are subsequently
   measured in wet lab.
   Results: We propose a novel approach that uses available quantitative
   information directly for classification rather than first using a
   regression scheme. It uses a new type of loss function called weighted
   biased regression. Application of this method to four widely studied
   datasets of the CoEPrA contest (Comparative Evaluation of Prediction
   Algorithms, http://coepra.org) shows that it can outperform simple
   classification methods that do not make use of this additional
   quantitative information.}},
DOI = {{10.1093/bioinformatics/btq021}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Knapp, Ernst-Walter/E-4493-2010}},
ORCID-Numbers = {{Knapp, Ernst-Walter/0000-0002-2858-0460}},
Unique-ID = {{ISI:000274973800003}},
}

@article{ ISI:000275523200008,
Author = {Escalera, Laura and Reguera, Beatriz and Moita, Teresa and Pazos,
   Yolanda and Cerejo, Marta and Manuel Cabanas, Jose and Ruiz-Villarreal,
   Manuel},
Title = {{Bloom dynamics of Dinophysis acuta in an upwelling system: In situ
   growth versus transport}},
Journal = {{HARMFUL ALGAE}},
Year = {{2010}},
Volume = {{9}},
Number = {{3}},
Pages = {{312-322}},
Month = {{MAR}},
Abstract = {{Blooms of Dinophysis spp. associated with lipophilic shellfish toxin
   outbreaks are common in Northwestern Iberia waters from spring to
   autumn. Blooms of Dinophysis acuta are very seasonal (late summer-early
   autumn); they start earlier in Northern Portuguese waters during the
   upwelling season, and reach their maximum values in the Galician Rias
   during downwelling events at the end of the upwelling season. There is
   controversy about whether sudden increases in cell concentrations in the
   rias result from cross-shelf transport of populations previously
   established in adjacent shelf waters, or are due to longshore transport
   that brings populations located off Portugal to the North. In 2005,
   record concentrations of D. acuta were observed in Portuguese waters (14
   x 10(4) cell L(-1)) off Aveiro in early September, while concentrations
   off the Galician coast were very moderate (10(2)-10(3) cell L(-1)).
   During the autumn transition from upwelling- to downwelling-favourable
   winds, D. acuta declined abruptly off Portugal while the annual maximum
   (up to 22 x 10(3) cell L(-1)) was found in the Galician Rias. A new
   approach was used that combined physical observations (SST, current
   measurements); weekly observations from monitoring programmes in Galicia
   and Portugal; weekly division rate (Amin) estimates of D. acuta in Ria
   de Vigo, together with monthly transects and additional ad hoc sampling
   in the Ria. During August and early September, division rates were high
   in Ria de Vigo but concentrations were low, whereas higher SST values in
   Portugal (a proxy for thermal stratification) seemed to promote the
   build up of high densities of D. acuta. During the last week of October
   and the first week of November, populations declined, whereas in
   Galicia, maximum concentrations were reached while division rate
   estimates were almost zero. Results presented here confirm that
   increased numbers in the Galician Rias are not due to intrinsic growth
   but to physically driven accumulation; a simple cell concentration
   budget calculated during the accumulation period suggests that the high
   net growth observed during downwelling, in the absence of cellular
   division, must be due to cells imported by longshore transport. (C) 2009
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.hal.2009.12.002}},
ISSN = {{1568-9883}},
ResearcherID-Numbers = {{Reguera, Beatriz/M-7371-2014
   Ruiz-Villarreal, Manuel/F-3389-2010
   Escalera, Laura/S-2836-2018
   Moita Garnel, Maria Teresa/M-4039-2013
   }},
ORCID-Numbers = {{Reguera, Beatriz/0000-0003-4582-9798
   Ruiz-Villarreal, Manuel/0000-0003-0169-0958
   Moita Garnel, Maria Teresa/0000-0002-8995-7516
   Escalera, Laura/0000-0003-0938-4250
   Cabanas, Jose/0000-0002-2140-0332}},
Unique-ID = {{ISI:000275523200008}},
}

@article{ ISI:000274518200010,
Author = {Zhang, Zhenxing and Dehoff, Andrew D. and Pody, Robert D.},
Title = {{New Approach to Identify Trend Pattern of Streamflows}},
Journal = {{JOURNAL OF HYDROLOGIC ENGINEERING}},
Year = {{2010}},
Volume = {{15}},
Number = {{3}},
Pages = {{244-248}},
Month = {{MAR}},
Abstract = {{Streamflow trend analyses have so far mainly focused on testing if
   trends for a specified period are significant. However, the traditional
   trend tests cannot characterize trend patterns, i.e., if the trend is
   gradual or abrupt. This study contributes a novel approach to identify
   the trend pattern of streamflows. This study proposes the use of
   repeated monotonic trend tests with varying beginning and ending times.
   The results of the repeated trend tests are plotted along the beginning
   and ending times. The sensitivity of trends with respect to the period
   of time is then employed to characterize the trend pattern. Application
   of the new approach to watersheds within the Susquehanna River Basin
   demonstrates that the new approach is capable of characterizing trend
   patterns. A comparison with the results of single monotonic trend tests
   shows that it is also useful for the exploration of all available data
   in contrast to a single monotonic trend test that only shows trends for
   a specified time period.}},
DOI = {{10.1061/(ASCE)HE.1943-5584.0000179}},
ISSN = {{1084-0699}},
Unique-ID = {{ISI:000274518200010}},
}

@article{ ISI:000288913700004,
Author = {Baker, Matthew E. and King, Ryan S.},
Title = {{A new method for detecting and interpreting biodiversity and ecological
   community thresholds}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2010}},
Volume = {{1}},
Number = {{1}},
Pages = {{25-37}},
Month = {{MAR}},
Abstract = {{1. Existing methods for identifying ecological community thresholds are
   designed for univariate indicators or multivariate dimension-reduction
   of community structure. Most are insensitive to responses of individual
   taxa with low occurrence frequencies or highly variable abundances,
   properties of the vast majority of taxa in community data sets. We
   introduce Threshold Indicator Taxa ANalysis (TITAN) to detect changes in
   taxa distributions along an environmental gradient over space or time,
   and assess synchrony among taxa change points as evidence for community
   thresholds.
   2. TITAN uses indicator species scores to integrate occurrence,
   abundance and directionality of taxa responses. It identifies the
   optimum value of a continuous variable, x, that partitions sample units
   while maximizing taxon-specific scores. Indicator z scores standardize
   original scores relative to the mean and SD of permuted samples along x,
   thereby emphasizing the relative magnitude of change and increasing the
   contributions of taxa with low occurrence frequencies but high
   sensitivity to the gradient. TITAN distinguishes negative (z-) and
   positive (z+) taxa responses and tracks cumulative responses of
   declining {[}sum(z-)] and increasing {[}sum(z+)] taxa in the community.
   Bootstrapping is used to estimate indicator reliability and purity as
   well as uncertainty around the location of individual taxa and community
   change points.
   3. Using two simulated data sets, TITAN correctly identified taxon and
   community thresholds in more than 99\% of 500 unique versions of each
   simulation. In contrast, multivariate change-point analysis did not
   distinguish directional taxa responses, resulting in much wider
   confidence intervals that in one instance failed to capture thresholds
   in 38\% of the iterations.
   4. Retrospective analysis of macroinvertebrate community response to a
   phosphorus gradient supported previous threshold estimates, although
   TITAN produced narrower confidence limits and revealed that several taxa
   declined at lower levels of phosphorus. Re-analysis of macroinvertebrate
   responses to an urbanization gradient illustrated disparate change
   points for declining (0.81-3.3\% urban land) and increasing (6.8-26.6\%)
   taxa, whereas the published threshold estimate (20-30\%) missed the
   declining-taxa threshold because it could not distinguish their
   synchronous decline from the gradual increase in ubiquitous taxa.
   5. Synthesis and applications. By deconstructing communities to assess
   synchrony of taxon-specific change points, TITAN provides a sensitive
   and precise alternative to existing methods for assessing community
   thresholds. TITAN has tremendous potential to inform conservation of
   rare or threatened species, develop species sensitivity models, identify
   reference conditions and to support development of numerical regulatory
   criteria.}},
DOI = {{10.1111/j.2041-210X.2009.00007.x}},
ISSN = {{2041-210X}},
Unique-ID = {{ISI:000288913700004}},
}

@article{ ISI:000288913700006,
Author = {Gal, Gideon and Anderson, William},
Title = {{A novel approach to detecting a regime shift in a lake ecosystem}},
Journal = {{METHODS IN ECOLOGY AND EVOLUTION}},
Year = {{2010}},
Volume = {{1}},
Number = {{1}},
Pages = {{45-52}},
Month = {{MAR}},
Abstract = {{1. Certain classes of change in the characteristics of an ecosystem,
   labelled regime shifts, have been observed in marine and freshwater
   ecosystems world-wide. Few tools, however, have been offered to detect
   and identify regime shifts in time-series data.
   2. We use a novel approach based on tools taken from the world of
   statistics, and econometrics to examine the occurrence of a regime shift
   in the predatory zooplankton population of Lake Kinneret, Israel. The
   tools are a free-knot spline mean function estimation method and a
   Markov-switching vector autoregression model.
   3. Our approach detected, with high probability, the occurrence of a
   regime shift in the zooplankton population in the early to mid-1990s.
   This was in-line with expectations based on similar events observed in
   the lake.
   4. The suggested approach is a step forward from existing approaches in
   that it does not require any pre-determent of threshold values but
   rather relies on a hidden underlying stochastic process that yields
   probabilities of regime shifts. Thus, it can therefore be applied
   without introducing any prior biases into the analysis. The approach is,
   therefore, an objective method in detecting the likely occurrence of a
   regime shift.}},
DOI = {{10.1111/j.2041-210X.2009.00006.x}},
ISSN = {{2041-210X}},
Unique-ID = {{ISI:000288913700006}},
}

@article{ ISI:000275107700006,
Author = {Peles, Dana N. and Simon, John D.},
Title = {{Direct Measurement of the Ultraviolet Absorption Coefficient of Single
   Retinal Melanosomes}},
Journal = {{PHOTOCHEMISTRY AND PHOTOBIOLOGY}},
Year = {{2010}},
Volume = {{86}},
Number = {{2}},
Pages = {{279-281}},
Month = {{MAR-APR}},
Abstract = {{A novel approach to photoemission electron microscopy is used to enable
   the first direct measurement of the absorption coefficient from intact
   melanosomes isolated from bovine retinal pigment epithelial cells. The
   difference in absorption between newborn and adult melanosomes is in
   good agreement with that predicted from the relative amounts of the
   monomeric precursors present in the constituent melanin as determined by
   chemical degradation analyses. The results demonstrate that for
   melanosomes containing eumelanins, there is a direct relation between
   the absorption coefficient and the relative 5,6-dihydroxyindole:
   5,6-dihydroxyindole-2-carboxylic acid (DHI:DHICA) content, with an
   increased UV absorption coefficient associated with increasing DHICA
   content.}},
DOI = {{10.1111/j.1751-1097.2009.00656.x}},
ISSN = {{0031-8655}},
EISSN = {{1751-1097}},
Unique-ID = {{ISI:000275107700006}},
}

@article{ ISI:000275003800015,
Author = {Beca, Pedro and Santos, Rui},
Title = {{Measuring sustainable welfare: A new approach to the ISEW}},
Journal = {{ECOLOGICAL ECONOMICS}},
Year = {{2010}},
Volume = {{69}},
Number = {{4}},
Pages = {{810-819}},
Month = {{FEB 15}},
Abstract = {{Sustainability and welfare assessment is a contemporary theme of major
   scientific and policy relevance, requiring the consideration of multiple
   dimensions and diverse perspectives. The economic approach to
   sustainability and welfare assessment has frequently relied on
   alternatives, or adjustments, to GDP, widely used as an indicator of
   macroeconomic performance.
   Several authors have proposed alternative indicators, such as the ISEW,
   which intend to measure sustainability and economic welfare in a way
   that avoids the limitations of GDP: namely accounting for the value of
   externalities, the distribution of income and natural resources
   depiction. Since Daly and Cobb (1989) there have been proposed
   improvements to the ISEW, however, its aptitude to represent a sound
   alternative to GDP is still the subject of scientific debate.
   This paper presents a new approach to the ISEW (named Modified ISEW),
   including new components and methodological changes for the estimation
   of the index. These have the purpose of avoiding some of the index
   shortcomings and allow for a direct comparison with the GDP, which are
   advantages over previous studies.
   An application is developed for the US case, taking advantage of wide
   data availability and the possibility of comparing the results with
   previous works. The results obtained provide a clearer picture of the
   success or failure of environmental and social policies. namely by
   avoiding the tampering effect resulting from the cumulative accounting
   of environmental externalities. This work also emphasizes the inadequacy
   of GDP as a welfare indicator, as well as the need to develop and adopt
   alternative indicators. (C) 2009 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolecon.2009.11.031}},
ISSN = {{0921-8009}},
ResearcherID-Numbers = {{Santos, Rui/Q-6837-2016
   }},
ORCID-Numbers = {{Santos, Rui/0000-0003-2829-6475
   Beca, Pedro/0000-0003-3821-0807}},
Unique-ID = {{ISI:000275003800015}},
}

@incollection{ ISI:000276891600005,
Author = {Boelcskei, Kata and Petho, Gabor and Szolcsanyi, Janos},
Editor = {{Szallasi, A}},
Title = {{Noxious Heat Threshold Measured with Slowly Increasing Temperatures:
   Novel Rat Thermal Hyperalgesia Models}},
Booktitle = {{ANALGESIA: METHODS AND PROTOCOLS}},
Series = {{Methods in Molecular Biology}},
Year = {{2010}},
Volume = {{617}},
Pages = {{57-66}},
Abstract = {{The conventional methods for the study of thermal pain in animals apply
   constant suprathreshold heat stimuli and measure the reflex latency of
   pain-avoiding reactions. The latency measured by these methods may
   greatly vary upon repeated measurements which is a major disadvantage
   concerning reliability. The presently introduced novel approach involves
   applying a slowly increasing thermal stimulus which allows determination
   of the noxious heat threshold i.e. the lowest temperature evoking
   pain-avoiding behaviour. An increasing-temperature hot plate and an
   increasing-temperature water bath are presented which are both suitable
   to determine the noxious heat threshold with high reproducibility. Acute
   thermal hyperalgesia models based on the drop of the heat threshold are
   also described for each equipment which proved to be highly sensitive to
   standard analgesics.}},
DOI = {{10.1007/978-1-60327-323-7\_5}},
ISSN = {{1064-3745}},
ISBN = {{978-1-60327-322-0}},
Unique-ID = {{ISI:000276891600005}},
}

@article{ ISI:000273581800010,
Author = {Diniz, P. R. B. and Murta-Junior, L. O. and Brum, D. G. and de Araujo,
   D. B. and Santos, A. C.},
Title = {{Brain tissue segmentation using q-entropy in multiple sclerosis magnetic
   resonance images}},
Journal = {{BRAZILIAN JOURNAL OF MEDICAL AND BIOLOGICAL RESEARCH}},
Year = {{2010}},
Volume = {{43}},
Number = {{1}},
Pages = {{77-84}},
Month = {{JAN}},
Abstract = {{The loss of brain volume has been used as a marker of tissue destruction
   and can be used as an index of the progression of neurodegenerative
   diseases, such as multiple sclerosis. In the present study, we tested a
   new method for tissue segmentation based on pixel intensity threshold
   using generalized Tsallis entropy to determine a statistical
   segmentation parameter for each single class of brain tissue. We
   compared the performance of this method using a range of different q
   parameters and found a different optimal q parameter for white matter,
   gray matter, and cerebrospinal fluid. Our results support the conclusion
   that the differences in structural correlations and scale invariant
   similarities present in each tissue class can be accessed by generalized
   Tsallis entropy, obtaining the intensity limits for these tissue class
   separations. In order to test this method, we used it for analysis of
   brain magnetic resonance images of 43 patients and 10 healthy controls
   matched for gender and age. The values found for the entropic q index
   were 0.2 for cerebrospinal fluid, 0.1 for white matter and 1.5 for gray
   matter. With this algorithm, we could detect an annual loss of 0.98\%
   for the patients, in agreement with literature data. Thus, we can
   conclude that the entropy of Tsallis adds advantages to the process of
   automatic target segmentation of tissue classes, which had not been
   demonstrated previously.}},
DOI = {{10.1590/S0100-879X2009007500019}},
ISSN = {{0100-879X}},
EISSN = {{1414-431X}},
ResearcherID-Numbers = {{Murta, Luiz/F-2010-2010
   Brum, Doralina/A-1225-2015
   Araujo, Draulio/I-6038-2012
   Santos, Antonio/F-5419-2012}},
ORCID-Numbers = {{Murta, Luiz/0000-0002-2197-6008
   Brum, Doralina/0000-0002-9050-3319
   Araujo, Draulio/0000-0002-6934-2485
   }},
Unique-ID = {{ISI:000273581800010}},
}

@article{ ISI:000272818700019,
Author = {Johnson, C. E. and Freel, C. D. and Kornbluth, S.},
Title = {{Features of programmed cell death in intact Xenopus oocytes and early
   embryos revealed by near-infrared fluorescence and real-time monitoring}},
Journal = {{CELL DEATH AND DIFFERENTIATION}},
Year = {{2010}},
Volume = {{17}},
Number = {{1}},
Pages = {{170-179}},
Month = {{JAN}},
Abstract = {{Factors influencing apoptosis of vertebrate eggs and early embryos have
   been studied in cell-free systems and in intact embryos by analyzing
   individual apoptotic regulators or caspase activation in static samples.
   A novel method for monitoring caspase activity in living Xenopus oocytes
   and early embryos is described here. The approach, using microinjection
   of a near-infrared caspase substrate that emits fluorescence only after
   its proteolytic cleavage by active effector caspases, has enabled the
   elucidation of otherwise cryptic aspects of apoptotic regulation. In
   particular, we show that brief caspase activity (10 min) is sufficient
   to cause apoptotic death in this system. We illustrate a cytochrome c
   dose threshold in the oocyte, which is lowered by Smac, a protein that
   binds thereby neutralizing the inhibitor of apoptosis proteins. We show
   that meiotic oocytes develop resistance to cytochrome c, and that the
   eventual death of oocytes arrested in meiosis is caspase-independent.
   Finally, data acquired through imaging caspase activity in the Xenopus
   embryo suggest that apoptosis in very early development is not
   cell-autonomous. These studies both validate this assay as a useful tool
   for apoptosis research and reveal subtleties in the cell death program
   during early development. Moreover, this method offers a potentially
   valuable screening modality for identifying novel apoptotic regulators.
   Cell Death and Differentiation (2010) 17, 170-179;
   doi:10.1038/cdd.2009.120; published online 4 September 2009}},
DOI = {{10.1038/cdd.2009.120}},
ISSN = {{1350-9047}},
EISSN = {{1476-5403}},
Unique-ID = {{ISI:000272818700019}},
}

@article{ ISI:000281080100002,
Author = {Brown, V. and White, K. A. J.},
Title = {{The HPV vaccination strategy: could male vaccination have a significant
   impact?}},
Journal = {{COMPUTATIONAL AND MATHEMATICAL METHODS IN MEDICINE}},
Year = {{2010}},
Volume = {{11}},
Number = {{3}},
Pages = {{223-237}},
Abstract = {{We investigate the potential success of the human papilloma virus (HPV)
   vaccine, taking into consideration possible waning immunity and the
   influence of behavioural parameters. We use a compartmental,
   population-level ordinary differential equation (ODE) model. We find the
   effective reproductive value for HPV, {[}image omitted], which measures
   the threshold for infection outbreak in a population that is not
   entirely susceptible, together with infection prevalence. We study the
   effects of different parameters on both of these quantities. Results
   show that waning immunity plays a large part in allowing infection to
   persist. The proportion of the population not sexually active when
   vaccination occurs affects {[}image omitted], as does the rate at which
   individuals become sexually active. In several cases, infection persists
   as a result of an infection reservoir in the male cohort. To explore
   this further, we introduce male vaccination and find the conditions for
   which vaccination of males could be considered appropriate.}},
DOI = {{10.1080/17486700903486613}},
Article-Number = {{PII 923354009}},
ISSN = {{1748-670X}},
ORCID-Numbers = {{Brown, Victoria/0000-0002-9751-0984}},
Unique-ID = {{ISI:000281080100002}},
}

@incollection{ ISI:000289827600012,
Author = {Kawai, Shigeharu and Oku, Yositaka and Okada, Yasumasa and Miwakeichi,
   Fumikazu and Ishiguro, Makio and Tamura, Yoshiyasu},
Editor = {{Chaovalitwongse, W and Pardalos, PM and Xanthopoulos, P}},
Title = {{Parametric Modeling Analysis of Optical Imaging Data on Neuronal
   Activities in the Brain}},
Booktitle = {{COMPUTATIONAL NEUROSCIENC E}},
Series = {{Springer Series in Optimization and Its Applications}},
Year = {{2010}},
Volume = {{38}},
Pages = {{213-225}},
Abstract = {{An optical imaging technique using a voltage-sensitive dye (voltage
   imaging) has been widely applied to the analyses of various brain
   functions. Because optical signals in voltage imaging are small and
   require several kinds of preprocessing, researchers who use voltage
   imaging often conduct signal averaging of multiple trials and correction
   of signals by cutting the noise near the baseline in order to improve
   the apparent signal-noise ratio. However, a noise cutting threshold
   level that is usually set arbitrarily largely affects the analyzed
   results. Therefore, we aimed to develop a new method to objectively
   evaluate optical imaging data on neuronal activities. We constructed a
   parametric model to analyze optical time series data. We have chosen the
   respiratory neuronal network in the brainstem as a representative system
   to test our method. In our parametric model we assumed an optical signal
   of each pixel as the input and the inspiratory motor nerve activity of
   the spinal cord as the output. The model consisted of a threshold
   function and a delay transfer function. Although it was a simple
   nonlinear dynamic model, it could provide precise}},
DOI = {{10.1007/978-0-387-88630-5\_12}},
ISSN = {{1931-6828}},
ISBN = {{978-0-387-88629-9}},
Unique-ID = {{ISI:000289827600012}},
}

@incollection{ ISI:000284343500009,
Author = {Prezioso, Maria},
Editor = {{Cancilla, R and Gargano, M}},
Title = {{THE SUSTAINABLE TERRITORIAL ENVIRONMENTAL/ECONOMIC MANAGEMENT APPROACH
   TO MANAGE POLICY IMPACTS AND EFFECTS}},
Booktitle = {{GLOBAL ENVIONMENTAL POLICIES: IMPACT, MANAGEMENT AND EFFECTS}},
Series = {{Environmental Science Engineering and Technology}},
Year = {{2010}},
Pages = {{185-225}},
Abstract = {{In this time, the Impact Assessment is the best process to structure and
   support the development of global/local environmental policies. It
   identifies territorial/spatial problems and objectives (by several ex
   ante and ex post steps) and the main political/programming scenarios and
   project options measuring impacts on economic, environmental, cultural
   and social fields. It outlines advantages and disadvantages for each
   option and it examines all possible synergies and trade-offs on the base
   of territorial context.
   In order to obtain the integration of different tools (Territorial
   Impact Assessment - TIA, Strategic Environmental Assessment - SEA,
   Environmental Impact Assessment - EIA), it wars necessary to work within
   a systemic vision (from Von Bertanlaffy General Theory, 1969; to Saaty
   analytical hierarchy process, 1980; to analytical planning, 1985),
   pursuing its application into economicterritorial analysis and planning
   choices (Prezioso, 1995, 2003 and 2007).
   To use Impact Assessment means to be able: i) to know effects of
   policies, programmes, planning on territorial status quo indicators
   before; ii) to measure the degree of risk of overtaking the carrying
   capacity threshold and the improvement in performance and
   competitiveness; iii) to build scenarios of funds allocations and
   management, according to indications provided by the Territorial
   Capability Framework.
   This way was already anticipated in 2005 by the European Commission,
   pursuing the way of ``Better Regulation{''}, who started several formal
   and informal initiatives. Among them, the Impact Assessment proposal was
   a new method to introduce a common support within the framework of the
   Better Regulation package and the European Sustainable Development
   Strategy.
   This proposal aroused great interest, since it could orientate further
   policy goals and choices differently. In fact, the Impact Assessment
   introduced a new method in 2002, integrating and replacing previous
   single-sector types of assessment.
   Impact Assessment (IA) is a process aimed at structuring and supporting
   the policies' development.
   It identifies and assesses the problem at stake and all objectives
   pursued. It identifies the main options to achieve objectives and it
   analyses their impacts in economic, environmental and social fields. It
   outlines advantages and disadvantages of each option and it examines
   possible synergies and trade-offs.
   The Impact Assessment is an aid to political decision, not a substitute
   to it. It informs decisionmakers about impacts of each proposal, but it
   leaves them free to take their decisions.
   This approach will be particularly important if it really aims to carry
   on a sustainable economic territorial growth. Of course, it requires
   institutions and policy makers to accept sustainability as a political
   permanently stance, as a principle for the period 2007-2013 and as an
   orientation for local/global (regional/national) European
   competitiveness.
   The sustainable development became part of the territorial and economic
   planning, it induced a gradual evolution within the selection of
   measures different from the monetary one and it suggested and supported
   methods, process, specific indicators for the phenomena evaluation and
   management, using units of measure different from the price, the
   principle indicator of the market esteem.}},
ISBN = {{978-1-60876-204-0}},
Unique-ID = {{ISI:000284343500009}},
}

@article{ ISI:000273576400007,
Author = {Naoghare, Pravin K. and Ki, Hyeon A. and Paek, Seung-Mann and Tak, Yu
   Kyung and Suh, Young-Ger and Kim, Sang Geon and Leeb, Kyeong-Hee and
   Song, Joon Myong},
Title = {{Simultaneous quantitative monitoring of drug-induced caspase cascade
   pathways in carcinoma cells}},
Journal = {{INTEGRATIVE BIOLOGY}},
Year = {{2010}},
Volume = {{2}},
Number = {{1}},
Pages = {{46-57}},
Abstract = {{Caspases are the key mediators of apoptosis. The caspase cascade
   includes a series of events leading to the activation of initiator and
   downstream caspases in a cell. Analysis of the caspase cascade in intact
   cells, however, has generally been limited as the simultaneous
   monitoring of upstream and downstream caspases is not well executed. In
   an effort to monitor the activation of caspase cascades in an intact
   cell, high-content cellular imaging that allows simultaneous
   quantitative monitoring of caspase activation has been developed. This
   has great significance for the exploration of various cellular caspases
   involved in apoptotic pathways as possible therapeutic targets in the
   process of drug discovery. To explore the potential of simultaneous
   monitoring of caspase-mediated apoptotic pathways, human myeloid
   leukemia HL-60 cells were treated with SH-03
   \{(7S,7aR,13aS)-9,10-dimethoxy-3,3-dimethyl-7,7a,13,13a-tetrahydro-3H-ch
   romeno {[}3,4-b]pyrano{[}2,3-h]chromen-7-ol\} (a newly synthesized
   candidate), camptothecin or naringenin (agents known to induce
   apoptosis) with or without caspase inhibitors. SH-03 or naringenin
   treatment initiated the caspase cascade through an intrinsic apoptotic
   pathway, whereas camptothecin treatment triggered both intrinsic and
   extrinsic caspase cascades. We now report a new approach based on
   uniform threshold intensity distribution that facilitates rapid,
   quantitative monitoring of drug-induced caspase cascades through
   multi-spectral and multicolor imaging cytometry.}},
DOI = {{10.1039/b916481b}},
ISSN = {{1757-9694}},
EISSN = {{1757-9708}},
Unique-ID = {{ISI:000273576400007}},
}

@article{ ISI:000274866800014,
Author = {Riemer, Nicole and West, Matthew and Zaveri, Rahul and Easter, Richard},
Title = {{Estimating black carbon aging time-scales with a particle-resolved
   aerosol model}},
Journal = {{JOURNAL OF AEROSOL SCIENCE}},
Year = {{2010}},
Volume = {{41}},
Number = {{1, SI}},
Pages = {{143-158}},
Month = {{JAN}},
Note = {{9th International Conference on Carbonaceous Particles in the
   Atmosphere, Berkeley, CA, 2009}},
Abstract = {{Understanding the aging process of aerosol particles is important for
   assessing their chemical reactivity, cloud condensation nuclei activity,
   radiative properties and health impacts. in this study we investigate
   the aging of black carbon containing particles in an idealized urban
   plume using a new approach, the particle-resolved aerosol model
   PartMC-MOSAIC. We present a method to estimate aging time-scales using
   an aging criterion based on cloud condensation nuclei activation. The
   results show a separation into a daytime regime where condensation
   dominates and a nighttime regime where coagulation dominates. There is
   also a strong dependence on supersaturation threshold. For the chosen
   urban plume scenario and supersaturations ranging from 0.1\% to 1\%, the
   aging time-scales vary between 11 and 0.068 h during the day, and
   between 54 and 6.4 h during the night. (C) 2009 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.jaerosci.2009.08.009}},
ISSN = {{0021-8502}},
EISSN = {{1879-1964}},
ResearcherID-Numbers = {{West, Matthew/A-7398-2012
   }},
ORCID-Numbers = {{West, Matthew/0000-0002-7605-0050
   Zaveri, Rahul/0000-0001-9874-8807}},
Unique-ID = {{ISI:000274866800014}},
}

@article{ ISI:000281565900001,
Author = {Chan, Kung-Sik and Zhang, Tianyang and Bailey, Kevin M.},
Title = {{Otolith biochronology reveals factors underlying dynamics in marine fish
   larvae}},
Journal = {{MARINE ECOLOGY PROGRESS SERIES}},
Year = {{2010}},
Volume = {{412}},
Pages = {{1-10}},
Abstract = {{Study of the dynamics in marine fish larvae is notoriously difficult,
   given their minute size in a vast, complex and variable marine
   environment. We demonstrate a novel statistical approach, utilizing a
   panel of 19 annual hatchdate distributions, to unravel environmental and
   potential harvesting effects on the spawning, hatching and survival
   processes for walleye pollock in the Gulf of Alaska. Hatchdates are
   determined from counting daily increments on otoliths of larvae. The
   hatchdate frequency distribution determined from sampling a population
   of larvae depends on factors influencing birth and death processes, i.e.
   spawning times and survival rates. Using a nonlinear and partly
   parametric statistical model, temperature, strong winds, and the age
   frequency of spawning adults were found to strongly influence
   hatchdates. Moreover, the interaction of both a climate regime shift and
   the `Exxon Valdez' oil spill in 1989 suppressed pollock larval abundance
   in 1989, with geometrically diminishing after-effects lasting until
   around 1992. The novel method presented here provides a general
   framework for unlocking the rich information hidden in hatchdate data
   about environmental and/or intervention effects on dynamics in marine
   fishes.}},
DOI = {{10.3354/meps08698}},
ISSN = {{0171-8630}},
EISSN = {{1616-1599}},
Unique-ID = {{ISI:000281565900001}},
}

@article{ ISI:000273432300022,
Author = {McIlhenny, Stephen E. and Hager, Eric S. and Grabo, Daniel J. and
   DiMatteo, Christopher and Shapiro, Irving M. and Tulenko, Thomas N. and
   DiMuzio, Paul J.},
Title = {{Linear Shear Conditioning Improves Vascular Graft Retention of
   Adipose-Derived Stem Cells by Upregulation of the alpha(5)beta(1)
   Integrin}},
Journal = {{TISSUE ENGINEERING PART A}},
Year = {{2010}},
Volume = {{16}},
Number = {{1}},
Pages = {{245-255}},
Month = {{JAN}},
Abstract = {{Use of adult adipose-derived stem cells (ASCs) as endothelial cell
   substitutes in vascular tissue engineering is attractive because of
   their availability. However, when seeded onto decellularized vascular
   scaffolding and exposed to physiological fluid shear force, ASCs are
   physically separated from the graft lumen. Herein we have investigated
   methods of increasing initial ASC attachment using luminal precoats and
   a novel protocol for the gradual introduction of shear stress to
   optimize ASC retention. Fibronectin coating of the graft lumen increased
   ASC attachment by nearly sixfold compared with negative controls.
   Gradual introduction of near physiological fluid shear stress using a
   novel bioreactor whereby flow rate was increased every second at a rate
   of 1.5 dynes/cm(2) per day resulted in complete luminal coverage
   compared with near complete cell loss following conventional daily
   abrupt increases. An upregulation of the alpha(5)beta(1) integrin was
   evinced following exposure to shear stress, which accounts for the
   observed increase in ASC retention on the graft lumen. These results
   indicated a novel method for seeding, conditioning, and retaining of
   adult stem cells on a decellularized vein scaffold within a high-shear
   stress microenvironment.}},
DOI = {{10.1089/ten.tea.2009.0238}},
ISSN = {{1937-3341}},
Unique-ID = {{ISI:000273432300022}},
}

@article{ ISI:000272581600003,
Author = {Nicolau, Miguel and Schoenauer, Marc},
Title = {{On the evolution of scale-free topologies with a gene regulatory network
   model}},
Journal = {{BIOSYSTEMS}},
Year = {{2009}},
Volume = {{98}},
Number = {{3}},
Pages = {{137-148}},
Month = {{DEC}},
Abstract = {{A novel approach to generating scale-free network topologies is
   introduced, based on an existing artificial gene regulatory network
   model. From this model, different interaction networks can be extracted,
   based on an activation threshold. By using an evolutionary computation
   approach, the model is allowed to evolve, in order to reach specific
   network statistical measures. The results obtained show that, when the
   model uses a duplication and divergence initialisation, such as seen in
   nature, the resulting regulation networks not only are closer in
   topology to scale-free networks, but also require only a few
   evolutionary cycles to achieve a satisfactory error Value. (C) 2009
   Elsevier Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/j.biosystems.2009.06.006}},
ISSN = {{0303-2647}},
Unique-ID = {{ISI:000272581600003}},
}


@article{ ISI:000273426400005,
Author = {Matthaeus, Franziska},
Title = {{THE SPREAD OF PRION DISEASES IN THE BRAIN - MODELS OF REACTION AND
   TRANSPORT ON NETWORKS}},
Journal = {{JOURNAL OF BIOLOGICAL SYSTEMS}},
Year = {{2009}},
Volume = {{17}},
Number = {{4}},
Pages = {{623-641}},
Month = {{DEC}},
Abstract = {{In this paper we will present a modeling approach to describe the
   progression and the spread of prion diseases in the brain. Although
   there exist a number of mathematical models for the interaction of
   prions with their native counterpart, prion transport and spread is
   usually neglected. The concentration dynamics of prions, and thus the
   dynamics of the disease progression, however, are influenced by prion
   transport, especially in a medium as complex as the brain. Therefore, we
   focus here on the interaction between prion concentration dynamics and
   prion transport. The model is constructed by combining a model of
   prion-prion interaction with transport on networks. The approach leads
   to a system of reaction-diffusion equations, whereby the diffusion term
   is discrete. The equations are solved numerically on domains given as
   large networks. We show that the prion concentration grows faster on
   networks characterized by a higher degree heterogeneity. Furthermore, we
   introduce cell death as a consequence of increasing prion concentration,
   leading to network decomposition. We show that infectious diseases
   destroy networks similarly to targeted attacks, namely by affecting the
   nodes with the highest degree first. Relating the incubation period and
   disease progression to the process of network decomposition, we find
   that, interestingly, a long incubation time followed by sudden onset and
   fast progression of the disease does not need to be reflected in the
   overall concentration dynamics of the infective agent.}},
DOI = {{10.1142/S0218339009003010}},
ISSN = {{0218-3390}},
Unique-ID = {{ISI:000273426400005}},
}

@article{ ISI:000272332900009,
Author = {Ferenc Jordan and Liu, Wei-chung and Mike, Agnes},
Title = {{Trophic field overlap: A new approach to quantify keystone species}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2009}},
Volume = {{220}},
Number = {{21, SI}},
Pages = {{2899-2907}},
Month = {{NOV 10}},
Note = {{6th European Conference on Ecological Modelling, Challenges for
   Ecological Modelling in a Changing World, Trieste, ITALY, NOV 27-30,
   2007}},
Abstract = {{It is a current challenge to better understand the relative importance
   of species in ecosystems, and the network perspective is able to offer
   quantitative tools for this. It is plausible to assume, in general, that
   well-linked species, being key interactors, are also more important for
   the community. Recently a number of methods have been suggested for
   quantifying the network position of species in ecological networks (like
   the topological importance metric, TI). Most of them are based on node
   centrality indices and it may happen that the two most important species
   in a food web have very similar interaction structure and they can
   essentially replace each other if one becomes extinct. For conservation
   considerations it is a challenge to identify species that are richly
   connected and, at the same time, have a relatively unique and
   irreplaceable interaction pattern. We present a new method and
   illustrate our approach by using the Kuosheng Bay trophic network in
   Taiwan. The new method is based on the interaction matrix, where the
   strength of the interaction between nodes i and j depends only on
   topology. By defining a threshold separating weak and strong
   interactors, we define the effective range of interactions for each
   graph node. If the overlaps between pairs of these ranges are
   quantified, we gain a metric expressing how unique is the interaction
   pattern of a focal node (TO). The combination of centrality (TI) and
   uniqueness (TO) is called topological functionality (TF). We compare the
   nodal importance rank provided by this metric to others based on a
   variety of centrality measures. The main conclusion is that shrimps seem
   to have the most unique interaction pattern despite that their
   structural importance has been underestimated by all conventional
   centrality indices. Also, our network analysis suggests that fisheries
   disturb the ecosystem in a more critical network position than the
   impingement by the local power plant. (C) 2008 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/j.ecolmodel.2008.12.003}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
ResearcherID-Numbers = {{Jordan, Ferenc/A-9940-2009}},
Unique-ID = {{ISI:000272332900009}},
}

@article{ ISI:000269277200015,
Author = {Cao, Xin and Chen, Jin and Imura, Hidefumi and Higashi, Osamu},
Title = {{A SVM-based method to extract urban areas from DMSP-OLS and SPOT VGT
   data}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2009}},
Volume = {{113}},
Number = {{10}},
Pages = {{2205-2209}},
Month = {{OCT}},
Abstract = {{Mapping urban areas at regional and global scales has become an urgent
   task because of the increasing pressures from rapid urbanization and
   associated environmental problems. Satellite imaging of stable
   anthropogenic lights from DMSP-OLS provides an accurate, economical, and
   straightforward way to map the global distribution of urban areas. To
   address problems in the thresholding methods that use empirical
   strategies or manual trial-and-error procedures, we proposed a support
   vector machine (SVM)-based region-growing algorithm to
   semi-automatically extract urban areas from DMSP-OLS and SPOT NDVI data.
   Several simple criteria were used to select SVM training sets of urban
   and non-urban pixels, and an iterative classification and training
   procedure was adopted to identify the urban pixels through region
   growing. The new method was validated using the extents of 25 Chinese
   cities, as classified by Landsat ETM + images, and then compared with
   two common thresholding methods. The results showed that the SVM-based
   algorithm could not only achieve comparable results to the
   local-optimized threshold method, but also avoid its tedious
   trial-and-error procedure, suggesting that the new method is an easy and
   simple alternative for extracting urban extent from DMSP-OLS and SPOT
   NDVI data. (C) 2009 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.rse.2009.06.001}},
ISSN = {{0034-4257}},
EISSN = {{1879-0704}},
ResearcherID-Numbers = {{li, dongsheng/B-2285-2012
   Chen, Lijun/Q-5012-2016
   Higashi, Osamu/A-3278-2014
   Chen, Jin/I-7666-2016
   Chen, Jin/A-6417-2011
   Cao, Xin/P-3814-2014}},
ORCID-Numbers = {{Higashi, Osamu/0000-0002-8087-0318
   Chen, Jin/0000-0002-6497-4141
   }},
Unique-ID = {{ISI:000269277200015}},
}

@article{ ISI:000271118900001,
Author = {Toronen, Petri and Ojala, Pauli J. and Marttinen, Pekka and Holm, Liisa},
Title = {{Robust extraction of functional signals from gene set analysis using a
   generalized threshold free scoring function}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2009}},
Volume = {{10}},
Month = {{SEP 23}},
Abstract = {{Background: A central task in contemporary biosciences is the
   identification of biological processes showing response in genome-wide
   differential gene expression experiments. Two types of analysis are
   common. Either, one generates an ordered list based on the differential
   expression values of the probed genes and examines the tail areas of the
   list for over-representation of various functional classes.
   Alternatively, one monitors the average differential expression level of
   genes belonging to a given functional class. So far these two types of
   method have not been combined.
   Results: We introduce a scoring function, Gene Set Z-score (GSZ), for
   the analysis of functional class over-representation that combines two
   previous analysis methods. GSZ encompasses popular functions such as
   correlation, hypergeometric test, Max-Mean and Random Sets as limiting
   cases. GSZ is stable against changes in class size as well as across
   different positions of the analysed gene list in tests with randomized
   data. GSZ shows the best overall performance in a detailed comparison to
   popular functions using artificial data. Likewise, GSZ stands out in a
   cross-validation of methods using split real data. A comparison of
   empirical p-values further shows a strong difference in favour of GSZ,
   which clearly reports better p-values for top classes than the other
   methods. Furthermore, GSZ detects relevant biological themes that are
   missed by the other methods. These observations also hold when comparing
   GSZ with popular program packages.
   Conclusion: GSZ and improved versions of earlier methods are a useful
   contribution to the analysis of differential gene expression. The
   methods and supplementary material are available from the website
   http://ekhidna.biocenter.helsinki.fi/users/petri/public/GSZ/GSZscore.htm
   l.}},
DOI = {{10.1186/1471-2105-10-307}},
Article-Number = {{307}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Marttinen, Pekka/N-6234-2015
   }},
ORCID-Numbers = {{Marttinen, Pekka/0000-0001-7078-7927
   toronen, petri/0000-0003-4764-9790}},
Unique-ID = {{ISI:000271118900001}},
}

@article{ ISI:000279191500010,
Author = {Pei, Yongzhen and Yang, Yong and Li, Changguo},
Title = {{BIFURCATION OF A MUTUALISTIC SYSTEM WITH VARIABLE COEFFICIENTS AND
   IMPULSIVE EFFECTS}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMATHEMATICS}},
Year = {{2009}},
Volume = {{2}},
Number = {{3}},
Pages = {{363-375}},
Month = {{SEP}},
Abstract = {{In this paper, we introduce and study an impulsive mutualistic model
   with variable coefficients. By using Floquent theorem, the invasion
   threshold and the stability of the boundary periodic solution are
   obtained. Furthermore, by using standard techniques of bifurcation
   theory, the existent condition of the positive periodic solution is
   obtained. Finally, numerical simulations are carried out to confirm the
   main theorems.}},
DOI = {{10.1142/S1793524509000649}},
ISSN = {{1793-5245}},
EISSN = {{1793-7159}},
Unique-ID = {{ISI:000279191500010}},
}

@article{ ISI:000267532500009,
Author = {Allen, Benjamin and Kon, Mark and Bar-Yam, Yaneer},
Title = {{A New Phylogenetic Diversity Measure Generalizing the Shannon Index and
   Its Application to Phyllostomid Bats}},
Journal = {{AMERICAN NATURALIST}},
Year = {{2009}},
Volume = {{174}},
Number = {{2}},
Pages = {{236-243}},
Month = {{AUG}},
Abstract = {{Protecting biodiversity involves preserving the maximum number and
   abundance of species while giving special attention to species with
   unique genetic or morphological characteristics. In balancing different
   priorities, conservation policymakers may consider quantitative measures
   that compare diversity across ecological communities. To serve this
   purpose, a measure should increase or decrease with changes in community
   composition in a way that reflects what is valued, including species
   richness, evenness, and distinctness. However, counterintuitively,
   studies have shown that established indices, including those that
   emphasize average interspecies phylogenetic distance, may increase with
   the elimination of species. We introduce a new diversity index, the
   phylogenetic entropy, which generalizes in a natural way the Shannon
   index to incorporate species relatedness. Phylogenetic entropy favors
   communities in which highly distinct species are more abundant, but it
   does not advocate decreasing any species proportion below a community
   structure dependent threshold. We contrast the behavior of multiple
   indices on a community of phyllostomid bats in the Selva Lacandona. The
   optimal genus distribution for phylogenetic entropy populates all genera
   in a linear relationship to their total phylogenetic distance to other
   genera. Two other indices favor eliminating 12 out of the 23 genera.}},
DOI = {{10.1086/600101}},
ISSN = {{0003-0147}},
ResearcherID-Numbers = {{Allen, Benjamin/F-1829-2011
   }},
ORCID-Numbers = {{Allen, Benjamin/0000-0002-9746-7613}},
Unique-ID = {{ISI:000267532500009}},
}

@article{ ISI:000268289300004,
Author = {Lampariello, Francesco},
Title = {{Ratio Analysis of Cumulatives for Labeled Cell Quantification from
   Immunofluorescence Histograms Derived from Cells Expressing Low Antigen
   Levels}},
Journal = {{CYTOMETRY PART A}},
Year = {{2009}},
Volume = {{75A}},
Number = {{8}},
Pages = {{665-674}},
Month = {{AUG}},
Abstract = {{The problem of the quantitative analysis of immunofluorescence
   histograms ``with weak labeling{''} is considered. In these
   distributions, the fluorescence intensity mean of labeled cells is
   slightly above that Of unlabeled cells, and therefore there is a
   considerable overlap between the two populations. The procedure of
   selecting ``by eye{''} an intensity level that separates labeled from
   unlabeled cells is evidently inadequate, leading to subjective and not
   reproducible results. Threshold and subtraction methods, although
   objective, quick, and easy to use, are also inaccurate. Till now,
   correct enough estimates of the percentage of labeled cells have been
   obtained only by applying mathematical modeling methods, that are,
   however, not very easy to handle. A new method for estimating the
   labeled cell proportion from immunofluorescence histograms is presented.
   It relies on the analysis of the ratio of the cumulative distributions
   associated with the routinely available test and negative control
   histograms. Since this ratio results in a well-defined
   ``sigma-shaped{''} distribution from a certain channel onwards, a
   suitable fitting function is used to obtain directly a preliminary
   estimate of the proportion of labeled cells. Then, an iterative
   procedure is designed for updating the estimate until it is judged
   acceptable, according to a suitable criterion. Moreover, to take into
   account the overall variability in the control data, the estimation
   process is repeated using the cumulatives associated with replicate
   negative control histograms. The accuracy of the method was tested with
   the same set of experimental data analyzed by means of a mathematical
   modeling method described in a previous article (Lampariello and Aiello,
   Cytometry 1998;32:241-254). There was a very good agreement between the
   known and estimated proportions of labeled cells, and the latter were,
   in many cases, even closer to the true values than those previously
   obtained. On the basis of the results reported, the method, which is
   objective, very simple, and Much easier to implement than our previously
   proposed one, appears to be effective in the entire range of the
   percentage values, and to provide results independent of the acquisition
   precision. (C) 2009 International Society for Advancement of Cytometry}},
DOI = {{10.1002/cyto.a.20755}},
ISSN = {{1552-4922}},
Unique-ID = {{ISI:000268289300004}},
}

@article{ ISI:000268031300004,
Author = {Mukhopadhyay, B. and Bhattacharyya, R.},
Title = {{DIFFUSION INDUCED SHIFT OF BIFURCATION POINT IN A MANGROVE ECOSYSTEM
   FOOD-CHAIN MODEL WITH HARVESTING}},
Journal = {{NATURAL RESOURCE MODELING}},
Year = {{2009}},
Volume = {{22}},
Number = {{3}},
Pages = {{415-436}},
Month = {{AUG}},
Abstract = {{The present paper deals with a detritus-based food-chain model within a
   mangrove ecosystem. The top predator (mainly fish) is assumed to have a
   commercial value and undergoes harvesting. Stability and bifurcation
   behavior of the model is studied and a threshold harvest rate is
   obtained. Next we introduce environmental nonhomogenity into the model
   equation. The resulting reaction diffusion system is investigated, and
   the criteria for supercritical Hopf bifurcation is obtained using the
   method of Lyapunov first coefficient. A comparison of the critical
   harvest rates under the homogeneous and the nonhomogeneous context is
   performed both analytically and numerically.}},
DOI = {{10.1111/j.1939-7445.2009.00043.x}},
ISSN = {{0890-8575}},
EISSN = {{1939-7445}},
Unique-ID = {{ISI:000268031300004}},
}

@article{ ISI:000269408400006,
Author = {Zhang, Shuqin and DeGraba, Thomas J. and Wang, Honghui and Hoehn, Gerard
   T. and Gonzales, Denise A. and Suffredini, Anthony F. and Ching, Wai-Ki
   and Ng, Michael K. and Zhou, Xiaobo and Wong, Stephen T. C.},
Title = {{A novel peak detection approach with chemical noise removal using
   short-time FFT for prOTOF MS data}},
Journal = {{PROTEOMICS}},
Year = {{2009}},
Volume = {{9}},
Number = {{15}},
Pages = {{3833-3842}},
Month = {{AUG}},
Abstract = {{Peak detection is a pivotal first step in biomarker discovery from MS
   data and can significantly influence the results of downstream data
   analysis steps. We developed a novel automatic peak detection method for
   prOTOF MS data, which does not require a priori knowledge of protein
   masses. Random noise is removed by an undecimated wavelet transform and
   chemical noise is attenuated by an adaptive short-time discrete Fourier
   transform. Isotopic peaks corresponding to a single protein are combined
   by extracting an envelope over them. Depending on the SIN, the desired
   peaks in each individual spectrum are detected and those with the
   highest intensity among their peak clusters are recorded. The common
   peaks among all the spectra are identified by choosing an appropriate
   cut-off threshold in the complete linkage hierarchical clustering. To
   remove the 1 Da shifting of the peaks, the peak corresponding to the
   same protein is determined as the detected peak with the largest number
   among its neighborhood. We validated this method using a data set of
   serial peptide and protein calibration standards. Compared with MoverZ
   program, our new method detects more peaks and significantly enhances
   SIN of the peak after the chemical noise removal. We then successfully
   applied this method to a data set from prOTOF MS spectra of albumin and
   albumin-bound proteins from serum samples of 59 patients with carotid
   artery disease compared to vascular disease-free patients to detect
   peaks with S/N >= 2. Our method is easily implemented and is highly
   effective to define peaks that will be used for disease classification
   or to highlight potential biomarkers.}},
DOI = {{10.1002/pmic.200800030}},
ISSN = {{1615-9853}},
EISSN = {{1615-9861}},
ResearcherID-Numbers = {{Ng, Michael/B-7189-2009
   HKBU, Mathematics/B-5086-2009
   }},
ORCID-Numbers = {{NG, Michael/0000-0001-6833-5227}},
Unique-ID = {{ISI:000269408400006}},
}

@article{ ISI:000269417500001,
Author = {Newberg, Lee A.},
Title = {{Error statistics of hidden Markov model and hidden Boltzmann model
   results}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2009}},
Volume = {{10}},
Month = {{JUL 9}},
Abstract = {{Background: Hidden Markov models and hidden Boltzmann models are
   employed in computational biology and a variety of other scientific
   fields for a variety of analyses of sequential data. Whether the
   associated algorithms are used to compute an actual probability or, more
   generally, an odds ratio or some other score, a frequent requirement is
   that the error statistics of a given score be known. What is the chance
   that random data would achieve that score or better? What is the chance
   that a real signal would achieve a given score threshold?
   Results: Here we present a novel general approach to estimating these
   false positive and true positive rates that is significantly more
   efficient than are existing general approaches. We validate the
   technique via an implementation within the HMMER 3.0 package, which
   scans DNA or protein sequence databases for patterns of interest, using
   a profile-HMM.
   Conclusion: The new approach is faster than general nave sampling
   approaches, and more general than other current approaches. It provides
   an efficient mechanism by which to estimate error statistics for hidden
   Markov model and hidden Boltzmann model results.}},
DOI = {{10.1186/1471-2105-10-212}},
Article-Number = {{212}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000269417500001}},
}

@article{ ISI:000265481300009,
Author = {Tzafriri, A. R. and Levin, A. D. and Edelman, E. R.},
Title = {{Diffusion-limited binding explains binary dose response for local
   arterial and tumour drug delivery}},
Journal = {{CELL PROLIFERATION}},
Year = {{2009}},
Volume = {{42}},
Number = {{3}},
Pages = {{348-363}},
Month = {{JUN}},
Abstract = {{Local drug delivery has transformed medicine, yet it remains unclear how
   drug efficacy depends on physicochemical properties and delivery
   kinetics. Most therapies seek to prolong release, yet recent studies
   demonstrate sustained clinical benefit following local bolus
   endovascular delivery.
   The purpose of the current study was to examine interplay between drug
   dose, diffusion and binding in determining tissue penetration and
   effect.
   We introduce a quantitative framework that balances dose, saturable
   binding and diffusion, and measured the specific binding parameters of
   drugs to target tissues.
   Model reduction techniques augmented by numerical simulations revealed
   that impact of saturable binding on drug transport and retention is
   determined by the magnitude of a binding potential, B-p, ratio of
   binding capacity to product of equilibrium dissociation constant and
   accessible tissue volume fraction. At low B-p (< 1), drugs are
   predominantly free and transport scales linearly with concentration. At
   high B-p (> 40), drug transport exhibits threshold dependence on applied
   surface concentration.
   In this paradigm, drugs and antibodies with large B-p penetrate faster
   and deeper into tissues when presented at high concentrations. Threshold
   dependence of tissue transport on applied surface concentration of
   paclitaxel and rapamycin may explain threshold dose dependence of in
   vivo biological efficacy of these drugs.}},
DOI = {{10.1111/j.1365-2184.2009.00602.x}},
ISSN = {{0960-7722}},
EISSN = {{1365-2184}},
Unique-ID = {{ISI:000265481300009}},
}

@article{ ISI:000265543000002,
Author = {Zhang, Juping and Jin, Zhen},
Title = {{DISCRETE TIME SI AND SIS EPIDEMIC MODELS WITH VERTICAL TRANSMISSION}},
Journal = {{JOURNAL OF BIOLOGICAL SYSTEMS}},
Year = {{2009}},
Volume = {{17}},
Number = {{2}},
Pages = {{201-212}},
Month = {{JUN}},
Abstract = {{Discrete time SI and SIS epidemic models with vertical transmission are
   presented in this paper. With regard to the SI model with constant or
   variable population size, we introduce an epidemic threshold parameter,
   the basic reproductive number R-0, for predicting disease dynamics. R-0
   > 1 implies that the disease tends to an endemic equilibrium, while R-0
   < 1 implies disease extinction. On the other hand, for the SIS epidemic
   model with another form force of infection, the basic reproduction
   number R-0 determines the persistence or extinction of the disease. In
   the same time, we also explore the relationship between the demographic
   equation and the epidemic process. In particular, we show that the
   epidemic model can exhibit bistability (alternative stable equilibria)
   over a wide range of parameter values.}},
DOI = {{10.1142/S0218339009002788}},
ISSN = {{0218-3390}},
EISSN = {{1793-6470}},
ResearcherID-Numbers = {{jin, zhen/B-6563-2015}},
ORCID-Numbers = {{jin, zhen/0000-0002-9763-707X}},
Unique-ID = {{ISI:000265543000002}},
}

@article{ ISI:000266511000006,
Author = {Kirchner, G. and Steiner, M. and Zaehringer, M.},
Title = {{A new approach to estimate nuclide ratios from measurements with
   activities close to background}},
Journal = {{JOURNAL OF ENVIRONMENTAL RADIOACTIVITY}},
Year = {{2009}},
Volume = {{100}},
Number = {{6}},
Pages = {{484-488}},
Month = {{JUN}},
Abstract = {{Measurements of low-level radioactivity often give results of the order
   of the detection limit. For many applications, interest is not only in
   estimating activity concentrations of a single radioactive isotope, but
   focuses on multi-isotope analyses, which often enable inference on the
   source of the activity detected (e.g. from activity ratios). Obviously,
   such conclusions become questionable if the measurement merely gives a
   detection limit for a specific isotope. This is particularly relevant if
   the presence of an isotope, which shows a low signal only (e.g. due to a
   short half-life or a small transition probability), is crucial for
   gaining the information of interest.
   This paper discusses a new approach which has the potential to solve
   these problems. Using Bayesian statistics, a method is presented which
   allows statistical inference on nuclide ratios taking into account both
   prior knowledge and all information collected from the measurements. it
   is shown that our method allows quantitative conclusion to be drawn if
   counts of single isotopes are low or become even negative after
   background subtraction. Differences to the traditional statistical
   approach of specifying decision thresholds or detection limits are
   highlighted.
   Application of this new approach is illustrated by a number of examples
   of environmental low-level radioactivity measurements. The capabilities
   of our approach for spectrum interpretation and source identification
   are demonstrated with real spectra from air filters, sewage sludge and
   soil samples. (C) 2009 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jenvrad.2009.03.009}},
ISSN = {{0265-931X}},
Unique-ID = {{ISI:000266511000006}},
}

@article{ ISI:000265722000013,
Author = {Zhang, Wei and Swinton, Scott M.},
Title = {{Incorporating natural enemies in an economic threshold for dynamically
   optimal pest management}},
Journal = {{ECOLOGICAL MODELLING}},
Year = {{2009}},
Volume = {{220}},
Number = {{9-10}},
Pages = {{1315-1324}},
Month = {{MAY 17}},
Note = {{Euopean Conference on Ecological Modelling, Trieste, ITALY, 2007}},
Abstract = {{The control of pests by their natural enemies represents an important
   regulating ecosystem service that helps maintain the stability of crop
   ecosystems. These services, however, are often ignored in pest
   management decision making. In addition, the use of broad-spectrum
   insecticides can damage the populations of natural enemies, reducing the
   cost-effectiveness of insecticide investment if unaccounted for in
   treatment decisions.
   The existing literature on modeling of biological control of insect
   pests has generally focused on simulations of the population dynamics of
   pest and natural enemy species and the processes underlying pest
   control. But agriculture is a managed ecosystem where predator-prey
   relationships are heavily influenced by human managers. In modeling
   managerial choices, this study develops an intra-seasonal dynamic
   bioeconomic optimization model for insecticide-based pest management
   that explicitly takes into account both the biological control effect of
   natural enemies on pest density and the nontarget mortality effect of
   insecticides on the level of natural pest control supplied. The model
   captures predator-prey interactions, linking them to crop growth and
   yield damage functions, which in turn are evaluated in a dynamic
   optimization framework. We introduce a new decision rule for judicious
   insecticide decisions using a natural enemy-adjusted economic threshold.
   This threshold represents the pest population density at which
   insecticide control becomes optimal in spite of the opportunity cost of
   injury to natural enemies of the target pest. Using field data from
   Michigan, the model is applied to the case of soybean aphid (Aphis
   glycines, Matsumura), a recent invasive pest of soybean (Glycine max),
   whose management is of both economic and environmental importance to the
   North Central region of the United States. As illustrated by the
   numerical examples, such natural enemy-adjusted threshold is likely to
   lead to fewer recommendations for insecticide use than naive models that
   ignore natural enemies, resulting in less insecticide use, while
   maintaining profitability for farmers that rely on chemical pest control
   methods.
   The bioeconomic model developed in this study can be used to conduct a
   wide variety of analyses such as identifying dynamically optimal spray
   strategies and estimating the implied economic value of natural control
   services. Furthermore, with the incorporation of inter-year carry-over
   factors, such as overwintering of pests and natural enemies, the current
   model can contribute to building multi-year models for studying
   long-term pest management. (C) 2009 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolmodel.2009.01.027}},
ISSN = {{0304-3800}},
EISSN = {{1872-7026}},
Unique-ID = {{ISI:000265722000013}},
}

@article{ ISI:000266174400020,
Author = {Rueda, Marta and Rebollo, Salvador and Galvez-Bravo, Lucia},
Title = {{Response to Delibes-Mateos et al. : Pellet size matters}},
Journal = {{ACTA OECOLOGICA-INTERNATIONAL JOURNAL OF ECOLOGY}},
Year = {{2009}},
Volume = {{35}},
Number = {{3}},
Pages = {{485-487}},
Month = {{MAY-JUN}},
Abstract = {{In Rueda et al. {[}Rueda, M., Rebollo, S., Galvez-Bravo, L., 2008. Age
   and season determine European rabbit habitat use in Mediterranean
   ecosystems. Acta Oecol. 34, 266-273] We used a threshold of 6 nun faecal
   pellet diameter to differentiate between adult and juvenile European
   rabbit (Oryctolagus cuniculus) habitat use. Delibes-Mateos et al.
   designed a housing experiment with 12 adult rabbits and criticised the
   choice of 6 mm as a threshold to separate adult and juvenile rabbit
   pellets, claiming that adults can produce pellets both larger and
   smaller than 6 mm in similar proportions. In response to their criticism
   we argue the following. The selection of a 6 mm threshold has a
   bibliographic basis, it is not a new method developed by Rueda et al.
   and produces consistent results When applied in the field. Assuming that
   Delibes-Mateos et al. results are accurate, we should have found a
   greater number of <6 mm pellets than >6 mm, overall and seasonally,
   which is not the case. We believe that the use of commercial pelleted
   food, keeping animals isolated in small cages for over a year. and the
   use of adult rabbits only, makes the experimental design used by these
   authors not Suitable to refute the usefulness of separating rabbit
   pellets smaller and larger than 6 mm diameter as indicators of changes
   in the relative abundance of juvenile and adult rabbits in the field.
   Finally, we agree with the authors that the use of indirect methods of
   animal aging would require case-specific validation Studies: however, we
   believe these studies should be correctly designed. (C) 2009 Elsevier
   Masson SAS. All rights reserved.}},
DOI = {{10.1016/j.actao.2009.01.006}},
ISSN = {{1146-609X}},
ResearcherID-Numbers = {{Rueda, Marta/K-5024-2014
   Galvez-Bravo, Lucia/L-7478-2017}},
ORCID-Numbers = {{Rueda, Marta/0000-0002-9754-6258
   Galvez-Bravo, Lucia/0000-0002-9684-9775}},
Unique-ID = {{ISI:000266174400020}},
}

@article{ ISI:000264944000005,
Author = {Joh, Richard I. and Wang, Hao and Weiss, Howard and Weitz, Joshua S.},
Title = {{Dynamics of Indirectly Transmitted Infectious Diseases with
   Immunological Threshold}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{2009}},
Volume = {{71}},
Number = {{4}},
Pages = {{845-862}},
Month = {{MAY}},
Abstract = {{There are numerous examples of human pathogens which persist in
   environmental reservoirs while infectious outbreaks remain rare. In this
   manuscript, we consider the dynamics of infectious diseases for which
   the primary mode of transmission is indirect and mediated by contact
   with a contaminated reservoir. We evaluate the realistic scenario in
   which the number of ingested pathogens must be above a critical
   threshold to cause infection in susceptible individuals. This minimal
   infectious dose is a consequence of the clearance effect of the innate
   immune system. Infected individuals shed pathogens back into the aquatic
   reservoir, indirectly increasing the transmittability of the pathogen to
   the susceptible. Building upon prior works in the study of cholera
   dynamics, we introduce and analyze a family of reservoir mediated SIR
   models with a threshold pathogen density for infection. Analyzing this
   family of models, we show that an outbreak can result from
   noninfinitesimal introductions of either infected individuals or
   additional pathogens in the reservoir. We devise two new measures of how
   likely it is that an environmentally persistent pathogen will cause an
   outbreak: (i) the minimum fraction of infected individuals; and (ii) the
   minimum fluctuation size of in-reservoir pathogens. We find an
   additional control parameter involving the shedding rate of infected
   individuals, which we term the pathogen enhancement ratio, which
   determines whether outbreaks lead to epidemics or endemic disease
   states. Thus, the ultimate outcome of disease is controlled by the
   strength of fluctuations and the global stability of a nonlinear
   dynamical system, as opposed to conventional analysis in which disease
   reflects the linear destabilization of a disease free equilibrium. Our
   model predicts that in the case of waterborne diseases, suppressing the
   pathogen density in aquatic reservoirs may be more effective than
   minimizing the number of infected individuals.}},
DOI = {{10.1007/s11538-008-9384-4}},
ISSN = {{0092-8240}},
ResearcherID-Numbers = {{Weitz, Joshua/I-6696-2012
   }},
ORCID-Numbers = {{Weitz, Joshua/0000-0002-3433-8312
   Joh, Richard/0000-0003-0583-8032}},
Unique-ID = {{ISI:000264944000005}},
}

@article{ ISI:000265010400003,
Author = {Gurarie, Eliezer and Andrews, Russel D. and Laidre, Kristin L.},
Title = {{A novel method for identifying behavioural changes in animal movement
   data}},
Journal = {{ECOLOGY LETTERS}},
Year = {{2009}},
Volume = {{12}},
Number = {{5}},
Pages = {{395-408}},
Month = {{MAY}},
Abstract = {{A goal of animal movement analysis is to reveal behavioural mechanisms
   by which organisms utilize complex and variable environments.
   Statistical analysis of movement data is complicated by the fact that
   the data are multidimensional, autocorrelated and often marked by error
   and irregular measurement intervals or gappiness. Furthermore, movement
   data reflect behaviours that are themselves heterogeneous. Here, we
   model movement data as a subsampling of a continuous stochastic
   processes, and introduce the behavioural change point analysis (BCPA), a
   likelihood-based method that allows for the identification of
   significant structural changes. The BCPA is robust to gappiness and
   measurement error, computationally efficient, easy to implement and
   reveals structure that is otherwise difficult to discern. We apply the
   analysis to a GPS movement track of a northern fur seal (Callorhinus
   ursinus), revealing an unexpectedly complex diurnal behavioural profile,
   and demonstrate its robustness to the greater errors associated with the
   ARGOS tracking system. By informing empirical interpretation of movement
   data, we suggest that the BCPA can eventually motivate the development
   of mechanistic behavioural models.}},
DOI = {{10.1111/j.1461-0248.2009.01293.x}},
ISSN = {{1461-023X}},
EISSN = {{1461-0248}},
ResearcherID-Numbers = {{Logger, Satellite/C-1379-2010
   }},
ORCID-Numbers = {{Gurarie, Eliezer/0000-0002-8666-9674}},
Unique-ID = {{ISI:000265010400003}},
}

@article{ ISI:000267408500009,
Author = {Arbia, G. and Copetti, M. and Lafratta, G.},
Title = {{Spatial dependence in the tails of air pollutant distributions:
   alternatives to the spatial correlogram}},
Journal = {{ENVIRONMETRICS}},
Year = {{2009}},
Volume = {{20}},
Number = {{3}},
Pages = {{331-345}},
Month = {{MAY}},
Abstract = {{The study of the pollutants needs a better understanding of their
   extreme behaviours which could potentially cause adverse health effects.
   When analysing spatial dependence of the Pollutant, the dependogram
   proposed by Arbia and Lafratta is preferred to the traditional
   correlogram used in the spatial statistics literature because it
   captures nonlinear relationships in the tails of the joint distributions
   and helps in detecting a pattern of spatial regularities. In this paper,
   we present a new method to estimate the spatial dependogram that uses
   univariate and bivariate threshold models. The method is applied to a
   set of hourly NO(2) data collected by seven monitoring stations in the
   city of Rome (Italy) during the years 2000 and 2001. Copyright (c) 2008
   John Wiley \& Sons. Ltd.}},
DOI = {{10.1002/env.934}},
ISSN = {{1180-4009}},
ResearcherID-Numbers = {{Copetti, Massimiliano/K-3186-2016}},
ORCID-Numbers = {{Copetti, Massimiliano/0000-0002-7960-5947}},
Unique-ID = {{ISI:000267408500009}},
}

@article{ ISI:000265560900005,
Author = {Bazan, Carlos and Miller, Michelle and Blomgren, Peter},
Title = {{Structure enhancement diffusion and contour extraction for electron
   tomography of mitochondria}},
Journal = {{JOURNAL OF STRUCTURAL BIOLOGY}},
Year = {{2009}},
Volume = {{166}},
Number = {{2}},
Pages = {{144-155}},
Month = {{MAY}},
Abstract = {{The interpretation and measurement of the architectural organization of
   mitochondria depend heavily upon the availability of good software tools
   for filtering, segmenting, extracting, measuring, and classifying the
   features of interest. Images of mitochondria contain many flow-like
   patterns and they are usually corrupted by large amounts of noise. Thus,
   it is necessary to enhance them by denoising and closing interrupted
   structures. We introduce a new approach based on anisotropic nonlinear
   diffusion and bilateral filtering for electron tomography of
   mitochondria. It allows noise removal and structure closure at certain
   scales, while preserving both the orientation and magnitude of
   discontinuities without the need for threshold switches. This technique
   facilitates image enhancement for subsequent segmentation, contour
   extraction, and improved visualization of the complex and intricate
   mitochondrial morphology. We perform the extraction of the
   structure-defining contours by employing a variational level set
   formulation. The propagating front for this approach is an approximate
   signed distance function which does not require expensive
   re-initialization. The behavior of the combined approach is tested for
   visualizing the structure of a HeLa cell mitochondrion and the results
   we obtain are very promising. (C) 2009 Elsevier Inc. All rights
   reserved.}},
DOI = {{10.1016/j.jsb.2009.02.009}},
ISSN = {{1047-8477}},
Unique-ID = {{ISI:000265560900005}},
}

@article{ ISI:000266394600017,
Author = {Wang Jing and Feng Zhou-Yan},
Title = {{A Novel Method for Multi-channel Neuronal Spike Detection and
   Classification}},
Journal = {{PROGRESS IN BIOCHEMISTRY AND BIOPHYSICS}},
Year = {{2009}},
Volume = {{36}},
Number = {{5}},
Pages = {{641-647}},
Month = {{MAY}},
Abstract = {{The detection and classification of extracellular action potentials
   (i.e. spike) of various single neurons from extracellular recordings are
   crucial for extracting neuronal spike sequences and thereby for
   investigating the mechanisms of neural information processing in the
   central nervous system. In order to increase the correctness of spike
   detecting and sorting, a new analysis algorithm for processing
   multi-channel spike signals recorded from rat hippocampi with silicon
   microelectrode arrays is presented. Four recording contacts on the
   electrode array are arranged close enough to simultaneously record
   spikes emitted from same neurons. Firstly, the algorithm extracts all
   spikes in the four channel recordings by using a multi-channel threshold
   detection method. Secondly, the algorithm classifies the spikes based on
   a principle component analysis for a specifically designed type of
   compound spike waveforms. The compound spike waveform is formed by
   linking four spike waveforms of a same neuronal firing in the four
   recording channels one by one in series. The test results with both
   synthetic datasets and experimental recordings reveal that compared with
   corresponding traditional single-channel algorithm, the multi-channel
   algorithm can significantly enhance both the number of extracted spikes
   and the correctness of spike classifications. The algorithm can also
   increase the number of isolated neurons from a single experimental
   preparation. These results indicate that the novel method is efficient
   for the automatic detection and classification of neuronal spikes.}},
DOI = {{10.3724/SP.J.1206.2008.00606}},
ISSN = {{1000-3282}},
Unique-ID = {{ISI:000266394600017}},
}

@article{ ISI:000265341600028,
Author = {Nissinen, A. and Parikka-Alhola, K. and Rita, H.},
Title = {{Environmental criteria in the public purchases above the EU threshold
   values by three Nordic countries: 2003 and 2005}},
Journal = {{ECOLOGICAL ECONOMICS}},
Year = {{2009}},
Volume = {{68}},
Number = {{6}},
Pages = {{1838-1849}},
Month = {{APR 15}},
Abstract = {{Green Public Procurement (GPP) has been considered as an important
   policy instrument in the context of sustainable consumption and
   production. The state and progress of GPP has earlier been measured by
   questionnaires and interviews, both methods being based on the
   assessment by the purchaser, and questionnaires having low response
   rates. Recently, a new method was developed, analyzing the existence of
   environmental criteria in the calls for tenders. However, the studies
   have dealt neither with the progress in GPP, nor the statistical
   evidence of differences between countries. Our aim was to analyze more
   thoroughly whether the differences in the proportions of `green' calls
   for tenders between the three Nordic countries, Denmark, Finland and
   Sweden, in 2003 and 2005 were real, and whether there had occurred any
   progress between the years concerned. The paper also presents the
   `GPP-record' method, which enables more valid measurement of the
   environmental soundness of public purchasing. The statistical analyses
   were done using logit models with country, year and product group as the
   explanatory factors. It proved to be relevant to take into account the
   variation that occurred from the random existence of product groups in
   the samples of calls for tender. There were less environmental criteria
   in the calls for tenders in Finland than in Denmark and Sweden in 2003,
   but in 2005 no significant difference between Finland and Denmark was
   observed. Both Finland and Sweden saw progress in this area between 2003
   and 2005. (C) 2008 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolecon.2008.12.005}},
ISSN = {{0921-8009}},
Unique-ID = {{ISI:000265341600028}},
}

@article{ ISI:000264651800014,
Author = {Georgii, Elisabeth and Dietmann, Sabine and Uno, Takeaki and Pagel,
   Philipp and Tsuda, Koji},
Title = {{Enumeration of condition-dependent dense modules in protein interaction
   networks}},
Journal = {{BIOINFORMATICS}},
Year = {{2009}},
Volume = {{25}},
Number = {{7}},
Pages = {{933-940}},
Month = {{APR 1}},
Abstract = {{Motivation: Modern systems biology aims at understanding how the
   different molecular components of a biological cell interact. Often,
   cellular functions are performed by complexes consisting of many
   different proteins. The composition of these complexes may change
   according to the cellular environment, and one protein may be involved
   in several different processes. The automatic discovery of functional
   complexes from protein interaction data is challenging. While previous
   approaches use approximations to extract dense modules, our approach
   exactly solves the problem of dense module enumeration. Furthermore,
   constraints from additional information sources such as gene expression
   and phenotype data can be integrated, so we can systematically mine for
   dense modules with interesting profiles.
   Results: Given a weighted protein interaction network, our method
   discovers all protein sets that satisfy a user-defined minimum density
   threshold. We employ a reverse search strategy, which allows us to
   exploit the density criterion in an efficient way. Our experiments show
   that the novel approach is feasible and produces biologically meaningful
   results. In comparative validation studies using yeast data, the method
   achieved the best overall prediction performance with respect to
   confirmed complexes. Moreover, by enhancing the yeast network with
   phenotypic and phylogenetic profiles and the human network with
   tissue-specific expression data, we identified condition-dependent
   complex variants.}},
DOI = {{10.1093/bioinformatics/btp080}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000264651800014}},
}

@article{ ISI:000265602600006,
Author = {Nam, Hojung and Lee, KiYoung and Lee, Doheon},
Title = {{Identification of temporal association rules from time-series microarray
   data sets}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2009}},
Volume = {{10}},
Number = {{3}},
Month = {{MAR 19}},
Note = {{2nd International Workshop on Data and Text Mining in Bioinformatics,
   Napa Valley, CA, OCT 30, 2008}},
Abstract = {{Background: One of the most challenging problems in mining gene
   expression data is to identify how the expression of any particular gene
   affects the expression of other genes. To elucidate the relationships
   between genes, an association rule mining (ARM) method has been applied
   to microarray gene expression data. However, a conventional ARM method
   has a limit on extracting temporal dependencies between gene
   expressions, though the temporal information is indispensable to
   discover underlying regulation mechanisms in biological pathways. In
   this paper, we propose a novel method, referred to as temporal
   association rule mining (TARM), which can extract temporal dependencies
   among related genes. A temporal association rule has the form {[}gene A
   up arrow, gene B down arrow] -> (7 min) {[}gene C up arrow], which
   represents that high expression level of gene A and significant
   repression of gene B followed by significant expression of gene C after
   7 minutes. The proposed TARM method is tested with Saccharomyces
   cerevisiae cell cycle time-series microarray gene expression data set.
   Results: In the parameter fitting phase of TARM, the fitted parameter
   set {[}threshold = +/- 0.8, support >= 3 transactions, confidence >=
   90\%] with the best precision score for KEGG cell cycle pathway has been
   chosen for rule mining phase. With the fitted parameter set, numbers of
   temporal association rules with five transcriptional time delays (0, 7,
   14, 21, 28 minutes) are extracted from gene expression data of 799
   genes, which are pre-identified cell cycle relevant genes. From the
   extracted temporal association rules, associated genes, which play same
   role of biological processes within short transcriptional time delay and
   some temporal dependencies between genes with specific biological
   processes are identified.
   Conclusion: In this work, we proposed TARM, which is an applied form of
   conventional ARM. TARM showed higher precision score than Dynamic
   Bayesian network and Bayesian network. Advantages of TARM are that it
   tells us the size of transcriptional time delay between associated
   genes, activation and inhibition relationship between genes, and sets of
   co-regulators.}},
DOI = {{10.1186/1471-2105-10-S3-S6}},
Article-Number = {{S6}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Lee, Doheon/B-7303-2011
   }},
ORCID-Numbers = {{Lee, Doheon/0000-0001-9070-4316
   Nam, Hojung/0000-0002-5109-9114}},
Unique-ID = {{ISI:000265602600006}},
}

@article{ ISI:000264589000002,
Author = {Walter, Christof and Stuetzel, Hartmut},
Title = {{A new method for assessing the sustainability of land-use systems (II):
   Evaluating impact indicators}},
Journal = {{ECOLOGICAL ECONOMICS}},
Year = {{2009}},
Volume = {{68}},
Number = {{5}},
Pages = {{1288-1300}},
Month = {{MAR 15}},
Abstract = {{In the past decade, numerous indicators and indicator sets for
   sustainable agriculture and sustainable land management have been
   proposed. In addition to their interest in comparing different
   management systems on an indicator by indicator basis, land managers are
   often interested in comparing individual indicators against a threshold,
   or, in order to study trade-offs, against each other. To this end it is
   necessary to (1) transform the original indicators into a comparable
   format, and (2) score these transformed indicators against a
   sustainability function.
   This paper introduces an evaluation method for land-use-related impact
   indicators, which was designed to accomplish these tasks. it is the
   second of a series of two papers, and as such it links into a larger
   framework for sustainability assessment of land use systems.
   The evaluation scheme introduced here comprises (1) a standardisation
   procedure, which aims at making different indicators comparable. In this
   procedure indicators are first normalised, by referencing them to the
   total impact they contribute towards, and then they are corrected by a
   factor describing the severity of this total impact in terms of
   exceeding a threshold. The procedure borrows conceptually from Life
   Cycle Assessment (LCA) Impact Analysis methodology; (2) a valuation
   procedure, which judges the individual standardised indicators with
   regard to sustainability.
   This methodology is then tested on an indicator set for the
   environmental impact of a spinach production system in Northwest
   Germany. The method highlights mineral resource consumption, greenhouse
   gas emission, eutrophication and impacts on soil quality as the most
   important environmental effects of the studied system.
   We then explore the effect of introducing weighting factors, reflecting
   the differing societal perception of diverse environmental issues. Two
   different sets of weighting factors are used.
   The influence of weighting is, however, small compared to that of the
   standardisation procedure introduced earlier.
   Finally, we explore the propagation of uncertainty (defined as a
   variable's 95\% confidence limits) throughout the standardisation
   procedure using a stochastic simulation approach. The uncertainty of the
   analysed standardised indicator was higher than that of the
   non-standardised indicators by a factor of 2.0 to 2.5. (C) 2008 Elsevier
   B.V. All rights reserved.}},
DOI = {{10.1016/j.ecolecon.2008.11.017}},
ISSN = {{0921-8009}},
ResearcherID-Numbers = {{Stutzel, Hartmut/A-6743-2008}},
ORCID-Numbers = {{Stutzel, Hartmut/0000-0002-0015-5550}},
Unique-ID = {{ISI:000264589000002}},
}

@article{ ISI:000264353400003,
Author = {Vorbau, Manuel and Hillemann, Lars and Stintz, Michael},
Title = {{Method for the characterization of the abrasion induced nanoparticle
   release into air from surface coatings}},
Journal = {{JOURNAL OF AEROSOL SCIENCE}},
Year = {{2009}},
Volume = {{40}},
Number = {{3}},
Pages = {{209-217}},
Month = {{MAR}},
Abstract = {{A novel method for the quantification of the nanoparticle release into
   air from surface coatings was developed. The method is based on the
   combination of a defined abrasion process with highly sensitive methods
   to quantify airborne particle concentration. A standardized system for
   abrasion testing was employed to adjust a defined wear process.
   Details of the abrasion system are presented in combination with a
   downstream aerosol measurement technique (scanning mobility particle
   sizer, SMPS and condensation particle counter, CPC) which focuses on the
   quantification of a release rate from particles smaller than 100 nm. The
   interconnection between the aerosol measurement system and the abrasion
   process is described in detail.
   After the proof of sensitivity of the measuring system first measuring
   results of abraded surface coatings revealed a reasonable
   reproducibility of the generated wear particles in terms of total mass
   and a rather weak reproducibility in terms of the total number
   concentration. However, the total number of generated submicrometer
   particles or nanoparticles was extremely low and laid below the
   statistic significance threshold. (C) 2008 Elsevier Ltd. All rights
   reserved.}},
DOI = {{10.1016/j.jaerosci.2008.10.006}},
ISSN = {{0021-8502}},
ResearcherID-Numbers = {{Stintz, Michael/D-7549-2011}},
Unique-ID = {{ISI:000264353400003}},
}

@article{ ISI:000263338600007,
Author = {Roy, Shovonlal},
Title = {{The coevolution of two phytoplankton species on a single resource:
   Allelopathy as a pseudo-mixotrophy}},
Journal = {{THEORETICAL POPULATION BIOLOGY}},
Year = {{2009}},
Volume = {{75}},
Number = {{1}},
Pages = {{68-75}},
Month = {{FEB}},
Abstract = {{Without the top-down effects and the external/physical forcing, a stable
   coexistence of two phytoplankton species under a single resource is
   impossible - a result well known from the principle of competitive
   exclusion. Here I demonstrate by analysis of a mathematical model that
   such a stable coexistence in a homogeneous media without any external
   factor would be possible, at least theoretically, provided (i) one of
   the two species is toxin producing thereby has an allelopathic effect on
   the other, and (ii) the allelopathic effect exceeds a critical level.
   The threshold level of allelopathy required for the coexistence has been
   derived analytically in terms of the parameters associated with the
   resource competition and the nutrient recycling. That the extra
   mortality of a competitor driven by allelopathy of a toxic species gives
   a positive feed back to the algal growth process through the recycling
   is explained. And that this positive feed back plays a pivotal role in
   reducing competition pressures and helping species succession in the
   two-species model is demonstrated. Based on these specific coexistence
   results, I introduce and explain theoretically the allelopathic effect
   of a toxic species as a `pseudo-mixotrophy' - a mechanism of `if you
   cannot beat them or eat them, just kill them by chemical weapons'. The
   impact of this mechanism of species succession by pseudo-mixotrophy in
   the form of alleopathy is discussed in the context of current
   understanding on straight mixotrophy and resource-species relationship
   among phytoplankton species. (C) 2008 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.tpb.2008.11.003}},
ISSN = {{0040-5809}},
ORCID-Numbers = {{Roy, Shovonlal/0000-0003-2543-924X}},
Unique-ID = {{ISI:000263338600007}},
}

@article{ ISI:000262005200016,
Author = {Pacheco, Jorge M. and Santos, Francisco C. and Souza, Max O. and Skyrms,
   Brian},
Title = {{Evolutionary dynamics of collective action in N-person stag hunt
   dilemmas}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2009}},
Volume = {{276}},
Number = {{1655}},
Pages = {{315-321}},
Month = {{JAN 22}},
Abstract = {{In the animal world, collective action to shelter, protect and nourish
   requires the cooperation of group members. Among humans, many situations
   require the cooperation of more than two individuals simultaneously.
   Most of the relevant literature has focused on an extreme case, the
   N-person Prisoner's Dilemma. Here we introduce a model in which a
   threshold less than the total group is required to produce benefits,
   with increasing participation leading to increasing productivity. This
   model constitutes a generalization of the two-person stag hunt game to
   an N-person game. Both finite and infinite population models are
   studied. In infinite populations this leads to a rich dynamics that
   admits multiple equilibria. Scenarios of defector dominance, pure
   coordination or coexistence may arise simultaneously. On the other hand,
   whenever one takes into account that populations are finite and when
   their size is of the same order of magnitude as the group size, the
   evolutionary dynamics is profoundly affected: it may ultimately invert
   the direction of natural selection, compared with the infinite
   population limit.}},
DOI = {{10.1098/rspb.2008.1126}},
ISSN = {{0962-8452}},
EISSN = {{1471-2954}},
ResearcherID-Numbers = {{Souza, Max/B-6399-2008
   Santos, Francisco/D-2394-2010
   Pacheco, Jorge/B-6116-2008}},
ORCID-Numbers = {{Souza, Max/0000-0001-8200-1899
   Santos, Francisco/0000-0002-9103-2862
   Pacheco, Jorge/0000-0002-2579-8499}},
Unique-ID = {{ISI:000262005200016}},
}

@article{ ISI:000261955100012,
Author = {Tennie, Claudio and Gilby, Ian C. and Mundry, Roger},
Title = {{The meat-scrap hypothesis: small quantities of meat may promote
   cooperative hunting in wild chimpanzees (Pan troglodytes)}},
Journal = {{BEHAVIORAL ECOLOGY AND SOCIOBIOLOGY}},
Year = {{2009}},
Volume = {{63}},
Number = {{3}},
Pages = {{421-431}},
Month = {{JAN}},
Abstract = {{A common explanation for hunting in groups is that doing so yields a
   greater per capita caloric benefit than hunting solitarily. This is
   logical for social carnivores, which rely exclusively on meat for
   energy, but arguably not for omnivores, which obtain calories from
   either plant or animal matter. The common chimpanzee, Pan troglodytes,
   is one of the few true omnivores that regularly hunts in groups. Studies
   to date have yielded conflicting data regarding the payoffs of group
   hunting in chimpanzees. Here, we interpret chimpanzee hunting patterns
   using a new approach. In contrast to the classical assumption that
   hunting with others maximizes per capita caloric intake, we propose that
   group hunting is favored because it maximizes an individual's likelihood
   of obtaining important micronutrients that may be found in small
   quantities of meat. We describe a mathematical model demonstrating that
   group hunting may evolve when individuals can obtain micronutrients more
   frequently by hunting in groups than by hunting solitarily, provided
   that group size is below a certain threshold. Twenty five years of data
   from Gombe National Park, Tanzania are consistent with this prediction.
   We propose that our `meat-scrap' hypothesis is a unifying approach that
   may explain group hunting by chimpanzees and other social omnivores.}},
DOI = {{10.1007/s00265-008-0676-3}},
ISSN = {{0340-5443}},
ORCID-Numbers = {{Tennie, Claudio/0000-0002-5302-4925}},
Unique-ID = {{ISI:000261955100012}},
}

@article{ ISI:000261678500010,
Author = {Bloomquist, Erik W. and Dorman, Karin S. and Suchard, Marc A.},
Title = {{StepBrothers: inferring partially shared ancestries among recombinant
   viral sequences}},
Journal = {{BIOSTATISTICS}},
Year = {{2009}},
Volume = {{10}},
Number = {{1}},
Pages = {{106-120}},
Month = {{JAN}},
Abstract = {{Phylogeneticists have developed several statistical methods to infer
   recombination among molecular sequences that are evolutionarily related.
   Of these methods, Markov change-point models currently provide the most
   coherent framework. Yet, the Markov assumption is faulty in that the
   inferred relatedness of homologous sequences across regions divided by
   recombinant events is not independent, particularly for nonrecombinant
   sequences as they share the same history. To correct this limitation, we
   introduce a novel random tips (RT) model. The model springs from the
   idea that a recombinant sequence inherits its characters from an unknown
   number of ancestral full-length sequences, of which one only observes
   the incomplete portions. The RT model decomposes recombinant sequences
   into their ancestral portions and then augments each portion onto the
   data set as unique partially observed sequences. This data augmentation
   generates a random number of sequences related to each other through a
   single inferable tree with the same random number of tips. While
   intuitively pleasing, this single tree corrects the independence
   assumptions plaguing previous methods while permitting the detection of
   recombination. The single tree also allows for inference of the relative
   times of recombination events and generalizes to incorporate multiple
   recombinant sequences. This generalization answers important questions
   with which previous models struggle. For example, we demonstrate that a
   group of human immunodeficiency type 1 recombinant viruses from
   Argentina, previously thought to have the same recombinant history,
   actually consist of 2 groups: one, a clonal expansion of a reference
   sequence and another that predates the formation of the reference
   sequence. In another example, we demonstrate that 2 hepatitis B virus
   recombinant strains share similar splicing locations, suggesting a
   common descent of the 2 viruses. We implement and run both examples in a
   software package called StepBrothers, freely available to interested
   parties.}},
DOI = {{10.1093/biostatistics/kxn019}},
ISSN = {{1465-4644}},
EISSN = {{1468-4357}},
ORCID-Numbers = {{Dorman, Karin/0000-0003-3650-0018}},
Unique-ID = {{ISI:000261678500010}},
}

@incollection{ ISI:000272402300009,
Author = {Zou, Fei},
Editor = {{DiPetrillo, K}},
Title = {{QTL Mapping in Intercross and Backcross Populations}},
Booktitle = {{CARDIOVASCULAR GENOMICS: METHODS AND PROTOCOLS}},
Series = {{Methods in Molecular Biology}},
Year = {{2009}},
Volume = {{573}},
Pages = {{157-173}},
Abstract = {{In the past two decades, various statistical approaches have been
   developed to identify quantitative trait locus with experimental
   organisms. In this chapter, we introduce several commonly used QTL
   mapping methods for intercross and backcross populations. Important
   issues related to QTL mapping, such as threshold and confidence interval
   calculations are also discussed. We list and describe five public domain
   QTL software packages commonly used by biologists.}},
DOI = {{10.1007/978-1-60761-247-6\_9}},
ISSN = {{1064-3745}},
ISBN = {{978-1-60761-246-9}},
Unique-ID = {{ISI:000272402300009}},
}

@article{ ISI:000278707200010,
Author = {Rockstrom, Johan and Steffen, Will and Noone, Kevin and Persson, Asa and
   Chapin, III, F. Stuart and Lambin, Eric and Lenton, Timothy M. and
   Scheffer, Marten and Folke, Carl and Schellnhuber, Hans Joachim and
   Nykvist, Bjorn and de Wit, Cynthia A. and Hughes, Terry and van der
   Leeuw, Sander and Rodhe, Henning and Sorlin, Sverker and Snyder, Peter
   K. and Costanza, Robert and Svedin, Uno and Falkenmark, Malin and
   Karlberg, Louise and Corell, Robert W. and Fabry, Victoria J. and
   Hansen, James and Walker, Brian and Liverman, Diana and Richardson,
   Katherine and Crutzen, Paul and Foley, Jonathan},
Title = {{Planetary Boundaries: Exploring the Safe Operating Space for Humanity}},
Journal = {{ECOLOGY AND SOCIETY}},
Year = {{2009}},
Volume = {{14}},
Number = {{2}},
Abstract = {{Anthropogenic pressures on the Earth System have reached a scale where
   abrupt global environmental change can no longer be excluded. We propose
   a new approach to global sustainability in which we define planetary
   boundaries within which we expect that humanity can operate safely.
   Transgressing one or more planetary boundaries may be deleterious or
   even catastrophic due to the risk of crossing thresholds that will
   trigger non-linear, abrupt environmental change within continental- to
   planetary-scale systems. We have identified nine planetary boundaries
   and, drawing upon current scientific understanding, we propose
   quantifications for seven of them. These seven are climate change (CO2
   concentration in the atmosphere <350 ppm and/or a maximum change of +1 W
   m(-2) in radiative forcing); ocean acidification (mean surface seawater
   saturation state with respect to aragonite >= 80\% of pre-industrial
   levels); stratospheric ozone (<5\% reduction in O-3 concentration from
   pre-industrial level of 290 Dobson Units); biogeochemical nitrogen (N)
   cycle (limit industrial and agricultural fixation of N-2 to 35 Tg N
   yr(-1)) and phosphorus (P) cycle (annual P inflow to oceans not to
   exceed 10 times the natural background weathering of P); global
   freshwater use (<4000 km(3) yr(-1) of consumptive use of runoff
   resources); land system change (<15\% of the ice-free land surface under
   cropland); and the rate at which biological diversity is lost (annual
   rate of <10 extinctions per million species). The two additional
   planetary boundaries for which we have not yet been able to determine a
   boundary level are chemical pollution and atmospheric aerosol loading.
   We estimate that humanity has already transgressed three planetary
   boundaries: for climate change, rate of biodiversity loss, and changes
   to the global nitrogen cycle. Planetary boundaries are interdependent,
   because transgressing one may both shift the position of other
   boundaries or cause them to be transgressed. The social impacts of
   transgressing boundaries will be a function of the social-ecological
   resilience of the affected societies. Our proposed boundaries are rough,
   first estimates only, surrounded by large uncertainties and knowledge
   gaps. Filling these gaps will require major advancements in Earth System
   and resilience science. The proposed concept of ``planetary
   boundaries{''} lays the groundwork for shifting our approach to
   governance and management, away from the essentially sectoral analyses
   of limits to growth aimed at minimizing negative externalities, toward
   the estimation of the safe space for human development. Planetary
   boundaries define, as it were, the boundaries of the ``planetary playing
   field{''} for humanity if we want to be sure of avoiding major
   human-induced environmental change on a global scale.}},
Article-Number = {{32}},
ISSN = {{1708-3087}},
ResearcherID-Numbers = {{Scheffer, Marten/C-1852-2012
   Steffen, Will/C-7651-2011
   Quinn, Patrick/B-5489-2010
   Costanza, Robert/A-4912-2008
   Richardson, Katherine/D-7592-2014
   Snyder, Peter/H-3063-2013
   de Wit, Cynthia/J-8063-2012
   Crutzen, Paul/F-6044-2012
   Walker, Brian/F-2386-2011
   Hughes, Terry/L-4721-2013
   Schellnhuber, Hans Joachim/B-2607-2012
   Lenton, Tim/X-1893-2018
   }},
ORCID-Numbers = {{Costanza, Robert/0000-0001-6348-8734
   Richardson, Katherine/0000-0003-3785-2787
   de Wit, Cynthia/0000-0001-8497-2699
   Hughes, Terry/0000-0002-5257-5063
   Schellnhuber, Hans Joachim/0000-0001-7453-4935
   Lenton, Tim/0000-0002-6725-7498
   Chapin III, F Stuart/0000-0002-2558-9910}},
Unique-ID = {{ISI:000278707200010}},
}

@article{ ISI:000262352400111,
Author = {Ruter, Jens and Barnett, Brian G. and Kryczek, Ilona and Brumlik,
   Michael J. and Daniel, Benjamin J. and Coukos, George and Zou, Weiping
   and Curiel, Tyler J.},
Title = {{Altering regulatory T cell function in cancer immunotherapy: a novel
   means to boost the efficacy of cancer vaccines}},
Journal = {{FRONTIERS IN BIOSCIENCE-LANDMARK}},
Year = {{2009}},
Volume = {{14}},
Pages = {{1761-1770}},
Month = {{JAN 1}},
Abstract = {{Cancers express tumor associated antigens that should elicit immune
   attack, but spontaneous immune rejection of established cancer is rare.
   Recent data demonstrate that specific and active tumor-mediated
   mechanisms hinder host anti-tumor immunity. CD4(+)CD25(+) T regulatory
   cells (Tregs) are important mediators of active immune evasion in
   cancer. Disrupting tumor-mediated mechanisms hindering host immunity is
   a novel approach to tumor immunotherapy. Treg depletion improves
   endogenous anti-tumor immunity and the efficacy of active immunotherapy
   in animal models for cancer, suggesting that inhibiting Treg function
   could also improve the limited successes of human cancer immunotherapy.
   We have identified five strategies to block Treg activity: depletion,
   interference with trafficking, inhibition of differentiation, blockade
   of function or raising the effector T cell threshold for suppression.
   Discovery of additional regulatory cell populations expands the
   potential targets for these approaches. The fusion toxin denileukin
   diftitox (Ontak) reduces Treg numbers and function in the blood of some
   patients with cancer. We discuss specific strategies to block Treg
   activity and present some of our preliminary data in this area.
   Combining Treg depletion with active vaccination and other approaches
   poses additional challenges that are discussed.}},
DOI = {{10.2741/3338}},
ISSN = {{1093-9946}},
EISSN = {{1093-4715}},
ORCID-Numbers = {{Coukos, George/0000-0001-8813-7367}},
Unique-ID = {{ISI:000262352400111}},
}

@article{ ISI:000262750800004,
Author = {Walker, Tony R. and Grant, Jon},
Title = {{Quantifying erosion rates and stability of bottom sediments at mussel
   aquaculture sites in Prince Edward Island, Canada}},
Journal = {{JOURNAL OF MARINE SYSTEMS}},
Year = {{2009}},
Volume = {{75}},
Number = {{1-2}},
Pages = {{46-55}},
Month = {{JAN}},
Abstract = {{Downward fluxes of organic biodeposits under suspended mussel culture
   cause benthic impacts such as microbial mat production. Quantifying
   sediment erosion in these coastal ecosystems is important for
   understanding how fluxes of organic matter and particulates contribute
   to benthic-pelagic coupling. Critical shear velocity (u{*}), erosion
   rates and particle size distributions of resuspended sediment were
   measured at two sites; an impacted muddy site with extensive mussel
   culture (site 1), and a coarser sandier site with less mussel influence
   (site 2), using a new method for assessing sediment erosion at Tracadie
   Bay, Prince Edward Island in August 2003. Shear forces were generated by
   vertically oscillating a perforated disc at controlled frequencies.
   These forces correspond to shear velocity, using a re-designed and
   calibrated Particle Erosion Simulator. Undisturbed sediment cores
   obtained by divers and grab (sub-cored using a Plexiglas (TM) cores)
   were exposed to shear stress to compare differences between collection
   methods. Microbial mats were present at site 1 which initially
   biostabilized sediment against erosion due to `armoring' of the
   sediment, but onset of erosion was abrupt once these mats failed.
   Erosion sequences at site 2 (without mat cover) were smoother resulting
   in less material being eroded. Mean mass of material eroded was 47 and
   23 g m(-2) min(-1) at sites 1 and 2 respectively. Mat area cover and
   shear velocity was strongly related. Critical shear velocities varied
   between 1.70 and 1.77 cm s(-1), with no obvious differences between
   location or collection method, so sediments from these two contrasting
   sites had identical mean critical shear velocities. Significant
   differences existed in the concentrations of chlorophyll a, colloidal
   and bulk carbohydrates, between mats and bare sediment from site 1.
   Particle sizes measured by videography of resuspended sediment at
   different shear velocities ranged from 100 mu m (the minimum diameter
   capable of being detected by the system), to large mat fragments of 1700
   mu m for both sites. These results provide evidence of the relevance of
   using a portable erosion device to indicate how sediment erodability is
   affected by mussel-microbial relationships. (c) 2008 Elsevier B.V. All
   rights reserved.}},
DOI = {{10.1016/j.jmarsys.2008.07.009}},
ISSN = {{0924-7963}},
ORCID-Numbers = {{Walker, Tony/0000-0001-9008-0697}},
Unique-ID = {{ISI:000262750800004}},
}

@article{ ISI:000261456700008,
Author = {Dang, Thanh Hai and Van Leemput, Koenraad and Verschoren, Alain and
   Laukens, Kris},
Title = {{Prediction of kinase-specific phosphorylation sites using conditional
   random fields}},
Journal = {{BIOINFORMATICS}},
Year = {{2008}},
Volume = {{24}},
Number = {{24}},
Pages = {{2857-2864}},
Month = {{DEC 15}},
Abstract = {{Motivation: Phosphorylation is a crucial post-translational protein
   modi. cation mechanism with important regulatory functions in biological
   systems. It is catalyzed by a group of enzymes called kinases, each of
   which recognizes certain target sites in its substrate proteins. Several
   authors have built computational models trained from sets of
   experimentally validated phosphorylation sites to predict these target
   sites for each given kinase. All of these models suffer from certain
   limitations, such as the fact that they do not take into account the
   dependencies between amino acid motifs within protein sequences in a
   global fashion.
   Results: We propose a novel approach to predict phosphorylation sites
   from the protein sequence. The method uses a positive dataset to train a
   conditional random field (CRF) model. The negative training dataset is
   used to specify the decision threshold corresponding to a desired false
   positive rate. Application of the method on experimentally verified
   benchmark phosphorylation data (Phospho. ELM) shows that it performs
   well compared to existing methods for most kinases. This is to our
   knowledge that the first report of the use of CRFs to predict
   post-translational modi. cation sites in protein sequences.}},
DOI = {{10.1093/bioinformatics/btn546}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Laukens, Kris/V-1121-2018}},
ORCID-Numbers = {{Laukens, Kris/0000-0002-8217-2564}},
Unique-ID = {{ISI:000261456700008}},
}

@article{ ISI:000261379600010,
Author = {Radhakrishnan, Arun and Goldstein, Joseph L. and McDonald, Jeffrey G.
   and Brown, Michael S.},
Title = {{Switch-like Control of SREBP-2 Transport Triggered by Small Changes in
   ER Cholesterol: A Delicate Balance}},
Journal = {{CELL METABOLISM}},
Year = {{2008}},
Volume = {{8}},
Number = {{6}},
Pages = {{512-521}},
Month = {{DEC 3}},
Abstract = {{Animal cells control their membrane lipid composition within narrow
   limits, but the sensing mechanisms underlying this control are largely
   unknown. Recent studies disclosed a protein network that controls the
   level of one lipid-cholesterol. This network resides in the endoplasmic
   reticulum (ER). A key component is Scap, a tetrameric ER membrane
   protein that binds cholesterol. Cholesterol binding prevents Scap, from
   transporting SREBPs to the Golgi for activation. Using a new method to
   purify ER membranes from cultured cells, we show that Scap responds
   cooperatively to ER cholesterol levels. When ER cholesterol exceeds 5\%
   of total ER lipids; (molar basis), SREBP-2 transport is abruptly
   blocked. Transport resumes when ER cholesterol falls below the 5\%
   threshold. The 5\% threshold is lowered to 3\% when cells overexpress
   Insig-1, Scap-binding protein. Cooperative interactions between
   cholesterol, Scap, and Insig create a sensitive switch that controls the
   cholesterol composition of cell membranes with remarkable precision.}},
DOI = {{10.1016/j.cmet.2008.10.008}},
ISSN = {{1550-4131}},
EISSN = {{1932-7420}},
ORCID-Numbers = {{Radhakrishnan, Arun/0000-0002-7266-7336}},
Unique-ID = {{ISI:000261379600010}},
}

@article{ ISI:000261566800023,
Author = {Sun, Wenjie and Stegmann, Barbara J. and Henne, Melinda and Cwherino,
   Williain H. and Segars, James H.},
Title = {{A new approach to ovarian reserve testing}},
Journal = {{FERTILITY AND STERILITY}},
Year = {{2008}},
Volume = {{90}},
Number = {{6}},
Pages = {{2196-2202}},
Month = {{DEC}},
Abstract = {{Objective: To critically examine ovarian reserve testing before assisted
   reproduction.
   Design: A PUBMED computer search to identify relevant literature.
   Setting: Multiple sites.
   Patient(s): Patients undergoing assisted reproduction.
   Intervention(S): Testing for ovarian reserve.
   Main Outcome Measure(S): Assisted reproductive technology (ART) and
   pregnancy outcomes.
   Result(S): The prevalence of ovarian insufficiency varies significantly
   for women aged 30-45 years. Gerieralization or averaging of threshold
   values across different aged women leads to very poor sensitivity,
   specificity, and positive predictive value for all tests of ovarian
   reserve. Because of the changing prevalence of ovarian insufficiency,
   there is no single, suitable threshold value for any screening test of
   ovarian reserve. Our analysis supports dividing impaired ovarian reserve
   into two groups: age-dependent ovarian aging (physiologic) and premature
   (non-physiologic) reductions in the oocyte pool. Interpretation of any
   screening test used requires that age is considered as a variable. To
   guide clinical interpretation of test results, We suggest using a
   nomogram of FSH values versus expected delivery rate-per-cycle-start
   with ART for a given age.
   Conclusion(s): Proper interpretation of screening tests for ovarian
   insufficiency in couples considering ART is important as the presence of
   impaired ovarian reserve is associated with a low likelihood of
   pregnancy. The condition of premature (nonphysiologic) ovarian
   insufficiency warrants additional research. (Fertil Steril(R)
   2008;90:2196202. (C)2008 by American Society for Reproductive Medicine.)}},
DOI = {{10.1016/j.fertnstert.2007.10.080}},
ISSN = {{0015-0282}},
EISSN = {{1556-5653}},
Unique-ID = {{ISI:000261566800023}},
}

@article{ ISI:000261812400006,
Author = {Walker, Tony R. and Grant, Jon and Cranford, Peter and Lintern, D. Gwyn
   and Hill, Paul and Jarvis, Peter and Barrell, Jeffrey and Nozais,
   Christian},
Title = {{Suspended sediment and erosion dynamics in Kugmallit Bay and Beaufort
   Sea during ice-free conditions}},
Journal = {{JOURNAL OF MARINE SYSTEMS}},
Year = {{2008}},
Volume = {{74}},
Number = {{3-4}},
Pages = {{794-809}},
Month = {{DEC 1}},
Abstract = {{The Mackenzie River is the largest river on the North American side of
   the Arctic and its huge freshwater and sediment load impacts the
   Canadian Beaufort Shelf. Huge quantities of sediment and associated
   organic carbon are transported in the Mackenzie plume into the interior
   of the Arctic Ocean mainly during the freshet (May to September).
   Changing climate scenarios portend increased coastal erosion and
   resuspension that lead to altered river-shelf-slope particle budgets. We
   measured sedimentation rates, suspended particulate matter (SPM),
   particle size and settling rates during ice-free conditions in Kugmallit
   Bay (3-5 m depth). Additionally, measurements of erosion rate, critical
   shear stress, particle size distribution and resuspension threshold of
   bottom sediments were examined at four regionally contrasting sites
   (33-523 m depth) on the Canadian Beaufort Shelf using a new method for
   assessing sediment erosion. Wind induced resuspension was evidenced by a
   strong relationship between SPM and wind speed in Kugmallit Bay.
   Deployment of sediment traps showed decreasing sedimentation rates at
   sites along an inshore-offshore transect ranging from 5400 to 3700 g
   m(-2) day(-1). Particle settling rates and size distributions measured
   using a Perspex settling chamber showed strong relationships between
   equivalent spherical diameter (ESD) and particle settling rates
   (r(2)=0.91). Mean settling rates were 0.72 cm s(-1) with corresponding
   ESD values of 0.9 mm. Undisturbed sediment cores were exposed to shear
   stress in an attempt to compare differences in sediment stability across
   the shelf during September to October 2003. Shear was generated by
   vertically oscillating a perforated disc at controlled frequencies
   corresponding to calibrated shear velocity using a piston grid erosion
   device. Critical (Type I) erosion thresholds (u.) varied between 1.1 and
   1.3 cm s(-1) with no obvious differences in location. Sediments at the
   deepest site Amundsen Gulf displayed the highest erosion rates (22-54 g
   m(-2) min(-1)) with resuspended particle sizes ranging from 100 to 930
   mu m for all sites. There was no indication of biotic influence on
   sediment stability, although our cores did not display a fluff layer of
   unconsolidated sediment. Concurrent studies in the delta and shelf
   region suggest the importance of a nepheloid layer which transports
   suspended particles to the slope. Continuous cycles of resuspension,
   deposition, and horizontaladvection may intensify with reduction of sea
   ice in this region. Our measurements coupled with studies of circulation
   and cross-shelf exchange allow parameterization and modeling of particle
   dynamics and carbon fluxes under various climate change scenarios. (C)
   2008 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.jmarsys.2008.01.006}},
ISSN = {{0924-7963}},
ORCID-Numbers = {{Walker, Tony/0000-0001-9008-0697
   Jarvis, Peter/0000-0003-2740-6740
   Barrell, Jeffrey/0000-0002-1740-2829}},
Unique-ID = {{ISI:000261812400006}},
}

@article{ ISI:000261540600010,
Author = {Inaba, Hisashi and Nishiura, Hiroshi},
Title = {{The state-reproduction number for a multistate class age structured
   epidemic system and its application to the asymptomatic transmission
   model}},
Journal = {{MATHEMATICAL BIOSCIENCES}},
Year = {{2008}},
Volume = {{216}},
Number = {{1}},
Pages = {{77-89}},
Month = {{NOV}},
Abstract = {{In this paper, we develop the theory of a state-reproduction number for
   a multistate class age structured epidemic system and apply it to
   examine the asymptomatic transmission model. We formulate a renewal
   integral equation system to describe the invasion of infectious diseases
   into a multistate class age structured host population. We define the
   state-reproduction number for a class age structured system, which is
   the net reproduction number of a specific host type and which plays an
   analogous role to the type-reproduction number {[}M.G. Roberts, J.A.P.
   Heesterbeek, A new method for estimating the effort required to control
   an infectious disease, Proc. R. Soc. Lond. B 270 (2003) 1359; J.A.P.
   Heesterbeek, M.G. Roberts, The type-reproduction number T in models for
   infectious disease control, Math. Biosci. 206 (2007) 3] in discussing
   the critical level of public health intervention. The renewal equation
   formulation permits computations not only of the state-reproduction
   number, but also of the generation time and the intrinsic growth rate of
   infectious diseases.
   Subsequently, the basic theory is applied to capture the dynamics of a
   directly transmitted disease within two types of infected populations,
   i.e., asymptomatic and symptomatic individuals, in which the symptomatic
   class is observable and hence a target host of the majority of
   interventions. The state-reproduction number of the symptomatic host is
   derived and expressed as a measurable quantity, leading to discussion on
   the critical level of case isolation. The serial interval and other
   epidemiologic indices are computed, clarifying the parameters on which
   these indices depend. As a practical example, we illustrate the
   eradication threshold for case isolation of smallpox. The generation
   time and serial interval are comparatively examined for pandernic
   influenza. (c) 2008 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.mbs.2008.08.005}},
ISSN = {{0025-5564}},
EISSN = {{1879-3134}},
ResearcherID-Numbers = {{Nishiura, Hiroshi/D-1426-2011
   }},
ORCID-Numbers = {{Nishiura, Hiroshi/0000-0003-0941-8537}},
Unique-ID = {{ISI:000261540600010}},
}

@article{ ISI:000259251000017,
Author = {Banaie, Masood and Sarbaz, Yashar and Gharibzadeh, Shahriar and
   Towhidkhah, Farzad},
Title = {{Huntington's disease: Modeling the gait disorder and proposing novel
   treatments}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2008}},
Volume = {{254}},
Number = {{2}},
Pages = {{361-367}},
Month = {{SEP 21}},
Abstract = {{Huntington's disease is a movement disorder originated from
   malfunctioning of Basal Ganglia (BG). There are some models for this
   disease, most of them being conceptual. So, it seems that considering
   all physiological information and structural specifications to develop a
   holistic model is needed. We introduce a computational model based on
   experimental and physiological findings. Parts of the brain known to be
   involved in Huntington's disease are all considered in our model and
   most features of the movement disorders have been appeared in the model.
   This mathematical model has considered the involved parts of the brain
   in a fairly accurate way, explaining the behavior and mechanism of the
   disease according to the physiological information. Our model has
   several advantages. It is able to simulate the normal and Huntington's
   disease stride time intervals. It shows how the present treatment, i.e.
   diazepam, is able to ameliorate the gait disorder. In this research we
   assessed the effects of changing some neurotransmitter levels in order
   to propose new treatments. Although we showed that gamma amino butyric
   acid (GABA) blockers reduce Huntington's disease movement disorder, but
   we discussed that it is unfair to use this route for treatment. We
   evaluated our model response to increment of GABA, alone and observed
   that the gait disorder was strengthened. Our novel idea in this regard
   is resuscitation of BG loop in order to maintain its major physiological
   functions, and at the same time raising the threshold in order to weaken
   the internal disturbances. Our last idea about BG treatment is to
   decrease glutamate. Our model was able to show the effectiveness of this
   treatment on Huntington's disease disturbances. We propose that
   experimental studies should be designed in which these two novel methods
   of treatment will be evaluated. This validation would implement a
   milestone in treatment of such a debilitating disease at Huntington. (C)
   2008 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2008.05.023}},
ISSN = {{0022-5193}},
ResearcherID-Numbers = {{Banaie, Masood/C-9880-2010
   }},
ORCID-Numbers = {{Towhidkhah, Farzad/0000-0003-3135-9609}},
Unique-ID = {{ISI:000259251000017}},
}

@article{ ISI:000258379200009,
Author = {Peros, Matthew C. and Gajewski, K. and Viau, Andre E.},
Title = {{Continental-scale tree population response to rapid climate change,
   competition and disturbance}},
Journal = {{GLOBAL ECOLOGY AND BIOGEOGRAPHY}},
Year = {{2008}},
Volume = {{17}},
Number = {{5}},
Pages = {{658-669}},
Month = {{SEP}},
Abstract = {{Aim Using a new approach to analyse fossil pollen data, we investigate
   temporal and spatial patterns in Populus (poplar, cottonwood, aspen)
   from the Late Glacial to the present at regional to continental scales.
   Location North America.
   Methods We extracted data on the timing and magnitude of the maximum
   value of Populus pollen from each pollen diagram in the North American
   Pollen Database (NAPD). The information was plotted in histograms of
   150-year bins to identify times when Populus was abundant on the
   landscape. We also mapped the maximum values to identify spatial
   patterns and their causes.
   Results Our analyses show that there have been several periods since the
   Late Glacial when Populus was abundant on the landscape: (1) from 12.35
   to 12.65 kyr BP, in eastern North America, largely in response to the
   opening of the forest following the onset of the Younger Dryas; (2) from
   10.85 to 11.75 kyr BP, following the termination of the Younger Dryas;
   and (3) during the last 150 years, as land was cleared for agricultural
   use, especially in the midwestern United States.
   Main conclusion Since the Late Glacial, changes in the abundance of
   Populus were caused more by the effects of abrupt climate change on its
   major competitors, rather than the direct effects of climate on Populus
   itself.}},
DOI = {{10.1111/j.1466-8238.2008.00406.x}},
ISSN = {{1466-822X}},
ResearcherID-Numbers = {{Gajewski, Konrad/L-5128-2017
   }},
ORCID-Numbers = {{Viau, Andre/0000-0001-5386-5612}},
Unique-ID = {{ISI:000258379200009}},
}

@article{ ISI:000260310500010,
Author = {Yasuda, Takako and Yoshimoto, Masami and Maeda, Keiko and Matsumoto,
   Atsuko and Maruyama, Kouichi and Ishikawa, Yuji},
Title = {{Rapid and Simple Method for Quantitative Evaluation of Neurocytotoxic
   Effects of Radiation on Developing Medaka Brain}},
Journal = {{JOURNAL OF RADIATION RESEARCH}},
Year = {{2008}},
Volume = {{49}},
Number = {{5}},
Pages = {{533-540}},
Month = {{SEP}},
Abstract = {{We describe a novel method for rapid and quantitative evaluation of the
   degree of radiation-induced apoptosis in the developing brain of medaka
   (Oryzias latipes). Embryos at stage 28 were irradiated with 1, 2, 3.5,
   and 5 Gy x-ray. Living embryos were stained with a vital dye, acridine
   orange (AO), for 1-2 h, and whole-mount brains were examined under an
   epifluorescence microscope. From 7 to 10 h after irradiation with 5 Gy
   x-ray, we found two morphologically different types of AO-stained
   structures, namely, small single nuclei and rosette-shaped nuclear
   clusters. Electron microscopy revealed that these two distinct types of
   structures were single apoptotic cells with condensed nuclei and
   aggregates of apoptotic cells, respectively. From 10 to 30 h after
   irradiation, a similar AO-staining pattern was observed. The numbers of
   AO-stained rosette-shaped nuclear clusters and AO-stained single nuclei
   increased in a dose-dependent manner in the optic tectum. We used the
   number of AO-stained rosette-shaped nuclear clusters/optic tectum as an
   index of the degree of radiation-induced brain cell death at 20-24 It
   after irradiation. The results showed that the number of rosette-shaped
   nuclear clusters/optic tectum in irradiated embryos exposed to 2 Gy or
   higher doses was highly significant compared to the number in
   nonirradiated control embryos, whereas no difference was detected at I
   Gy. Thus, the threshold dose for brain cell death in medaka embryos was
   taken as being between 1-2 Gy, which may not be so extraordinarily large
   compared to those for rodents and humans. The results show that medaka
   embryos are useful for quantitative evaluation of developmental
   neurocytotoxic effects of radiation.}},
DOI = {{10.1269/jrr.08030}},
ISSN = {{0449-3060}},
Unique-ID = {{ISI:000260310500010}},
}

@article{ ISI:000258556100019,
Author = {Struck, Torsten H. and Nesnidal, Maximilian P. and Purschke, Guenter and
   Halanych, Kenneth M.},
Title = {{Detecting possibly saturated positions in 18S and 28S sequences and
   their influence on phylogenetic reconstruction of Annelida
   (Lophotrochozoa)}},
Journal = {{MOLECULAR PHYLOGENETICS AND EVOLUTION}},
Year = {{2008}},
Volume = {{48}},
Number = {{2}},
Pages = {{628-645}},
Month = {{AUG}},
Abstract = {{Phylogenetic reconstructions may be hampered by multiple substitutions
   in nucleotide positions obliterating signal, a phenomenon called
   saturation. Traditionally, plotting ti/tv ratios against genetic
   distances has been used to reveal saturation by assessing when ti/tv
   stabilizes at 1. However, interpretation of results and assessment of
   comparability between different data sets or partitions are rather
   subjective. Herein, we present the new C factor, which quantifies
   convergence of ti/tv ratios, thus allowing comparability. Furthermore,
   we introduce a comparative value for homoplasy, the O/E ratio, based on
   alterations of tree length. Simulation studies and an empirical example,
   based on annelid rRNA-gene sequences, show that the C factor correlates
   with noise, tree length and genetic distance and therefore is a proxy
   for saturation. The O/E ratio correlates with the C factor, which does
   not provide an intrinsic threshold of exclusion, and thus both together
   can objectively guide decisions to exclude saturated nucleotide
   positions. However, analyses also showed that, for reconstructing
   annelid phylogeny using Maximum Likelihood, an increase in numbers of
   positions improves tree reconstruction more than does the exclusion of
   saturated positions. (c) 2008 Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.ympev.2008.05.015}},
ISSN = {{1055-7903}},
EISSN = {{1095-9513}},
ResearcherID-Numbers = {{Halanych, Ken/A-9480-2009}},
ORCID-Numbers = {{Halanych, Ken/0000-0002-8658-9674}},
Unique-ID = {{ISI:000258556100019}},
}

@article{ ISI:000259015600002,
Author = {Guescini, Michele and Sisti, Davide and Rocchi, Marco B. L. and Stocchi,
   Laura and Stocchi, Vilberto},
Title = {{A new real-time PCR method to overcome significant quantitative
   inaccuracy due to slight amplification inhibition}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2008}},
Volume = {{9}},
Month = {{JUL 30}},
Abstract = {{Background: Real-time PCR analysis is a sensitive DNA quantification
   technique that has recently gained considerable attention in
   biotechnology, microbiology and molecular diagnostics. Although, the
   cycle-threshold (Ct) method is the present ``gold standard{''}, it is
   far from being a standard assay. Uniform reaction efficiency among
   samples is the most important assumption of this method. Nevertheless,
   some authors have reported that it may not be correct and a slight PCR
   efficiency decrease of about 4\% could result in an error of up to 400\%
   using the Ct method. This reaction efficiency decrease may be caused by
   inhibiting agents used during nucleic acid extraction or copurified from
   the biological sample.
   We propose a new method (Cy(0)) that does not require the assumption of
   equal reaction efficiency between unknowns and standard curve.
   Results: The Cy(0) method is based on the fit of Richards' equation to
   real-time PCR data by nonlinear regression in order to obtain the best
   fit estimators of reaction parameters. Subsequently, these parameters
   were used to calculate the Cy(0) value that minimizes the dependence of
   its value on PCR kinetic.
   The Ct, second derivative (Cp), sigmoidal curve fitting method (SCF) and
   Cy(0) methods were compared using two criteria: precision and accuracy.
   Our results demonstrated that, in optimal amplification conditions,
   these four methods are equally precise and accurate. However, when PCR
   efficiency was slightly decreased, diluting amplification mix quantity
   or adding a biological inhibitor such as IgG, the SCF, Ct and Cp methods
   were markedly impaired while the Cy(0) method gave significantly more
   accurate and precise results.
   Conclusion: Our results demonstrate that Cy(0) represents a significant
   improvement over the standard methods for obtaining a reliable and
   precise nucleic acid quantification even in sub-optimal amplification
   conditions overcoming the underestimation caused by the presence of some
   PCR inhibitors.}},
DOI = {{10.1186/1471-2105-9-326}},
Article-Number = {{326}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Rocchi, marco/0000-0002-0056-5795
   Guescini, Michele/0000-0001-9372-7038
   davide, sisti/0000-0002-7925-7495}},
Unique-ID = {{ISI:000259015600002}},
}

@article{ ISI:000257436300001,
Author = {Lehning, M. and Loewe, H. and Ryser, M. and Raderschall, N.},
Title = {{Inhomogeneous precipitation distribution and snow transport in steep
   terrain}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2008}},
Volume = {{44}},
Number = {{7}},
Month = {{JUL 3}},
Abstract = {{The inhomogeneous snow distribution found in alpine terrain is the
   result of wind and precipitation interacting with the (snow) surface
   over topography. We introduce and explain preferential deposition of
   precipitation as the deposition process without erosion of previously
   deposited snow and thus in absence of saltation. A numerical model is
   developed, describing the relevant processes of saltation, suspension,
   and preferential deposition. The model uses high-resolution wind fields
   calculated with a meteorological model, ARPS. The model is used to
   simulate a 120 h snow storm period over a steep alpine ridge, for which
   snow distribution measurements are available. The comparison to
   measurements shows that the model captures the larger-scale snow
   distribution patterns and predicts the total additional lee slope
   loading well. However, the spatial resolution of 25 m is still
   insufficient to capture the smaller-scale deposition features observed.
   The model suggests that the snow distribution on the ridge scale is
   primarily caused by preferential deposition and that this result is not
   sensitive to model parameters such as turbulent diffusivity, drift
   threshold, or concentration in the saltation layer.}},
DOI = {{10.1029/2007WR006545}},
Article-Number = {{W07404}},
ISSN = {{0043-1397}},
ResearcherID-Numbers = {{Lowe, Henning/B-6279-2009}},
ORCID-Numbers = {{Lowe, Henning/0000-0001-7515-6809}},
Unique-ID = {{ISI:000257436300001}},
}

@article{ ISI:000257696400017,
Author = {Joliot, Pierre and Joliot, Anne},
Title = {{Quantification of the electrochemical proton gradient and activation of
   ATP synthase in leaves}},
Journal = {{BIOCHIMICA ET BIOPHYSICA ACTA-BIOENERGETICS}},
Year = {{2008}},
Volume = {{1777}},
Number = {{7-8}},
Pages = {{676-683}},
Month = {{JUL-AUG}},
Note = {{15th European Bioenergetic Conference, Trinity Coll, Dublin, IRELAND,
   JUL 19-24, 2008}},
Abstract = {{We have developed a new method to quantify the transmembrane
   electrochemical proton gradient present in chloroplasts of dark-adapted
   leaves. When a leaf is illuminated by a short pulse of intense light, we
   observed that the light-induced membrane potential changes, measured by
   the difference of absorption (520 nm-546 nm), reach a maximum value
   (similar to 190 mV) determined by ion leaks that occur above a threshold
   level of the electrochemical proton gradient. After the light-pulse, the
   decay of the membrane potential follows a multiphasic kinetics. A marked
   slowdown of the rate of membrane potential decay occurs similar to 100
   ms after the light-pulse, which has been previously interpreted as
   reflecting the switch from an activated to an inactivated state of the
   ATP synthase (junge, W., Rumberg, B. and Schroder, H., Eur. J. Biochem.
   14 (1970) 575-581). This transition occurs at similar to 110 mV, thereby
   providing a second reference level. On this basis, we have estimated the
   Delta mu(H+) level that pre-exists in the dark. Depending upon the
   physiological state of the leaf, this level varies from 40 to 70 mV. In
   the dark, the Delta mu(H+) collapses upon addition of inhibitors of the
   respiratory chain, thus showing that it results from the hydrolysis of
   ATP of mitochondrial origin. Illumination of the leaf for a period
   longer than several seconds induces a long-lived Delta mu(H+) increase
   (up to similar to 150 mV) that reflects the light-induced increase in
   ATP concentration. Following the illumination, Delta mu(H+) relaxes to
   its dark-adapted value according a multiphasic kinetics that is
   completed in more than 1 h. In mature leaf, the deactivation of the
   Benson-Calvin cycle follows similar kinetics as Delta mu(H+) decay,
   showing that its state of activation is mainly controlled by ATP
   concentration. (c) 2008 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.bbabio.2008.04.010}},
ISSN = {{0005-2728}},
Unique-ID = {{ISI:000257696400017}},
}

@article{ ISI:000256635800027,
Author = {Kuss, Patrick and Rees, Mark and Aegisdottir, Hafdis Hanna and Ellner,
   Stephen P. and Stocklin, Jurg},
Title = {{Evolutionary demography of long-lived monocarpic perennials: a
   time-lagged integral projection model}},
Journal = {{JOURNAL OF ECOLOGY}},
Year = {{2008}},
Volume = {{96}},
Number = {{4}},
Pages = {{821-832}},
Month = {{JUL}},
Abstract = {{1. The evolution of flowering strategies (when and at what size to
   flower) in monocarpic perennials is determined by balancing current
   reproduction with expected future reproduction, and these are largely
   determined by size-specific patterns of growth and survival. However,
   because of the difficulty in following long-lived individuals throughout
   their lives, this theory has largely been tested using short-lived
   species (< 5 years).
   2. Here, we tested this theory using the long-lived monocarpic perennial
   Campanula thyrsoides which can live up to 16 years. We used a novel
   approach that combined permanent plot and herb chronology data from a
   3-year field study to parameterize and validate integral projection
   models (IPMs).
   3. Similar to other monocarpic species, the rosette leaves of C.
   thyrsoides wither over winter and so size cannot be measured in the year
   of flowering. We therefore extended the existing IPM framework to
   incorporate an additional time delay that arises because flowering
   demography must be predicted from rosette size in the year before
   flowering.
   4. We found that all main demographic functions (growth, survival
   probability, flowering probability and fecundity) were strongly
   size-dependent and there was a pronounced threshold size of flowering.
   There was good agreement between the predicted distribution of flowering
   ages obtained from the IPMs and that estimated in the field. Mostly,
   there was good agreement between the IPM predictions and the direct
   quantitative field measurements regarding the demographic parameters
   lambda, R-0 and T. We therefore conclude that the model captures the
   main demographic features of the field populations.
   5. Elasticity analysis indicated that changes in the survival and growth
   function had the largest effect (c. 80\%) on lambda and this was
   considerably larger than in short-lived monocarps. We found only weak
   selection pressure operating on the observed flowering strategy which
   was close to the predicted evolutionary stable strategy.
   6. Synthesis. The extended IPM accurately described the demography of a
   long-lived monocarpic perennial using data collected over a relatively
   short period. We could show that the evolution of flowering strategies
   in short- and long-lived monocarps seem to follow the same general rules
   but with a longevity-related emphasis on survival over fecundity.}},
DOI = {{10.1111/j.1365-2745.2008.01374.x}},
ISSN = {{0022-0477}},
EISSN = {{1365-2745}},
ResearcherID-Numbers = {{Stocklin, Jurg/F-5029-2012}},
Unique-ID = {{ISI:000256635800027}},
}

@article{ ISI:000258188600001,
Author = {Gonzalez, Juan R. and Carrasco, Josep L. and Armengol, Lluis and
   Villatoro, Sergi and Jover, Lluis and Yasui, Yutaka and Estivill, Xavier},
Title = {{Probe-specific mixed-model approach to detect copy number differences
   using multiplex ligation-dependent probe amplification (MLPA)}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2008}},
Volume = {{9}},
Month = {{JUN 4}},
Abstract = {{Background: MLPA method is a potentially useful semi-quantitative method
   to detect copy number alterations in targeted regions. In this paper, we
   propose a method for the normalization procedure based on a non-linear
   mixed-model, as well as a new approach for determining the statistical
   significance of altered probes based on linear mixed-model. This method
   establishes a threshold by using different tolerance intervals that
   accommodates the specific random error variability observed in each test
   sample.
   Results: Through simulation studies we have shown that our proposed
   method outperforms two existing methods that are based on simple
   threshold rules or iterative regression. We have illustrated the method
   using a controlled MLPA assay in which targeted regions are variable in
   copy number in individuals suffering from different disorders such as
   Prader-Willi, DiGeorge or Autism showing the best performace.
   Conclusion: Using the proposed mixed-model, we are able to determine
   thresholds to decide whether a region is altered. These threholds are
   specific for each individual, incorporating experimental variability,
   resulting in improved sensitivity and specificity as the examples with
   real data have revealed.}},
DOI = {{10.1186/1471-2105-9-261}},
Article-Number = {{261}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Carrasco, Josep/D-3660-2011
   Estivill, Xavier/E-2957-2012
   Jover, Lluis/D-2192-2011
   Estivill, Xavier/A-3125-2013
   Yasui, Yutaka/E-2564-2015
   Gonzalez, Juan R/B-7860-2018}},
ORCID-Numbers = {{Carrasco, Josep/0000-0003-1184-0753
   Jover, Lluis/0000-0003-0631-1398
   Estivill, Xavier/0000-0002-0723-2256
   Yasui, Yutaka/0000-0002-7717-8638
   Gonzalez, Juan R/0000-0003-3267-2146}},
Unique-ID = {{ISI:000258188600001}},
}

@article{ ISI:000255269200006,
Author = {Kim, Sunshin and Kang, Jaewoo and Chung, Yong Je and Li, Jinyan and Ryu,
   Keun Ho},
Title = {{Clustering orthologous proteins across phylogenetically distant species}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS}},
Year = {{2008}},
Volume = {{71}},
Number = {{3}},
Pages = {{1113-1122}},
Month = {{MAY 15}},
Abstract = {{The quality of orthologous protein clusters (OPCs) is largely dependent
   on the results of the reciprocal BLAST (basic local alignment search
   tool) hits among genomes. The BLAST algorithm is very efficient and
   fast, but it is very difficult to get optimal solution among
   phylogenetically distant species because the genomes with large
   evolutionary distance typically have low similarity in their protein
   sequences. To reduce the false positives in the OPCs, thresholding is
   often employed on the BLAST scores. However, the thresholding also
   eliminates large numbers of true positives as the ortho-logs from
   distant species likely have low BLAST scores. To rectify this problem,
   we introduce a new hybrid method combining the Recursive and the Markov
   CLuster (MCL) algorithms without using the BLAST thresholding. In the
   first step, we use InParanoid to produce n(n-1)/2 ortholog tables from n
   genomes. After combining all the tables into one, our clustering
   algorithm clusters ortholog pairs recursively in the table. Then, our
   method employs MCL algorithm to compute the clusters and refines the
   clusters by adjusting the inflation factor. We tested our method using
   six different genomes and evaluated the results by comparing against
   Kegg Orthology (KO) OPCs, which are generated from manually curated
   pathways. To quantify the accuracy of the results, we introduced a new
   intuitive similarity measure based on our Least-move algorithm that
   computes the consistency between two OPCs. We compared the resulting
   OPCs with the KO OPCs using this measure. We also evaluated the
   performance of our method using InParanoid as the baseline approach. The
   experimental results show that, at the inflation factor 1.3, we produced
   54\% more orthologs than InParanoid sacrificing a little less accuracy
   (1.7\% less) than InParanoid, and at the factor 1.4, produced not only
   15\% more orthologs than InParanoid but also a higher accuracy (1.4\%
   more) than InParanoid.}},
DOI = {{10.1002/prot.21792}},
ISSN = {{0887-3585}},
ORCID-Numbers = {{Li, Jinyan/0000-0003-1833-7413}},
Unique-ID = {{ISI:000255269200006}},
}

@article{ ISI:000255901600013,
Author = {Na, Heung Sik and Choi, Soonwook and Kim, Junesun and Park, Joonoh and
   Shin, Hee-Sup},
Title = {{Attenuated neuropathic pain in Ca(V)3.1 null mice}},
Journal = {{MOLECULES AND CELLS}},
Year = {{2008}},
Volume = {{25}},
Number = {{2}},
Pages = {{242-246}},
Month = {{APR 30}},
Abstract = {{To assess the role of alpha(1G) T-type Ca2+ channels in neuropathic pain
   after L5 spinal nerve ligation, we examined behavioral pain
   susceptibility in mice lacking Ca(V)3.1 (alpha(-/-)(1G)), the gene
   encoding the pore-forming units of these channels. Reduced spontaneous
   pain responses and an increased threshold for paw withdrawal in response
   to mechanical stimulation were observed in these mice. The a(1G)(-/-)
   mice also showed attenuated thermal hyperalgesia in response to both
   low-(IR30) and high-intensity (IR60) infrared stimulation. Our results
   reveal the importance of alpha(1G) T-type Ca2+ channels in the
   development of neuropathic pain, and suggest that selective modulation
   of alpha(1G) subtype channels may provide a novel approach to the
   treatment of allodynia and hyperalgesia.}},
ISSN = {{1016-8478}},
Unique-ID = {{ISI:000255901600013}},
}

@article{ ISI:000255285000001,
Author = {Gabow, Aaron P. and Leach, Sonia M. and Baumgartner, William A. and
   Hunter, Lawrence E. and Goldberg, Debra S.},
Title = {{Improving protein function prediction methods with integrated literature
   data}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2008}},
Volume = {{9}},
Month = {{APR 15}},
Abstract = {{Background: Determining the function of uncharacterized proteins is a
   major challenge in the post-genomic era due to the problem's complexity
   and scale. Identifying a protein's function contributes to an
   understanding of its role in the involved pathways, its suitability as a
   drug target, and its potential for protein modifications. Several
   graph-theoretic approaches predict unidentified functions of proteins by
   using the functional annotations of better-characterized proteins in
   protein-protein interaction networks. We systematically consider the use
   of literature co-occurrence data, introduce a new method for quantifying
   the reliability of co-occurrence and test how performance differs across
   species. We also quantify changes in performance as the prediction
   algorithms annotate with increased specificity.
   Results: We find that including information on the co-occurrence of
   proteins within an abstract greatly boosts performance in the Functional
   Flow graph-theoretic function prediction algorithm in yeast, fly and
   worm. This increase in performance is not simply due to the presence of
   additional edges since supplementing protein-protein interactions with
   co-occurrence data outperforms supplementing with a comparably-sized
   genetic interaction dataset. Through the combination of protein-protein
   interactions and co-occurrence data, the neighborhood around unknown
   proteins is quickly connected to well-characterized nodes which global
   prediction algorithms can exploit. Our method for quantifying
   co-occurrence reliability shows superior performance to the other
   methods, particularly at threshold values around 10\% which yield the
   best trade off between coverage and accuracy. In contrast, the
   traditional way of asserting co-occurrence when at least one abstract
   mentions both proteins proves to be the worst method for generating
   co-occurrence data, introducing too many false positives. Annotating the
   functions with greater specificity is harder, but co-occurrence data
   still proves beneficial.
   Conclusion: Co-occurrence data is a valuable supplemental source for
   graph-theoretic function prediction algorithms. A rapidly growing
   literature corpus ensures that co-occurrence data is a readily-available
   resource for nearly every studied organism, particularly those with
   small protein interaction databases. Though arguably biased toward known
   genes, co-occurrence data provides critical additional links to
   well-studied regions in the interaction network that graph-theoretic
   function prediction algorithms can exploit.}},
DOI = {{10.1186/1471-2105-9-198}},
Article-Number = {{198}},
ISSN = {{1471-2105}},
ORCID-Numbers = {{Hunter, Lawrence/0000-0003-1455-3370}},
Unique-ID = {{ISI:000255285000001}},
}

@article{ ISI:000255152200034,
Author = {Harmanci, Arif Ozgun and Sharma, Gaurav and Mathews, David H.},
Title = {{PARTS: Probabilistic Alignment for RNA joinT Secondary structure
   prediction}},
Journal = {{NUCLEIC ACIDS RESEARCH}},
Year = {{2008}},
Volume = {{36}},
Number = {{7}},
Pages = {{2406-2417}},
Month = {{APR}},
Abstract = {{A novel method is presented for joint prediction of alignment and common
   secondary structures of two RNA sequences. The joint consideration of
   common secondary structures and alignment is accomplished by structural
   alignment over a search space defined by the newly introduced motif
   called matched helical regions. The matched helical region formulation
   generalizes previously employed constraints for structural alignment and
   thereby better accommodates the structural variability within RNA
   families. A probabilistic model based on pseudo free energies obtained
   from precomputed base pairing and alignment probabilities is utilized
   for scoring structural alignments. Maximum a posteriori (MAP) common
   secondary structures, sequence alignment and joint posterior
   probabilities of base pairing are obtained from the model via a dynamic
   programming algorithm called PARTS. The advantage of the more general
   structural alignment of PARTS is seen in secondary structure predictions
   for the RNase P family. For this family, the PARTS MAP predictions of
   secondary structures and alignment perform significantly better than
   prior methods that utilize a more restrictive structural alignment
   model. For the tRNA and 5S rRNA families, the richer structural
   alignment model of PARTS does not offer a benefit and the method
   therefore performs comparably with existing alternatives. For all RNA
   families studied, the posterior probability estimates obtained from
   PARTS offer an improvement over posterior probability estimates from a
   single sequence prediction. When considering the base pairings predicted
   over a threshold value of confidence, the combination of sensitivity and
   positive predictive value is superior for PARTS than for the single
   sequence prediction. PARTS source code is available for download under
   the GNU public license at http://rna.urmc.rochester.edu.}},
DOI = {{10.1093/nar/gkn043}},
ISSN = {{0305-1048}},
ResearcherID-Numbers = {{Sharma, Gaurav/A-1154-2007}},
ORCID-Numbers = {{Sharma, Gaurav/0000-0001-9735-9519}},
Unique-ID = {{ISI:000255152200034}},
}

@article{ ISI:000254010400016,
Author = {Graziano, Frank M. and Kettoola, Samira Y. and Munshower, Judy M. and
   Stapleton, Jack T. and Towfic, George J.},
Title = {{Effect of spatial distribution of T-Cells and HIV load on HIV
   progression}},
Journal = {{BIOINFORMATICS}},
Year = {{2008}},
Volume = {{24}},
Number = {{6}},
Pages = {{855-860}},
Month = {{MAR}},
Abstract = {{Motivation: We present a spatial-temporal (ST) human immunodeficiency
   virus (HIV) simulation model to investigate the spatial distribution of
   viral load and T-cells during HIV progression. The proposed model uses
   the Finite Element (FE) method to divide a considered infected region
   into interconnected subregions each containing viral population and
   T-cells. HIV T-cells and viral load are traced and counted within and
   between subregions to estimate their effect upon neighboring regions.
   The objective is to estimate overall ST changes of HIV progression and
   to study the ST therapeutic effect upon HIV dynamics in spatial and
   temporal domains. We introduce sub-regional (spatial) parameters of
   T-cells and viral load production and elimination to estimate the
   spatial propagation and interaction of HIV dynamics under the influence
   of a 3TC D4T Reverse Transcriptase Inhibitors (RTI) drug regimen.
   Results: In terms of percentage change standard deviation, we show that
   the average rate per 10 weeks (throughout a 10-year clinical trial) of
   the ST CD4+ change is 5.35\% 1.3 different than that of the CD4+ rate of
   change in laboratory datasets, and the average rate of change of the ST
   CD8+ is 4.98\% 1.93 different than that of the CD8+ rate of change. The
   half-life of the ST CD4+ count is 1.68\% 3.381 different than the actual
   half-life of the CD4+ count obtained from laboratory datasets. The
   distribution of the viral load and T-cells in a considered region tends
   to cluster during the HIV progression once a threshold of 86-89\% viral
   accumulation is reached.}},
DOI = {{10.1093/bioinformatics/btn008}},
ISSN = {{1367-4803}},
ORCID-Numbers = {{Stapleton, Jack/0000-0002-2302-9055}},
Unique-ID = {{ISI:000254010400016}},
}

@article{ ISI:000253632200006,
Author = {Ninomiya, Yoshiyuki and Yoshimoto, Atsushi},
Title = {{Statistical method for detecting structural change in the growth process}},
Journal = {{BIOMETRICS}},
Year = {{2008}},
Volume = {{64}},
Number = {{1}},
Pages = {{46-53}},
Month = {{MAR}},
Abstract = {{Due to competition among individual trees and other exogenous factors
   that change the growth environment, each tree grows following its own
   growth trend with some structural changes in growth over time. In the
   present article, a new method is proposed to detect a structural change
   in the growth process. We formulate the method as a simple statistical
   test for signal detection without constructing any specific model for
   the structural change. To evaluate the p-value of the test, the tube
   method is developed because the regular distribution theory is
   insufficient. Using two sets of tree diameter growth data sampled from
   planted forest stands of Cryptomeria japonica in Japan, we conduct an
   analysis of identifying the effect of thinning on the growth process as
   a structural change. Our results demonstrate that the proposed method is
   useful to identify the structural change caused by thinning. We also
   provide the properties of the method in terms of the size and power of
   the test.}},
DOI = {{10.1111/j.1541-0420.2007.00844.x}},
ISSN = {{0006-341X}},
EISSN = {{1541-0420}},
ResearcherID-Numbers = {{U-ID, Kyushu/C-5291-2016
   }},
ORCID-Numbers = {{Yoshimoto, Atsushi/0000-0002-4641-6605}},
Unique-ID = {{ISI:000253632200006}},
}

@article{ ISI:000253034800004,
Author = {Pawlowski, Christopher W. and Cabezas, Heriberto},
Title = {{Identification of regime shifts in time series using neighborhood
   statistics}},
Journal = {{ECOLOGICAL COMPLEXITY}},
Year = {{2008}},
Volume = {{5}},
Number = {{1}},
Pages = {{30-36}},
Month = {{MAR}},
Abstract = {{Sudden and significant changes in biotic and abiotic variables have been
   observed across a variety of systems. The identification of such regime
   shifts in time series includes both model-fitting and statistical
   approaches. We introduce two methods that use state- or
   measurement-space neighborhood statistics to pick out regime shifts.
   Analysis of simulated and real data sets shows that these methods can be
   an effective means of identifying regime shifts for single variable as
   well as multivariable time series. In addition, these methods can be
   used on systems with non-equilibrium steady states. However, care must
   be taken in interpreting results as these methods do respond to changes
   in time series that are not consistent with the regime shift concept.
   (C) 2007 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecocom.2007.07.005}},
ISSN = {{1476-945X}},
Unique-ID = {{ISI:000253034800004}},
}

@article{ ISI:000252538300003,
Author = {Jimenez-Guerrero, Pedro and Jorba, Oriol and Baidasanoa, Jose M. and
   Gasso, Santiago},
Title = {{The use of a modelling system as a tool for air quality management:
   Annual high-resolution simulations and evaluation}},
Journal = {{SCIENCE OF THE TOTAL ENVIRONMENT}},
Year = {{2008}},
Volume = {{390}},
Number = {{2-3}},
Pages = {{323-340}},
Month = {{FEB 15}},
Abstract = {{The high levels of air pollutants over the North-Western Mediterranean
   (NWM) exceed the thresholds set in current air quality regulations. They
   demand a detailed diagnosis of those areas where the exceedances of
   thresholds related to human health are found. In this sense, there is a
   need for modelling studies for the specific area of the NWM that take
   into account the annual cycle to address the diagnosis of air pollution.
   A new approach to the modelling of air quality in the NWM has been
   adopted by combining the WRF-EMICAT-CMAQ-DREAM modelling system to
   diagnose the current status of the levels of photochemical air pollution
   (focusing on ozone, O-3; nitrogen dioxide, NO2; carbon monoxide, CO; and
   particulate matter, PM10) in the area during an annual cycle (year
   2004). The complexity of the area of study requires the application of
   high spatial and temporal resolution (2 km and 1 h). The annual
   simulations need to cover the complex different meteorological
   situations and types of episodes of air pollution in the area of study.
   The outputs of the modelling system are evaluated against observations
   from 52 meteorological and 59 air quality stations belonging to the
   Environmental Department of the Catalonia Government (Spain), which
   involve a dense and accurate spatial distribution of stations in the
   territory (32,215 km(2)). The results indicate a good behaviour of the
   model in both coastal and inland areas of the NWM, with a slight trend
   to the overestimation of tropospheric O-3 concentrations and the
   underestimation of other photochemical pollutants (NO2, CO and PM10).
   The modelling diagnosis indicates that the main air quality-related
   problems in the NWM are the exceedances of the 1-hrO(3) information
   threshold set in the Directive 2002/3/EC (180 mu g m(-3)) as a
   consequence of the transport of O-3 precursors downwind the Barcelona
   Greater Area (BGA); and the exceedances of the annual value for the
   protection of human health for NO2 and PM10 (40 mu g m(-3), Directive
   1999/30/EC), both in the BGA, as a consequence of the high
   traffic-related emissions. (c) 2007 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.scitotenv.2007.10.025}},
ISSN = {{0048-9697}},
ResearcherID-Numbers = {{Gasso-Domingo, Santiago/D-8972-2014
   }},
ORCID-Numbers = {{Gasso-Domingo, Santiago/0000-0003-0481-4522
   Jorba, Oriol/0000-0001-5872-0244}},
Unique-ID = {{ISI:000252538300003}},
}

@article{ ISI:000252468700005,
Author = {Paninski, Liam and Haith, Adrian and Szirtes, Gabor},
Title = {{Integral equation methods for computing likelihoods and their
   derivatives in the stochastic integrate-and-fire model}},
Journal = {{JOURNAL OF COMPUTATIONAL NEUROSCIENCE}},
Year = {{2008}},
Volume = {{24}},
Number = {{1}},
Pages = {{69-79}},
Month = {{FEB}},
Abstract = {{We recently introduced likelihood-based methods for fitting stochastic
   integrate-and-fire models to spike train data. The key component of this
   method involves the likelihood that the model will emit a spike at a
   given time t. Computing this likelihood is equivalent to computing a
   Markov first passage time density (the probability that the model
   voltage crosses threshold for the first time at time t). Here we detail
   an improved method for computing this likelihood, based on solving a
   certain integral equation. This integral equation method has several
   advantages over the techniques discussed in our previous work: in
   particular, the new method has fewer free parameters and is easily
   differentiable (for gradient computations). The new method is also
   easily adaptable for the case in which the model conductance, not just
   the input current, is time-varying. Finally, we describe how to
   incorporate large deviations approximations to very small likelihoods.}},
DOI = {{10.1007/s10827-007-0042-x}},
ISSN = {{0929-5313}},
Unique-ID = {{ISI:000252468700005}},
}

@article{ ISI:000251822000003,
Author = {Jobin, Benoit and Labrecque, Sandra and Grenier, Marcelle and Falardeau,
   Gilles},
Title = {{Object-based classification as an alternative approach to the
   traditional pixel-based classification to identify potential habitat of
   the Grasshopper Sparrow}},
Journal = {{ENVIRONMENTAL MANAGEMENT}},
Year = {{2008}},
Volume = {{41}},
Number = {{1}},
Pages = {{20-31}},
Month = {{JAN}},
Abstract = {{The traditional method of identifying wildlife habitat distribution over
   large regions consists of pixel-based classification of satellite images
   into a suite of habitat classes used to select suitable habitat patches.
   Object-based classification is a new method that can achieve the same
   objective based on the segmentation of spectral bands of the image
   creating homogeneous polygons with regard to spatial or spectral
   characteristics. The segmentation algorithm does not solely rely on the
   single pixel value, but also on shape, texture, and pixel spatial
   continuity. The object-based classification is a knowledge base process
   where an interpretation key is developed using ground control points and
   objects are assigned to specific classes according to threshold values
   of determined spectral and/or spatial attributes. We developed a model
   using the eCognition software to identify suitable habitats for the
   Grasshopper Sparrow, a rare and declining species found in southwestern
   Quebec. The model was developed in a region with known breeding sites
   and applied on other images covering adjacent regions where potential
   breeding habitats may be present. We were successful in locating
   potential habitats in areas where dairy farming prevailed but failed in
   an adjacent region covered by a distinct Landsat scene and dominated by
   annual crops. We discuss the added value of this method, such as the
   possibility to use the contextual information associated to objects and
   the ability to eliminate unsuitable areas in the segmentation and land
   cover classification processes, as well as technical and logistical
   constraints. A series of recommendations on the use of this method and
   on conservation issues of Grasshopper Sparrow habitat is also provided.}},
DOI = {{10.1007/s00267-007-9031-0}},
ISSN = {{0364-152X}},
EISSN = {{1432-1009}},
Unique-ID = {{ISI:000251822000003}},
}

@article{ ISI:000258001200004,
Author = {Kamo, Masashi and Naito, Wataru},
Title = {{A novel approach to determining a population-level threshold in
   ecological risk assessment: A case study of zinc}},
Journal = {{HUMAN AND ECOLOGICAL RISK ASSESSMENT}},
Year = {{2008}},
Volume = {{14}},
Number = {{4}},
Pages = {{714-727}},
Abstract = {{A novel approach to population-level assessment was applied in order to
   demonstrate its utility in estimating and managing the risk of zinc in a
   water environment. Much attention has been paid to population-level risk
   assessment, but there have been no attempts to determine a safe
   population-level concentration as an environmental criterion. Based on
   the published results of toxicity tests for various species, we first
   theoretically derived a threshold concentration at which a population
   size is unchanged due to the adverse effects of zinc exposure. To derive
   a zinc concentration that will protect populations in natural
   environments, we adopted the concept of species sensitivity
   distribution. Assuming the threshold concentrations of a set of species
   are log-normally distributed, we calculated the 95\% protection level of
   zinc (PHC5 :population-level hazardous concentration of 5\% of species),
   which is 107 mu g/L. Meanwhile, the 95\% protection criterion (HC5)
   based on conventional individual-level chronic toxicity, was calculated
   to be 14.6 mu g/L. The environmentally safe concentration for a
   population-level endpoint is about 7 times greater than that for an
   individual-level endpoint. The proposed method provides guidance for a
   pragmatic approach to population-level ecological risk assessment and
   the management of chemicals.}},
DOI = {{10.1080/10807030802235110}},
ISSN = {{1080-7039}},
EISSN = {{1549-7860}},
ResearcherID-Numbers = {{Kamo, Masashi/C-6049-2015
   Naito, Wataru/M-1928-2018}},
ORCID-Numbers = {{Kamo, Masashi/0000-0002-6972-3333
   Naito, Wataru/0000-0003-2064-7170}},
Unique-ID = {{ISI:000258001200004}},
}

@article{ ISI:000255407500009,
Author = {Weaver, Christina M. and Wearne, Susan L.},
Title = {{Neuronal firing sensitivity to morphologic and active membrane
   parameters}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2008}},
Volume = {{4}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Both the excitability of a neuron's membrane, driven by active ion
   channels, and dendritic morphology contribute to neuronal firing
   dynamics, but the relative importance and interactions between these
   features remain poorly understood. Recent modeling studies have shown
   that different combinations of active conductances can evoke similar
   firing patterns, but have neglected how morphology might contribute to
   homeostasis. Parameterizing the morphology of a cylindrical dendrite, we
   introduce a novel application of mathematical sensitivity analysis that
   quantifies how dendritic length, diameter, and surface area influence
   neuronal firing, and compares these effects directly against those of
   active parameters. The method was applied to a model of neurons from
   goldfish Area II. These neurons exhibit, and likely contribute to,
   persistent activity in eye velocity storage, a simple model of working
   memory. We introduce sensitivity landscapes, defined by local
   sensitivity analyses of firing rate and gain to each parameter,
   performed globally across the parameter space. Principal directions over
   which sensitivity to all parameters varied most revealed intrinsic
   currents that most controlled model output. We found domains where
   different groups of parameters had the highest sensitivities, suggesting
   that interactions within each group shaped firing behaviors within each
   specific domain. Application of our method, and its characterization of
   which models were sensitive to general morphologic features, will lead
   to advances in understanding how realistic morphology participates in
   functional homeostasis. Significantly, we can predict which active
   conductances, and how many of them, will compensate for a given age- or
   development-related structural change, or will offset a morphologic
   perturbation resulting from trauma or neurodegenerative disorder, to
   restore normal function. Our method can be adapted to analyze any
   computational model. Thus, sensitivity landscapes, and the quantitative
   predictions they provide, can give new insight into mechanisms of
   homeostasis in any biological system.}},
DOI = {{10.1371/journal.pcbi.0040011}},
Article-Number = {{e11}},
ISSN = {{1553-7358}},
Unique-ID = {{ISI:000255407500009}},
}

@article{ ISI:000251928300001,
Author = {Cumming, Graeme S. and Barnes, Grenville},
Title = {{Characterizing land tenure dynamics by comparing spatial and temporal
   variation at multiple scales}},
Journal = {{LANDSCAPE AND URBAN PLANNING}},
Year = {{2007}},
Volume = {{83}},
Number = {{4}},
Pages = {{219-227}},
Month = {{DEC 7}},
Abstract = {{The dynamics of landscape change are closely linked to the dynamics of
   land tenure-the societal institutions (organizations, rules, fights and
   restrictions) that control the allocation and use of land and its
   associated resources by people. Land tenure occurs over a characteristic
   set of spatial and temporal scales. Understanding the nature of
   variation in land tenure in space and time is an important step towards
   developing a deeper understanding of land tenure change and its impacts
   on societies and ecosystems. In this paper we develop a new method of
   characterizing and exploring spatial and temporal variation in land
   tenure, focusing on the potential for dynamic feedbacks between
   variation in space and variation in time. The primary objective of this
   research is to develop a robust set of methods that can be used to make
   inferences about pattern-process linkages across multiple scales in
   landscapes that contain interlinked social and ecological systems. We
   apply our approach to the analysis of a 30-year data set of land
   ownership in part of Hamilton County (north central Florida) and find
   strong evidence for positive feedbacks between spatial and temporal
   variation. Our results suggest that spatial and temporal variation in
   land tenure interact strongly, raising the interesting possibility that
   strategic purchases of land in ecologically important areas may serve to
   limit habitat fragmentation. (c) 2007 Elsevier B.V.. All rights
   reserved.}},
DOI = {{10.1016/j.landurbplan.2007.04.007}},
ISSN = {{0169-2046}},
ResearcherID-Numbers = {{Cumming, Graeme/B-6551-2008}},
ORCID-Numbers = {{Cumming, Graeme/0000-0002-3678-1326}},
Unique-ID = {{ISI:000251928300001}},
}

@article{ ISI:000253895300011,
Author = {Howard, J. A. E. and Jarre, A. and Clark, A. E. and Moloney, C. L.},
Title = {{Application of the sequential t-test algorithm for analysing regime
   shifts to the southern Benguela ecosystem}},
Journal = {{AFRICAN JOURNAL OF MARINE SCIENCE}},
Year = {{2007}},
Volume = {{29}},
Number = {{3}},
Pages = {{437-451}},
Month = {{DEC}},
Abstract = {{Long-term ecosystem changes, such as regime shifts, have occurred in
   several marine ecosystems worldwide. Multivariate statistical methods
   have been used to detect such changes. A new method known as the
   sequential t-test algorithm for analysing regime shifts (STARS) is
   applied to a set of biological state variables as well as environmental
   and anthropogenic forcing variables in the southern Benguela. The method
   is able to correct for auto-correlation within time-series by a process
   known as prewhitening. All variables were tested with and without
   prewhitening. Shifts that were detected with both methods were termed
   robust. The STARS method detected shifts in relatively short time-series
   and identified when these shifts occurred without a priori hypotheses.
   Shifts were generally well detected at the end of time-series, but
   further development of the method is needed to enhance its performance
   for auto-correlated time-series. Since 1950, two major long-term
   ecosystem changes were identified for the southern Benguela. The first
   change occurred during the 1960s, caused predominantly by heavy fishing
   pressure but with some environmental forcing. The second change occurred
   in the early 2000s, caused mainly by environmental forcing. To
   strengthen these findings, further analyses should be carried out using
   different methods.}},
DOI = {{10.2989/AJMS.2007.29.3.11.341}},
ISSN = {{1814-232X}},
ResearcherID-Numbers = {{Moloney, Coleen/B-4363-2009}},
ORCID-Numbers = {{Moloney, Coleen/0000-0001-6663-8814}},
Unique-ID = {{ISI:000253895300011}},
}

@article{ ISI:000250724400015,
Author = {Gicquel, Stphanie and Marion-Gallois, Roland},
Title = {{Randomization with a posteriori constraints: Description and properties}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2007}},
Volume = {{26}},
Number = {{27}},
Pages = {{5033-5045}},
Month = {{NOV 30}},
Note = {{5th International Meeting on Statistical Methods in Biopharmacy, Paris,
   FRANCE, SEP 26-27, 2006}},
Organization = {{French Soc Stat}},
Abstract = {{The use of randomization for assigning patients to treatment groups in
   clinical trials is firmly acknowledged as providing the best quality
   results.
   Two standard methods are used in order to achieve well-balanced groups
   with respect to prognostic factors (i.e. factors influencing the disease
   outcome): stratification and minimization. Stratification is recommended
   when the number of strata is not too high-otherwise, minimization is
   preferred. However, minimization may compromise blinding (since the
   search for balance is performed a priori) and, furthermore, use of the
   technique has been questioned by the European Agency for the Evaluation
   of Medicinal Products. We have developed a new procedure for adaptive
   randomization, which we have named `randomization with a posteriori
   constraints'. By using a search for balance a posteriori, this procedure
   ensures that patient groups are similar with respect to prognostic
   factors while being less vulnerable to selection bias. The aim of this
   work was to describe the new method and to compare it (using
   simulations) with stratification and minimization.
   In the case of trials with few prognostic factors, the recourse to
   minimization or `randomization with a posteriori constraints' does not
   appear to be useful. In such a context, stratification has suitable
   properties and its simplicity of implementation encourages its use.
   However, when the number of prognostic factors is higher, `randomization
   with a posteriori constraints' is less predictable than minimization and
   the chance of imbalance is lower than for stratification. In conclusion,
   `randomization with a posteriori constraints' with an adequate threshold
   seems to be a good compromise between minimization and stratification.
   (C) 2007 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/sim.2953}},
ISSN = {{0277-6715}},
Unique-ID = {{ISI:000250724400015}},
}

@article{ ISI:000250673900020,
Author = {Wang, Zhanfeng and Chang, Yuan-Chin I. and Ying, Zhiliang and Zhu, Liang
   and Yang, Yaning},
Title = {{A parsimonious threshold-independent protein feature selection method
   through the area under receiver operating characteristic curve}},
Journal = {{BIOINFORMATICS}},
Year = {{2007}},
Volume = {{23}},
Number = {{20}},
Pages = {{2788-2794}},
Month = {{OCT 15}},
Abstract = {{Motivation: Protein expression profiling for differences indicative of
   early cancer holds promise for improving diagnostics. Due to their high
   dimensionality, statistical analysis of proteomic data from mass
   spectrometers is challenging in many aspects such as dimension
   reduction, feature subset selection as well as construction of
   classification rules. Search of an optimal feature subset, commonly
   known as the feature subset selection (FSS) problem, is an important
   step towards disease classification/diagnostics with biomarkers.
   Methods: We develop a parsimonious threshold-independent feature
   selection (PTIFS) method based on the concept of area under the curve
   (AUC) of the receiver operating characteristic (ROC). To reduce
   computational complexity to a manageable level, we use a sigmoid
   approximation to the empirical AUC as the criterion function. Starting
   from an anchor feature, the PTIFS method selects a feature subset
   through an iterative updating algorithm. Highly correlated features that
   have similar discriminating power are precluded from being selected
   simultaneously. The classification rule is then determined from the
   resulting feature subset. Results: The performance of the proposed
   approach is investigated by extensive simulation studies, and by
   applying the method to two mass spectrometry data sets of prostate
   cancer and of liver cancer. We compare the new approach with the
   threshold gradient descent regularization (TGDR) method. The results
   show that our method can achieve comparable performance to that of the
   TGDR method in terms of disease classification, but with fewer features
   selected.}},
DOI = {{10.1093/bioinformatics/btm442}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000250673900020}},
}

@article{ ISI:000250320800036,
Author = {Okushima, Shimehiro and Tamura, Makoto},
Title = {{Multiple calibration decomposition analysis: Energy use and carbon
   dioxide emissions in the Japanese economy, 1970-1995}},
Journal = {{ENERGY POLICY}},
Year = {{2007}},
Volume = {{35}},
Number = {{10}},
Pages = {{5156-5170}},
Month = {{OCT}},
Abstract = {{The purpose of this paper is to present a new approach to evaluating
   structural change of the economy in a multisector general equilibrium
   framework. The multiple calibration technique is applied to an ex post
   decomposition analysis of structural change between periods, enabling
   the distinction between price substitution and technological change to
   be made for each sector. This approach has the advantage of sounder
   microtheoretical underpinnings when compared with conventional
   decomposition methods. The proposed technique is empirically applied to
   changes in energy use and carbon dioxide (CO2) emissions in the Japanese
   economy from 1970 to 1995. The results show that technological change is
   of great importance for curtailing energy use and CO2 emissions in
   Japan. Total CO2 emissions increased during this period primarily
   because of economic growth, which is represented by final demand
   effects. On the other hand, the effects such as technological change for
   labor or energy mitigated the increase in CO2 emissions. (C) 2007
   Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.enpol.2007.04.001}},
ISSN = {{0301-4215}},
Unique-ID = {{ISI:000250320800036}},
}

@article{ ISI:000250160800002,
Author = {Frenkel, Zakharia M. and Trifonov, Edward N.},
Title = {{Evolutionary networks in the formatted protein sequence space}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2007}},
Volume = {{14}},
Number = {{8}},
Pages = {{1044-1057}},
Month = {{OCT}},
Abstract = {{In our recent work, a new approach to establish sequence relatedness, by
   walking through the protein sequence space, was introduced. The sequence
   space is built from 20 amino acid long fragments of proteins from a very
   large collection of fully sequenced prokaryotic genomes. The fragments,
   points in the space, are connected, if they are closely related (high
   sequence identity). The connected fragments form variety of networks of
   sequence kinship. In this research the networks in the formatted
   sequence space and their topology are analyzed. For lower identity
   thresholds a huge network of complex structure is formed, involving up
   to 10\% points of the space. When the threshold is increased, the major
   network splits into a set of smaller clusters with a wide diversity of
   sizes and topologies. Such ``evolutionary networks{''} may serve as a
   powerful sequence annotation tool that allows one to reveal fine details
   in the evolutionary history of proteins.}},
DOI = {{10.1089/cmb.2007.0066}},
ISSN = {{1066-5277}},
EISSN = {{1557-8666}},
Unique-ID = {{ISI:000250160800002}},
}

@article{ ISI:000249572400011,
Author = {Ito, Hiroyuki},
Title = {{Bootstrap significance test of synchronous spike events - A case study
   of oscillatory spike trains}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2007}},
Volume = {{26}},
Number = {{21}},
Pages = {{3976-3996}},
Month = {{SEP 20}},
Abstract = {{The purpose of this monograph is two folds. Firstly, we introduce
   challenging spike data to the statistical analysis. The data of two
   neurons recorded from the cat visual pathway show various non-stationary
   characteristics not fitted by the Poisson spike train. Spike firings of
   both neurons are strongly periodic and tightly synchronized. Our second
   purpose is a case study of applications of various statistical methods
   for the significance test of the time-varying spike synchrony. We
   provide various general remarks to the statistical analysis of the
   synchronous spike activities. At first, we apply the unitary event
   analysis. The significance limit for the coincident spike events by the
   Poisson distribution is compared with the limit given by the
   non-parametric test based on the bootstrap samplings. The bootstrap test
   performs superior to the Poisson test in two respects: (1) avoids false
   positives due to the sudden change of spike density; and (2) takes into
   account the non-stationary change of the spiking pattern at different
   sampling windows. When the spike trains are highly periodic, the
   histogram of the number of accidental coincident spike events over the
   bootstrap samples has a systematically larger variance than the Poisson
   distribution. We find that a large variance originates from the
   correlation between the successive coincident spike events in the
   structured spike trains. The significance of the time-varying synchrony
   is tested by another statistical method by Ventura et al., which is
   based on the adaptive smoothing method and the bootstrap significance
   test. Copyright (c) 2007 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/sim.2962}},
ISSN = {{0277-6715}},
EISSN = {{1097-0258}},
Unique-ID = {{ISI:000249572400011}},
}

@article{ ISI:000249818700003,
Author = {Wan, Xiu-Feng and Chen, Guorong and Luo, Feng and Emch, Michael and
   Donis, Ruben},
Title = {{A quantitative genotype algorithm reflecting H5N1 Avian influenza niches}},
Journal = {{BIOINFORMATICS}},
Year = {{2007}},
Volume = {{23}},
Number = {{18}},
Pages = {{2368-2375}},
Month = {{SEP 15}},
Abstract = {{Motivation: Computational genotyping analyses are critical for
   characterizing molecular evolutionary footprints, thus providing
   important information for designing the strategies of influenza
   prevention and control. Most of the current methods that are available
   are based on multiple sequence alignment and phylogenetic tree
   construction, which are time consuming and limited by the number of
   taxa. Arbitrarily defining genotypes further complicates the
   interpretation of genotyping results. Methods: In this study, we
   describe a quantitative influenza genotyping algorithm based on the
   theory of quasispecies. First, the complete composition vector (CCV) was
   utilized to calculate the pairwise evolutionary distance between
   genotypes. Next, Hierarchical Bayesian Modeling using the Gibbs Sampling
   algorithm was applied to identify the segment genotype threshold, which
   is used to identify influenza segment genotype through a modularity
   calculation. The viral genotype was defined by combining eight segment
   genotypes based on the genetic reassortment feature of influenza A
   viruses. Results: We applied this method for H5N1 avian influenza
   viruses and identified 107 niches among 283 viruses with a complete
   genome set. The diversity of viral genotypes, and their correlation with
   geographic locations suggests that these viruses form local niches after
   being introduced to a new ecological environment through poultry trade
   or bird migration. This novel method allows us to define genotypes in a
   robust, quantitative as well as hierarchical manner.}},
DOI = {{10.1093/bioinformatics/btm354}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000249818700003}},
}

@article{ ISI:000249500900019,
Author = {Fujiwara, Masami},
Title = {{Extinction-effective population index: Incorporating life-history
   variations in population viability analysis}},
Journal = {{ECOLOGY}},
Year = {{2007}},
Volume = {{88}},
Number = {{9}},
Pages = {{2345-2353}},
Month = {{SEP}},
Abstract = {{Viability status of populations is a commonly used measure for
   decision-making in the management of populations. One of the challenges
   faced by managers is the need to consistently allocate management effort
   among populations. This allocation should in part be based on comparison
   of extinction risks among populations. Unfortunately, common criteria
   that use minimum viable population size or count-based population
   viability analysis (PVA) often do not provide results that are
   comparable among populations, primarily because they lack consistency in
   determining population size measures and threshold levels of population
   size (e.g., minimum viable population size and quasi-extinction
   threshold). Here I introduce a new index called the
   ``extinction-effective population index,{''} which accounts for
   differential effects of demographic stochasticity among organisms with
   different life-history strategies and among individuals in different
   life stages. This index is expected to become a new way of determining
   minimum viable population size criteria and also complement the
   count-based PVA. The index accounts for the difference in life-history
   strategies of organisms, which are modeled using matrix population
   models. The extinction-effective population index, sensitivity, and
   elasticity are demonstrated in three species of Pacific salmonids. The
   interpretation of the index is also provided by comparing them with
   existing demographic indices. Finally, a measure of
   life-history-specific effect of demographic stochasticity is derived.}},
DOI = {{10.1890/06-1405.1}},
ISSN = {{0012-9658}},
EISSN = {{1939-9170}},
ResearcherID-Numbers = {{Fujiwara, Masami/C-3115-2012}},
ORCID-Numbers = {{Fujiwara, Masami/0000-0002-9255-6043}},
Unique-ID = {{ISI:000249500900019}},
}

@article{ ISI:000249309900006,
Author = {Li, Jinyan and Yang, Qiang},
Title = {{Strong compound-risk factors: Efficient discovery through emerging
   patterns and contrast sets}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE}},
Year = {{2007}},
Volume = {{11}},
Number = {{5}},
Pages = {{544-552}},
Month = {{SEP}},
Abstract = {{Odds ratio (OR), relative risk (RR) (risk ratio), and absolute risk
   reduction (ARR) (risk difference) are biostatistics measurements that
   are widely used for identifying significant risk factors in dichotomous
   groups of subjects. In the past, they have often been used to assess
   simple risk factors. In this paper, we introduce the concept of
   compound-risk factors to broaden the applicability of these statistical
   tests for assessing factor interplays. We observe that compound-risk
   factors with a high risk ratio or a big risk difference have an
   one-to-one correspondence to strong emerging patterns or strong contrast
   sets-two types of patterns that have been extensively studied in the
   data mining field. Such a relationship has been unknown to researchers
   in the past, and efficient algorithms for discovering strong
   compound-risk factors have been lacking. In this paper, we propose a
   theoretical framework and a new algorithm that unify the discovery of
   compound-risk factors that have a strong OR, risk ratio, or a risk
   difference. Our method guarantees that all patterns meeting a certain
   test threshold can be efficiently discovered. Our contribution thus
   represents the first of its kind in linking the risk ratios and ORs to
   pattern mining algorithms, making it possible to find compound-risk
   factors in large-scale data sets. In addition, we show that using
   compound-risk factors can improve classification accuracy in
   probabilistic learning algorithms on several disease data sets, because
   these compound-risk factors capture the interdependency between
   important data attributes.}},
DOI = {{10.1109/TITB.2007.891163}},
ISSN = {{1089-7771}},
EISSN = {{1558-0032}},
ORCID-Numbers = {{Yang, Qiang/0000-0001-5059-8360
   Li, Jinyan/0000-0003-1833-7413}},
Unique-ID = {{ISI:000249309900006}},
}

@article{ ISI:000249706800001,
Author = {Nicolas, Pierre and Bessieres, Philippe and Ehrlich, S. Dusko and
   Maguin, Emmanuelle and van de Guchte, Maarten},
Title = {{Extensive horizontal transfer of core genome genes between two
   Lactobacillus species found in the gastrointestinal tract}},
Journal = {{BMC EVOLUTIONARY BIOLOGY}},
Year = {{2007}},
Volume = {{7}},
Month = {{AUG 20}},
Abstract = {{Background: While genes that are conserved between related bacterial
   species are usually thought to have evolved along with the species,
   phylogenetic trees reconstructed for individual genes may contradict
   this picture and indicate horizontal gene transfer. Individual trees are
   often not resolved with high confidence, however, and in that case
   alternative trees are generally not considered as contradicting the
   species tree, although not confirming it either. Here we conduct an
   in-depth analysis of 401 protein phylogenetic trees inferred with
   varying levels of confidence for three lactobacilli from the acidophilus
   complex. At present the relationship between these bacteria, isolated
   from environments as diverse as the gastrointestinal tract (
   Lactobacillus acidophilus and Lactobacillus johnsonii) and yogurt (
   Lactobacillus delbrueckii ssp. bulgaricus), is ambiguous due to
   contradictory phenotypical and 16S rRNA based classifications.
   Results: Among the 401 phylogenetic trees, those that could be
   reconstructed with high confidence support the 16S-rRNA tree or one
   alternative topology in an astonishing 3: 2 ratio, while the third
   possible topology is practically absent. Lowering the confidence
   threshold for trees to be taken into consideration does not
   significantly affect this ratio, and therefore suggests that gene
   transfer may have affected as much as 40\% of the core genome genes.
   Gene function bias suggests that the 16S rRNA phylogeny of the
   acidophilus complex, which indicates that L. acidophilus and L.
   delbrueckii ssp. bulgaricus are the closest related of these three
   species, is correct. A novel approach of comparison of interspecies
   protein divergence data employed in this study allowed to determine that
   gene transfer most likely took place between the lineages of the two
   species found in the gastrointestinal tract.
   Conclusion: This case-study reports an unprecedented level of
   phylogenetic incongruence, presumably resulting from extensive
   horizontal gene transfer. The data give a first indication of the large
   extent of gene transfer that may take place in the gastrointestinal
   tract and its accumulated effect. For future studies, our results should
   encourage a careful weighing of data on phylogenetic tree topology,
   confidence and distribution to conclude on the absence or presence and
   extent of horizontal gene transfer.}},
DOI = {{10.1186/1471-2148-7-141}},
Article-Number = {{141}},
ISSN = {{1471-2148}},
Unique-ID = {{ISI:000249706800001}},
}

@article{ ISI:000249818300009,
Author = {Elo, Laura L. and Jaervenpaeae, Henna and Oresic, Matej and Lahesmaa,
   Riitta and Aittokallio, Tero},
Title = {{Systematic construction of gene coexpression networks with applications
   to human T helper cell differentiation process}},
Journal = {{BIOINFORMATICS}},
Year = {{2007}},
Volume = {{23}},
Number = {{16}},
Pages = {{2096-2103}},
Month = {{AUG 15}},
Abstract = {{Motivation: Coexpression networks have recently emerged as a novel
   holistic approach to microarray data analysis and interpretation.
   Choosing an appropriate cutoff threshold, above which a gene-gene
   interaction is considered as relevant, is a critical task in most
   network-centric applications, especially when two or more networks are
   being compared.
   Results: We demonstrate that the performance of traditional approaches,
   which are based on a pre-defined cutoff or significance level, can vary
   drastically depending on the type of data and application. Therefore, we
   introduce a systematic procedure for estimating a cutoff threshold of
   coexpression networks directly from their topological properties. Both
   synthetic and real datasets show clear benefits of our data-driven
   approach under various practical circumstances. In particular, the
   procedure provides a robust estimate of individual degree distributions,
   even from multiple microarray studies performed with different array
   platforms or experimental designs, which can be used to discriminate the
   corresponding phenotypes. Application to human T helper cell
   differentiation process provides useful insights into the components and
   interactions controlling this process, many of which would have remained
   unidentified on the basis of expression change alone. Moreover, several
   human-mouse orthologs showed conserved topological changes in both
   systems, suggesting their potential importance in the differentiation
   process.
   Contact: laliel@utu.fi
   Supplementary information: Supplementary data are available at
   Bioinformatics online.}},
DOI = {{10.1093/bioinformatics/btm309}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Aittokallio, Tero/B-6583-2009
   Oresic, Matej/K-7673-2016
   }},
ORCID-Numbers = {{Aittokallio, Tero/0000-0002-0886-9769
   Oresic, Matej/0000-0002-2856-9165
   Elo, Laura/0000-0001-5648-4532}},
Unique-ID = {{ISI:000249818300009}},
}

@article{ ISI:000248178600013,
Author = {Imhof, Lorens A. and Fudenberg, Drew and Nowak, Martin A.},
Title = {{Tit-for-tat or win-stay, lose-shift?}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2007}},
Volume = {{247}},
Number = {{3}},
Pages = {{574-580}},
Month = {{AUG 7}},
Abstract = {{The repeated Prisoner's Dilemma is usually known as a story of
   tit-for-tat (TFT). This remarkable strategy has won both of Robert
   Axelrod's tournaments. TFT does whatever the opponent has done in the
   previous round. It will cooperate if the opponent has cooperated, and it
   will defect if the opponent has defected. But TFT has two weaknesses:
   (i) it cannot correct mistakes (erroneous moves) and (ii) a population
   of TFT players is undermined by random drift when mutant strategies
   appear which play always-cooperate (ALLC). Another equally simple
   strategy called `win-stay, lose-shift' (WSLS) has neither of these two
   disadvantages. WSLS repeats the previous move if the resulting payoff
   has met its aspiration level and changes otherwise. Here, we use a novel
   approach of stochastic evolutionary game dynamics in finite populations
   to study mutation-selection dynamics in the presence of erroneous moves.
   We compare four strategies: always-defect (ALLD), ALLC, TFT and WSLS.
   There are two possible outcomes: if the benefit of cooperation is below
   a critical value then ALLD is selected; if the benefit of cooperation is
   above this critical value then WSLS is selected. TFT is never selected
   in this evolutionary process, but lowers the selection threshold for
   WSLS. (C) 2007 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2007.03.027}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
ResearcherID-Numbers = {{Nowak, Martin/A-6977-2008}},
Unique-ID = {{ISI:000248178600013}},
}

@article{ ISI:000249116000025,
Author = {Rotem, Naama and Sestieri, Emanuel and Cohen, Dana and Paulin, Mike and
   Meiri, Hanoch and Yarom, Yosef},
Title = {{The functional architecture of the shark's dorsal-octavolateral nucleus:
   an in vitro study}},
Journal = {{JOURNAL OF EXPERIMENTAL BIOLOGY}},
Year = {{2007}},
Volume = {{210}},
Number = {{15}},
Pages = {{2730-2742}},
Month = {{AUG 1}},
Abstract = {{Learning to predict the component in the sensory information resulting
   from the organism's own activity enables it to respond appropriately to
   unexpected stimuli. For example, the elasmobranch dorsal octavolateral
   nucleus (DON) can apparently extract the unexpected component (i.e.
   generated by nearby organisms) from the incoming electrosensory signals.
   Here we introduce a novel and unique experimental approach that combines
   the advantages of in vitro preparations with the integrity of in vivo
   conditions. In such an experimental system one can study, under control
   conditions, the cellular and network mechanisms that underlie
   cancellation of expected sensory inputs. Using extracellular and
   intracellular recordings we compared the dynamics and spatiotemporal
   organization of the electrosensory afferent nerve and parallel fiber
   inputs to the DON. The afferent nerve has a low threshold and high
   conduction velocity; a stimulus that recruits a small number of fibers
   is sufficient to activate the principal neurons. The excitatory
   postsynaptic potential in the principal cells evoked by afferent nerve
   fibers has fast kinetics that efficiently reach the threshold for action
   potential. In contrast, the parallel fibers have low conduction
   velocity, high threshold and extensive convergence on the principal
   neurons of the DON. The excitatory postsynaptic response has slow
   kinetics that provides a wide time window for integration of inputs. The
   highly efficient connection between the afferent nerve and the principal
   neurons in the DON indicates that filtration occurring in the DON cannot
   be mediated simply by summation of the parallel fibers' signals with the
   afferent sensory signals. Hence we propose that the filtering may be
   mediated via secondary neurons that adjust the principal neurons'
   sensitivity to afferent inputs.}},
DOI = {{10.1242/jeb.001784}},
ISSN = {{0022-0949}},
Unique-ID = {{ISI:000249116000025}},
}

@article{ ISI:000248588000024,
Author = {Deans, Tara L. and Cantor, Charles R. and Collins, James J.},
Title = {{A tunable genetic switch based on RNAi and repressor proteins for
   regulating gene expression in mammalian cells}},
Journal = {{CELL}},
Year = {{2007}},
Volume = {{130}},
Number = {{2}},
Pages = {{363-372}},
Month = {{JUL 27}},
Abstract = {{Here, we introduce an engineered, tunable genetic switch that couples
   repressor proteins and an RNAi target design to effectively turn any
   gene off. We used the switch to regulate the expression of EGFP in mouse
   and human cells and found that it offers > 99\% repression as well as
   the ability to tune gene expression. To demonstrate the system's
   modularity and level of gene silencing, we used the switch to tightly
   regulate the expression of diphtheria toxin and Cre recombinase,
   respectively. We also used the switch to tune the expression of a
   proapoptotic gene and show that a threshold expression level is required
   to induce apoptosis. This work establishes a system for tight, tunable
   control of mammalian gene expression that can be used to explore the
   functional role of various genes as well as to determine whether a
   phenotype is the result of a threshold response to changes in gene
   expression.}},
DOI = {{10.1016/j.cell.2007.05.045}},
ISSN = {{0092-8674}},
Unique-ID = {{ISI:000248588000024}},
}

@article{ ISI:000248500300001,
Author = {Scott, Michelle S. and Barton, Geoffrey J.},
Title = {{Probabilistic prediction and ranking of human protein-protein
   interactions}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2007}},
Volume = {{8}},
Month = {{JUL 5}},
Abstract = {{Background: Although the prediction of protein-protein interactions has
   been extensively investigated for yeast, few such datasets exist for the
   far larger proteome in human. Furthermore, it has recently been
   estimated that the overall average false positive rate of available
   computational and high-throughput experimental interaction datasets is
   as high as 90\%.
   Results: The prediction of human protein-protein interactions was
   investigated by combining orthogonal protein features within a
   probabilistic framework. The features include co-expression, orthology
   to known interacting proteins and the full-Bayesian combination of
   subcellular localization, co-occurrence of domains and
   post-translational modifications. A novel scoring function for local
   network topology was also investigated. This topology feature greatly
   enhanced the predictions and together with the full-Bayes combined
   features, made the largest contribution to the predictions. Using a
   conservative threshold, our most accurate predictor identifies 37606
   human interactions, 32892 (80\%) of which are not present in other
   publicly available large human interaction datasets, thus substantially
   increasing the coverage of the human interaction map. A subset of the
   32892 novel predicted interactions have been independently validated.
   Comparison of the prediction dataset to other available human
   interaction datasets estimates the false positive rate of the new method
   to be below 80\% which is competitive with other methods. Since the new
   method scores and ranks all human protein pairs, smaller subsets of
   higher quality can be generated thus leading to even lower false
   positive prediction rates.
   Conclusion: The set of interactions predicted in this work increases the
   coverage of the human interaction map and will help determine the
   highest confidence human interactions.}},
DOI = {{10.1186/1471-2105-8-239}},
Article-Number = {{239}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Barton, Geoffrey/C-6267-2011
   }},
ORCID-Numbers = {{Barton, Geoffrey/0000-0002-9014-5355}},
Unique-ID = {{ISI:000248500300001}},
}

@article{ ISI:000249330700009,
Author = {Zhu, Chengsong and Huang, Ju and Zhang, Yuan-Ming},
Title = {{Mapping binary trait loci in the F-2 : 3 design}},
Journal = {{JOURNAL OF HEREDITY}},
Year = {{2007}},
Volume = {{98}},
Number = {{4}},
Pages = {{337-344}},
Month = {{JUL-AUG}},
Abstract = {{In the inheritance analysis of quantitative trait with low heritability,
   the precision is relatively low. In this situation, an F-2:3 design,
   which is genotyped in F2 plants and phenotyped in the F2:3 progeny, is
   applied to increase the precision in the detection of quantitative trait
   loci (QTL). This is because that residual variance on the basis of
   family-mean-based observations has been significantly decreased by
   increasing the number of F2:3 progeny. Our previous results showed that
   the mixture distribution for the F2,3 family of heterozygous F2 plant
   can significantly increase the power of QTL detection relative to the
   classical F2 design. In this article, we extended our previous method
   from continuous traits to binary traits in the F2:3 design. The method
   here also takes full advantage of the mixture distribution. However, the
   method presented here differs from our previous method in 2 aspects. One
   is that the penetrance model is integrated with the liability model for
   mapping binary trait loci (BTL), and another is that the phenotypic data
   used in the analysis are the sum of phenotypic values of F2:3 progeny
   derived from each F2 plant rather than the average Of F2:3 progeny due
   to the fact that the distribution of the sum follows binomial
   distribution. in addition, the threshold in the liability model could
   also be estimated. Therefore, a new framework of mapping BTL on the
   basis of a single BTL model was set up and implemented via the
   Expectation-Maximization algorithm. Results of simulated studies showed
   that the proposed method provides accurate estimates for both the
   effects and the locations of BTL, with high statistical power even under
   the low heritability. With the new method, we are ready to map BTL, as
   we can do for quantitative traits under the F2:3 design. The computer
   program performing the analysis of the simulated data is available to
   users for real data analysis.}},
DOI = {{10.1093/jhered/esm041}},
ISSN = {{0022-1503}},
ResearcherID-Numbers = {{Zhu, Chengsong/F-8045-2011
   }},
ORCID-Numbers = {{Zhang, Yuan-Ming/0000-0003-2317-2190}},
Unique-ID = {{ISI:000249330700009}},
}

@article{ ISI:000247187600042,
Author = {Lienert, Judit and Gudel, Karin and Escher, Beate I.},
Title = {{Screening method for ecotoxicological hazard assessment of 42
   pharmaceuticals considering human metabolism and excretory routes}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2007}},
Volume = {{41}},
Number = {{12}},
Pages = {{4471-4478}},
Month = {{JUN 15}},
Abstract = {{We assessed the ecotoxicological hazard potential of 42 pharmaceuticals
   from 22 therapeutic groups, including metabolites formed in humans. We
   treated each parent drug and its metabolites as a mixture of similarly
   acting compounds. If physicochemical or effect literature data were
   missing, we estimated these with quantitative structure-activity
   relationships (QSAR). Additionally, we estimated micropollutant removal
   efficiency of urine source separation using pharmaceutical information.
   On average, 50\% of a parent drug was metabolized, and 70\% was excreted
   with urine, albeit with large variations among drugs. Metabolism reduced
   the toxic potential of all but eight drugs. The subsequently modeled
   risk quotient was mostly below the threshold of one. However, ibuprofen
   and its metabolites in a mixture could pose an ecotoxicologal risk; and
   possibly also acetylsalicylic acid, bezafibrate, carbamazepine,
   diclofenac, fenofibrate, and paracetamol. Lipophilicity and sale
   quantities of parent drugs alone were insufficient to estimate their
   ecotoxicological risk. Urine separation could decrease the
   ecotoxicological risk of some, but not all drugs. The estimated risk
   quotients were equal in urine and feces, again with large variations
   among compounds. Because of scientific limitations of the model and
   inconsistent literature data the results are somewhat uncertain.
   However, this new approach allows first tier screening of single drugs,
   thus supporting decision-making.}},
DOI = {{10.1021/es0627693}},
ISSN = {{0013-936X}},
ResearcherID-Numbers = {{PRAZERES, KELLY/O-7715-2015
   Escher, Beate/C-7992-2009
   Lienert, Judit/B-1138-2013}},
ORCID-Numbers = {{Escher, Beate/0000-0002-5304-706X
   Lienert, Judit/0000-0003-1925-3895}},
Unique-ID = {{ISI:000247187600042}},
}

@article{ ISI:000246382700039,
Author = {Ayala, Marcela and Roman, Rosa and Vazquez-Duhalt, Rafael},
Title = {{A catalytic approach to estimate the redox potential of heme-peroxidases}},
Journal = {{BIOCHEMICAL AND BIOPHYSICAL RESEARCH COMMUNICATIONS}},
Year = {{2007}},
Volume = {{357}},
Number = {{3}},
Pages = {{804-808}},
Month = {{JUN 8}},
Abstract = {{The redox potential of heme-peroxidases varies according to a
   combination of structural components within the active site and its
   vicinities. For each peroxidase, this redox potential imposes a
   thermodynamic threshold to the range of oxidizable substrates. However,
   the instability of enzymatic intermediates during the catalytic cycle
   precludes the use of direct voltammetry to measure the redox potential
   of most peroxidases. Here we describe a novel approach to estimate the
   redox potential of peroxidases, which directly depends on the catalytic
   performance of the activated enzyme. Selected p-substituted phenols are
   used as substrates for the estimations. The results obtained with this
   catalytic approach correlate well with the oxidative capacity predicted
   by the redox potential of the Fe(III)/Fe(II) couple. (c) 2007 Elsevier
   Inc. All rights reserved.}},
DOI = {{10.1016/j.bbrc.2007.04.020}},
ISSN = {{0006-291X}},
EISSN = {{1090-2104}},
ResearcherID-Numbers = {{Vazquez-Duhalt, Rafael/E-2751-2016
   Ayala, Marcela/A-4876-2008}},
ORCID-Numbers = {{Vazquez-Duhalt, Rafael/0000-0003-1612-2996
   }},
Unique-ID = {{ISI:000246382700039}},
}

@article{ ISI:000249940600010,
Author = {Mahecha, Miguel D. and Martinez, Alfredo and Lischeid, Gunnar and Beck,
   Erwin},
Title = {{Nonlinear dimensionality reduction: Alternative ordination approaches
   for extracting and visualizing biodiversity patterns in tropical montane
   forest vegetation data}},
Journal = {{ECOLOGICAL INFORMATICS}},
Year = {{2007}},
Volume = {{2}},
Number = {{2}},
Pages = {{138-149}},
Month = {{JUN 1}},
Abstract = {{Ecological patterns are difficult to extract directly from vegetation
   data. The respective surveys provide a high number of interrelated
   species occurrence variables. Since often only a limited number of
   ecological gradients determine species distributions, the data might be
   represented by much fewer but effectively independent variables. This
   can be achieved by reducing the dimensionality of the data. Conventional
   methods are either limited to linear feature extraction (e.g., principal
   component analysis, and Classical Multidimensional Scaling, CMDS) or
   require a priori assumptions on the intrinsic data dimensionality (e.g.,
   Nonmetric Multidimensional Scaling, NMDS, and self organized maps, SOM).
   In this study we explored the potential of Isometric Feature Mapping
   (Isomap). This new method of dimensionality reduction is a nonlinear
   generalization of CMDS. Isomap is based on a nonlinear geodesic
   inter-point distance matrix. Estimating geodesic distances requires one
   free threshold parameter, which defines linear geometry to be preserved
   in the global nonlinear distance structure. We compared Isomap to its
   linear (CMDS) and nonmetric NMDS) equivalents. Furthermore, the use of
   geodesic distances allowed also extending NMDS to a version that we
   called NMDS-G. In addition we investigated a supervised Isomap valiant
   (S-Isomap) and showed that all these techniques are interpretable within
   a single methodical framework.
   As an example we investigated seven plots (subdivided in 456 subplots)
   in different secondary tropical montane forests with 773 species of
   vascular plants. A key problem for the study of tropical vegetation data
   is the heterogeneous small scale variability implying large ranges of
   beta-diversity. The CMDS and NMDS methods did not reduce the data
   dimensionality reasonably. On the contrary, Isomap, explained 95\% of
   the data variance in the first five dimensions and provided ecologically
   interpretable visualizations; NMDS-G yielded similar results. The main
   shortcoming of the latter was the high computational cost and the
   requirement to predefine the dimension of the embedding space. The
   S-Isomap learning scheme did not improve the Isomap variant for an
   optimal threshold parameter but substantially improved the nonoptimal
   solutions.
   We conclude that Isomap as a new ordination method allows effective
   representations of high dimensional vegetation data sets. The method is
   promising since it does not require a priori assumptions, and is
   computationally highly effective.}},
DOI = {{10.1016/j.ecoinf.2007.05.002}},
ISSN = {{1574-9541}},
EISSN = {{1878-0512}},
ResearcherID-Numbers = {{Mahecha, Miguel/F-2443-2010
   Lischeid, Gunnar/F-9383-2016}},
ORCID-Numbers = {{Mahecha, Miguel/0000-0003-3031-613X
   Lischeid, Gunnar/0000-0003-3700-6062}},
Unique-ID = {{ISI:000249940600010}},
}

@article{ ISI:000248241500008,
Author = {Meng, Xinzhu and Song, Zhitao and Chen, Lansun},
Title = {{A new mathematical model for optimal control strategies of integrated
   pest management}},
Journal = {{JOURNAL OF BIOLOGICAL SYSTEMS}},
Year = {{2007}},
Volume = {{15}},
Number = {{2}},
Pages = {{219-234}},
Month = {{JUN}},
Abstract = {{A state-dependent impulsive SI epidemic model for integrated pest
   management (IPM) is proposed and investigated. We shall examine an
   optimal impulsive control problem in the management of an epidemic to
   control a pest population. We introduce a small amount of pathogen into
   a pest population with the expectation that it will generate an epidemic
   and that it will subsequently be endemic such that the number of pests
   is no larger than the given economic threshold (ET), so that the pests
   cannot cause economic damage. This is the biological control strategy
   given in the present paper. The combination strategy of pulse capturing
   (susceptible individuals) and pulse releasing (infective individuals) is
   implemented in the model if the number of pests (susceptible) reaches
   the ET. Firstly, the impulsive control problem is to drive the pest
   population below a given pest level and to do so in a manner which
   minimizes a weighted sum of the cost of using the control. Hence, for a
   one time impulsive effect we obtain the optimal strategy in terms of
   total cost such that the number of pests is no larger than the given ET.
   Secondly, we show the existence of periodic solution with the number of
   pests no larger than ET, and by using the Analogue of the Poincare
   Criterion we prove that it is asymptotically stable under a planned
   impulsive control strategy. Further, the period T of the periodic
   solution is calculated, which can be used to estimate how long the pest
   population will take to return back to its pre-control level. The main
   feature of the present paper is to apply an SI infectious disease model
   to IPM, and some pests control strategies are given.}},
DOI = {{10.1142/S0218339007002143}},
ISSN = {{0218-3390}},
EISSN = {{1793-6470}},
ORCID-Numbers = {{Meng, Xinzhu/0000-0002-6553-9686}},
Unique-ID = {{ISI:000248241500008}},
}

@article{ ISI:000247270600005,
Author = {Golubtsov, Peter V. and McKelvey, Robert},
Title = {{The incomplete-information split-stream fish war: Examining the
   implications of competing risks}},
Journal = {{NATURAL RESOURCE MODELING}},
Year = {{2007}},
Volume = {{20}},
Number = {{2}},
Pages = {{263-300}},
Month = {{SUM}},
Note = {{Conference on Economic Effects of Climate Change on Fisheries, Bergen,
   NORWAY, JUN, 2005}},
Abstract = {{It is now widely recognized that climactic regime shifts, which
   aperiodically alter a harvested fish stock's biomass and spatial
   distribution, may lead to distorted fisheries management decisions which
   negatively impact the fishery, both biologically and economically. This
   is particularly true for trans-boundary migratory stocks, where optimal
   management relies on coordination among independent nation-states.
   Unanticipated changes in stock distribution and abundance can upset
   expectations of national authorities, leading them to sanction
   inappropriate harvesting levels by their separately managed fleets
   targeting the same breeding fish stock.
   Our theoretical studies are based on a spatially-distributed stochastic
   model, which we have called the ``split-stream model,{''} where two
   separately managed fleets harvest simultaneously at two separate sites.
   Our key assumption is that competing fleet managers, when harvesting
   noncooperatively, hold incomplete and asymmetric private information of
   current stock recruitment and spatial distribution. When subsequently
   negotiating to coordinate their harvests, they agree that they will
   share their information and then bargain over partition of the gains
   from their cooperation. This bargaining process takes into account the
   fleet's relative competitive strengths, particularly due to private
   information asymmetries.
   In this present article we introduce a more complex information
   structure than had been assumed in our earlier work (McKelvey and
   Golubtsov {[}2002], McKelvey, Miller and Golubtsov {[}20031, Mckelvey et
   al. {[}2004]). Specifically, both stock-growth and stock-split
   parameters vary stochastically and asynchronously. Thus, when harvesting
   noncooperatively, each fleet may possess private knowledge which is
   unavailable to the other. We examine the interplay of the harvesting
   game's information structure with other fishery characteristics. such as
   the fleets' economics and operating characteristics and their attitudes
   toward risk, to determine the implications of such structure for the
   outcome of the harvesting game. All of these changes axe made to capture
   new conceptual phenomena. and expand the range of applicability of the
   model.}},
ISSN = {{0890-8575}},
Unique-ID = {{ISI:000247270600005}},
}

@article{ ISI:000247817500018,
Author = {Sahoo, Debashis and Dill, David L. and Tibshirani, Rob and Plevritis,
   Sylvia K.},
Title = {{Extracting binary signals from microarray time-course data}},
Journal = {{NUCLEIC ACIDS RESEARCH}},
Year = {{2007}},
Volume = {{35}},
Number = {{11}},
Pages = {{3705-3712}},
Month = {{JUN}},
Abstract = {{This article presents a new method for analyzing microarray time courses
   by identifying genes that undergo abrupt transitions in expression
   level, and the time at which the transitions occur. The algorithm
   matches the sequence of expression levels for each gene against temporal
   patterns having one or two transitions between two expression levels.
   The algorithm reports a P-value for the matching pattern of each gene,
   and a global false discovery rate can also be computed. After matching,
   genes can be sorted by the direction and time of transitions. Genes can
   be partitioned into sets based on the direction and time of change for
   further analysis, such as comparison with Gene Ontology annotations or
   binding site motifs. The method is evaluated on simulated and actual
   timecourse data. On microarray data for budding yeast, it is shown that
   the groups of genes that change in similar ways and at similar times
   have significant and relevant Gene Ontology annotations.}},
DOI = {{10.1093/nar/gkm284}},
ISSN = {{0305-1048}},
Unique-ID = {{ISI:000247817500018}},
}

@article{ ISI:000245521100008,
Author = {Pueyo, Salvador},
Title = {{Self-organised criticality and the response of wildland fires to climate
   change}},
Journal = {{CLIMATIC CHANGE}},
Year = {{2007}},
Volume = {{82}},
Number = {{1-2}},
Pages = {{131-161}},
Month = {{MAY}},
Abstract = {{Here I present a new approach to forecasting the effects of climate
   change on catastrophic events, based on the `self-organised criticality'
   concept from statistical physics. In particular, I develop the
   `self-organised critical fuel succession model' (SOCFUS), which deals
   with wildland fires. I show that there is good agreement between model
   and data for the response pattern of the whole fire size statistical
   distribution to weather fluctuations in a boreal forest region. I
   tentatively predict the fire regime in this region for an instance of
   possible climate change scenario. I show that the immediate response is
   sharper than usually thought, but part of the added burning rate might
   not persist indefinitely. A large fraction of the extra burning in the
   transition period is likely to be concentrated in a few `climate change
   fires', much larger than the largest fires that could currently occur. I
   also suggest that the major fire events recently observed in some
   tropical rainforest regions belong to a qualitatively different, even
   more abrupt type of response, which is also predicted by the model.}},
DOI = {{10.1007/s10584-006-9134-2}},
ISSN = {{0165-0009}},
Unique-ID = {{ISI:000245521100008}},
}

@article{ ISI:000246166500014,
Author = {Xu, Lisheng and Zhang, David and Wang, Kuanquan and Li, Naimin and Wang,
   Xiaoyun},
Title = {{Baseline wander correction in pulse waveforms using wavelet-based
   cascaded adaptive filter}},
Journal = {{COMPUTERS IN BIOLOGY AND MEDICINE}},
Year = {{2007}},
Volume = {{37}},
Number = {{5}},
Pages = {{716-731}},
Month = {{MAY}},
Abstract = {{Pulse diagnosis is a convenient, inexpensive, painless, and non-invasive
   diagnosis method. Quantifying pulse diagnosis is to acquire and record
   pulse waveforms by a set of sensor firstly, and then analyze these pulse
   waveforms. However, respiration and artifact motion during pulse
   waveform acquisition can introduce baseline wander. It is necessary,
   therefore, to remove the pulse waveform's baseline wander in order to
   perform accurate pulse waveform analysis. This paper presents a
   wavelet-based cascaded adaptive filter (CAF) to remove the baseline
   wander of pulse waveform. To evaluate the level of baseline wander, we
   introduce a criterion: energy ratio (ER) of pulse waveform to its
   baseline wander. If the ER is more than a given threshold, the baseline
   wander can be removed only by cubic spline estimation; otherwise it must
   be filtered by, in sequence, discrete Meyer wavelet filter and the cubic
   spline estimation. Compared with traditional methods such as cubic
   spline estimation, morphology filter and Linear-phase finite impulse
   response (FIR) least-squares-error digital filter, the experimental
   results on 50 simulated and 500 real pulse signals demonstrate the power
   of CAF filter both in removing baseline wander and in preserving the
   diagnostic information of pulse waveforms. This CAF filter also can be
   used to remove the baseline wander of other physiological signals, such
   as ECG and so on. (c) 2006 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.compbiomed.2006.06.014}},
ISSN = {{0010-4825}},
ResearcherID-Numbers = {{XU, Lisheng/C-2974-2008
   Zhang, David/O-9396-2016}},
ORCID-Numbers = {{Zhang, David/0000-0002-5027-5286}},
Unique-ID = {{ISI:000246166500014}},
}

@article{ ISI:000246378700013,
Author = {Zhou, Yongxin and Bai, Jing},
Title = {{Multiple abdominal organ segmentation: An atlas-based fuzzy
   connectedness approach}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE}},
Year = {{2007}},
Volume = {{11}},
Number = {{3}},
Pages = {{348-352}},
Month = {{MAY}},
Abstract = {{Organ segmentation is an important step in various medical image
   applications. In this paper, a presegmented atlas is incorporated into
   the fuzzy connectedness (FC) framework for automatic segmentation of
   abdominal organs. First, the atlas is registered onto the subject to
   provide an initial segmentation. Then, a novel method is applied to
   estimate the necessary FC parameters such as organ intensity features,
   seeds, and optimal FC threshold automatically and subject adaptively. In
   order to overcome the intensity overlapping between the neighboring
   organs, a shape modification approach based on Euclidean distance and
   watershed segmentation is used. This atlas-based segmentation method has
   been tested on some abdominal CT and MRI images from Chinese patients.
   Experimental results indicate the validity of this segmentation method
   for various image modalities.}},
DOI = {{10.1109/TITB.2007.892695}},
ISSN = {{1089-7771}},
EISSN = {{1558-0032}},
Unique-ID = {{ISI:000246378700013}},
}

@article{ ISI:000246040200004,
Author = {Braunewell, Stefan and Bornholdt, Stefan},
Title = {{Superstability of the yeast cell-cycle dynamics: Ensuring causality in
   the presence of biochemical stochasticity}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2007}},
Volume = {{245}},
Number = {{4}},
Pages = {{638-643}},
Month = {{APR 21}},
Abstract = {{Gene regulatory dynamics are governed by molecular processes and
   therefore exhibits an inherent stochasticity. However, for the survival
   of an organism it is a strict necessity that this intrinsic noise does
   not prevent robust functioning of the system. It is still an open
   question how dynamical stability is achieved in biological systems
   despite the omnipresent fluctuations. In this paper we investigate the
   cell cycle of the budding yeast Saccharomyces cerevisiae as an example
   of a well-studied organism. We study a genetic network model of 11 genes
   that coordinate the cell-cycle dynamics using a modeling framework which
   generalizes the concept of discrete threshold dynamics. By allowing for
   fluctuations in the process times, we introduce noise into the model,
   accounting for the effects of biochemical stochasticity. We study the
   dynamical attractor of the cell cycle and find a remarkable robustness
   against fluctuations of this kind. We identify mechanisms that ensure
   reliability in spite of fluctuations: `Catcher states' and persistence
   of activity levels contribute significantly to the stability of the
   yeast cell cycle despite the inherent stochasticity. (c) 2006 Elsevier
   Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2006.11.012}},
ISSN = {{0022-5193}},
Unique-ID = {{ISI:000246040200004}},
}

@article{ ISI:000244774700023,
Author = {Sikorsky, Jan A. and Primerano, Donald A. and Fenger, Terry W. and
   Denvir, James},
Title = {{DNA damage reduces Taq DNA polymerase fidelity and PCR amplification
   efficiency}},
Journal = {{BIOCHEMICAL AND BIOPHYSICAL RESEARCH COMMUNICATIONS}},
Year = {{2007}},
Volume = {{355}},
Number = {{2}},
Pages = {{431-437}},
Month = {{APR 6}},
Abstract = {{DNA damage blocks DNA polymerase progression and increases miscoding. In
   this study, we assessed the effects of specific lesions on Taq DNA
   polymerase fidelity and amplification efficiency. In the presence of
   8-oxo-7,8-dihydro-2'-deoxyguanosine (8-oxodG), Taq DNA polymerase
   inserted dCMP and to a lesser extent dAMP.
   8-Oxo-7,8-dihydro-2'-deoxyadenosine (8-oxodA) instructed the
   incorporation of dTMP and caused a pronounced n - 1 deletion not
   observed in other systems. The presence of an abasic lesion led to dAMP
   incorporation and n - 1 deletions. In addition, we introduce the mean
   modified efficiency (MME) as a more precise method for determining PCR
   amplification efficiency of damaged templates. Using this method, we
   were able to quantify reductions in amplification efficiency of
   templates containing 8-oxodG (single or multiple), 8-oxodA, or abasic
   sites. Because the MME method can detect small reductions in
   amplification efficiency, it may be useful in comparing the extent of
   damage in environmentally degraded or archival DNA specimens. (c) 2007
   Elsevier Inc. All rights reserved.}},
DOI = {{10.1016/j.bbrc.2007.01.169}},
ISSN = {{0006-291X}},
Unique-ID = {{ISI:000244774700023}},
}

@article{ ISI:000245484900001,
Author = {Hamilton, Nicholas A. and Pantelic, Radosav S. and Hanson, Kelly and
   Teasdale, Rohan D.},
Title = {{Fast automated cell phenotype image classification}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2007}},
Volume = {{8}},
Month = {{MAR 30}},
Abstract = {{Background: The genomic revolution has led to rapid growth in sequencing
   of genes and proteins, and attention is now turning to the function of
   the encoded proteins. In this respect, microscope imaging of a protein's
   sub-cellular localisation is proving invaluable, and recent advances in
   automated fluorescent microscopy allow protein localisations to be
   imaged in high throughput. Hence there is a need for large scale
   automated computational techniques to efficiently quantify, distinguish
   and classify sub-cellular images. While image statistics have proved
   highly successful in distinguishing localisation, commonly used measures
   suffer from being relatively slow to compute, and often require cells to
   be individually selected from experimental images, thus limiting both
   throughput and the range of potential applications. Here we introduce
   threshold adjacency statistics, the essence which is to threshold the
   image and to count the number of above threshold pixels with a given
   number of above threshold pixels adjacent. These novel measures are
   shown to distinguish and classify images of distinct sub-cellular
   localization with high speed and accuracy without image cropping.
   Results: Threshold adjacency statistics are applied to classification of
   protein sub-cellular localization images. They are tested on two image
   sets (available for download), one for which fluorescently tagged
   proteins are endogenously expressed in 10 sub-cellular locations, and
   another for which proteins are transfected into 11 locations. For each
   image set, a support vector machine was trained and tested.
   Classification accuracies of 94.4\% and 86.6\% are obtained on the
   endogenous and transfected sets, respectively. Threshold adjacency
   statistics are found to provide comparable or higher accuracy than other
   commonly used statistics while being an order of magnitude faster to
   calculate. Further, threshold adjacency statistics in combination with
   Haralick measures give accuracies of 98.2\% and 93.2\% on the endogenous
   and transfected sets, respectively.
   Conclusion: Threshold adjacency statistics have the potential to greatly
   extend the scale and range of applications of image statistics in
   computational image analysis. They remove the need for cropping of
   individual cells from images, and are an order of magnitude faster to
   calculate than other commonly used statistics while providing comparable
   or better classification accuracy, both essential requirements for
   application to large-scale approaches.}},
DOI = {{10.1186/1471-2105-8-110}},
Article-Number = {{110}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Hamilton, Nicholas/A-6033-2010
   Teasdale, Rohan/B-2538-2009}},
ORCID-Numbers = {{Hamilton, Nicholas/0000-0003-0331-3427
   Teasdale, Rohan/0000-0001-7455-5269}},
Unique-ID = {{ISI:000245484900001}},
}

@article{ ISI:000245511800006,
Author = {Brodzik, Andrzej K.},
Title = {{Quaternionic periodicity transform: an algebraic solution to the tandem
   repeat detection problem}},
Journal = {{BIOINFORMATICS}},
Year = {{2007}},
Volume = {{23}},
Number = {{6}},
Pages = {{694-700}},
Month = {{MAR 15}},
Abstract = {{Motivation: One of the main tasks of DNA sequence analysis is
   identification of repetitive patterns. DNA symbol repetitions play a key
   role in a number of applications, including prediction of gene and exon
   locations, identification of diseases, reconstruction of human
   evolutionary history and DNA forensics.
   Results: A new approach towards identification of tandem repeats in DNA
   sequences is proposed. The approach is a refinement of previously
   considered method, based on the complex periodicity transform. The
   refinement is obtained, among others, by mapping of DNA symbols to pure
   quaternions. This mapping results in an enhanced, symbol-balanced
   sensitivity of the transform to DNA patterns, and an unambiguous
   threshold selection criterion. Computational efficiency of the transform
   is further improved, and coupling of the computation with the period
   value is removed, thereby facilitating parallel implementation of the
   algorithm. Additionally, a post-processing stage is inserted into the
   algorithm, enabling unambiguous display of results in a convenient
   graphical format. Comparison of the quaternionic periodicity transform
   with two well-known pattern detection techniques shows that the new
   approach is competitive with these two techniques in detection of exact
   and approximate repeats.}},
DOI = {{10.1093/bioinformatics/btl674}},
ISSN = {{1367-4803}},
ORCID-Numbers = {{, andrzej/0000-0001-5077-7561}},
Unique-ID = {{ISI:000245511800006}},
}

@article{ ISI:000244839800008,
Author = {Samia, Noelle I. and Chan, Kung-Sik and Stenseth, Nils Chr.},
Title = {{A generalized threshold mixed model for analyzing nonnormal nonlinear
   time series, with application to plague in Kazakhstan}},
Journal = {{BIOMETRIKA}},
Year = {{2007}},
Volume = {{94}},
Number = {{1}},
Pages = {{101-118}},
Month = {{MAR}},
Abstract = {{We introduce the generalized threshold mixed model for piecewise-linear
   stochastic regression with possibly nonnormal time-series data. It is
   assumed that the conditional probability distribution of the response
   variable belongs to the exponential family, and the conditional mean
   response is linked to some piecewise-linear stochastic regression
   function. We study the particular case where the response variable
   equals zero in the lower regime. Some large-sample properties of a
   likelihood-based estimation scheme are derived. Our approach is
   motivated by the need for modelling nonlinearity in serially correlated
   epizootic events. Data coming from monitoring conducted in a natural
   plague focus in Kazakhstan are used to illustrate this model by
   obtaining biologically meaningful conclusions regarding the threshold
   relationship between prevalence of plague and some covariates including
   past abundance of great gerbils and other climatic variables.}},
DOI = {{10.1093/biomet/asm006}},
ISSN = {{0006-3444}},
ResearcherID-Numbers = {{Stenseth, Nils Chr./G-5212-2016}},
ORCID-Numbers = {{Stenseth, Nils Chr./0000-0002-1591-5399}},
Unique-ID = {{ISI:000244839800008}},
}

@article{ ISI:000244469500003,
Author = {Pinto, Francisco R. and Carrico, Joao A. and Ramirez, Mario and Almeida,
   Jonas S.},
Title = {{Ranked Adjusted Rand: integrating distance and partition information in
   a measure of clustering agreement}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2007}},
Volume = {{8}},
Month = {{FEB 7}},
Abstract = {{Background: Biological information is commonly used to cluster or
   classify entities of interest such as genes, conditions, species or
   samples. However, different sources of data can be used to classify the
   same set of entities and methods allowing the comparison of the
   performance of two data sources or the determination of how well a given
   classification agrees with another are frequently needed, especially in
   the absence of a universally accepted ``gold standard{''}
   classification.
   ]Results: Here, we describe a novel measure-the Ranked Adjusted Rand
   (RAR) index. RAR differs from existing methods by evaluating the extent
   of agreement between any two groupings, taking into account the
   intercluster distances. This characteristic is relevant to evaluate
   cases of pairs of entities grouped in the same cluster by one method and
   separated by another. The latter method may assign them to close
   neighbour clusters or, on the contrary, to clusters that are far apart
   from each other. RAR is applicable even when intercluster distance
   information is absent for both or one of the groupings. In the first
   case, RAR is equal to its predecessor, Adjusted Rand ( HA) index.
   Artificially designed clusterings were used to demonstrate situations in
   which only RAR was able to detect differences in the grouping patterns.
   A study with larger simulated clusterings ensured that in realistic
   conditions, RAR is effectively integrating distance and partition
   information. The new method was applied to biological examples to
   compare 1) two microbial typing methods, 2) two gene regulatory network
   distances and 3) microarray gene expression data with pathway
   information. In the first application, one of the methods does not
   provide intercluster distances while the other originated a hierarchical
   clustering. RAR proved to be more sensitive than HA in the choice of a
   threshold for defining clusters in the hierarchical method that
   maximizes agreement between the results of both methods.
   Conclusion: RAR has its major advantage in combining cluster distance
   and partition information, while the previously available methods used
   only the latter. RAR should be used in the research problems were HA was
   previously used, because in the absence of inter cluster distance
   effects it is an equally effective measure, and in the presence of
   distance effects it is a more complete one.}},
DOI = {{10.1186/1471-2105-44}},
Article-Number = {{44}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Ramirez, Mario/B-4993-2008
   Pinto, Francisco/A-7443-2008
   PTMS, RNEM/C-1589-2014
   Carrico, Joao/A-7367-2008}},
ORCID-Numbers = {{Ramirez, Mario/0000-0002-4084-6233
   Pinto, Francisco/0000-0002-4217-0054
   Carrico, Joao/0000-0002-5274-2722}},
Unique-ID = {{ISI:000244469500003}},
}

@article{ ISI:000208317200024,
Author = {Bridges, Susan M. and Magee, G. Bryce and Wang, Nan and Williams, W.
   Paul and Burgess, Shane C. and Nanduri, Bindu},
Title = {{ProtQuant: a tool for the label-free quantification of MudPIT proteomics
   data}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2007}},
Volume = {{8}},
Number = {{7}},
Abstract = {{Background: Effective and economical methods for quantitative analysis
   of high throughput mass spectrometry data are essential to meet the
   goals of directly identifying, characterizing, and quantifying proteins
   from a particular cell state. Multidimensional Protein Identification
   Technology (MudPIT) is a common approach used in protein identification.
   Two types of methods are used to detect differential protein expression
   in MudPIT experiments: those involving stable isotope labelling and the
   so-called label-free methods. Label-free methods are based on the
   relationship between protein abundance and sampling statistics such as
   peptide count, spectral count, probabilistic peptide identification
   scores, and sum of peptide Sequest XCorr scores (Sigma XCorr). Although
   a number of label-free methods for protein quantification have been
   described in the literature, there are few publicly available tools that
   implement these methods. We describe ProtQuant, a Java-based tool for
   label-free protein quantification that uses the previously published
   Sigma XCorr method for quantification and includes an improved method
   for handling missing data.
   Results: ProtQuant was designed for ease of use and portability for the
   bench scientist. It implements the Sigma XCorr method for label free
   protein quantification from MudPIT datasets. ProtQuant has a graphical
   user interface, accepts multiple file formats, is not limited by the
   size of the input files, and can process any number of replicates and
   any number of treatments. In addition, ProtQuant implements a new method
   for dealing with missing values for peptide scores used for
   quantification. The new algorithm, called Sigma XCorr{*}, uses ``below
   threshold{''} peptide scores to provide meaningful non-zero values for
   missing data points. We demonstrate that Sigma XCorr{*} produces an
   average reduction in false positive identifications of differential
   expression of 25\% compared to Sigma XCorr.
   Conclusion: ProtQuant is a tool for protein quantification built for
   multi-platform use with an intuitive user interface. ProtQuant
   efficiently and uniquely performs label-free quantification of protein
   datasets produced with Sequest and provides the user with facilities for
   data management and analysis. Importantly, ProtQuant is available as a
   self-installing executable for the Windows environment used by many
   bench scientists.}},
DOI = {{10.1186/1471-2105-8-S7-S24}},
Article-Number = {{S24}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000208317200024}},
}

@incollection{ ISI:000270717700001,
Author = {Giraldi, Gilson A. and Silva, Rodrigo L. S. and Rodrigues, Paulo S. S.
   and Jimenez, Walter and Strauss, Edilberto and Oliveira, Antonio A. F.
   and Suri, Jasjit S.},
Editor = {{Suri, JS and Farag, AA}},
Title = {{T-SURFACES FRAMEWORK FOR OFFSET GENERATION AND SEMIAUTOMATIC 3D
   SEGMENTATION}},
Booktitle = {{DEFORMABLE MODELS: THEORY AND BIOMATERIAL APPLICATIONS}},
Series = {{Topics in Biomedical Engineering}},
Year = {{2007}},
Pages = {{1-29}},
Abstract = {{This chapter describes a new approach that integrates the T-Surfaces
   model and isosurface generation methods in a general framework for
   segmentation and surface reconstruction in 3D medical images. Besides,
   the T-Surfaces model is applied for offset generation in the context of
   geometry extraction. T-Surfaces is a parametric deformable model based
   on a triangulation of the image domain, a discrete surface model, and an
   image threshold. Two types of isosurface generation methods are
   considered in this work: continuation and marching. The continuation
   approach is useful during reparameterization of T-Surfaces, while the
   latter is suitable to initialize the model closer the boundary. First,
   the T-Surfaces grid and the threshold are used to define a coarser image
   resolution. This field is thresholded to obtained a 0-1 function that is
   processed by a marching method to generate polygonal surfaces whose
   interior may contain the desired objects. If a polygonal surface
   involves more than one object, the resolution is increased in that
   specific region, and the marching procedure is applied again. Next, we
   apply T-Surfaces to improve the result. If the obtained topology remains
   incorrect, we enable the user to modify the topology by an interactive
   method based on the T-Surfaces framework. Finally, we discuss the
   utility of diffusion methods and implicit deformable models for our
   approach.}},
ISSN = {{1568-1033}},
ISBN = {{978-0-387-68343-0}},
Unique-ID = {{ISI:000270717700001}},
}

@article{ ISI:000243191400005,
Author = {Ichiyanagi, Kenji and Nakajima, Ryo and Kajikawa, Masaki and Okada,
   Norihiro},
Title = {{Novel retrotransposon analysis reveals multiple mobility pathways
   dictated by hosts}},
Journal = {{GENOME RESEARCH}},
Year = {{2007}},
Volume = {{17}},
Number = {{1}},
Pages = {{33-41}},
Month = {{JAN}},
Abstract = {{Autonomous non-long-terminal-repeat retrotransposons (NLRs) proliferate
   by retrotransposition via coordinated reactions of target DNA cleavage
   and reverse transcription by a mechanism called target-primed reverse
   transcription (TPRT). Whereas this mechanism guarantees the covalent
   attachment of the NLR and its target site at the 3' junction, mechanisms
   for the joining at the 5' junction have been conjectural. To better
   understand the retrotransposition pathways, we analyzed target - NLR
   junctions of zebrafish NLRs with a new method of identifying genomic
   copies that reside within other transposons, termed ``target analysis of
   nested transposons{''} (TANT). Application of the TANT method revealed
   various features of the zebrafish NLR integrants; for example, half of
   the integrants carry extra nucleotides at the 5' junction, which is in
   stark contrast to the major human NLR, LINE-1. Interestingly, in a cell
   culture assay, retrotransposition of the zebrafish NLR in heterologous
   human cells did not bear extra 5' nucleotides, indicating that the
   choice of the 5' joining pathway is affected by the host. Our results
   suggest that several pathways exist for NLR retrotransposition and argue
   in favor of host protein involvement. With genomic sequence information
   accumulating exponentially, our data demonstrate the general
   applicability of the TANT method for the analysis of a wide variety of
   retrotransposons.}},
DOI = {{10.1101/gr.5542607}},
ISSN = {{1088-9051}},
ResearcherID-Numbers = {{Ichiyanagi, Kenji/J-2141-2015
   }},
ORCID-Numbers = {{Ichiyanagi, Kenji/0000-0002-6831-6525}},
Unique-ID = {{ISI:000243191400005}},
}

@article{ ISI:000242825300006,
Author = {Inaba, Hisashi},
Title = {{Age-structured homogeneous epidemic systems with application to the
   MSEIR epidemic model}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2007}},
Volume = {{54}},
Number = {{1}},
Pages = {{101-146}},
Month = {{JAN}},
Abstract = {{In this paper, we develop a new approach to deal with asymptotic
   behavior of the age-structured homogeneous epidemic systems and discuss
   its application to the MSEIR epidemic model. For the homogeneous system,
   there is no attracting nontrivial equilibrium, instead we have to
   examine existence and stability of persistent solutions. Assuming that
   the host population dynamics can be described by the stable population
   model, we rewrite the basic system into the system of ratio age
   distribution, which is the age profile divided by the stable age
   profile. If the host population has the stable age profile, the ratio
   age distribution system is reduced to the normalized system. Then we
   prove the stability principle that the local stability or instability of
   steady states of the normalized system implies that of the corresponding
   persistent solutions of the original homogeneous system. In the latter
   half of this paper, we prove the threshold and stability results for the
   normalized system of the age-structured MSEIR epidemic model.}},
DOI = {{10.1007/s00285-006-0033-y}},
ISSN = {{0303-6812}},
EISSN = {{1432-1416}},
Unique-ID = {{ISI:000242825300006}},
}

@article{ ISI:000243621200004,
Author = {Konings, Maurits K.},
Title = {{A new method for spatially selective, non-invasive activation of
   neurons: concept and computer simulation}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2007}},
Volume = {{45}},
Number = {{1}},
Pages = {{7-24}},
Month = {{JAN}},
Abstract = {{Currently available non-invasive neurostimulation devices, using skin
   electrodes or externally applied magnetic coils, are not capable of
   producing a local stimulation maximum deep inside a homogeneous
   conductor, because of a fundamental limitation inherent to the Laplace
   equation. In this paper, a new neurostimulation method (the DeepFocus
   method) is presented, which avoids this limitation by using an indirect
   method of producing electric currents inside tissues: First,
   cylinder-shaped ferromagnetic rotating disks of non-permanent magnetic
   material are placed near the skin and magnetized by a non-rotating
   magnetic coil. Each of the disks rotates at high speed around its own
   axis of symmetry, thus producing a purely electric Lorentz force field
   having a non-zero divergence outside the disk, and therefore giving rise
   to charge accumulations inside the tissues. Subsequently, the magnetic
   field is switched off suddenly, causing a re-distribution of charge, and
   hence short-lived electrical currents, which can be used to activate
   neurons. Two magnet configurations are presented in this paper, and
   analyzed by computer simulation, showing that the DeepFocus method
   produces a maximum current density (the `focus') deep inside the
   conducting body. The field strength thus created in the focus (7.9 V/m)
   is strong enough to activate thick myelinated fibers, but can be kept
   below the threshold for C-fibers, which makes the new method a possible
   tool for pain mitigation by targeted neurostimulation.}},
DOI = {{10.1007/s11517-006-0136-z}},
ISSN = {{0140-0118}},
EISSN = {{1741-0444}},
ResearcherID-Numbers = {{Konings, Maurits/E-8664-2013}},
Unique-ID = {{ISI:000243621200004}},
}

@article{ ISI:000243621200009,
Author = {Fisher, A. C. and El-Deredy, W. and Hagan, R. P. and Brown, M. C. and
   Lisboa, P. J. G.},
Title = {{Removal of eye movement artefacts from single channel recordings of
   retinal evoked potentials using synchronous dynamical embedding and
   independent component analysis}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{2007}},
Volume = {{45}},
Number = {{1}},
Pages = {{69-77}},
Month = {{JAN}},
Abstract = {{A system is described for the removal of eye movement and blink
   artefacts from single channel pattern reversal electroretinogram
   recordings of very poor signal-to-noise ratios. Artefacts are detected
   and removed by using a blind source separation technique based on the
   jadeR independent component analysis algorithm. The single channel data
   are arranged as a series of overlapping time-delayed vectors forming a
   dynamical embedding matrix. The structure of this matrix is constrained
   to the phase of the stimulation epoch: the term synchronous dynamical
   embedding is coined. A novel method using a marker channel with a
   non-independent synchronous feature is employed to identify the single
   most relevant source estimation for reconstruction and signal recovery.
   This method is non-lossy, all underlying signal being recovered. In
   synthetic datasets of defined noise content and in standardised real
   data recordings, the performance of this technique is compared to
   conventional fixed-threshold hard-limit rejection. The most significant
   relative improvements are achieved when movement and blink artefacts are
   greatest: no improvement is demonstrable for the random noise only
   situation.}},
DOI = {{10.1007/s11517-006-0123-4}},
ISSN = {{0140-0118}},
ResearcherID-Numbers = {{El-deredy, Wael/H-6521-2015}},
ORCID-Numbers = {{El-deredy, Wael/0000-0002-9822-1092}},
Unique-ID = {{ISI:000243621200009}},
}

@article{ ISI:000244894400004,
Author = {Farias, Ariel A. and Jaksic, Fabian M.},
Title = {{Assessing the relative contribution of functional divergence and guild
   aggregation to overall functional structure of species assemblages}},
Journal = {{ECOLOGICAL INFORMATICS}},
Year = {{2006}},
Volume = {{1}},
Number = {{4}},
Pages = {{367-375}},
Month = {{DEC}},
Abstract = {{The study of functional structure in species assemblages emphasizes the
   detection of significant guild aggregation patterns. Thus, protocols
   based on intensive resampling of empirical data have been proposed to
   assess guild structure. Such protocols obtain the frequency distribution
   of a given functional similarity metric, and identify a threshold value
   (often the 95th percentile) beyond which clusters in a functional
   dendrogram are considered as significant guilds (using one-tailed
   tests). An alternative approach sequentially searches for significant
   differences between clusters at decreasing levels of similarity in a
   dendrogram until one is detected, then assumes that all subsequent nodes
   should also be significant. Nevertheless, these protocols do not test
   both the significance and sign of deviations from random at all levels
   of functional similarity within a dendrogram. Here, we propose a new
   bootstrapping approach that: (1) overcomes such pitfalls by performing
   two-tailed tests for each node in a dendrogram of functional similarity
   after separately determining their respective sample distributions, and
   (2) enables the quantification of the relative contribution of guild
   aggregation and functional divergence to the overall functional
   structure of the entire assemblage. We exemplify this approach by using
   long-term data on guild dynamics in a vertebrate predator assemblage of
   central Chile. Finally, we illustrate how the interpretation of
   functional structure is improved by applying this new approach to the
   data set available. (c) 2006 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.ecoinf.2006.09.003}},
ISSN = {{1574-9541}},
ResearcherID-Numbers = {{jaksic, Fabian/G-1705-2014
   }},
ORCID-Numbers = {{Farias, Ariel/0000-0001-7510-2253}},
Unique-ID = {{ISI:000244894400004}},
}

@article{ ISI:000241005500003,
Author = {Koop, Gary and Tole, Lise},
Title = {{An investigation of thresholds in air pollution-mortality effects}},
Journal = {{ENVIRONMENTAL MODELLING \& SOFTWARE}},
Year = {{2006}},
Volume = {{21}},
Number = {{12}},
Pages = {{1662-1673}},
Month = {{DEC}},
Abstract = {{In this paper we introduce and implement new techniques to investigate
   threshold effects in air pollution-mortality relationships. Our key
   interest is in measuring the dose-response relationship above and below
   a given threshold level where we allow for a large number of potential
   explanatory variables to trigger the threshold effect. This is in
   contrast to existing approaches that usually focus on a single threshold
   trigger. We allow for a myriad of threshold effects within a Bayesian
   statistical framework that accounts for model uncertainty (i.e.
   uncertainty about which threshold trigger and explanatory variables are
   appropriate). We apply these techniques in an empirical exercise using
   daily data from Toronto for 1992-1997. We investigate the existence and
   nature of threshold effects in the relationship between mortality and
   ozone (03), total particulate matter (PM) and an index of other
   conventionally occurring air pollutants. In general, we find the effects
   of the pollutants we consider on mortality to be statistically
   indistinguishable from zero with no evidence of thresholds. The one
   exception is ozone, for which results present an ambiguous picture.
   Ozone has no significant effect on mortality when we exclude threshold
   effects from the analysis. Allowing for thresholds we find a positive
   and significant effect for this pollutant when the threshold trigger is
   the average change in ozone two days ago. However, this significant
   effect is not observed after controlling for PM. (c) 2005 Elsevier Ltd.
   All rights reserved.}},
DOI = {{10.1016/j.envsoft.2005.07.012}},
ISSN = {{1364-8152}},
EISSN = {{1873-6726}},
ResearcherID-Numbers = {{Wang, Linden/M-6617-2014
   }},
ORCID-Numbers = {{Koop, Gary/0000-0002-6091-378X}},
Unique-ID = {{ISI:000241005500003}},
}

@article{ ISI:000243482000005,
Author = {de Ridder, Jeroen and Uren, Anthony and Kool, Jaap and Reinders, Marcel
   and Wessels, Lodewyk},
Title = {{Detecting statistically significant common insertion sites in retroviral
   insertional mutagenesis screens}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2006}},
Volume = {{2}},
Number = {{12}},
Pages = {{1530-1542}},
Month = {{DEC}},
Abstract = {{Retroviral insertional mutagenesis screens, which identify genes
   involved in tumor development in mice, have yielded a substantial number
   of retroviral integration sites, and this number is expected to grow
   substantially due to the introduction of high-throughput screening
   techniques. The data of various retroviral insertional mutagenesis
   screens are compiled in the publicly available Retroviral Tagged Cancer
   Gene Database (RTCGD). Integrally analyzing these screens for the
   presence of common insertion sites (CISs, i.e., regions in the genome
   that have been hit by viral insertions in multiple independent tumors
   significantly more than expected by chance) requires an approach that
   corrects for the increased probability of finding false CISs as the
   amount of available data increases. Moreover, significance estimates of
   CISs should be established taking into account both the noise, arising
   from the random nature of the insertion process, as well as the bias,
   stemming from preferential insertion sites present in the genome and the
   data retrieval methodology. We introduce a framework, the kernel
   convolution (KC) framework, to find CISs in a noisy and biased
   environment using a predefined significance level while controlling the
   family-wise error (FWE) (the probability of detecting false CISs). Where
   previous methods use one, two, or three predetermined fixed scales, our
   method is capable of operating at any biologically relevant scale. This
   creates the possibility to analyze the CISs in a scale space by varying
   the width of the CISs, providing new insights in the behavior of CISs
   across multiple scales. Our method also features the possibility of
   including models for background bias. Using simulated data, we evaluate
   the KC framework using three kernel functions, the Gaussian, triangular,
   and rectangular kernel function. We applied the Gaussian KC to the data
   from the combined set of screens in the RTCGD and found that 53\% of the
   CISs do not reach the significance threshold in this combined setting.
   Still, with the FWE under control, application of our method resulted in
   the discovery of eight novel CISs, which each have a probability less
   than 5\% of being false detections.}},
DOI = {{10.1371/journal.pcbi.0020166}},
Article-Number = {{e166}},
ISSN = {{1553-734X}},
ORCID-Numbers = {{de Ridder, Jeroen/0000-0002-0828-3477}},
Unique-ID = {{ISI:000243482000005}},
}

@article{ ISI:000242429400008,
Author = {Yun, Sungcheol and Lee, Youngjo},
Title = {{Robust estimation in mixed linear models with non-monotone missingness}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2006}},
Volume = {{25}},
Number = {{22}},
Pages = {{3877-3892}},
Month = {{NOV 30}},
Abstract = {{We introduce a model to account for abrupt changes among repeated
   measures with non-monotone missingness. Development of likelihood
   inferences for such models is hard because it involves intractable
   integration to obtain the marginal likelihood. We use hierarchical
   likelihood to overcome such difficulty. Abrupt changes among repeated
   measures can be well described by introducing random effects in the
   dispersion. A simulation study shows that the resulting estimator is
   efficient, robust against misspecification of fatness of tails. For
   illustration we use a schizophrenic behaviour data presented by Rubin
   and Wu. Copyright (c) 2005 John Wiley \& Sons, Ltd.}},
DOI = {{10.1002/sim.2479}},
ISSN = {{0277-6715}},
EISSN = {{1097-0258}},
Unique-ID = {{ISI:000242429400008}},
}

@article{ ISI:000240259500010,
Author = {Mueller, Johannes and Kuttler, Christina and Hense, Burkard A. and
   Rothballer, Michael and Hartmann, Anton},
Title = {{Cell-cell communication by quorum sensing and dimension-reduction}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2006}},
Volume = {{53}},
Number = {{4}},
Pages = {{672-702}},
Month = {{OCT}},
Abstract = {{Several bacterial taxa change their behavior if the population density
   exceeds a certain threshold. This phenomenon is the consequence of a
   communication system between the bacteria and is called quorum sensing
   (QS). Up to now, this phenomenon is mostly modeled at population level.
   However, new experimental techniques allow for single cell analysis. We
   introduce a modeling approach for the description of this QS system,
   including a discussion of the regulatory network and its bistable
   behavior. Based on this single-cell model we develop and analyze a
   spatially structured model for a cell population. Special attention is
   given to the scaling behavior w.r.t. the cell size (leading to an
   approximation theorem for stationary solutions) and its consequences for
   the interpretation of cell communication (QS versus diffusion sensing).
   Concluding, we apply the modeling approach to spatially structured
   experimental data.}},
DOI = {{10.1007/s00285-006-0024-z}},
ISSN = {{0303-6812}},
ResearcherID-Numbers = {{Rothballer, Michael/B-5219-2015
   Hartmann, Anton/D-8070-2013}},
ORCID-Numbers = {{Rothballer, Michael/0000-0003-4881-2705
   }},
Unique-ID = {{ISI:000240259500010}},
}

@article{ ISI:000241631400004,
Author = {Kleinman, Ken P. and Abrams, Allyson M.},
Title = {{Assessing surveillance using sensitivity, specificity and timeliness}},
Journal = {{STATISTICAL METHODS IN MEDICAL RESEARCH}},
Year = {{2006}},
Volume = {{15}},
Number = {{5}},
Pages = {{445-464}},
Month = {{OCT}},
Abstract = {{Monitoring ongoing processes of illness to detect sudden changes is an
   important aspect of practical epidemiology and medicine more generally.
   Most commonly, the monitoring has been restricted to a unidimensional
   stream of data over time. In such situations, analytic results from the
   industrial process monitoring have suggested optimal approaches to
   monitor the data streams. Data streams including spatial location as
   well as temporal sequence are becoming available. Monitoring methods
   that incorporate spatial data may prove superior to those that ignore
   it. However, analytically, optimal methods for spatial surveillance data
   may not exist. In the present article, we introduce and discuss
   evaluation metrics that can be used to compare the performance of
   statistical methods of surveillance. Our general approach is to
   generalize receiver operating characteristic (ROC) curves to incorporate
   the time of detection in addition to the usual test characteristics of
   sensitivity and specificity. In addition to weighting ordinary ROC
   curves by two measures of timeliness, we describe three
   three-dimensional generalizations of ROC curves that result in
   timeliness-ROC surfaces. Working in the context of surveillance of cases
   of disease to detect a sudden outbreak, we demonstrate these in an
   artificial example and in a previously described simulation context and
   show how the metrics differ. We also discuss the differences and under
   which circumstances one might prefer a given method.}},
DOI = {{10.1177/0962280206071641}},
ISSN = {{0962-2802}},
Unique-ID = {{ISI:000241631400004}},
}

@article{ ISI:000240588800004,
Author = {Vernikos, Georgios S. and Parkhill, Julian},
Title = {{Interpolated variable order motifs for identification of horizontally
   acquired DNA: revisiting the Salmonella pathogenicity islands}},
Journal = {{BIOINFORMATICS}},
Year = {{2006}},
Volume = {{22}},
Number = {{18}},
Pages = {{2196-2203}},
Month = {{SEP 15}},
Abstract = {{Motivation: There is a growing literature on the detection of Horizontal
   Gene Transfer (HGT) events by means of parametric, non-comparative
   methods. Such approaches rely only on sequence information and utilize
   different low and high order indices to capture compositional deviation
   from the genome backbone; the superiority of the latter over the former
   has been shown elsewhere. However even high order k-mers may be poor
   estimators of HGT, when insufficient information is available, e.g. in
   short sliding windows. Most of the current HGT prediction methods
   require pre-existing annotation, which may restrict their application on
   newly sequenced genomes.
   Results: We introduce a novel computational method, Interpolated
   Variable Order Motifs (IVOMs), which exploits compositional biases using
   variable order motif distributions and captures more reliably the local
   composition of a sequence compared with fixed-order methods. For optimal
   localization of the boundaries of each predicted region, a second order,
   two-state hidden Markov model (HMM) is implemented in a change-point
   detection framework. We applied the IVOM approach to the genome of
   Salmonella enterica serovar Typhi CT18, a well-studied prokaryote in
   terms of HGT events, and we show that the IVOMs outperform
   state-of-the-art low and high order motif methods predicting not only
   the already characterized Salmonella Pathogenicity Islands (SPI-1 to
   SPI-10) but also three novel SPIs (SPI-15, SPI-16, SPI-17) and other HGT
   events.}},
DOI = {{10.1093/bioinformatics/btl369}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Parkhill, Julian/G-4703-2011}},
ORCID-Numbers = {{Parkhill, Julian/0000-0002-7069-5958}},
Unique-ID = {{ISI:000240588800004}},
}

@article{ ISI:000240649100002,
Author = {Regonda, Satish Kumar and Rajagopalan, Balaji and Clark, Martyn},
Title = {{A new method to produce categorical streamflow forecasts}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2006}},
Volume = {{42}},
Number = {{9}},
Month = {{SEP 12}},
Abstract = {{Categorical forecasts of streamflow are important for effective water
   resources management. Typically, these are obtained by generating
   ensemble forecasts of streamflow and counting the proportion of
   ensembles in the desired category. Here we develop a simple and direct
   method to produce categorical streamflow forecasts at multiple sites.
   The method involves predicting the probability of the leading mode (or
   principal component) of the basin streamflows above a given threshold
   and subsequently translating the predicted probabilities to all the
   sites in the basin. The categorical probabilistic forecasts are obtained
   via logistic regression using a set of large-scale climate predictors.
   Application to categorical forecasts of the spring (April-June)
   streamflows at six locations in the Gunnison River Basin exhibited
   significant long-lead forecast skill.}},
DOI = {{10.1029/2006WR004984}},
Article-Number = {{W09501}},
ISSN = {{0043-1397}},
ResearcherID-Numbers = {{Rajagopalan, Balaji/A-5383-2013
   Clark, Martyn/A-5560-2015
   }},
ORCID-Numbers = {{Rajagopalan, Balaji/0000-0002-6883-7240
   Clark, Martyn/0000-0002-2186-2625
   BALAJI, RAJAGOPALAN/0000-0003-1433-7520}},
Unique-ID = {{ISI:000240649100002}},
}

@article{ ISI:000240403300020,
Author = {Boesel, Luciano F. and Azevedo, Helena S. and Reis, Rui L.},
Title = {{Incorporation of alpha-amylase enzyme and a bioactive filler into
   hydrophilic, partially degradable, and bioactive cements (HDBCs) as a
   new approach to tailor simultaneously their degradation and bioactive
   behavior}},
Journal = {{BIOMACROMOLECULES}},
Year = {{2006}},
Volume = {{7}},
Number = {{9}},
Pages = {{2600-2609}},
Month = {{SEP 11}},
Abstract = {{Hydrophilic, partially degradable, and bioactive cements (HDBCs) are
   starch-containing cements intended to degrade partially in the human
   body and, in so doing, allow for bone ingrowth inside the pores formed
   during degradation. Therefore, the study of degradation and bioactivity
   behavior was performed to assess the suitability of the current HDBCs
   formulations to achieve those aims. The degradation profile of HDBCs was
   studied under different conditions, including incubation in
   phosphate-buffered saline (PBS) and PBS supplemented with R-amylase at
   different concentrations. Thermostable alpha-amylase was also added to
   some formulations to allow control of the degradation rate and its
   extent. In a second stage the simultaneous phenomena of enzymatic
   degradation and bioactivity ( both in vitro) was studied. We observed
   that the degradation of starch present in HDBCs can be easily controlled
   by the amount of alpha-amylase added to the cement and high values of
   degradation may be achieved if high enough quantities of enzyme are
   incorporated. However, the maximum degradation extent is much more
   dependent on the total amount of starch present in the formulation than
   on the amount of enzyme added to it: for full pore connectivity, the
   amount of starch should be higher than the percolation threshold for a
   3D specimen. Nonetheless, calcium phosphate was able to nucleate and
   spread in inner pores of the cement, formed due to degradation, if they
   were interconnected. For a more thorough covering of the pores with
   calcium phosphates the amount of starch present in HDBCs should be
   increased to be higher than the percolation threshold.}},
DOI = {{10.1021/bm060387j}},
ISSN = {{1525-7797}},
ResearcherID-Numbers = {{Reis, Rui L./A-8938-2008
   Group, 3Bs/E-4374-2012
   Azevedo, Helena/A-8298-2011}},
ORCID-Numbers = {{Reis, Rui L./0000-0002-4295-6129
   Group, 3Bs/0000-0002-5195-3456
   Azevedo, Helena/0000-0002-5470-1844}},
Unique-ID = {{ISI:000240403300020}},
}

@article{ ISI:000239033000006,
Author = {Meyer, Carsten and Fernandez Gavela, Jose and Harris, Matthew},
Title = {{Combining algorithms in automatic detection of QRS complexes in ECG
   signals}},
Journal = {{IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE}},
Year = {{2006}},
Volume = {{10}},
Number = {{3}},
Pages = {{468-475}},
Month = {{JUL}},
Note = {{18th IEEE Symposium on Computer-Based Medical Systems, Dublin, IRELAND,
   JUN 23-24, 2005}},
Organization = {{IEEE Comp Soc Tech Comm Computat Med; Trinity Coll Dublin, Dept Comp
   Sci; Sci Fdn Ireland}},
Abstract = {{QRS complex and specifically R-Peak detection is the crucial first step
   in every automatic electrocardiogram analysis. Much work has been
   carried out in this field, using various methods ranging from filtering
   and threshold methods, through wavelet methods, to neural networks and
   others.. Performance is generally good, but each method has situations
   where it fails. In this paper, we suggest an approach to automatically
   combine different QRS complex detection algorithms, here the
   Pan-Tompkins and wavelet algorithms, to benefit from the strengths of
   both methods. In particular, we introduce parameters allowing to balance
   the contribution of the individual algorithms; these parameters are
   estimated in a data-driven way. Experimental results and analysis are
   provided on the Massachusetts Institute of Technology-Beth Israel
   Hospital (MIT-BIH) Arrhythmia Database. We show that our combination
   approach outperforms both individual algorithms.}},
DOI = {{10.1109/TITB.2006.875662}},
ISSN = {{1089-7771}},
Unique-ID = {{ISI:000239033000006}},
}

@article{ ISI:000240057000004,
Author = {Barring, Lars and Holt, Tom and Linderson, Maj-Lena and Radziejewski,
   Madej and Moriondo, Marco and Palutikof, Jean P.},
Title = {{Defining dry/wet spells for point observations, observed area averages,
   and regional climate model gridboxes in Europe}},
Journal = {{CLIMATE RESEARCH}},
Year = {{2006}},
Volume = {{31}},
Number = {{1}},
Pages = {{35-49}},
Month = {{JUN 26}},
Abstract = {{A new method for optimising threshold values of dry/wet spells is
   evaluated. A set of indices is used to find the best threshold giving
   good correspondence between the frequency of dry/wet spells in Hadley
   Centre regional model (HadRM3) output, reference observations with
   predetermined thresholds, and area-averaged observations. The analyses
   focus on selected model gridboxes in 3 different European climate
   regimes (Sweden, UK, Italy), where station data are available from
   several locations. In addition, a pan-European analysis using the
   European Climate Assessment (ECA) dataset is carried out. Generally,
   there is good agreement between point observations and the corresponding
   area average using the common thresholds of 0.1 or 1.0 mm with
   observational data. Applying the optimal thresholds on the model output
   is important, as it typically results in substantially better agreement
   between the simulated and observed series of dry/wet days. The fitted
   optimal pan-European dry/wet threshold is (1) 0.47 or 0.15 mm, depending
   on model version, for the observed point data threshold of 0.1 mm, and
   (2) 1.2 or 0.56 mm, depending on model version, for the threshold of 1.0
   mm.}},
DOI = {{10.3354/cr031035}},
ISSN = {{0936-577X}},
ORCID-Numbers = {{Moriondo, Marco/0000-0002-8356-7517}},
Unique-ID = {{ISI:000240057000004}},
}

@article{ ISI:000236943100015,
Author = {Triantis, KA and Vardinoyannis, K and Tsolaki, EP and Botsaris, I and
   Lika, K and Mylonas, M},
Title = {{Re-approaching the small island effect}},
Journal = {{JOURNAL OF BIOGEOGRAPHY}},
Year = {{2006}},
Volume = {{33}},
Number = {{5}},
Pages = {{914-923}},
Month = {{MAY}},
Abstract = {{Aim To propose a new approach to the small island effect (SIE) and a
   simple mathematical procedure for the estimation of its upper limit. The
   main feature of the SIE is that below an upper size threshold an
   increase of species number with increase of area in small islands is not
   observed.
   Location Species richness patterns from different taxa and insular
   systems are analysed.
   Methods Sixteen different data sets from 12 studies are analysed. Path
   analysis was used for the estimation of the upper limit of the SIE. We
   studied each data set in order to detect whether there was a certain
   island size under which the direct effects of area were eliminated. This
   detection was carried out through the sequential exclusion of islands
   from the largest to the smallest. For the cases where an SIE was
   detected, a log-log plot of species number against area is presented.
   The relationships between habitat diversity, species number and area are
   studied within the limits of the SIE. In previous studies only area was
   used for the detection of the SIE, whereas we also encompass habitat
   diversity, a parameter with well documented influence on species
   richness, especially at small scales.
   Results An SIE was detected in six out of the 16 studied cases. The
   upper limit of the SIE varies, depending on the characteristics of the
   taxon and the archipelago under study. In general, the values of the
   upper limit of the SIE calculated according to the approach undertaken
   in our study differ from the values calculated in previous studies.
   Main conclusions Although the classical species-area models have been
   used to estimate the upper limit of the SIE, we propose that the
   detection of this phenomenon should be undertaken independently from the
   species-area relationship, so that the net effects of area are
   calculated excluding the surrogate action of area on other variables,
   such as environmental heterogeneity. The SIE appears when and where area
   ceases to influence species richness directly. There are two distinct
   SIE patterns: (1) the classical SIE where both the direct and indirect
   effects of area are eliminated and (2) the cryptic SIE where area
   affects species richness indirectly. Our approach offers the opportunity
   of studying the different factors influencing biodiversity on small
   scales more accurately. The SIE cannot be considered a general pattern
   with fixed behaviour that can be described by the same model for
   different island groups and taxa. The SIE should be recognized as a
   genuine but idiosyncratic phenomenon.}},
DOI = {{10.1111/j.1365-2699.2006.01464.x}},
ISSN = {{0305-0270}},
EISSN = {{1365-2699}},
ResearcherID-Numbers = {{Triantis, Kostas/A-1018-2009
   Mylonas, Moisis/E-8825-2011}},
Unique-ID = {{ISI:000236943100015}},
}

@article{ ISI:000236436300010,
Author = {Li, HZ and Gui, J},
Title = {{Gradient directed regularization for sparse Gaussian concentration
   graphs, with applications to inference of genetic networks}},
Journal = {{BIOSTATISTICS}},
Year = {{2006}},
Volume = {{7}},
Number = {{2}},
Pages = {{302-317}},
Month = {{APR}},
Abstract = {{Large-scale microarray gene expression data provide the possibility of
   constructing genetic networks or biological pathways. Gaussian graphical
   models have been suggested to provide an effective method for
   constructing such genetic networks. However, most of the available
   methods for constructing Gaussian graphs do not account for the sparsity
   of the networks and are computationally more demanding or infeasible,
   especially in the settings of high dimension and low sample size. We
   introduce a threshold gradient descent (TGD) regularization procedure
   for estimating the sparse precision matrix in the setting of Gaussian
   graphical models and demonstrate its application to identifying genetic
   networks. Such a procedure is computationally feasible and can easily
   incorporate prior biological knowledge about the network structure.
   Simulation results indicate that the proposed method yields a better
   estimate of the precision matrix than the procedures that fail to
   account for the sparsity of the graphs. We also present the results on
   inference of a gene network for isoprenoid biosynthesis in Arabidopsis
   thaliana. These results demonstrate that the proposed procedure can
   indeed identify biologically meaningful genetic networks based on
   microarray gene expression data.}},
DOI = {{10.1093/biostatistics/kxj008}},
ISSN = {{1465-4644}},
EISSN = {{1468-4357}},
Unique-ID = {{ISI:000236436300010}},
}

@article{ ISI:000238266600014,
Author = {Hauschild, Michael Z. and Potting, Jose and Hertel, Ole and Schoepp,
   Wolfgang and Bastrup-Birk, Annemarie},
Title = {{Spatial differentiation in the characterisation of photochemical ozone
   formation - The EDIP2003 methodology}},
Journal = {{INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT}},
Year = {{2006}},
Volume = {{11}},
Number = {{1}},
Pages = {{72-80}},
Month = {{APR}},
Abstract = {{Background, Aims and Scope. In the life cycle of a product, emissions
   take place at many different locations. The location of the source and
   its surrounding conditions influence the fate of the emitted pollutant
   and the subsequent exposure it causes. This source of variation is
   normally neglected in Life Cycle Impact Assessment (LCIA), although it
   is well known that the impacts predicted by site-generic LCIA in some
   cases differ significantly from the actual impacts. Environmental
   impacts of photochemical ozone (ground-level ozone) depend on parameters
   with a considerable geographical variability (like emission patterns and
   population densities). A spatially differentiated characterisation model
   thus seems relevant.
   Methods and Results. The European RAINS model is applied for calculation
   of site-dependent characterisation factors for Non-Methane Volatile
   Organic Compounds (NMVOCs) and nitrogen oxides (NOx) for 41 countries or
   regions within Europe, and compatible characterisation factors for
   carbon monoxide (CO) are developed based on expert judgement. These
   factors are presented for three emission years (1990, 1995 and 2010),
   and they address human health impacts and vegetation impacts in two
   separate impacts categories, derived from AOT40 and AOT60 values
   respectively. Compatible site-generic chatacterisation factors for
   NMVOC, NOx, CO and methane (CH4) are calculated as emission-weighted
   European averages to be applied on emissions for which the location is
   unknown. The site-generic and site-dependent characterisation factors
   are part of the EDIP2003 LCIA methodology. The factors are applied in a
   specific case study, and it is demonstrated how the inclusion of spatial
   differentiation may alter the results of the photochemical ozone
   characterisation of life cycle impact assessment.
   Discussion and Conclusions. Compared to traditional midpoint
   characterisation modelling, this novel approach is spatially resolved
   and comprises a larger part of the cause-effect chain including exposure
   assessment and exceeding of threshold values. This positions it closer
   to endpoint modelling and makes the results easier to interpret. In
   addition, the developed model allows inclusion of the contributions from
   NO., which are neglected when applying the traditional approaches based
   on Photochemical Ozone Creation Potentials (POCPs). The variation in
   site-dependent characterisation factors is far larger than the variation
   in POCP factors. It thus seems more important to represent the spatially
   determined variation in exposure than the difference in POCP among the
   substances.}},
DOI = {{10.1065/lca2006.04.014}},
ISSN = {{0948-3349}},
EISSN = {{1614-7502}},
ResearcherID-Numbers = {{Bastrup-Birk, Annemarie/C-5868-2011
   Hertel, Ole/L-4346-2013
   QSA, DTU/J-4787-2014
   Hauschild, Michael/G-4335-2011
   Hauschild, Michael/L-6059-2015}},
ORCID-Numbers = {{Hertel, Ole/0000-0003-0972-7735
   Hauschild, Michael/0000-0002-8331-7390}},
Unique-ID = {{ISI:000238266600014}},
}

@article{ ISI:000236114100005,
Author = {Coutinho, R and Fernandez, B and Lima, R and Meyroneinc, A},
Title = {{Discrete time piecewise affine models of genetic regulatory networks}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2006}},
Volume = {{52}},
Number = {{4}},
Pages = {{524-570}},
Month = {{APR}},
Abstract = {{We introduce simple models of genetic regulatory networks and we proceed
   to the mathematical analysis of their dynamics. The models are discrete
   time dynamical systems generated by piecewise affine contracting
   mappings whose variables represent gene expression levels. These models
   reduce to boolean networks in one limiting case of a parameter, and
   their asymptotic dynamics approaches that of a differential equation in
   another limiting case of this parameter. For intermediate values, the
   model present an original phenomenology which is argued to be due to
   delay effects. This phenomenology is not limited to piecewise affine
   model but extends to smooth nonlinear discrete time models of regulatory
   networks.
   In a first step, our analysis concerns general properties of networks on
   arbitrary graphs (characterisation of the attractor, symbolic dynamics,
   Lyapunov stability, structural stability, symmetries, etc). In a second
   step, focus is made on simple circuits for which the attractor and its
   changes with parameters are described. In the negative circuit of 2
   genes, a thorough study is presented which concern stable
   (quasi-)periodic oscillations governed by rotations on the unit circle -
   with a rotation number depending continuously and monotonically on
   threshold parameters. These regular oscillations exist in negative
   circuits with arbitrary number of genes where they are most likely to be
   observed in genetic systems with non-negligible delay effects.}},
DOI = {{10.1007/s00285-005-0359-x}},
ISSN = {{0303-6812}},
ResearcherID-Numbers = {{Coutinho, Ricardo/M-7139-2013}},
ORCID-Numbers = {{Coutinho, Ricardo/0000-0002-1348-5364}},
Unique-ID = {{ISI:000236114100005}},
}

@article{ ISI:000237495000012,
Author = {Jiang Wei and Li Xia and Guo Zheng and Li Chuanxing and Wang Lihong and
   Rao Shaoqi},
Title = {{A novel model-free approach for reconstruction of time-delayed gene
   regulatory networks}},
Journal = {{SCIENCE IN CHINA SERIES C-LIFE SCIENCES}},
Year = {{2006}},
Volume = {{49}},
Number = {{2}},
Pages = {{190-200}},
Month = {{APR}},
Abstract = {{Reconstruction of genetic networks is one of the key scientific
   challenges in functional genomics. This paper describes a novel approach
   for addressing the regulatory dependencies between genes whose
   activities can be delayed by multiple units of time. The aim of the
   proposed approach termed TdGRN (time-delayed gene regulatory networking)
   is to reversely engineer the dynamic mechanisms of gene regulations,
   which is realized by identifying the time-delayed gene regulations
   through supervised decision-tree analysis of the newly designed
   time-delayed gene expression matrix, derived from the original
   time-series microarray data. A permutation technique is used to
   determine the statistical classification threshold of a tree, from which
   a gene regulatory rule(s) is extracted. The proposed TdGRN is a
   model-free approach that attempts to learn the underlying regulatory
   rules without relying on any model assumptions. Compared with
   model-based approaches, it has several significant advantages: it
   requires neither any arbitrary threshold for discretization of gene
   transcriptional values nor the definition of the number of regulators
   (k). We have applied this novel method to the publicly available data
   for budding yeast cell cycling. The numerical results demonstrate that
   most of the identified time-delayed gene regulations have current
   biological knowledge supports.}},
DOI = {{10.1007/s11427-006-0190-7}},
ISSN = {{1006-9305}},
ORCID-Numbers = {{Rao, Shaoqi/0000-0001-7809-3700}},
Unique-ID = {{ISI:000237495000012}},
}

@article{ ISI:000235826200001,
Author = {Wu, J and Hu, ZJ and DeLisi, C},
Title = {{Gene annotation and network inference by phylogenetic profiling}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2006}},
Volume = {{7}},
Month = {{FEB 17}},
Abstract = {{Background: Phylogenetic analysis is emerging as one of the most
   informative computational methods for the annotation of genes and
   identification of evolutionary modules of functionally related genes.
   The effectiveness with which phylogenetic profiles can be utilized to
   assign genes to pathways depends on an appropriate measure of
   correlation between gene profiles, and an effective decision rule to use
   the correlate. Current methods, though useful, perform at a level well
   below what is possible, largely because performance of the latter
   deteriorates rapidly as coverage increases.
   Results: We introduce, test and apply a new decision rule, correlation
   enrichment (CE), for assigning genes to functional categories at various
   levels of resolution. Among the results are: (1) CE performs better than
   standard guilt by association (SGA, assignment to a functional category
   when a simple correlate exceeds a pre-specified threshold) irrespective
   of the number of genes assigned (i.e. coverage); improvement is greatest
   at high coverage where precision (positive predictive value) of CE is
   approximately 6-fold higher than that of SGA. (2) CE is estimated to
   allocate each of the 2918 unannotated orthologs to KEGG pathways with an
   average precision of 49\% (approximately 7-fold higher than SGA) (3) An
   estimated 94\% of the 1846 unannotated orthologs in the COG ontology can
   be assigned a function with an average precision of 0.4 or greater. (4)
   Dozens of functional and evolutionarily conserved cliques or
   quasi-cliques can be identified, many having previously unannotated
   genes.
   Conclusion: The method serves as a general computational tool for
   annotating large numbers of unknown genes, uncovering evolutionary and
   functional modules. It appears to perform substantially better than
   extant stand alone high throughout methods.}},
DOI = {{10.1186/1471-2105-7-80}},
Article-Number = {{80}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Hu, Zhenjun/B-4309-2011}},
Unique-ID = {{ISI:000235826200001}},
}

@article{ ISI:000236709300001,
Author = {He, F and Zeng, AP},
Title = {{In search of functional association from time-series microarray data
   based on the change trend and level of gene expression}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2006}},
Volume = {{7}},
Month = {{FEB 15}},
Abstract = {{Background: The increasing availability of time-series expression data
   opens up new possibilities to study functional linkages of genes.
   Present methods used to infer functional linkages between genes from
   expression data are mainly based on a point-to-point comparison. Change
   trends between consecutive time points in time- series data have been so
   far not well explored.
   Results: In this work we present a new method based on extracting main
   features of the change trend and level of gene expression between
   consecutive time points. The method, termed as trend correlation (TC),
   includes two major steps: 1, calculating a maximal local alignment of
   change trend score by dynamic programming and a change trend correlation
   coefficient between the maximal matched change levels of each gene pair;
   2, inferring relationships of gene pairs based on two statistical
   extraction procedures. The new method considers time shifts and inverted
   relationships in a similar way as the local clustering (LC) method but
   the latter is merely based on a point-to-point comparison. The TC method
   is demonstrated with data from yeast cell cycle and compared with the LC
   method and the widely used Pearson correlation coefficient (PCC) based
   clustering method. The biological significance of the gene pairs is
   examined with several large-scale yeast databases. Although the TC
   method predicts an overall lower number of gene pairs than the other two
   methods at a same p-value threshold, the additional number of gene pairs
   inferred by the TC method is considerable: e. g. 20.5\% compared with
   the LC method and 49.6\% with the PCC method for a p-value threshold of
   2.7E-3. Moreover, the percentage of the inferred gene pairs consistent
   with databases by our method is generally higher than the LC method and
   similar to the PCC method. A significant number of the gene pairs only
   inferred by the TC method are process-identity or function-similarity
   pairs or have well-documented biological interactions, including 443
   known protein interactions and some known cell cycle related regulatory
   interactions. It should be emphasized that the overlapping of gene pairs
   detected by the three methods is normally not very high, indicating a
   necessity of combining the different methods in search of functional
   association of genes from time- series data. For a p-value threshold of
   1E-5 the percentage of process-identity and function-similarity gene
   pairs among the shared part of the three methods reaches 60.2\% and
   55.6\% respectively, building a good basis for further experimental and
   functional study. Furthermore, the combined use of methods is important
   to infer more complete regulatory circuits and network as exemplified in
   this study.
   Conclusion: The TC method can significantly augment the current major
   methods to infer functional linkages and biological network and is well
   suitable for exploring temporal relationships of gene expression in
   time- series data.}},
DOI = {{10.1186/1471-2105-7-69}},
Article-Number = {{69}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{He, Feng/F-3387-2012
   }},
ORCID-Numbers = {{HE, Feng/0000-0003-2657-7361}},
Unique-ID = {{ISI:000236709300001}},
}

@article{ ISI:000234770700017,
Author = {Bach, LA and Helvik, T and Christiansen, FB},
Title = {{The evolution of n-player cooperation - threshold games and ESS
   bifurcations}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2006}},
Volume = {{238}},
Number = {{2}},
Pages = {{426-434}},
Month = {{JAN 21}},
Abstract = {{An evolutionary game of individuals cooperating to obtain it collective
   benefit is here modelled as an n-player Prisoner's Dilemma game. With
   reference to biological situations, such its group foraging, we
   introduce a threshold condition in the number of cooperators required to
   obtain the collective benefit. In the simplest version, a three-player
   game, complex behaviour appears as the replicator dynamics exhibits a
   catastrophic event separating it parameter region allowing for
   coexistence of cooperators and defectors and a region of pure defection.
   Cooperation emerges through an ESS bifurcation, and cooperators only
   thrive beyond a critical point in cost-benefit space. Moreover, a
   repelling fixed point of the dynamics acts as a barrier to the
   introduction of cooperation in defecting populations. The results
   illustrate the qualitative difference between two-player games and
   multiple player games and thus the limitations to the generality of
   conclusions from two-player games. We present a procedure to find the
   evolutionarily stable strategies in any n-player game with cost and
   benefit depending on the number of cooperators. This was previously done
   by Motro {[}1991. Co-operation and defection: playing the field and the
   ESS. J. Theor. Biol. 151, 145-154] in the special cases of convex and
   concave benefit functions and constant cost. (c) 2005 Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/j.jtbi.2005.06.007}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
ResearcherID-Numbers = {{Christiansen, Freddy/I-7478-2013
   }},
ORCID-Numbers = {{Christiansen, Freddy/0000-0002-0793-0513
   Bach, Lars A./0000-0001-8610-7834}},
Unique-ID = {{ISI:000234770700017}},
}

@article{ ISI:000235824400001,
Author = {Yang, JJ and Yang, MCK},
Title = {{An improved procedure for gene selection from microarray experiments
   using false discovery rate criterion}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2006}},
Volume = {{7}},
Month = {{JAN 11}},
Abstract = {{Background: A large number of genes usually show differential
   expressions in a microarray experiment with two types of tissues, and
   the p-values of a proper statistical test are often used to quantify the
   significance of these differences. The genes with small p-values are
   then picked as the genes responsible for the differences in the tissue
   RNA expressions. One key question is what should be the threshold to
   consider the p-values small. There is always a trade off between this
   threshold and the rate of false claims. Recent statistical literature
   shows that the false discovery rate (FDR) criterion is a powerful and
   reasonable criterion to pick those genes with differential expression.
   Moreover, the power of detection can be increased by knowing the number
   of non-differential expression genes. While this number is unknown in
   practice, there are methods to estimate it from data. The purpose of
   this paper is to present a new method of estimating this number and use
   it for the FDR procedure construction.
   Results: A combination of test functions is used to estimate the number
   of differentially expressed genes. Simulation study shows that the
   proposed method has a higher power to detect these genes than other
   existing methods, while still keeping the FDR under control. The
   improvement can be substantial if the proportion of true differentially
   expressed genes is large. This procedure has also been tested with good
   results using a real dataset.
   Conclusion: For a given expected FDR, the method proposed in this paper
   has better power to pick genes that show differentiation in their
   expression than two other well known methods.}},
DOI = {{10.1186/1471-2105-7-15}},
Article-Number = {{15}},
ISSN = {{1471-2105}},
Unique-ID = {{ISI:000235824400001}},
}

@article{ ISI:000243785300011,
Author = {Koehler, P. and Fischer, H. and Schmitt, J. and Munhoven, G.},
Title = {{On the application and interpretation of Keeling plots in paleo climate
   research - deciphering delta C-13 of atmospheric CO2 measured in ice
   cores}},
Journal = {{BIOGEOSCIENCES}},
Year = {{2006}},
Volume = {{3}},
Number = {{4}},
Pages = {{539-556}},
Abstract = {{The Keeling plot analysis is an interpretation method widely used in
   terrestrial carbon cycle research to quantify exchange processes of
   carbon between terrestrial reservoirs and the atmosphere. Here, we
   analyse measured data sets and artificial time series of the partial
   pressure of atmospheric carbon dioxide (pCO(2)) and of delta C-13 of CO2
   over industrial and glacial/interglacial time scales and investigate to
   what extent the Keeling plot methodology can be applied to longer time
   scales. The artificial time series are simulation results of the global
   carbon cycle box model BICYCLE. The signals recorded in ice cores caused
   by abrupt terrestrial carbon uptake or release loose information due to
   air mixing in the firn before bubble enclosure and limited sampling
   frequency. Carbon uptake by the ocean cannot longer be neglected for
   less abrupt changes as occurring during glacial cycles. We introduce an
   equation for the calculation of long-term changes in the isotopic
   signature of atmospheric CO2 caused by an injection of terrestrial
   carbon to the atmosphere, in which the ocean is introduced as third
   reservoir. This is a paleo extension of the two reservoir mass balance
   equations of the Keeling plot approach. It gives an explanation for the
   bias between the isotopic signature of the terrestrial release and the
   signature deduced with the Keeling plot approach for long-term
   processes, in which the oceanic reservoir cannot be neglected. These
   deduced isotopic signatures are similar (-8.6 parts per thousand) for
   steady state analyses of long-term changes in the terrestrial and marine
   biosphere which both perturb the atmospheric carbon reservoir. They are
   more positive than the delta C-13 signals of the sources, e.g. the
   terrestrial carbon pools themselves (similar to -25 parts per thousand).
   A distinction of specific processes acting on the global carbon cycle
   from the Keeling plot approach is not straightforward. In general,
   processes related to biogenic fixation or release of carbon have lower
   y-intercepts in the Keeling plot than changes in physical processes,
   however in many case they are indistinguishable (e.g. ocean circulation
   from biogenic carbon fixation).}},
ISSN = {{1726-4170}},
EISSN = {{1726-4189}},
ResearcherID-Numbers = {{Kohler, Peter/F-7293-2010
   Schmitt, Jochen/B-7893-2009
   Fischer, Hubertus/A-1211-2014}},
ORCID-Numbers = {{Kohler, Peter/0000-0003-0904-8484
   Schmitt, Jochen/0000-0003-4695-3029
   Fischer, Hubertus/0000-0002-2787-4221}},
Unique-ID = {{ISI:000243785300011}},
}

@article{ ISI:000235365600001,
Author = {Juda, L},
Title = {{The report of the US Commission on Ocean Policy: State perspectives}},
Journal = {{COASTAL MANAGEMENT}},
Year = {{2006}},
Volume = {{34}},
Number = {{1}},
Pages = {{1-16}},
Month = {{JAN-MAR}},
Abstract = {{In its final report, An Ocean Blueprint for the 21st Century, the U. S.
   Commission on Ocean Policy, created by the Oceans Act of 2000, strongly
   endorsed regional scale, ecosystem-based management of coastal/ocean
   areas. To advance such an approach the Commission made numerous
   recommendations, including suggestions for both structural change in
   government and also for change in policy. As mandated by Congress, the
   process leading to the ultimate issuance of the Commission's final
   report specifically allowed for state governors to comment on a
   preliminary version of that report. The Commission was then to take
   those comments into account before issuing its final report. Thus, state
   governors were given a rare opportunity to indicate their preferences
   and concerns on the totality of U. S. coastal/ocean governance efforts.
   It is clear that implementation of the Commission's recommendations will
   require political support from the states, both in terms of encouraging
   the Administration and Congress to adopt change and in ensuring required
   federal-state cooperation. Accordingly, it is of great importance to
   understand the views of the states on the issues addressed by the
   Commission. This article outlines and examines key concerns raised by
   the nation's governors on the proposed new approach to coastal/ocean
   governance.}},
DOI = {{10.1080/08920750500364930}},
ISSN = {{0892-0753}},
Unique-ID = {{ISI:000235365600001}},
}

@article{ ISI:000234197200005,
Author = {Diaz, J and Garcia-Herrera, R and Trigo, RM and Linares, C and Valente,
   MA and de Miguel, J and Hernandez, E},
Title = {{The impact of the summer 2003 heat wave in Iberia: how should we measure
   it?}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETEOROLOGY}},
Year = {{2006}},
Volume = {{50}},
Number = {{3}},
Pages = {{159-166}},
Month = {{JAN}},
Abstract = {{We present a new approach to improve the reliability of quantifying the
   impact of a heat wave on mortality rates. We show, for the recent
   European summer 2003 heat wave, that the use of absolute maximum
   temperature values, or number of days above a given threshold, can be
   misleading. Here, we have assessed the impact of the heat wave on
   Iberian mortality by applying a four step procedure: (1) calculating,
   for each observatory, the local maximum temperature (T (max))
   distributions, (2) calculating the corresponding 95th percentile values
   (T (threshold)), (3) locally defining extremely hot days (EHD) as those
   days on which the local threshold of the 95th percentile of the series
   is exceeded, and (4) calculating the total degrees-days (DD) of
   exceedance, by calculating the difference T (max)-T (threshold) and
   summing these values for all days above T (threshold). We show that the
   relationship between summer mortality rates and the DD index is
   non-linear and can be described by a logarithmic function, with a
   correlation coefficient of 0.78, which explains 60.6\% of the mortality
   variance (F value of 24.64, significant at P < 0.0001). Using maximum
   temperatures, no significant relationship is found with mortality,
   whereas the EHD frequency shows a significant association with
   mortality, albeit weaker than that obtained with DD.}},
DOI = {{10.1007/s00484-005-0005-8}},
ISSN = {{0020-7128}},
EISSN = {{1432-1254}},
ResearcherID-Numbers = {{De Miguel, Jesus/B-8228-2011
   Linares Gil, Cristina/F-4695-2016
   Valente, Maria Antonia/A-5535-2013
   Trigo, Ricardo/B-7044-2008
   }},
ORCID-Numbers = {{De Miguel, Jesus/0000-0003-1631-7670
   Valente, Maria Antonia/0000-0001-8040-0829
   Trigo, Ricardo/0000-0002-4183-9852
   Diaz, Julio/0000-0003-4282-4959
   GARCIA HERRERA, RICARDO FRANCISCO/0000-0002-3845-7458}},
Unique-ID = {{ISI:000234197200005}},
}

@article{ ISI:000202958400008,
Author = {Dalevi, Daniel and Dubhashi, Devdatt and Hermansson, Malte},
Title = {{A new order estimator for fixed and variable length Markov models with
   applications to DNA sequence similarity}},
Journal = {{STATISTICAL APPLICATIONS IN GENETICS AND MOLECULAR BIOLOGY}},
Year = {{2006}},
Volume = {{5}},
Abstract = {{Recently Peres and Shields discovered a new method for estimating the
   order of a stationary fixed order Markov chain. They showed that the
   estimator is consistent by proving a threshold result. While this
   threshold is valid asymptotically in the limit, it is not very useful
   for DNA sequence analysis where data sizes are moderate. In this paper
   we give a novel interpretation of the Peres-Shields estimator as a sharp
   transition phenomenon. This yields a precise and powerful estimator that
   quickly identifies the core dependencies in data. We show that it
   compares favorably to other estimators, especially in the presence of
   variable dependencies. Motivated by this last point, we extend the
   Peres-Shields estimator to Variable Length Markov Chains. We compare it
   to a well-established estimator and show that it is superior in terms of
   the predictive likelihood. We give an application to the problem of
   detecting DNA sequence similarity in plasmids.}},
Article-Number = {{8}},
ISSN = {{1544-6115}},
Unique-ID = {{ISI:000202958400008}},
}

@inproceedings{ ISI:000240364200092,
Author = {Zvara, Peter and Braas, Karen M. and May, Victor and Vizzard, Margaret
   A.},
Editor = {{Vaudry, H and Laburthe, M}},
Title = {{A role for pituitary adenylate cyclase activating polypeptide (PACAP) in
   detrusor hyperreflexia after spinal cord injury (SCI)}},
Booktitle = {{VIP, PACAP, AND RELATED PEPTIDES: FROM GENE TO THERAPY}},
Series = {{ANNALS OF THE NEW YORK ACADEMY OF SCIENCES}},
Year = {{2006}},
Volume = {{1070}},
Pages = {{622-628}},
Note = {{7th International Symposium on VIP, PACAP and Related Peptides, Rouen,
   FRANCE, SEP 11-14, 2005}},
Organization = {{Conseil Reg Haute-Normandie; Agglomerat Rouen; Inst Fed Rech
   Multidisciplinaires Peptides; Inst Natl Sante Rech Med; Municipal Rouen;
   Sci Act Haute-Normandie; Tech Chime-Biol Sante; Univ Paris 7; Univ Rouen}},
Abstract = {{Intrathecal administration of the PAC1 receptor antagonist, PACAP6-38
   (10 nM), significantly (P <= 0.05) reduced intermicturition, threshold
   and micturition pressures in chronic (3-6 weeks) spinal cord injured
   rats but intravesical administration (100-300 nM) was without effect.
   Intrathecal PACAP6-38 reduced the number and amplitude of nonvoiding
   bladder contractions observed after spinal cord injury (SCI). PACAP may
   contribute to detrusor hyperreflexia induced by SCI and PACAP
   antagonists may be a novel approach to reduce detrusor hyperreflexia
   after SCI.}},
DOI = {{10.1196/annals.1317.092}},
ISSN = {{0077-8923}},
ISBN = {{1-57331-550-8}},
ResearcherID-Numbers = {{Zvara, Peter/G-6858-2016
   }},
ORCID-Numbers = {{Zvara, Peter/0000-0002-6972-6980}},
Unique-ID = {{ISI:000240364200092}},
}

@article{ ISI:000233849400007,
Author = {Ma, SG and Huang, J},
Title = {{Regularized ROC method for disease classification and biomarker
   selection with microarray data}},
Journal = {{BIOINFORMATICS}},
Year = {{2005}},
Volume = {{21}},
Number = {{24}},
Pages = {{4356-4362}},
Month = {{DEC 15}},
Abstract = {{Motivation: An important application of microarrays is to discover
   genomic biomarkers, among tens of thousands of genes assayed, for
   disease classification. Thus there is a need for developing statistical
   methods that can efficiently use such high-throughput genomic data,
   select biomarkers with discriminant power and construct classification
   rules. The ROC (receiver operator characteristic) technique has been
   widely used in disease classification with low-dimensional biomarkers
   because (1) it does not assume a parametric form of the class
   probability as required for example in the logistic regression method;
   (2) it accommodates case-control designs and (3) it allows treating
   false positives and false negatives differently. However, due to
   computational difficulties, the ROC-based classification has not been
   used with microarray data. Moreover, the standard ROC technique does not
   incorporate built-in biomarker selection.
   Results: We propose a novel method for biomarker selection and
   classification using the ROC technique for microarray data. The proposed
   method uses a sigmoid approximation to the area under the ROC curve as
   the objective function for classification and the threshold gradient
   descent regularization method for estimation and biomarker selection.
   Tuning parameter selection based on the V-fold cross validation and
   predictive performance evaluation are also investigated. The proposed
   approach is demonstrated with a simulation study, the Colon data and the
   Estrogen data. The proposed approach yields parsimonious models with
   excellent classification performance.}},
DOI = {{10.1093/bioinformatics/bti724}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ORCID-Numbers = {{HUANG, Jian/0000-0002-5218-9269}},
Unique-ID = {{ISI:000233849400007}},
}

@article{ ISI:000233938100041,
Author = {Arponen, A and Heikkinen, RK and Thomas, CD and Moilanen, A},
Title = {{The value of biodiversity in reserve selection: Representation, species
   weighting, and benefit functions}},
Journal = {{CONSERVATION BIOLOGY}},
Year = {{2005}},
Volume = {{19}},
Number = {{6}},
Pages = {{2009-2014}},
Month = {{DEC}},
Abstract = {{The limited availability of resources for conservation has led to the
   development of many quantitative methods for selecting reserves that aim
   to maximize The biodiversity value of reserve networks, in published
   analyses, species are often considered equal, although some are in much
   greater need of protection than others. Furthermore, representation is
   usually treated as a threshold a species is either represented or not,
   but varying levels of representation over or under a given target level
   are not valued differently. We propose that a higher representation
   level should also have higher value. We introduce a framework for
   reserve selection that includes species weights and benefit functions
   for under- and overrepresentation (number of locations for each
   species). We applied the method to conservation planning for herb-rich
   forests in southern Finland. Our use of benefit functions and weighting
   changed the identity of about 50\% of the selected sites at different
   funding levels and improved the representation of rare and threatened
   species. We also identified a small area of additional land that would
   substantially enhance the existing reserve network. We suggest that
   benefit functions and species weighting should be considered as standard
   options in reserve-selection applications.}},
DOI = {{10.1111/j.1523-1739.2005.00218.x}},
ISSN = {{0888-8892}},
ResearcherID-Numbers = {{Moilanen, Atte/A-5005-2011
   Thomas, Chris/A-1460-2014
   Thomas, Chris/A-1894-2012
   }},
ORCID-Numbers = {{Thomas, Chris/0000-0003-2822-1334
   Thomas, Chris/0000-0003-2822-1334
   Arponen, Anni/0000-0002-6721-7795}},
Unique-ID = {{ISI:000233938100041}},
}

@article{ ISI:000233035200006,
Author = {Nazzareno, D and Ceccarelli, M},
Title = {{Environinformatics in ecological risk assessment of agroecosystems
   pollutant leaching}},
Journal = {{STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT}},
Year = {{2005}},
Volume = {{19}},
Number = {{4}},
Pages = {{292-300}},
Month = {{OCT}},
Abstract = {{In this paper, a novel approach for mapping leaching risk at large
   sub-regional scale under limited information is presented, with acronym
   environinformatics in ecological risk assessment. The problem consists
   into quantifying the exchange frequency of the plant available soil
   water (Narula et al. in J Geogr Inform Decision Anal 7(1)32-46, 2002),
   this frequency can be adopted as a measure indicating the nutrient and
   contaminant leaching risk for a site. Our approach is based on
   integrating soil water balance with spatial analysis tools. However, any
   decision involved in scientific risk evaluation requires the accurate
   quantification of the degree of uncertainty arising from sampling,
   modelling and interpolation errors. The non-parametric geostatistical
   procedure of Indicator Kriging enables to circumvent this problem by
   estimating the probability that the true value exceed a set of threshold
   values. The transformation of leaching data to a binary response
   variable, known as ``indicator{''}, can lead to a soft description of
   leaching. Such soft description can mitigate the uncertainty in exchange
   frequency estimates of the plant available soil water. The approach was
   applied to a test site in Beneventan agroecosystem (South Italy) by
   using a long-term hydrological water balance acquired in a 40-years
   period. In this way, about 400 km(2) (25\%) of the total 2,000 km(2) of
   the Benevento province were classified as areas sensitive to nutrient
   and contaminant leaching.}},
DOI = {{10.1007/s00477-005-0233-9}},
ISSN = {{1436-3240}},
EISSN = {{1436-3259}},
ORCID-Numbers = {{Ceccarelli, Michele/0000-0002-4702-6617
   Diodato, Nazzareno/0000-0001-9549-1583}},
Unique-ID = {{ISI:000233035200006}},
}

@article{ ISI:000231664800004,
Author = {Piga, R and Micheletto, R and Kawakami, Y},
Title = {{Nano-probing of the membrane dynamics of rat pheochromocytoma by
   near-field optics}},
Journal = {{BIOPHYSICAL CHEMISTRY}},
Year = {{2005}},
Volume = {{117}},
Number = {{2}},
Pages = {{141-146}},
Month = {{SEP 1}},
Abstract = {{High-resolution analysis of activities of live cells is limited by the
   use of non-invasive methods. Apparatuses such as SEM, STM or AFM are not
   practicable because the necessary treatment or the harsh contact with
   system probe will disturb or destroy the cell. Optical methods are
   purely non-invasive, but they are usually diffraction limited and then
   their resolution is limited to approximately I gm. To overcome these
   restrictions, we introduce here the study of membrane activity of a live
   cell sample using a Scanning Near-field Optical Microscope (SNOM). A
   near field optical microscope is able to detect tiny vertical movement
   on the cell membrane in the range of only 1 nm or less, about 3 orders
   of magnitude better than conventional optical microscopes. It is a
   purely non-invasive, non-contact method, so the natural life activity of
   the sample is unperturbed. In this report, we demonstrated the
   nanometer-level resolving ability of our SNOM system analyzing
   cardiomyocytes samples of which membrane movement is known, and then we
   present new intriguing data of sharp 40 mn cell membrane sudden events
   on rat pheochromocytoma. cell line PC12. All the measurements are
   carried out in culture medium with alive and unperturbed samples. We
   believe that this methodology will open a new approach to investigate
   live samples. The extreme sensitivity of SNOM allows measurements that
   are not possible with any other method on live biomaterial paving the
   way for a broad range of novel studies and applications. (c) 2005
   Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.bpc.2005.04.018}},
ISSN = {{0301-4622}},
EISSN = {{1873-4200}},
ResearcherID-Numbers = {{Micheletto, Ruggero/F-6942-2011
   }},
ORCID-Numbers = {{Micheletto, Ruggero/0000-0003-1707-6493}},
Unique-ID = {{ISI:000231664800004}},
}

@article{ ISI:000230536200010,
Author = {Leung, KMY and Bjorgesaeter, A and Gray, JS and Li, WK and Lui, GCS and
   Wang, Y and Lam, PKS},
Title = {{Deriving sediment quality guidelines from field-based species
   sensitivity distributions}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2005}},
Volume = {{39}},
Number = {{14}},
Pages = {{5148-5156}},
Month = {{JUL 15}},
Abstract = {{The determination of predicted no-effect concentrations (PNECs) and
   sediment quality guidelines (SQGs) of toxic chemicals in marine sediment
   is extremely important in ecological risk assessment. However, current
   methods of deriving sediment PNECs or threshold effect levels (TELs) are
   primarily based on laboratory ecotoxicity bioassays that may not be
   ecologically and environmentally relevant. This study explores the
   possibility of utilizing field data of benthic communities and
   contaminant loadings concurrently measured in sediment samples collected
   from the Norwegian continental shelf to derive SQGs. This unique dataset
   contains abundance data for ca. 2200 benthic species measured at over
   4200 sampling stations, along with co-occurring concentration data for >
   25 chemical species. Using barium, cadmium, and total polycyclic
   aromatic hydrocarbons (PAHs) as examples, this paper describes a novel
   approach that makes use of the above data set for constructing
   field-based species sensitivity distributions (f-SSDs). Field-based SQGs
   are then derived based on the f-SSDs and HCx values {[}hazardous
   concentration for x\% of species or the (100 - x)\% protection level] by
   the nonparametric bootstrap method, Our results for Cd and total PAHs
   indicate that there are some discrepancies between the SOGs currently in
   use in various countries and our field-data-derived SQGs. The
   field-data-derived criteria appear to be more environmentally relevant
   and realistic. Here, we suggest that the f-SSDs can be directly used as
   benchmarks for probabilistic risk assessment, while the
   field-data-derived SQGs can be used as site-specific guidelines or
   integrated into current SQGs.}},
DOI = {{10.1021/es050450x}},
ISSN = {{0013-936X}},
EISSN = {{1520-5851}},
ResearcherID-Numbers = {{Leung, Kenneth/C-1055-2009
   LAM, Paul/B-9121-2008
   Li, Wai/I-7954-2015
   }},
ORCID-Numbers = {{LAM, Paul/0000-0002-2134-3710
   Leung, Kenneth Mei Yee/0000-0002-2164-4281}},
Unique-ID = {{ISI:000230536200010}},
}

@article{ ISI:000230255400014,
Author = {Minin, VN and Dorman, KS and Fang, F and Suchard, MA},
Title = {{Dual multiple change-point model leads to more accurate recombination
   detection}},
Journal = {{BIOINFORMATICS}},
Year = {{2005}},
Volume = {{21}},
Number = {{13}},
Pages = {{3034-3042}},
Month = {{JUL 1}},
Abstract = {{Motivation: We introduce a dual multiple change-point (MCP) model for
   recombination detection among aligned nucleotide sequences. The dual MCP
   model is an extension of the model introduced previously by Suchard and
   co-workers. In the original single MCP model, one change-point process
   is used to model spatial phylogenetic variation. Here, we show that
   using two change-point processes, one for spatial variation of tree
   topologies and the other for spatial variation of substitution process
   parameters, increases recombination detection accuracy. Statistical
   analysis is done in a Bayesian framework using reversible jump Markov
   chain Monte Carlo sampling to approximate the joint posterior
   distribution of all model parameters.
   Results: We use primate mitochondrial DNA data with simulated
   recombination break-points at specific locations to compare the two
   models. We also analyze two real HIV sequences to identify recombination
   break-points using the dual MCIP model.}},
DOI = {{10.1093/bioinformatics/bti459}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Minin, Vladimir/A-8488-2008
   }},
ORCID-Numbers = {{Minin, Vladimir/0000-0002-1917-9288
   Dorman, Karin/0000-0003-3650-0018}},
Unique-ID = {{ISI:000230255400014}},
}

@article{ ISI:000230715600002,
Author = {Gilbert, PB and Sun, YQ},
Title = {{Failure time analysis of HIV vaccine effects on viral load and
   antiretroviral therapy initiation}},
Journal = {{BIOSTATISTICS}},
Year = {{2005}},
Volume = {{6}},
Number = {{3}},
Pages = {{374-394}},
Month = {{JUL}},
Abstract = {{The world's first efficacy trial of a preventive HIV vaccine was
   completed in 2003. Study participants who became HIV infected were
   followed for 2 years and monitored for HIV viral load and initiation of
   antiretroviral therapy (ART). In order to determine if vaccination may
   have altered HIV progression in persons who acquired HIV, a
   pre-specified objective was to compare the time until a composite
   endpoint between the vaccine and placebo arms, where the composite
   endpoint is the first event of ART initiation or viral failure (HIV
   viral load exceeds a threshold x(vl) copies/ml). Specifically, with
   vaccine efficacy, VE(tau, x(vl)), defined as one minus the ratio
   (vaccine/placebo) of the cumulative probability of the composite
   endpoint (with failure threshold xvl) occurring by tau months, the aim
   was to estimate the four parameters \{VE(tau, x(vl)): x(vl) is an
   element of \{1500, 10 000, 20 000, 55 000\} copies/ml\} with
   simultaneous 95\% confidence bands. A Gaussian multipliers simulation
   method is devised for constructing confidence bands for VE(tau, x(vl))
   with x(vl) spanning multiple discrete values or a continuous range. The
   new method is evaluated in simulations and is applied to the vaccine
   trial data set.}},
DOI = {{10.1093/biostatistics/kci014}},
ISSN = {{1465-4644}},
Unique-ID = {{ISI:000230715600002}},
}

@article{ ISI:000230361800011,
Author = {Hunt, D and Rai, SN},
Title = {{Testing threshold and hormesis in a random effects dose-response model
   applied to developmental toxicity data}},
Journal = {{BIOMETRICAL JOURNAL}},
Year = {{2005}},
Volume = {{47}},
Number = {{3}},
Pages = {{319-328}},
Month = {{JUN}},
Abstract = {{Here we describe a random effects threshold dose-response model for
   clustered binary-response data from developmental toxicity studies. For
   our model we assume that a hormetic effect occurs in addition to a
   threshold effect. Therefore, the dose-response curve is based on two
   components: relationships below the threshold (hormetic u-shaped model)
   and those above the threshold (logistic model). In the absence of
   hormesis and threshold effects, the estimation procedure is
   straightforward. We introduce score tests that are derived from a random
   effects hormetic-threshold dose-response model. The model and tests are
   applied to clustered binary data from developmental toxicity studies of
   animals to test for hormesis and threshold effects. We also compare the
   score test and likelihood ratio test to test for hormesis and threshold
   effects in a simulated study.}},
DOI = {{10.1002/bimj.200310129}},
ISSN = {{0323-3847}},
Unique-ID = {{ISI:000230361800011}},
}

@article{ ISI:000230151800002,
Author = {Bhattacharyya, S and Bhattacharya, DK},
Title = {{A new approach to pest management problem}},
Journal = {{JOURNAL OF BIOLOGICAL SYSTEMS}},
Year = {{2005}},
Volume = {{13}},
Number = {{2}},
Pages = {{117-130}},
Month = {{JUN}},
Note = {{Workshop and Conference on Differential Equations in Biology and
   Medicine, Bedlewo, POLAND, SEP 29-OCT 03, 2003}},
Abstract = {{The paper deals with the effect of pesticide on the pest affecting a
   paddy field and also on the fishes living in the soil water. An
   objective function is formed in terms of projected loss of individual
   fish, the projected profit of killing a pest and the cost of spraying
   insecticide for a finite time period. Next the corresponding pest
   management model is formed by considering the growth equations of paddy,
   pest, and fish of the soil water. The model is analyzed by control
   theoretic means to determine the threshold limits of pesticide to be
   used. Lastly, the optimal value of pesticide is obtained. It is shown
   that the optimal value lies within the threshold limits. Finally
   corresponding to this optimal threshold value of pesticide, the optimal
   values of the biomass of paddy and that of the fish of the soil water
   are also obtained. The results are finally verified numerically by
   special choice of parameters compatible with the model.}},
DOI = {{10.1142/S0218339005001422}},
ISSN = {{0218-3390}},
EISSN = {{1793-6470}},
Unique-ID = {{ISI:000230151800002}},
}

@article{ ISI:000228779200006,
Author = {Adamczak, R and Porollo, A and Meller, J},
Title = {{Combining prediction of secondary structure and solvent accessibility in
   proteins}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS}},
Year = {{2005}},
Volume = {{59}},
Number = {{3}},
Pages = {{467-475}},
Month = {{MAY 15}},
Abstract = {{Owing to the use of evolutionary information and advanced machine
   learning protocols, secondary structures of amino acid residues in
   proteins can be predicted from the primary sequence with more than 75\%
   per-residue accuracy for the 3-state (i.e., helix, beta-strand, and
   coil) classification problem. In this work we investigate whether
   further progress may be achieved by incorporating the relative solvent
   accessibility (RSA) of an amino acid residue as a fingerprint of the
   overall topology of the protein. Toward that goal, we developed a novel
   method for secondary structure prediction that uses predicted RSA in
   addition to attributes derived from evolutionary profiles. Our general
   approach follows the 2-stage protocol of Rost and Sander, with a number
   of Elman-type recurrent neural networks (NNs) combined into a consensus
   predictor. The RSA is predicted using our recently developed
   regression-based method that provides real-valued RSA, with the overall
   correlation coefficients between the actual and predicted RSA of about
   0.66 in rigorous tests on independent control sets. Using the predicted
   RSA, we were able to improve the performance of our secondary structure
   prediction by up to 1.4\% and achieved the overall per-residue accuracy
   between 77.0\% and 78.4\% for the 3-state classification problem on
   different control sets comprising, together, 603 proteins without
   homology to proteins included in the training. The effects of including
   solvent accessibility depend on the quality of RSA prediction. In the
   limit of perfect prediction (i.e., when using the actual RSA values
   derived from known protein structures), the accuracy of secondary
   structure prediction increases by up to 4\%. We also observed that
   projecting real-valued RSA into 2 discrete classes with the commonly
   used threshold of 25\% RSA decreases the classification accuracy for
   secondary structure prediction. While the level of improvement of
   secondary structure prediction may be different for prediction protocols
   that implicitly account for RSA in other ways, we conclude that an
   increase in the 3-state classification accuracy may be achieved when
   combining RSA with a state-of-theart protocol utilizing evolutionary
   profiles. The new method is available through a Web server at
   http://sable.cchme.org. (c) 2005 Wiley-Liss, Inc.}},
DOI = {{10.1002/prot.20441}},
ISSN = {{0887-3585}},
ResearcherID-Numbers = {{de Sousa, Miguel/A-3877-2009
   Meller, Jaroslaw/A-1971-2011
   Adamczak, Rafal/F-2965-2014
   }},
ORCID-Numbers = {{Meller, Jaroslaw/0000-0002-1162-8253
   Adamczak, Rafal/0000-0002-0261-8184
   Porollo, Alexey/0000-0002-3202-5099}},
Unique-ID = {{ISI:000228779200006}},
}

@article{ ISI:000228254000029,
Author = {Marrero-Ponce, Y and Medina-Marrero, R and Castillo-Garit, JA and
   Romero-Zaldivar, V and Torrens, F and Castro, EA},
Title = {{Protein linear indices of the `macromolecular pseudograph alpha-carbon
   atom adjacency matrix' in bioinformatics. Part 1: Prediction of protein
   stability effects of a complete set of alanine substitutions in Arc
   repressor}},
Journal = {{BIOORGANIC \& MEDICINAL CHEMISTRY}},
Year = {{2005}},
Volume = {{13}},
Number = {{8}},
Pages = {{3003-3015}},
Month = {{APR 15}},
Abstract = {{A novel approach to bio-macromolecular design from a linear algebra
   point of view is introduced. A protein's total (whole protein) and local
   (one or more amino acid) linear indices are a new set of
   bio-macromolecular descriptors of relevance to protein QSAR/QSPR
   studies. These amino-acid level biochemical descriptors are based on the
   calculation of linear maps on R-n{[}f(k)(x(mi)) : R-n -> R-n] in
   canonical basis. These bio-macromolecular indices are calculated from
   the k (th) power of the macromolecular pseudograph alpha-carbon atom
   adjacency matrix. Total linear indices are linear functional on R-n.
   That is, the k (th) total linear indices are linear maps from R-n to the
   scalar R{[}f (k)(x(m)) : Rn -> R]. Thus, the k th total linear indices
   are calculated by summing the amino-acid linear indices of all amino
   acids in the protein molecule. A study of the protein stability effects
   for a complete set of alanine substitutions in the Arc repressor
   illustrates this approach. A quantitative model that discriminates near
   wild-type stability alanine mutants from the reduced-stability ones in a
   training series was obtained. This model permitted the correct
   classification of 97.56\% (40/41) and 91.67\% (11/12) of proteins in the
   training and test set, respectively. It shows a high Matthews
   correlation coefficient (MCC = 0.952) for the training set and an MCC =
   0.837 for the external prediction set. Additionally, canonical
   regression analysis corroborated the statistical quality of the
   classification model (R-canc = 0.824). This analysis was also used to
   compute biological stability canonical scores for each Are alanine
   mutant. On the other hand, the linear piecewise regression model
   compared favorably with respect to the linear regression one on
   predicting the melting temperature (t(m)) of the Are alanine mutants.
   The linear model explains almost 81\% of the variance of the
   experimental t(m) (R = 0.90 and s = 4.29) and the LOO press statistics
   evidenced its predictive ability (q(2) = 0.72 and s(cv) = 4.79).
   Moreover, the TOMOCOMD-CAMPS method produced a linear piecewise
   regression (R = 0.97) between protein backbone descriptors and tm values
   for alanine mutants of the Arc repressor. A break-point value of 51.87
   degrees C characterized two mutant clusters and coincided perfectly with
   the experimental scale. For this reason, we can use the linear
   discriminant analysis and piecewise models in combination to classify
   and predict the stability of the mutant Arc homodimers. These models
   also permitted the interpretation of the driving forces of such folding
   process, indicating that topologic/topographic protein backbone
   interactions control the stability profile of wild-type Arc and its
   alanine mutants. (c) 2005 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.bmc.2005.01.062}},
ISSN = {{0968-0896}},
EISSN = {{1464-3391}},
ResearcherID-Numbers = {{Marrero-Ponce, Yovani/H-5724-2011
   Castillo-Garit, Juan/J-1648-2015}},
ORCID-Numbers = {{Marrero-Ponce, Yovani/0000-0003-2721-1142
   Castillo-Garit, Juan/0000-0003-0896-9484}},
Unique-ID = {{ISI:000228254000029}},
}

@article{ ISI:000227977800011,
Author = {Itoh, M and Goto, S and Akutsu, T and Kanehisa, M},
Title = {{Fast and accurate database homology search using upper bounds of local
   alignment scores}},
Journal = {{BIOINFORMATICS}},
Year = {{2005}},
Volume = {{21}},
Number = {{7}},
Pages = {{912-921}},
Month = {{APR 1}},
Abstract = {{Motivation: It is widely recognized that homology search and ortholog
   clustering are very useful for analyzing biological sequences. However,
   recent growth of sequence database size makes homolog detection
   difficult, and rapid and accurate methods are required.
   Results: We present a novel method for fast and accurate homology
   detection, assuming that the Smith-Waterman (SW) scores between all
   similar sequence pairs in a target database are computed and stored. In
   this method, SW alignment is computed only if the upper bound, which is
   derived from our novel inequality, is higher than the given threshold.
   In contrast to other methods such as FASTA and BLAST, this method is
   guaranteed to find all sequences whose scores against the query are
   higher than the specified threshold. Results of computational
   experiments suggest that the method is dozens of times faster than
   SSEARCH if genome sequence data of closely related species are
   available.}},
DOI = {{10.1093/bioinformatics/bti076}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Kanehisa, Minoru/R-3081-2018}},
Unique-ID = {{ISI:000227977800011}},
}

@article{ ISI:000228826600004,
Author = {Wang, W and Bian, ZZ and Wu, YJ and Miao, YL},
Title = {{A novel approach of low-frequency ultrasonic naked plasmid gene delivery
   and its assessment}},
Journal = {{BIOMEDICAL AND ENVIRONMENTAL SCIENCES}},
Year = {{2005}},
Volume = {{18}},
Number = {{2}},
Pages = {{87-95}},
Month = {{APR}},
Abstract = {{Objective To deliver the naked genes into cells through the bioeffects
   of cell membrane porous produced by low-frequency ultrasound (US) and to
   investigate the safety by determining the threshold of cell damage and
   membrane permeability.
   Methods The suspension of red cells from chickens, rabbits, rats, and S
   180 cells was exposed to calibrated US field with different parameters
   in still and flowing state. Laser scanning confocal microscopy,
   fluorescent microscopy, scanning electron microscopy, flow cytometry and
   spectrophotometry were used to examine cell morphology, membrane
   permeability, enzymes, free radicals, naked gone expression efficiency,
   threshold of cell damage and cell viability.
   Results The plasmid of green fluorescent protein (GFP) as a reporter
   gene was delivered into S 180 cells under optimal conditions without
   cell damage and cytotoxicity. The transfection rate was (35.83 +/-
   2.53)\% (n=6) in viable cells, and the cell viability was (90.17 +/-
   1.47)\% (n=6). Also, malondialdehyde, hydroxyl free radical, alkaline
   phosphatase, and acid phosphatase showed a S-shaped growth model (r=0.98
   +/- 0.01.) in response to the permeability change and alteration of cell
   morphology. The constant E of energy accumulation in US delivery at 90\%
   cell viability was an optimal control factor, and at 80\% cell viability
   was the damage threshold.
   Conclusion US under optimal conditions is a versatile gene therapy tool.
   The intensity of GFP expression in US group has a higher fluorescent
   peak than that in AVV-GFP group and control group (P < 0.001). The
   optimal gene uptakes, expression of gone and safety depend on E, which
   can be applied to control gene delivery efficiency in combination with
   other parameters. The results are helpful for development of a novel
   clinical naked gene therapeutic system and non-hyperthermia cancer
   therapeutic system.}},
ISSN = {{0895-3988}},
Unique-ID = {{ISI:000228826600004}},
}

@article{ ISI:000227380100001,
Author = {Saijo, S and Yamada, Y and Sato, T and Tanaka, N and Matsui, T and
   Sazaki, G and Nakajima, K and Matsuura, Y},
Title = {{Structural consequences of hen egg-white lysozyme orthorhombic crystal
   growth in a high magnetic field: validation of X-ray diffraction
   intensity, conformational energy searching and quantitative analysis of
   B factors and mosaicity}},
Journal = {{ACTA CRYSTALLOGRAPHICA SECTION D-STRUCTURAL BIOLOGY}},
Year = {{2005}},
Volume = {{61}},
Number = {{3}},
Pages = {{207-217}},
Month = {{MAR}},
Abstract = {{A novel method has been developed to improve protein-crystal perfection
   during crystallization in a high magnetic field and structural studies
   have been undertaken. The three-dimensional structure of orthorhombic
   hen egg-white (HEW) lysozyme crystals grown in a homogeneous and static
   magnetic field of 10 T has been determined and refined to a resolution
   of 1.13 Angstrom and an R factor of 17.0\%. The 10 T crystals belonged
   to space group P2(1)2(1)2(1), with unit-cell parameters a = 56.54 (3), b
   = 73.86 (6), c = 30.50 (2) Angstrom and one molecule per asymmetric
   unit. A comparison of the structures of the 0 T and 10 T crystals has
   been carried out. The magnitude of the structural changes, with a
   root-mean-square deviation value of 0.75 Angstrom for the positions of
   all protein atoms, is similar to that observed when an identical protein
   structure is resolved in two different crystalline lattices. The
   structures remain similar, with the exception of a few residues e.g.
   Arg68, Arg73, Arg128 and Gln121. The shifts of the arginine residues
   result in very significant structural fluctuations, which can have large
   effects on a protein's crystallization properties. The high magnetic
   field contributed to an improvement in diffraction intensity by (i) the
   displacement of the charged side chains of Arg68 and Arg73 in the
   flexible loop and of Arg128 at the C-terminus and (ii) the removal of
   the alternate conformations of the charged side chains of Arg21, Lys97
   or Arg114. The improvement in crystal perfection might arise from the
   magnetic effect on molecular orientation without structural change and
   differences in molecular interactions. X-ray diffraction and
   molecular-modelling studies of lysozyme crystals grown in a 10 T field
   have indicated that the field contributes to the stability of the
   dihedral angle. The average difference in conformational energy has a
   value of -578 U mol(-1) per charged residue in favour of the crystal
   grown in the magnetic field. For most protein atoms, the average B
   factor in the 10 T crystal shows an improvement of 1.8 Angstrom(2) over
   that for the 0 T control; subsequently, the difference in diffraction
   intensity between the 10 T and 0 T crystals corresponds to an increase
   of 22.6\% at the resolution limit. The mosaicity of the 10 T crystal was
   better than that of the 0 T crystal. More highly isotropic values of
   0.0065, 0.0049 and 0.0048degrees were recorded along the a, b and c
   axes, respectively. Anisotropic mosaicity analysis indicated that
   crystal growth is most perfect in the direction that corresponds to the
   favoured growth direction of the crystal, and that the crystal grown in
   the magnetic field had domains that were three times the volume of those
   of the control crystal. Overall, the magnetic field has improved the
   quality of these crystals and the diffracted intensity has increased
   significantly with the magnetic field, leading to a higher resolution.}},
DOI = {{10.1107/S0907444904030926}},
ISSN = {{2059-7983}},
ResearcherID-Numbers = {{Sazaki, Gen/D-8035-2012
   Nakajima, Kazuo/B-8616-2009}},
ORCID-Numbers = {{Sazaki, Gen/0000-0002-2320-5442
   }},
Unique-ID = {{ISI:000227380100001}},
}

@article{ ISI:000227252500030,
Author = {Wang, YM and Silverman, SK},
Title = {{Directing the outcome of deoxyribozyme selections to favor native 3 `-5
   `' RNA ligation}},
Journal = {{BIOCHEMISTRY}},
Year = {{2005}},
Volume = {{44}},
Number = {{8}},
Pages = {{3017-3023}},
Month = {{MAR 1}},
Abstract = {{Previous experiments have identified numerous RNA ligase deoxyribozymes,
   each of which can synthesize either 2',5'-branched RNA, linear
   2'-5'-linked RNA, or linear 3'-5'-linked RNA. These products may be
   formed by reaction of a 2'-hydroxyl or 3'-hydroxyl of one RNA substrate
   with the 5'-triphosphate of a second RNA substrate. Here the inherent
   propensities for nucleophilic reactivity of specific hydroxyl groups
   were assessed using RNA substrates related to the natural sequences of
   spliceosome substrates and group II introns. With the spliceosome
   substrates, nearly half of the selected deoxyribozymes mediate a
   ligation reaction involving the natural branch-point adenosine as the
   nucleophile. In contrast, mostly linear RNA is obtained with the group
   II intron substrates. Because the two sets of substrates differ at only
   three nucleotides, we conclude that the location of the newly created
   ligation junction in DNA-catalyzed branch formation depends sensitively
   on the RNA substrate sequences. During the experiment that led primarily
   to branched RNA, we abruptly altered the selection strategy to demand
   that the deoxyribozymes create linear 3'-5' linkages by introducing an
   additional selection step involving the 3'-5'-selective 8-17
   deoxyribozyme. Although no 3'-5' linkages (less than or equal to1\%)
   were detectable in the pool products at the point that the 3'-5'
   selection pressure was applied, deoxyribozymes that specifically create
   3'-5' linkages quickly emerged within a few selection rounds. Our
   success in obtaining 3'-5' linkages via this approach shows that the
   outcome of deoxyribozyme selection experiments can be dramatically
   redirected by strategic changes in the selection procedure, even at a
   late stage. These results relate to natural selection, in which abrupt
   environmental variation can provide a rapid change in selection
   pressure. Linear 3'-5' RNA linkages are an important practical objective
   because the native backbone is desirable in site-specifically modified
   ribozymes assembled by ligation. Therefore, this new approach to obtain
   3'-5'-selective RNA ligase deoxyribozymes is particularly important for
   ongoing selection efforts.}},
DOI = {{10.1021/bi0478291}},
ISSN = {{0006-2960}},
ORCID-Numbers = {{, Yangming/0000-0001-6974-6060
   Silverman, Scott/0000-0001-8166-3460}},
Unique-ID = {{ISI:000227252500030}},
}

@article{ ISI:000228083800001,
Author = {Malinen, T and Tuomaala, A and Peltonen, H},
Title = {{Hydroacoustic fish stock assessment in the presence of dense
   aggregations of Chaoborus larvae}},
Journal = {{CANADIAN JOURNAL OF FISHERIES AND AQUATIC SCIENCES}},
Year = {{2005}},
Volume = {{62}},
Number = {{2}},
Pages = {{245-249}},
Month = {{FEB}},
Abstract = {{A new method for eliminating reverberation due to Chaoborus larvae from
   hydroacoustic recordings is presented based on an assumption of a
   constant dependence between the area backscattering strength (s(a)) with
   a high-volume backscattering threshold (s(v)) and s(a) with a low s(v)
   threshold for fish. The idea was to analyze data with a threshold high
   enough to eliminate reverberation and then convert the estimate to
   coincide with the result that would have been achieved with a low
   threshold containing all backscattering from fish. The approach was
   validated with a secondary dataset, and the magnitude of overestimation
   of fish density by reverberation was evaluated using data from four
   surveys conducted in a clay-turbid lake, where small planktivorous fish,
   smelt (Osmerus eperlanus), and larvae of Chaoborus flavicans coexist in
   the water column. With the presented method, estimation of smelt density
   was possible even when Chaoborus density was > 200 individuals center
   dot m(-3). The analyses revealed that the overestimation of fish density
   could be as high as 50\% if the reverberation is not taken into account.
   The presented method might also be applicable for eliminating
   reverberation due to other unwanted targets, because it is based on the
   acoustic properties of fish rather than those of unwanted targets.}},
DOI = {{10.1139/F05-038}},
ISSN = {{0706-652X}},
Unique-ID = {{ISI:000228083800001}},
}

@article{ ISI:000229239300003,
Author = {Nogueira, V and Devin, A and Walter, L and Rigoulet, M and Leverve, X
   and Fontaine, E},
Title = {{Effects of decreasing mitochondrial volume on the regulation of the
   permeability transition pore}},
Journal = {{JOURNAL OF BIOENERGETICS AND BIOMEMBRANES}},
Year = {{2005}},
Volume = {{37}},
Number = {{1}},
Pages = {{25-33}},
Month = {{FEB}},
Abstract = {{The permeability transition pore (PTP) is a Ca2+-sensitive mitochondrial
   inner membrane channel involved in several models of cell death. Because
   the matrix concentration of PTP regulatory factors depends on matrix
   volume, we have investigated the role of the mitochondrial volume in PTP
   regulation. By incubating rat liver mitochondria in media of different
   osmolarity, we found that the Ca2+ threshold required for PTP opening
   dramatically increased when mitochondrial volume decreased relative to
   the standard condition. This shrinkage-induced PTP inhibition was not
   related to the observed changes in protonmotive force, or pyridine
   nucleotide redox state and persisted when mitochondria were depleted of
   adenine nucleotides. On the other hand, mitochondrial volume did not
   affect PTP regulation when mitochondria were depleted of Mg2+. By
   studying the effects of Mg2+, cyclosporin A (CsA) and ubiquinone 0
   (Ub(0)) on PTP regulation, we found that mitochondrial shrinkage
   increased the efficacy of Mg2+ and Ub(0) at PTP inhibition, whereas it
   decreased that of CsA. The ability of mitochondrial volume to alter the
   activity of several PTP regulators represents a hitherto unrecognized
   characteristic of the pore that might lead to a new approach for its
   pharmacological modulation.}},
DOI = {{10.1007/s10863-005-4120-3}},
ISSN = {{0145-479X}},
EISSN = {{1573-6881}},
ResearcherID-Numbers = {{Walter, Ludivine/S-4456-2017
   }},
ORCID-Numbers = {{Walter, Ludivine/0000-0001-7214-0676
   Fontaine, Eric/0000-0002-5204-9477}},
Unique-ID = {{ISI:000229239300003}},
}

@article{ ISI:000231905600024,
Author = {Enberg, K},
Title = {{Benefits of threshold strategies and age-selective harvesting in a
   fluctuating fish stock of Norwegian spring spawning herring Clupea
   harengus}},
Journal = {{MARINE ECOLOGY PROGRESS SERIES}},
Year = {{2005}},
Volume = {{298}},
Pages = {{277-286}},
Abstract = {{The current state of the world's fisheries resources requires further
   investigation into the means and methods for sustainable use of fish
   stocks. In this study, I assess the performance of different harvesting
   strategies on a population model developed for Norwegian spring spawning
   herring Clupea harengus, a stock historically known as one of the
   largest and most valuable fish stocks in the world. This stock is
   further characterized by strong long-term fluctuations in stock size,
   which considerably complicate successful population management. The
   results support the use of threshold strategies, where harvesting is
   only allowed if the population size of the target population is above a
   predetermined threshold. Threshold strategies are beneficial for
   maintaining spawning biomass, yield, and prevention of population
   collapse. The downside of a threshold approach is the fisheries
   moratoria associated with low stock size. To overcome this, I introduce
   a precautionary strategy including 2 thresholds for reducing the
   likelihood and length of moratoria, such that if the population size
   decreases below an upper threshold, the harvest ratio is decreased in
   order to achieve fast recovery to the target population size. I also
   test the effect of changing the timing of fish entering the harvestable
   stock by means of age-selective harvesting. I show that allowing
   reproduction, preferably more than once, before the fish become
   vulnerable to harvesting increases the spawning biomass and yield, and
   decreases the variability in yield and the risk of stock collapse.}},
DOI = {{10.3354/meps298277}},
ISSN = {{0171-8630}},
ResearcherID-Numbers = {{Enberg, Katja/C-8630-2009}},
ORCID-Numbers = {{Enberg, Katja/0000-0002-0045-7604}},
Unique-ID = {{ISI:000231905600024}},
}

@article{ ISI:000226784800010,
Author = {Karp, NA and Griffin, JL and Lilley, KS},
Title = {{Application of partial least squares discriminant analysis to
   two-dimensional difference gel studies in expression proteomics}},
Journal = {{PROTEOMICS}},
Year = {{2005}},
Volume = {{5}},
Number = {{1}},
Pages = {{81-90}},
Month = {{JAN}},
Abstract = {{Two-dimensional difference gel electrophoresis (DIGE) is a tool for
   measuring changes in protein expression between samples involving
   pre-electrophoretic labeling with cyanine dyes. In multi-gel
   experiments, univariate statistical tests have been used to identify
   differential expression between sample types by looking for significant
   changes in spot volume. Multivariate statistical tests, which look for
   correlated changes between sample types, provide an alternate approach
   for identifying spots with differential expression. Partial least
   squares-discriminant analysis (PLS-DA), a multivariate statistical
   approach, was combined with an iterative threshold process to identify
   which protein spots had the greatest contribution to the model, and
   compared to univariate tests for three datasets. This included one
   dataset where no biological difference was expected. The novel
   multivariate approach, detailed here, represents a method to complement
   the univariate approach in identification of differentially expressed
   protein spots. This new approach has the advantages of reduced risk of
   false-positives and the identification of spots that are significantly
   altered in terms of correlated expression rather than absolute
   expression values.}},
DOI = {{10.1002/pmic.200400881}},
ISSN = {{1615-9853}},
EISSN = {{1615-9861}},
Unique-ID = {{ISI:000226784800010}},
}

@article{ ISI:000188974000002,
Author = {Wang, SC and Marshall, CR},
Title = {{Improved confidence intervals for estimating the position of a mass
   extinction boundary}},
Journal = {{PALEOBIOLOGY}},
Year = {{2004}},
Volume = {{30}},
Number = {{1}},
Pages = {{5-18}},
Month = {{WIN}},
Abstract = {{Marshall (1995) used the distribution of the endpoints of 50\% range
   extensions added to the stratigraphic ranges of individual taxa to
   bracket the position of an extinction boundary. Here we describe two
   improvements to Marshall's method. First, we show that more precise
   estimates of the position of such a boundary may be obtained using range
   extensions with confidence levels of less than 50\% (e.g., 20\%).
   Second, we introduce a new method of calculating confidence intervals
   that explicitly takes into account the position of the highest fossil
   find. Incorporating these improvements leads to confidence intervals for
   simulated data sets that are approximately four times more precise than
   those obtained by using Marshall's (1995) original method and
   approximately twice as precise as those using other published methods.
   We provide a look-up table that shows for different numbers of taxa the
   confidence level that should be used to maximize the precision of the
   estimated position of the extinction boundary, while ensuring that the
   boundary still lies within the stratigraphic interval bounded by at
   least one range extension. Unlike some other methods, our method is
   nonparametric and does not make the restrictive assumption of uniform
   preservation and recovery potential. We apply the method to Macellari's
   (1986) ammonite data from the late Cretaceous of Seymour Island,
   Antarctica.}},
DOI = {{10.1666/0094-8373(2004)030<0005:ICIFET>2.0.CO;2}},
ISSN = {{0094-8373}},
ORCID-Numbers = {{Wang, Steve/0000-0002-8953-285X}},
Unique-ID = {{ISI:000188974000002}},
}

@article{ ISI:000225361400024,
Author = {Satten, GA and Datta, S and Moura, H and Woolfitt, AR and Carvalho, MD
   and Carlone, GM and De, BK and Pavlopoulos, A and Barr, JR},
Title = {{Standardization and denoising algorithms for mass spectra to classify
   whole-organism bacterial specimens}},
Journal = {{BIOINFORMATICS}},
Year = {{2004}},
Volume = {{20}},
Number = {{17}},
Pages = {{3128-3136}},
Month = {{NOV 22}},
Abstract = {{Motivation: Application of mass spectrometry in proteomics is a
   breakthrough in high-throughput analyses. Early applications have
   focused on protein expression profiles to differentiate among various
   types of tissue samples (e.g. normal versus tumor). Here our goal is to
   use mass spectra to differentiate bacterial species using whole-organism
   samples. The raw spectra are similar to spectra of tissue samples,
   raising some of the same statistical issues (e.g. non-uniform baselines
   and higher noise associated with higher baseline), but are substantially
   noisier. As a result, new preprocessing procedures are required before
   these spectra can be used for statistical classification.
   Results: In this study, we introduce novel preprocessing steps that can
   be used with any mass spectra. These comprise a standardization step and
   a denoising step. The noise level for each spectrum is determined using
   only data from that spectrum. Only spectral features that exceed a
   threshold defined by the noise level are subsequently used for
   classification. Using this approach, we trained the Random Forest
   program to classify 240 mass spectra into four bacterial types. The
   method resulted in zero prediction errors in the training samples and in
   two test datasets having 240 and 300 spectra, respectively.}},
DOI = {{10.1093/bioinformatics/bth372}},
ISSN = {{1367-4803}},
ORCID-Numbers = {{Satten, Glen/0000-0001-7275-5371}},
Unique-ID = {{ISI:000225361400024}},
}

@article{ ISI:000225250100016,
Author = {Smid, M and Dorssers, LCJ},
Title = {{GO-Mapper: functional analysis of gene expression data using the
   expression level as a score to evaluate Gene Ontology terms}},
Journal = {{BIOINFORMATICS}},
Year = {{2004}},
Volume = {{20}},
Number = {{16}},
Pages = {{2618-2625}},
Month = {{NOV 1}},
Abstract = {{Motivation: Retrieval of information on biological processes from
   large-scale expression data is still a time-consuming task. An automated
   analysis utilizing all expression information would greatly increase our
   understanding of the samples under study.
   Results: We describe here a novel method to obtain a functional analysis
   of complex gene expression data. Instead of applying a predefined
   expression threshold, Gene Ontology (GO) terms are weighted using the
   actual measured levels of expression of all associated genes. Based on
   this concept, the application GO-Mapper was developed to quantitatively
   link gene expression levels to GO-terms for multiple experiments in an
   automated way. The applicability of GO-Mapper was developed and
   validated on in house and public human microarray data and mouse SAGE
   data. We demonstrate that the GO-Mapper allows for interrelating
   relevant biological functions with the experiments under study.}},
DOI = {{10.1093/bioinformatics/bth293}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
Unique-ID = {{ISI:000225250100016}},
}

@article{ ISI:000224344700001,
Author = {Stenseth, NC and Chan, KS and Tavecchia, G and Coulson, T and Mysterud,
   A and Clutton-Brock, T and Grenfell, B},
Title = {{Modelling non-additive and nonlinear signals from climatic noise in
   ecological time series: Soay sheep as an example}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2004}},
Volume = {{271}},
Number = {{1552}},
Pages = {{1985-1993}},
Month = {{OCT 7}},
Abstract = {{Understanding how climate can interact with other factors in determining
   patterns of species abundance is a persistent challenge in ecology.
   Recent research has suggested that the dynamics exhibited by some
   populations may be a non-additive function of climate, with climate
   affecting population growth more strongly at high density than at low
   density. However, we lack methodologies to adequately explain patterns
   in population growth generated as a result of interactions between
   intrinsic factors and extrinsic climatic variation in non-linear
   systems. We present a novel method (the Functional Coefficient Threshold
   Auto-Regressive (FCTAR) method) that can identify interacting influences
   of climate and density on population dynamics from time-series data. We
   demonstrate its use on count data on the size of the Soay sheep
   population, which is known to exhibit dynamics generated by nonlinear
   and non-additive interactions between density and climate, living on
   Hirta in the St Kilda archipelago. The FCTAR method suggests that
   climate fluctuations can drive the Soay sheep population between
   different dynamical regimes-from stable population size through limit
   cycles and non-periodic fluctuations.}},
DOI = {{10.1098/rspb.2004.2794}},
ISSN = {{0962-8452}},
ResearcherID-Numbers = {{Tavecchia, Giacomo/N-3961-2014
   Stenseth, Nils Chr./G-5212-2016
   }},
ORCID-Numbers = {{Tavecchia, Giacomo/0000-0001-5435-2691
   Stenseth, Nils Chr./0000-0002-1591-5399
   Mysterud, Atle/0000-0001-8993-7382
   Coulson, Tim/0000-0001-9371-9003}},
Unique-ID = {{ISI:000224344700001}},
}

@article{ ISI:000224564900004,
Author = {Tolvanen, OK},
Title = {{Exposure to bioaerosols and noise at a Finnish dry waste treatment plant}},
Journal = {{WASTE MANAGEMENT \& RESEARCH}},
Year = {{2004}},
Volume = {{22}},
Number = {{5}},
Pages = {{346-357}},
Month = {{OCT}},
Abstract = {{Repeated measurements were carried out during two different campaigns
   between 1998 and 2001 to assess the occupational hygiene at a Finnish
   dry waste treatment plant. The analytical determinations were done in
   four different places within the processing hall of the plant: near a
   conveyor belt, near a jigger screen, near an after-crusher and near a
   bailer. Measurements were also carried out in a coffee room for
   employees. Concentrations of bacteria, fungi and actinomycetes were
   determined by two methods (six-stage impactor and Camnea method) and
   levels of endotoxins, dust and noise were also investigated. High
   concentrations of microbes and endotoxins and the noise level were found
   to be a real problem in the waste processing hall. Microbe
   concentrations were highest during management of the dry waste fraction.
   Endotoxin concentrations all exceeded the threshold value of 200 EU
   m(-3) irrespective of the measurement place, with the only exception
   near the after-crusher where the average concentration was always as low
   as 60 EU m(-3). The noise level exceeded the Finnish threshold value of
   85 dBA. Problems were not easily solved through technical modifications
   and more radical improvements are needed. Improvements in reliability
   are also required in the measuring methods before their application in
   waste treatment plants. In particular, a new method of dust collection
   is recommended.}},
DOI = {{10.1177/0734242X04045427}},
ISSN = {{0734-242X}},
EISSN = {{1096-3669}},
Unique-ID = {{ISI:000224564900004}},
}

@article{ ISI:000223932000008,
Author = {Bentele, M and Lavrik, I and Ulrich, M and Stosser, S and Heermann, DW
   and Kalthoff, H and Krammer, PH and Eils, R},
Title = {{Mathematical modeling reveals threshold mechanism in CD95-induced
   apoptosis}},
Journal = {{JOURNAL OF CELL BIOLOGY}},
Year = {{2004}},
Volume = {{166}},
Number = {{6}},
Pages = {{839-851}},
Month = {{SEP 13}},
Abstract = {{Mathematical modeling is required for understanding the complex behavior
   of large signal transduction networks. Previous attempts to model signal
   transduction pathways were often limited to small systems or based on
   qualitative data only. Here, we developed a mathematical modeling
   framework for understanding the complex signaling behavior of
   CD95(APO-1/Fas)-mediated apoptosis. Defects in the regulation of
   apoptosis result in serious diseases such as cancer, autoimmunity, and
   neurodegeneration. During the last decade many of the molecular
   mechanisms of apoptosis signaling have been examined and elucidated. A
   systemic understanding of apoptosis is, however, still missing. To
   address the complexity of apoptotic signaling we subdivided this system
   into subsystems of different information qualities. A new approach for
   sensitivity analysis within the mathematical model was key for the
   identification of critical system parameters and two essential system
   properties: modularity and robustness. Our model describes the
   regulation of apoptosis on a systems level and resolves the important
   question of a threshold mechanism for the regulation of apoptosis.}},
DOI = {{10.1083/jcb.200404158}},
ISSN = {{0021-9525}},
EISSN = {{1540-8140}},
ResearcherID-Numbers = {{Kalthoff, Holger/B-1618-2010
   Lavrik, Inna/C-1700-2009
   Eils, Roland/B-6121-2009}},
ORCID-Numbers = {{Lavrik, Inna/0000-0002-9324-309X
   Eils, Roland/0000-0002-0034-4036}},
Unique-ID = {{ISI:000223932000008}},
}

@article{ ISI:000223827000001,
Author = {Ihmels, J and Bergmann, S and Barkai, N},
Title = {{Defining transcription modules using large-scale gene expression data}},
Journal = {{BIOINFORMATICS}},
Year = {{2004}},
Volume = {{20}},
Number = {{13}},
Pages = {{1993-2003}},
Month = {{SEP 1}},
Abstract = {{Motivation: Large-scale gene expression data comprising a variety of
   cellular conditions hold the promise of a global view on the
   transcription program. While conventional clustering algorithms have
   been successfully applied to smaller datasets, the utility of many
   algorithms for the analysis of large-scale data is limited by their
   inability to capture combinatorial and condition-specific co-regulation.
   In addition, there is an increasing need to integrate the rapidly
   accumulating body of other high-throughput biological data with the
   expression analysis. In a previous work, we introduced the signature
   algorithm, which overcomes the problems of conventional clustering and
   allows for intuitive integration of additional biological data. However,
   this approach is constrained by the comprehensiveness of relevant
   external data and its lacking ability to capture hierarchical
   modularity.
   Methods: We present a novel method for the analysis of large-scale
   expression data, which assigns genes into context-dependent and
   potentially overlapping regulatory units. We introduce the notion of a
   transcription module as a self-consistent regulatory unit consisting of
   a set of coregulated genes as well as the experimental conditions that
   induce their co-regulation. Self-consistency is defined by a rigorous
   mathematical criterion. We propose an efficient algorithm to identify
   such modules, which is based on the iterative application of the
   signature algorithm. A threshold parameter that determines the
   resolution of the modular decomposition is introduced.
   Results: The method is applied systematically to over 1000 expression
   profiles of the yeast Saccharomyces cerevisiae, and the results are
   presented using two complementary visualization schemes we developed.
   The average biological coherence, as measured by the conservation of
   putative cis-regulatory motifs between four related yeast species, is
   higher for transcription modules than for clusters identified by other
   methods applied to the same dataset. Our method is related to singular
   value decomposition (SVD) and to the pairwise average linkage clustering
   algorithm. It extends SVD by filtering out noise in the expression data
   and offering variable resolution to reveal hierarchical organization. It
   furthermore has the advantage over both methods of capturing overlapping
   modules in the presence of combinatorial regulation.}},
DOI = {{10.1093/bioinformatics/bth166}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ORCID-Numbers = {{Bergmann, Sven/0000-0002-6785-9034}},
Unique-ID = {{ISI:000223827000001}},
}

@article{ ISI:000223216500012,
Author = {Adamczak, R and Porollo, A and Meller, J},
Title = {{Accurate prediction of solvent accessibility using neural networks-based
   regression}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND BIOINFORMATICS}},
Year = {{2004}},
Volume = {{56}},
Number = {{4}},
Pages = {{753-767}},
Month = {{SEP 1}},
Abstract = {{Accurate prediction of relative solvent accessibilities (RSAs) of amino
   acid residues in proteins may be used to facilitate protein structure
   prediction and functional annotation. Toward that goal we developed a
   novel method for improved prediction of RSAs. Contrary to other machine
   learning-based methods from the literature, we do not impose a
   classification problem with arbitrary boundaries between the classes.
   Instead, we seek a continuous approximation of the real-value RSA using
   nonlinear regression, with several feed forward and recurrent neural
   networks, which are then combined into a consensus predictor. A set of
   860 protein structures derived from the PFAM database was used for
   training, whereas validation of the results was carefully performed on
   several nonredundant control sets comprising a total of 603 structures
   derived from new Protein Data Bank structures and had no homology to
   proteins included in the training. Two classes of alternative predictors
   were developed for comparison with the regression-based approach: one
   based on the standard classification approach and the other based on a
   semicontinuous approximation with the so-called thermometer encoding.
   Furthermore, a weighted approximation, with errors being scaled by the
   observed levels of variability in RSA for equivalent residues in
   families of homologous structures, was applied in order to improve the
   results. The effects of including evolutionary profiles and the growth
   of sequence databases were assessed. In accord with the observed levels
   of variability in RSA for different ranges of RSA values, the regression
   accuracy is higher for buried than for exposed residues, with overall
   15.3-15.8\% mean absolute errors and correlation coefficients between
   the predicted and experimental values of 0.64-0.67 on different control
   sets. The new method outperforms classification-based algorithms when
   the real value predictions are projected onto two-class classification
   problems with several commonly used thresholds to separate exposed and
   buried residues. For example, classification accuracy of about 77\% is
   consistently achieved on all control sets with a threshold of 25\% RSA.
   A web server that enables RSA prediction using the new method and
   provides customizable graphical representation of the results is
   available at http://sable.cchmc.org. (C) 2004 Wiley-Liss, Inc.}},
DOI = {{10.1002/prot.20176}},
ISSN = {{0887-3585}},
ResearcherID-Numbers = {{Adamczak, Rafal/F-2965-2014
   Meller, Jaroslaw/A-1971-2011
   }},
ORCID-Numbers = {{Adamczak, Rafal/0000-0002-0261-8184
   Meller, Jaroslaw/0000-0002-1162-8253
   Porollo, Alexey/0000-0002-3202-5099}},
Unique-ID = {{ISI:000223216500012}},
}

@article{ ISI:000223019900007,
Author = {Laubenbacher, R and Stigler, B},
Title = {{A computational algebra approach to the reverse engineering of gene
   regulatory networks}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2004}},
Volume = {{229}},
Number = {{4}},
Pages = {{523-537}},
Month = {{AUG 21}},
Abstract = {{This paper proposes a new method to reverse engineer gene regulatory
   networks from experimental data. The modeling framework used is
   time-discrete deterministic dynamical systems, with a finite set of
   states for each of the variables. The simplest examples of such models
   are Boolean networks, in which variables have only two possible states.
   The use of a larger number of possible states allows a finer
   discretization of experimental data and more than one possible mode of
   action for the variables, depending on threshold values. Furthermore,
   with a suitable choice of state set, one can employ powerful tools from
   computational algebra, that underlie the reverse-engineering algorithm,
   avoiding costly enumeration strategies. To perform well, the algorithm
   requires wildtype together with perturbation time courses. This makes it
   suitable for small to meso-scale networks rather than networks on a
   aenome-wide scale. An analysis of the complexity of the algorithm is
   performed. The algorithm is validated on a recently published Boolean
   network model of segment polarity development in Drosophila
   melanogaster. (C) 2004 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.jtbi.2004.04.037}},
ISSN = {{0022-5193}},
EISSN = {{1095-8541}},
Unique-ID = {{ISI:000223019900007}},
}

@article{ ISI:000208392400005,
Author = {Beissbarth, Tim and Hyde, Lavinia and Smyth, Gordon K. and Job, Chris
   and Boon, Wee-Ming and Tan, Seong-Seng and Scott, Hamish S. and Speed,
   Terence P.},
Title = {{Statistical modeling of sequencing errors in SAGE libraries}},
Journal = {{BIOINFORMATICS}},
Year = {{2004}},
Volume = {{20}},
Number = {{1}},
Pages = {{31-39}},
Month = {{AUG 4}},
Abstract = {{Motivation: Sequencing errors may bias the gene expression measurements
   made by Serial Analysis of Gene Expression (SAGE). They may introduce
   non-existent tags at low abundance and decrease the real abundance of
   other tags. These effects are increased in the longer tags generated in
   Long-SAGE libraries. Current sequencing technology generates quite
   accurate estimates of sequencing error rates. Here we make use of the
   sequence neighborhood of SAGE tags and error estimates from the
   base-calling software to correct for such errors.
   Results: We introduce a statistical model for the propagation of
   sequencing errors in SAGE and suggest an Expectation-Maximization (EM)
   algorithm to correct for them given observed sequences in a library and
   base-calling error estimates. We tested our method using simulated and
   experimental SAGE libraries. When comparing SAGE libraries, we found
   that sequencing errors can introduce considerable bias. High abundance
   tags may be falsely called as significantly differentially expressed,
   especially when comparing libraries with different levels of sequencing
   errors and/or of different size. Truly, differentially expressed tags
   have decreased significance as `true'-tag counts are generally
   underestimated. This may alter if tags near the threshold of
   differential expression are called significant. Moreover, the number of
   different transcripts present in a library is overestimated as false
   tags are introduced at low abundance. Our correction method adjusts the
   tag counts to be closer to the true counts and is able to partly correct
   for biases introduced by sequencing errors.}},
DOI = {{10.1093/bioinformatics/bth924}},
ISSN = {{1367-4803}},
ResearcherID-Numbers = {{Smyth, Gordon/B-5276-2008
   }},
ORCID-Numbers = {{Smyth, Gordon/0000-0001-9221-2892
   Scott, Hamish/0000-0002-5813-631X
   Boon, Wee-Ming/0000-0002-0240-7416}},
Unique-ID = {{ISI:000208392400005}},
}

@article{ ISI:000208392400041,
Author = {Szklarczyk, Radek and Heringa, Jaap},
Title = {{Tracking repeats using significance and transitivity}},
Journal = {{BIOINFORMATICS}},
Year = {{2004}},
Volume = {{20}},
Number = {{1}},
Pages = {{311-317}},
Month = {{AUG 4}},
Abstract = {{Motivation: Internal repeats in coding sequences correspond to
   structural and functional units of proteins. Moreover, duplication of
   fragments of coding sequences is known to be a mechanism to facilitate
   evolution. Identification of repeats is crucial to shed light on the
   function and structure of proteins, and explain their evolutionary past.
   The task is difficult because during the course of evolution many
   repeats diverged beyond recognition.
   Results: We introduce a new method TRUST, for ab initio determination of
   internal repeats in proteins. It provides an improvement in prediction
   quality as compared to alternative state-of-the-art methods. The
   increased sensitivity and accuracy of the method is achieved by
   exploiting the concept of transitivity of alignments. Starting from
   significant local suboptimal alignments, the application of transitivity
   allows us to (1) identify distant repeat homologues for which no
   alignments were found; (2) gain confidence about consistently
   well-aligned regions; and (3) recognize and reduce the contribution of
   non-homologous repeats. This re-assessment step enables us to derive a
   virtually noise-free profile representing a generalized repeat with high
   fidelity. We also obtained superior specificity by employing rigid
   statistical testing for self-sequence and profile-sequence alignments.
   Assessment was done using a database of repeat annotations based on
   structural superpositioning. The results show that TRUST is a useful and
   reliable tool for mining tandem and non-tandem repeats in protein
   sequence databases, capable of predicting multiple repeat types with
   varying intervening segments within a single sequence.}},
DOI = {{10.1093/bioinformatics/bth911}},
ISSN = {{1367-4803}},
Unique-ID = {{ISI:000208392400041}},
}

@article{ ISI:000222402400004,
Author = {Floter, A and Nicolas, J and Schaub, T and Selbig, J},
Title = {{Threshold extraction in metabolite concentration data}},
Journal = {{BIOINFORMATICS}},
Year = {{2004}},
Volume = {{20}},
Number = {{10}},
Pages = {{1491-1494}},
Month = {{JUL 1}},
Note = {{18th German Conference on Bioinformatics, Tech Univ Munchen, Garching,
   GERMANY, OCT 12-14, 2003}},
Organization = {{GSF Natl Res Ctr}},
Abstract = {{Motivation: Continued development of analytical techniques based on gas
   chromatography and mass spectrometry now facilitates the generation of
   larger sets of metabolite concentration data. An important step towards
   the understanding of metabolite dynamics is the recognition of stable
   states where metabolite concentrations exhibit a simple behaviour. Such
   states can be characterized through the identification of significant
   thresholds in the concentrations. But general techniques for finding
   discretization thresholds in continuous data prove to be practically
   insufficient for detecting states due to the weak conditional
   dependences in concentration data.
   Results: We introduce a method of recognizing states in the framework of
   decision tree induction. It is based upon a global analysis of decision
   forests where stability and quality are evaluated. It leads to the
   detection of thresholds that are both comprehensible and robust. Applied
   to metabolite concentration data, this method has led to the discovery
   of hidden states in the corresponding variables. Some of these reflect
   known properties of the biological experiments, and others point to
   putative new states.}},
DOI = {{10.1093/bioinformatics/bth107}},
ISSN = {{1367-4803}},
Unique-ID = {{ISI:000222402400004}},
}

@article{ ISI:000221838500014,
Author = {Tolocka, MP and Lake, DA and Johnston, MV and Wexler, AS},
Title = {{Number concentrations of fine and ultrafine particles containing metals}},
Journal = {{ATMOSPHERIC ENVIRONMENT}},
Year = {{2004}},
Volume = {{38}},
Number = {{20}},
Pages = {{3263-3273}},
Month = {{JUN}},
Abstract = {{Typical classification schemes for large data sets of single-particle
   mass spectra involve statistical or neural network analysis. In this
   work, a new approach is evaluated in which particle spectra are
   pre-selected on the basis of an above threshold signal intensity at a
   specified m/z (mass to charge ratio). This provides a simple way to
   identify candidate particles that may contain the specific chemical
   component associated with that m/z. Once selected, the candidate
   particle spectra are then classified by the fast adaptive resonance
   algorithm, ART 2-a, to confirm the presence of the targeted component in
   the particle and to study the intra-particle associations with other
   chemical components. This approach is used to characterize metals in a
   75,000 particle data set obtained in Baltimore, Maryland. Particles
   containing a specific metal are identified and then used to determine
   the size distribution, number concentration, time/wind dependencies and
   intra-particle correlations with other metals. Four representative
   elements are considered in this study: vanadium, iron, arsenic and lead.
   Number concentrations of ambient particles containing these elements can
   exceed 10,000 particles cm(-3) at the measurement site. Vanadium, a
   primary marker for fuel oil combustion, is observed from all wind
   directions during this time period. Iron and lead are observed from the
   east-northeast. Most particles from this direction that contain iron
   also contain lead and most particles that contain lead also contain
   iron, suggesting a common emission source for the two. Arsenic and lead
   are observed from the south-southeast. Particles from this direction
   contain either arsenic or lead but rarely both, suggesting different
   sources for each element. (C) 2004 Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.atmosenv.2004.03.010}},
ISSN = {{1352-2310}},
EISSN = {{1873-2844}},
ResearcherID-Numbers = {{Tolocka, Michael/C-7800-2011}},
Unique-ID = {{ISI:000221838500014}},
}

@article{ ISI:000188411000015,
Author = {Kalinina, OV and Mironov, AA and Gelfand, MS and Rakhmaninova, AB},
Title = {{Automated selection of positions determining functional specificity of
   proteins by comparative analysis of orthologous groups in protein
   families}},
Journal = {{PROTEIN SCIENCE}},
Year = {{2004}},
Volume = {{13}},
Number = {{2}},
Pages = {{443-456}},
Month = {{FEB}},
Abstract = {{The increasing volume of genomic data opens new possibilities for
   analysis of protein function. We introduce a method for automated
   selection of residues that determine the functional specificity of
   proteins with a common general function (the specificity-determining
   positions {[}SDP] prediction method). Such residues are assumed to be
   conserved within groups of orthologs (that may be assumed to have the
   same specificity) and to vary between paralogs. Thus, considering a
   multiple sequence alignment of a protein family divided into orthologous
   groups, one can select positions where the distribution of amino acids
   correlates with this division. Unlike previously published techniques,
   the introduced method directly takes into account nonuniformity of amino
   acid substitution frequencies. In addition, it does not require setting
   arbitrary thresholds. Instead, a formal procedure for threshold
   selection using the Bernoulli estimator is implemented. We tested the
   SDP prediction method on the LacI family of bacterial transcription
   factors and a sample of bacterial water and glycerol transporters
   belonging to the major intrinsic protein (MIP) family. In both cases,
   the comparison with available experimental and structural data strongly
   supported our predictions.}},
DOI = {{10.1110/ps.03191704}},
ISSN = {{0961-8368}},
EISSN = {{1469-896X}},
ResearcherID-Numbers = {{Kalinina, Olga/M-8482-2016
   Mironov, Andrey/C-8024-2012
   Gelfand, Mikhail/F-3425-2012
   }},
ORCID-Numbers = {{Kalinina, Olga/0000-0002-9445-477X
   Gelfand, Mikhail/0000-0003-4181-0846}},
Unique-ID = {{ISI:000188411000015}},
}

@article{ ISI:000187990000012,
Author = {Hartley, S and Kunin, WE and Lennon, JJ and Pocock, MJO},
Title = {{Coherence and discontinuity in the scaling of species' distribution
   patterns}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2004}},
Volume = {{271}},
Number = {{1534}},
Pages = {{81-88}},
Month = {{JAN 7}},
Abstract = {{The spatial distribution of a species can be characterized at many
   different spatial scales, from fine-scale measures of local population
   density to coarse-scale geographical-range structure. Previous studies
   have shown a degree of correlation in species' distribution patterns
   across narrow ranges of scales, making it possible to predict fine-scale
   properties from coarser-scale distributions. To test the limits of such
   extrapolation, we have compiled distributional information on 16 species
   of British plants, at scales ranging across six orders of magnitude in
   linear resolution (1 in to 100 km). As expected, the correlation between
   patterns at different spatial scales tends to degrade as the scales
   become more widely separated. There is, however, an abrupt breakdown in
   cross-scale correlations across intermediate (ca. 0.5 km) scales,
   suggesting that local and regional patterns are influenced by
   essentially non-overlapping sets of processes. The scaling discontinuity
   may also reflect characteristic scales of human land use in Britain,
   suggesting a novel method for analysing the `footprint' of humanity on a
   landscape.}},
DOI = {{10.1098/rspb.2003.2531}},
ISSN = {{0962-8452}},
EISSN = {{1471-2954}},
ResearcherID-Numbers = {{Pocock, Michael/A-5632-2012
   Hartley, Stephen/A-2794-2008
   Lennon, Jack/F-2475-2011}},
ORCID-Numbers = {{Pocock, Michael/0000-0003-4375-0445
   Hartley, Stephen/0000-0002-9049-5072
   }},
Unique-ID = {{ISI:000187990000012}},
}

@article{ ISI:000220130800002,
Author = {Raupach, MR and Lu, H},
Title = {{Representation of land-surface processes in aeolian transport models}},
Journal = {{ENVIRONMENTAL MODELLING \& SOFTWARE}},
Year = {{2004}},
Volume = {{19}},
Number = {{2}},
Pages = {{93-112}},
Abstract = {{Mathematical modelling provides an essential tool for understanding
   aeolian transport of dust and sand, its spatial and temporal dynamics,
   and its role in the earth system. As much of the crucial action takes
   place at or just above land surfaces, adequate representation of
   land-surface processes is essential. In this paper, we review progress
   in aeolian transport modelling during last two decades. with emphasis on
   the representation of several key land-surface processes: (1) saltation;
   (2) dust uplift; (3) the threshold for particle motion, including the
   effects of surface sheltering and cohesion; (4) turbulent diffusion near
   the ground; and (5) deposition of particles to the surface. These
   process models are then placed in the context of integrated models of
   aeolian transport, in which the conservation equation for atmospheric
   dust is solved over a large region or the globe. We identify and assess
   four challenges in large-scale aeolian transport modelling: (1) the
   fidelity of process representations, (2) upscaling point-scale process
   models in the presence of unresolved heterogeneity in space and time
   (where a new approach is suggested), (3) availability of spatial data on
   model inputs and boundary conditions, and (4) large-scale parameter
   estimation. Crown copyright (C) 2003 Published by Elsevier Ltd. All
   rights reserved.}},
DOI = {{10.1016/S1364-8152(03)00113-0}},
ISSN = {{1364-8152}},
EISSN = {{1873-6726}},
Unique-ID = {{ISI:000220130800002}},
}

@article{ ISI:000220234300005,
Author = {Spiro, PA and Macura, N},
Title = {{A local alignment metric for accelerating biosequence database search}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2004}},
Volume = {{11}},
Number = {{1}},
Pages = {{61-82}},
Abstract = {{We introduce a metric for local sequence alignments that has utility for
   accelerating optimal alignment searches without loss of sensitivity. The
   metric's triangle inequality property permits identification of
   redundant database entries guaranteed to have optimal alignments to the
   query sequence that fall below a specified score threshold, thereby
   permitting comparisons to these entries to be skipped. We prove the
   existence of the metric for a variety of scoring systems, including the
   most commonly used ones, and show that a triangle inequality can be
   established as well for nucleotide-to-protein sequence comparisons. We
   discuss a database clustering and search strategy that takes advantage
   of the triangle inequality. The strategy permits moderate but
   significant acceleration of searches against the widely used ``nr{''}
   protein database. It also provides a theoretically based method for
   database clustering in general and provides a standard against which to
   compare heuristic clustering strategies.}},
DOI = {{10.1089/106652704773416894}},
ISSN = {{1066-5277}},
EISSN = {{1557-8666}},
Unique-ID = {{ISI:000220234300005}},
}

@article{ ISI:000188322300013,
Author = {Williams, AEP and Hunt, ER},
Title = {{Accuracy assessment for detection of leafy spurge with hyperspectral
   imagery}},
Journal = {{JOURNAL OF RANGE MANAGEMENT}},
Year = {{2004}},
Volume = {{57}},
Number = {{1}},
Pages = {{106-112}},
Month = {{JAN}},
Abstract = {{When flowering, leafy spurge (Euphorbia esula L.) has conspicuous
   yellow-green bracts that are spectrally distinct from other vegetation
   and may be distinguished with hyperspectral remote sensing. In July
   1999, Airborne Visible Infrared Imaging Spectrometer (AVIRIS) data were
   acquired in northeastern Wyoming, near Devils Tower National Monument.
   Using the reflectance spectrum of flowering leafy spurge, leafy spurge
   occurrence was determined using a new method of spectral mixture
   analysis, Mixture Tuned Matched Filtering (MTMF). Ground reference data
   (146 sites) were obtained 2 weeks before and after AVIRIS overflight to
   test the classification accuracy of leafy spurge. For 3 land cover
   types: mixed prairie, riparian, and coniferous woodlands, the presence
   or absence leafy spurge was detected with an overall accuracy of 95\%
   using a 0.10 threshold for detection. Differences in classification
   thresholds resulted in a trade-off between false positives, pixels that
   were mapped as leafy spurge but did not contain leafy spurge on the
   ground, and false negatives, areas that had leafy spurge on the ground
   but were not mapped as leafy spurge. Detection of leafy spurge
   occurrence was best for mixed prairie and riparian cover types, and
   somewhat less successful for conifer woodlands because of interference
   from tree crowns and their shadows. The advantage of the MTMF technique
   is it allows automated processing of hyperspectral imagery to generate
   accurate maps of leafy spurge occurrence.}},
DOI = {{10.2307/4003961}},
ISSN = {{0022-409X}},
Unique-ID = {{ISI:000188322300013}},
}

@article{ ISI:000189394100005,
Author = {Hall, O and Hay, GJ and Bouchard, A and Marceau, DJ},
Title = {{Detecting dominant landscape objects through multiple scales: An
   integration of object-specific methods and watershed segmentation}},
Journal = {{LANDSCAPE ECOLOGY}},
Year = {{2004}},
Volume = {{19}},
Number = {{1}},
Pages = {{59-76}},
Abstract = {{Complex systems, such as landscapes, are composed of different critical
   levels of organization where interactions are stronger within levels
   than among levels, and where each level operates at relatively distinct
   time and spatial scales. To detect significant features occurring at
   specific levels of organization in a landscape, two steps are required.
   First, a multiscale dataset must be generated from which these features
   can emerge. Second, a procedure must be developed to delineate
   individual image-objects and identify them as they change through scale.
   In this paper, we introduce a framework for the automatic definition of
   multiscale landscape features using object-specific techniques and
   marker-controlled watershed segmentation. By applying this framework to
   a high-resolution satellite scene, image-objects of varying size and
   shape can be delineated and studied individually at their characteristic
   scale of expression. This framework involves three main steps: 1)
   multiscale dataset generation using an object-specific analysis and
   upscaling technique, 2) marker-controlled watershed transformation to
   automatically delineate individual image-objects as they evolve through
   scale, and 3) landscape feature identification to assess the
   significance of these image-objects in terms of meaningful landscape
   features. This study was conducted on an agro-forested region in
   southwest Quebec, Canada, using IKONOS satellite data. Results show that
   image-objects tend to persist within one or two scale domains, and then
   suddenly disappear at the next, while new image-objects emerge at
   coarser scale domains. We suggest that these patterns are associated to
   sudden shifts in the entire image structure at certain scale domains,
   which may correspond to critical landscape thresholds.}},
DOI = {{10.1023/B:LAND.0000018371.43447.1f}},
ISSN = {{0921-2973}},
ORCID-Numbers = {{Hall, Ola/0000-0002-9231-4028}},
Unique-ID = {{ISI:000189394100005}},
}

@article{ ISI:000187501100009,
Author = {Zhang, WY and Yao, QW and Tong, H and Stenseth, NC},
Title = {{Smoothing for spatiotemporal models and its application to modeling
   muskrat-mink interaction}},
Journal = {{BIOMETRICS}},
Year = {{2003}},
Volume = {{59}},
Number = {{4}},
Pages = {{813-821}},
Month = {{DEC}},
Abstract = {{For a set of spatially dependent dynamical models, we propose a method
   for estimating parameters that control temporal dynamics by spatial
   smoothing. The new approach is particularly relevant for analyzing
   spatially distributed panels of short time series. The asymptotic
   results show that spatial smoothing will improve the estimation in the
   presence of nugget effect, even when the sample size in each location is
   large. The proposed methodology is used to analyze the annual mink and
   muskrat data collected in a period of 25 years in 81 Canadian locations.
   Based on the proposed method, we are able to model the temporal dynamics
   which reflects the food chain interaction of the two species.}},
DOI = {{10.1111/j.0006-341X.2003.00095.x}},
ISSN = {{0006-341X}},
ResearcherID-Numbers = {{Stenseth, Nils Chr./G-5212-2016
   }},
ORCID-Numbers = {{Stenseth, Nils Chr./0000-0002-1591-5399
   Yao, Qiwei/0000-0003-2065-8486}},
Unique-ID = {{ISI:000187501100009}},
}

@article{ ISI:000187969400005,
Author = {Paraskevis, D and Lemey, P and Salemi, M and Suchard, M and Van de Peer,
   Y and Vandamme, AM},
Title = {{Analysis of the evolutionary relationships of HIV-1 and SIVcpz sequences
   using Bayesian inference: Implications for the origin of HIV-1}},
Journal = {{MOLECULAR BIOLOGY AND EVOLUTION}},
Year = {{2003}},
Volume = {{20}},
Number = {{12}},
Pages = {{1986-1996}},
Month = {{DEC}},
Abstract = {{The most plausible origin of HIV-1 group M is an SIV lineage currently
   represented by SIVcpz isolated from the chimpanzee subspecies Pan
   troglodytes troglodytes. The origin of HIV-1 group 0 is less clear.
   Putative recombination between any of the HIVA-1 and SlVcpz sequences
   was tested using bootscanning and Bayesian-scanning plots, as well as a
   new method using a Bayesian multiple change-point (BMCP) model to infer
   parental sequences and crossing-over points. We found that in the case
   of highly divergent sequences, such as HIV-1/SIVcpz, Bayesian scanning
   and BMCP methods are more appropriate than bootscanning analysis to
   investigate spatial phylogenetic variation, including estimating the
   boundaries of the regions with discordant evolutionary relationships and
   the levels of support of the phylogenetic clusters under study.
   According to the Bayesian scanning plots and BMCP method, there was
   strong evidence for discordant phylogenetic clustering throughout the
   genome: (1) HIV-1 group 0 clustered with SIVcpzANT/ TAN in middle pol,
   and partial vif/env; (2) SIVcpzGab1 clustered with SIVcpzANT/TAN in
   3'pol/vif, and middle env; (3) HIV-1 group 0 grouped with SIVcpzCamUS
   and SIVcpzGab1 in pl7/p24; (4) HIV-1 group M was more closely related to
   SIVcpzCamUS in 3'gag/pol and in middle pol, whereas in partial gp120
   group M clustered with group O. Conditionally independent phylogenetic
   analysis inferred by maximum likelihood (ML) and Bayesian methods
   further confirmed these findings. The discordant phylogenetic
   relationships between the HIV-1/SlVcpz sequences may have been caused by
   ancient recombination events, but they are also due, at least in part,
   to altered rates of evolution between parental SIVcpz lineages.}},
DOI = {{10.1093/molbev/msg207}},
ISSN = {{0737-4038}},
EISSN = {{1537-1719}},
ResearcherID-Numbers = {{Van de Peer, Yves/D-4388-2009
   Lemey, Philippe/C-3755-2018
   Vandamme, Anne-Mieke/I-4127-2012
   }},
ORCID-Numbers = {{Van de Peer, Yves/0000-0003-4327-3730
   Lemey, Philippe/0000-0003-2826-5353
   Vandamme, Anne-Mieke/0000-0002-6594-2766
   Paraskevis, Dimitrios/0000-0001-6167-7152}},
Unique-ID = {{ISI:000187969400005}},
}

@article{ ISI:000186913200010,
Author = {Pierce, KE and Rice, JE and Sanchez, JA and Wangh, LJ},
Title = {{Detection of cystic fibrosis alleles from single cells using molecular
   beacons and a novel method of asymmetric real-time PCR}},
Journal = {{MOLECULAR HUMAN REPRODUCTION}},
Year = {{2003}},
Volume = {{9}},
Number = {{12}},
Pages = {{815-820}},
Month = {{DEC}},
Abstract = {{We present a method for rapid and accurate identification of the normal
   and DeltaF508 alleles of the cystic fibrosis (CF) gene in single human
   cells that utilizes LATE (linear after the exponential)-PCR, a newly
   invented form of asymmetric PCR. Detection of the single-stranded
   amplicon is carried out in real time, using allele-specific molecular
   beacons. The LATE-PCR method permits controlled abrupt transition from
   exponential to linear amplification and thereby enhances the fluorescent
   signals and reduces variability between replicate samples relative to
   those obtained using typical real-time PCR. Of 239 single lymphoblasts
   generating amplification signals, 227 (95\%) exhibited signals that met
   objective quantitative criteria required for diagnosis. Among these
   samples, 222 were genotyped correctly, for an assay accuracy of 98\%.
   The small number of diagnostic errors was due to allele drop-out among
   heterozygous lymphoblasts, 4/119 (3.4\%), and contamination among
   homozygous DeltaF508 lymphoblasts, 1/57 (1.8\%). LATE-PCR offers a new
   strategy for preimplantation genetic diagnosis and other fields in which
   accurate quantitative detection of single copy genes is important.}},
DOI = {{10.1093/molehr/gag100}},
ISSN = {{1360-9947}},
Unique-ID = {{ISI:000186913200010}},
}

@article{ ISI:000185544500008,
Author = {Tran, D and Fournier, E and Durrieu, G and Massabuau, JC},
Title = {{Copper detection in the Asiatic clam Corbicula fluminea: optimum valve
   closure response}},
Journal = {{AQUATIC TOXICOLOGY}},
Year = {{2003}},
Volume = {{65}},
Number = {{3}},
Pages = {{317-327}},
Month = {{NOV 19}},
Abstract = {{When exposed to a contaminant, bivalves close their shell as a
   protective strategy. The aim of the present study was to estimate the
   maximum expected dissolved copper sensitivity in the freshwater bivalve
   Corbicula fluminea using a new approach to determine their potential and
   limit to detect contaminants. To take into account the rate of
   spontaneous closures, we integrated stress problems associated with
   fixation by a valve in usual valvometers and the spontaneous rhythm
   associated with nycthemeral activity, to optimize the response in
   conditions where the probability of spontaneous closing was lowest.
   Moreover, we used an original system with impedance valvometry, using
   lightweight impedance electrodes, to study free-ranging animals in low
   stress conditions combined with an analytical approach describing
   dose-response curves by logistic regression, with valve closure reaction
   as a function of response time and concentration of contaminant. In C
   fluminea, we estimated that copper concentrations >4 mug/l (95\%
   confidence interval (CI95\%), 2.3-8.8 mug/l) must be detected within 5 h
   after Cut addition. Lower values could not be distinguished from
   background noise. The threshold values were 2.5 times higher than the
   values reported in the literature. (C) 2003 Elsevier B.V. All rights
   reserved.}},
DOI = {{10.1016/S0166-445X(03)00156-5}},
ISSN = {{0166-445X}},
EISSN = {{1879-1514}},
ORCID-Numbers = {{Massabuau, Jean-Charles/0000-0003-1302-1719}},
Unique-ID = {{ISI:000185544500008}},
}

@article{ ISI:000187363200010,
Author = {Anten, NPR and Martinez-Ramos, M and Ackerly, DD},
Title = {{Defoliation and growth in an understory palm: Quantifying the
   contributions of compensatory responses}},
Journal = {{ECOLOGY}},
Year = {{2003}},
Volume = {{84}},
Number = {{11}},
Pages = {{2905-2918}},
Month = {{NOV}},
Abstract = {{We analyzed to what extent and by what mechanisms plants of the tropical
   understory palm Chamaedorea elegans are able to mitigate the negative
   effects of defoliation on performance (i.e., plant size, total growth,
   leaf lamina growth, and reproduction) and how this is related to light
   availability. For this purpose we developed a new approach that allowed
   us to quantify the performance of defoliated plants relative not only to
   the performance of undamaged plants, but also relative to the estimated
   performance of hypothetical defoliated plants that do not exhibit any.
   mechanisms of compensatory growth. The latter provides a way to quantify
   the adaptive value of compensation with reference to a hypothetical
   noncompensating alternative state. C. elegans plants were grown in a
   greenhouse at two light levels (5\% and 16\% of natural daylight) and
   subjected to five defoliation treatments (a control and four levels of
   defoliation). Defoliation was repeated every three months. Growth
   analysis revealed that defoliated plants allocated considerably more
   mass to the production of leaf laminas (f(lam)) than control plants, at
   the expense of allocation to other organs, particularly reproductive
   structures. Average growth rates per unit leaf area (NAR) and per unit
   plant mass (RGR), both measured on the basis of above-ground mass,
   increased with the level of defoliation at high light but not at low
   light. We estimated that the increases in f(lam) and NAR enabled C.
   elegans to compensate for part of the potential loss in performance
   caused by defoliation, even in cases where their RGR values were lower
   than those of control plants. Sensitivity analysis indicated that
   changes in NAR contributed more to this compensation than f(lam), but
   the importance of f(lam) increased with defoliation level and with
   decreasing light availability. The degree of compensation was higher in
   the high- than in the low-light treatment, suggesting that the
   possession of traits associated with compensatory growth. may be more
   important in sunny than in shaded environments. The degree of
   compensation differed depending on the measure of performance.
   Defoliated plants fully compensated for the potential reduction in
   lamina growth but compensated for <20\% of estimated loss in
   reproductive output. Since survival of C. elegans plants appears to be.
   strongly associated with their total leaf area, the greater compensation
   for lamina growth is important in relation to population dynamics.}},
DOI = {{10.1890/02-0454}},
ISSN = {{0012-9658}},
ResearcherID-Numbers = {{Ackerly, David/A-1247-2009}},
ORCID-Numbers = {{Ackerly, David/0000-0002-1847-7398}},
Unique-ID = {{ISI:000187363200010}},
}

@article{ ISI:000186357000003,
Author = {Djordjevic, M and Sengupta, AM and Shraiman, BI},
Title = {{A biophysical approach to transcription factor binding site discovery}},
Journal = {{GENOME RESEARCH}},
Year = {{2003}},
Volume = {{13}},
Number = {{11}},
Pages = {{2381-2390}},
Month = {{NOV}},
Abstract = {{Identification of transcription factor binding sites within regulatory
   segments of genomic DNA is ail important step toward understanding of
   the regulatory circuits that control expression of genes. Here, we
   describe a novel bioinformatics method that bases classification of
   potential binding sites explicitly on the estimate of sequence-specific
   binding energy of a given transcription factor. The method also
   estimates the chemical potential of the factor that defines the
   threshold of binding. In contrast with the widely used
   information-theoretic weight matrix method, the new approach correctly
   describes saturation in the transcription factor/DNA binding
   probability. This results in a significant improvement in the number of
   expected false positives, particularly in the ubiquitous case of
   low-specificity factors. In the strong binding limit, the algorithm is
   related to the ``support vector machine{''} approach to pattern
   recognition. The new method is used to identify likely genomic binding
   sites for the L coli transcription factors collected in the DPInteract
   database. In addition, for CRP (a global regulatory factor), the likely
   regulatory modality (i.e., repressor or activator) of predicted binding
   sites is determined.}},
DOI = {{10.1101/gr.1271603}},
ISSN = {{1088-9051}},
ResearcherID-Numbers = {{Djordjevic, Marko/H-1745-2016
   Sengupta, Anirvan/G-2508-2012}},
Unique-ID = {{ISI:000186357000003}},
}

@article{ ISI:000185696500046,
Author = {Gross, A and Besov, A and Reck, DD and Sorek, S and Ben-Dor, G and
   Britan, A and Palchikov, E},
Title = {{Application of waves for remediation of contaminated aquifers}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2003}},
Volume = {{37}},
Number = {{19}},
Pages = {{4481-4486}},
Month = {{OCT 1}},
Abstract = {{A theory developed suggested that significant displacement of solute in
   saturated porous media results from the propagation of compression
   waves. Four independent one-dimensional experimental setups and a
   variety of laboratory methods were used to confirm the predictions of
   the theory, specifically aimed at developing a novel method of inducing
   compression waves for use in remediation of contaminated aquifers.
   Compaction and shock waves were emitted through granular porous media
   saturated with saline water. The changes in solute concentration at
   observation points along the propagating wave were used to verify the
   validity of theory. The first setup was designed mainly to provide a
   qualitative assessment (i.e., changes in pressure due to the propagating
   wave were not recorded). In situ quantitative measurements of the
   pressure and electrical conductivity profiles along a sand column were
   done with the second and third experimental setups, respectively, to
   short and long shock waves. In the fourth setup, solute displacement was
   visualized by X-ray absorption. The findings were consistent with the
   theory in all experimental setups.}},
DOI = {{10.1021/es026297d}},
ISSN = {{0013-936X}},
ResearcherID-Numbers = {{GROSS, AMIT/F-1464-2012
   Besov, Alexey/E-9479-2012
   }},
ORCID-Numbers = {{Besov, Alexey/0000-0002-9188-5419
   Ben-Dor, Gabi/0000-0001-6890-8363
   Gross, Amit/0000-0002-6223-7084}},
Unique-ID = {{ISI:000185696500046}},
}

@article{ ISI:000185596600009,
Author = {Massot, M and Huey, RB and Tsuji, J and van Berkum, FH},
Title = {{Genetic, prenatal, and postnatal correlates of dispersal in hatchling
   fence lizards (Sceloporus occidentalis)}},
Journal = {{BEHAVIORAL ECOLOGY}},
Year = {{2003}},
Volume = {{14}},
Number = {{5}},
Pages = {{650-655}},
Month = {{SEP}},
Abstract = {{Similarity of dispersal behavior among siblings is common in
   vertebrates. However, little is known about the factors (genetic,
   prenatal, postnatal) generating this similarity. Here we analyzed
   potential influences on the dispersal patterns of multiple families of
   hatchling fence lizards, Sceloporus occidentalis. We captured near-term
   females from the field, incubated their eggs in the laboratory, measured
   various traits of the hatchlings and dams, and then released the
   hatchlings at a number of sites in nature. We recaptured hatchlings 5-6
   weeks later and measured the direct distance to the release site.
   Because we treated hatchlings (from eggs to release) randomly with
   respect to sibship, we eliminated the possibility that any observed
   sibling similarity in dispersal is merely an artifact of common
   postnatal influences. To analyze dispersal, we developed a new method
   that does not make an arbitrary choice of a threshold distance
   separating dispersers from nondispersers. We found a significant family
   effect on dispersal. We suspect that this family effect originates from
   genetic influences rather than from prenatal ones. Indeed, hatchling
   dispersal was remarkably unrelated to numerous traits (of clutches,
   mothers, or hatchlings) that might reflect prenatal effects. However, we
   did find that males were more likely to disperse than females, as
   predicted for polygynous species. Finally, characteristics of the
   release site did not appear to influence dispersal.}},
DOI = {{10.1093/beheco/arg056}},
ISSN = {{1045-2249}},
EISSN = {{1465-7279}},
ResearcherID-Numbers = {{Huey, Raymond/F-1597-2010
   }},
ORCID-Numbers = {{Huey, Raymond/0000-0002-4962-8670}},
Unique-ID = {{ISI:000185596600009}},
}

@article{ ISI:000185337300013,
Author = {Heil, SG and Kluijtmans, LAJ and Spiegelstein, O and Finnell, RH and
   Blom, HJ},
Title = {{Gene-specific monitoring of T7-based RNA amplification by real-time
   quantitative PCR}},
Journal = {{BIOTECHNIQUES}},
Year = {{2003}},
Volume = {{35}},
Number = {{3}},
Pages = {{502+}},
Month = {{SEP}},
Abstract = {{T7-based RNA amplification is applied when there is insufficient RNA.
   The overall extent of amplification can be
   measured-spectrophotometrically (i.e., quantifying RNA yields), but this
   measurement does not provide information about the RNA amplification of
   individual genes. Here we describe a method applying real-time
   quantitative PCR, which enables the monitoring of RNA amplification of
   individual genes. The amount of RNA before and after T7-based RNA
   amplification was determined by real-time quantitative PCR for three
   housekeeping genes: beta-2-microglobulin, porphobilinogen deaminase, and
   serine dehydratase, which are, respectively, a high, intermediate/low,
   and low copy transcript. Real-time quantitative PCR appeared to be
   suitable to-determine the extent of RNA amplification, as was reflected
   by the low intra- and inter-run coefficients of variation of cycle
   threshold of 1.1\%-2.1\%. The application of real-time quantitative PCR
   showed that T7-based RNA amplification is reproducible but might
   introduce a sequence-specific bias. Real-time quantitative PCR is a
   novel approach to monitor RNA amplification and is particularly suited
   to study RNA amplification of individual genes.}},
DOI = {{10.2144/03353st03}},
ISSN = {{0736-6205}},
ResearcherID-Numbers = {{Kluijtmans, Leo/L-4437-2015
   }},
ORCID-Numbers = {{Finnell, Richard/0000-0002-5962-8754}},
Unique-ID = {{ISI:000185337300013}},
}

@article{ ISI:000185832800001,
Author = {Vargas, JA and del Castillo, RF},
Title = {{Nuclear androdioecy and gynodioecy}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{2003}},
Volume = {{47}},
Number = {{3}},
Pages = {{199-221}},
Month = {{SEP}},
Abstract = {{We formulate two single-locus Mendelian models, one for androdioecy and
   the other one for gynodioecy, each with 3 parameters: t the male
   (female) fertility rate of males (females) to hermaphrodites, s the
   fraction of the progeny derived from selfing; and g the fitness of
   inbreeders. Each model is expressed as a transformation of a 3
   dimensional zygotic algebra, which we interpret as a rational map of the
   projective plane. We then study the dynamics for the evolution of each
   reproductive system; and compare our results with similar published
   models. In this process, we introduce a general concept of fitness and
   list some of its properties, obtaining a relative measure of population
   growth, computable as an eigenvalue of a mixed mating transformation for
   a population in equilibrium.
   Our results concur with previous models of the evolution of androdioecy
   and gynodioecy regarding the threshold values above which the sexual
   polymophism is stable, although the previous models assume constant the
   fraction of ovules from hermaphrodites that are self pollinated, while
   we assume constant the fraction of the progeny derived from selfing. A
   stable androdioecy requires more stringent conditions than a stable
   gynodioecy if the amount of pollen used for selfing is negligible in
   comparison with the total amount of pollen produced by hermaphrodites.
   Otherwise, both models are identical. We show explicitly that the
   genotype fitnesses depend linearly on their frequencies. Simulations
   show that any population not at equilibrium always converges to the
   equilibrium point of higher fitness. However, at intermediate steps, the
   fitness function occasionally decreases.}},
DOI = {{10.1007/s00285-003-0200-3}},
ISSN = {{0303-6812}},
Unique-ID = {{ISI:000185832800001}},
}

@article{ ISI:000184161600005,
Author = {Roberts, MG and Heesterbeek, JAP},
Title = {{A new method for estimating the effort required to control an infectious
   disease}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2003}},
Volume = {{270}},
Number = {{1522}},
Pages = {{1359-1364}},
Month = {{JUL 7}},
Abstract = {{We propose a new threshold quantity for the analysis of the epidemiology
   of infectious diseases. The quantity is similar in concept to the
   familiar basic reproduction ratio, R-0, but it singles out particular
   host types instead of providing a criterion that is uniform for all host
   types. Using this methodology we are able to identify the long-term
   effects of disease-control strategies for particular subgroups of the
   population, to estimate the level of control necessary when targeting
   control effort at a subset of host types, and to identify host types
   that constitute a reservoir of infection. These insights cannot be
   obtained by using R-0 alone.}},
DOI = {{10.1098/rspb.2003.2339}},
ISSN = {{0962-8452}},
Unique-ID = {{ISI:000184161600005}},
}

@article{ ISI:000183818900026,
Author = {Koyama, M and Heerdt, PM and Levi, R},
Title = {{Increased severity of reperfusion arrhythmias in mouse hearts lacking
   histamine H-3-receptors}},
Journal = {{BIOCHEMICAL AND BIOPHYSICAL RESEARCH COMMUNICATIONS}},
Year = {{2003}},
Volume = {{306}},
Number = {{3}},
Pages = {{792-796}},
Month = {{JUL 4}},
Abstract = {{We had previously reported that activation of histamine H-3-receptors
   (H3R) on cardiac adrenergic nerve terminals decreases norepinephrine
   (NE) overflow from ischemic hearts and alleviates reperfusion
   arrhythmias. Thus, we used transgenic mice lacking H3R (H3R-/-) to
   investigate whether ischemic arrhythmias might be more severe in H3R-/-
   hearts than in hearts with intact H3R (H3R+/+). We report a greater
   incidence and longer duration of ventricular fibrillation (VF) in H3R-/-
   hearts subjected to ischemia. VF duration was linearly correlated with
   NE overflow, suggesting a possible cause-effect relationship between
   magnitude of NE release and severity of reperfusion arrhythmias. Thus,
   our findings strengthen a protective antiarrhythmic role of H3R in
   myocardial ischemia. Since malignant tachyarrhythmias cause sudden death
   in ischemic heart disease, attenuation of NE release by selective H3R
   agonists may represent a new approach in the prevention and treatment of
   ischemic arrhythmias. (C) 2003 Elsevier Science (USA). All rights
   reserved.}},
DOI = {{10.1016/S0006-291X(03)01010-6}},
ISSN = {{0006-291X}},
Unique-ID = {{ISI:000183818900026}},
}

@article{ ISI:000207434200023,
Author = {Johansson, Oe. and Alkema, W. and Wasserman, W. W. and Lagergren, J.},
Title = {{Identification of functional clusters of transcription factor binding
   motifs in genome sequences: the MSCAN algorithm}},
Journal = {{BIOINFORMATICS}},
Year = {{2003}},
Volume = {{19}},
Number = {{1}},
Pages = {{i169-i176}},
Month = {{JUL}},
Abstract = {{Motivation: The identification of regulatory control regions within
   genomes is a major challenge. Studies have demonstrated that regulating
   regions can be described as locally dense clusters or modules of
   cis-acting transcription factor binding sites (TFBS). For well-described
   biological contexts, it is possible to train predictive algorithms to
   discern novel modules in genome sequences. However, utility of module
   detection methods has been severely limited by insufficient training
   data. For only a few tissues can one obtain sufficient numbers of
   literature-derived regulatory modules.
   Results: We present a novel method, MSCAN, that circumvents the training
   data problem by measuring the statistical significance of any
   non-overlapping combination of TFBS in a window. Given a set of
   transcription factor binding profiles, a significance threshold, and a
   genomic sequence, MSCAN returns putative regulatory regions. We assess
   performance on two curated collections of regulatory regions; one each
   for tissue-specific expression in liver and skeletal muscle cells. The
   efficiency of MSCAN allows for predictive screens of entire genomes.}},
DOI = {{10.1093/bioinformatics/btg1021}},
ISSN = {{1367-4803}},
EISSN = {{1460-2059}},
ResearcherID-Numbers = {{Alkema, Wynand/G-5826-2018
   Wasserman, Wyeth/I-4866-2015
   }},
ORCID-Numbers = {{Alkema, Wynand/0000-0002-5523-9217
   Wasserman, Wyeth/0000-0001-6098-6412}},
Unique-ID = {{ISI:000207434200023}},
}

@article{ ISI:000184092500010,
Author = {Schwinning, S and Starr, BI and Ehleringer, JR},
Title = {{Dominant cold desert plants do not partition warm season precipitation
   by event size}},
Journal = {{OECOLOGIA}},
Year = {{2003}},
Volume = {{136}},
Number = {{2}},
Pages = {{252-260}},
Month = {{JUL}},
Abstract = {{We conducted experiments to examine the quantitative relationships
   between rainfall event size and rainwater uptake and use by four common
   native plant species of the Colorado Plateau, including two perennial
   grasses, Hilaria jamesii (C-4) and Oryzopsis hymenoides (C-3), and two
   shrubs, Ceratoides lanata (C-3), and Gutierrezia sarothrae (C-3).
   Specifically, we tested the hypothesis that grasses use small rainfall
   events more efficiently than shrubs and lose this advantage when events
   are large. Rainfall events between 2 and 20 mm were simulated in spring
   and summer by applying pulses of deuterium-labeled irrigation water.
   Afterwards, pulse water fractions in stems and the rates of leaf gas
   exchange were monitored for 9 days. Cumulative pulse water uptake over
   this interval (estimated by integrating the product of pulse fraction in
   stem water and daytime transpiration rate over time) was approximately
   linearly related to the amount of pulse water added to the ground in all
   four species. Across species, consistently more pulse water was taken up
   in summer than in spring. Relative to their leaf areas, the two grass
   species took up more pulse water than the two shrub species, across all
   event sizes and in both seasons, thus refuting the initial hypothesis.
   In spring, pulse water uptake did not significantly increase
   photosynthetic rates and in summer, pulse water uptake had similar, but
   relatively small effects on the photosynthetic rates of the three C-3
   plants, and a larger effect on the C-4 plant H. jamesii. Based on these
   data, we introduce an alternative hypothesis for the responses of plant
   functional types to rainfall events of different sizes, building on
   cost-benefit considerations for active physiological responses to
   sudden, unpredictable changes in water availability.}},
DOI = {{10.1007/s00442-003-1255-y}},
ISSN = {{0029-8549}},
ResearcherID-Numbers = {{Schwinning, Susanne/G-6412-2015}},
ORCID-Numbers = {{Schwinning, Susanne/0000-0002-9740-0291}},
Unique-ID = {{ISI:000184092500010}},
}

@article{ ISI:000180936300009,
Author = {Holmes, CC and Heard, NA},
Title = {{Generalized monotonic regression using random change points}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2003}},
Volume = {{22}},
Number = {{4}},
Pages = {{623-638}},
Month = {{FEB 28}},
Abstract = {{We introduce a procedure for generalized monotonic curve fitting that is
   based on a Bayesian analysis of the isotonic regression model.
   Conventional isotonic regression fits monotonically increasing step
   functions to data. In our approach we treat the number and location of
   the steps as random. For each step level we adopt the conjugate prior to
   the sampling distribution of the data as if the curve was unconstrained.
   We then propose to use Markov chain Monte Carlo simulation to draw
   samples from the unconstrained model space and retain only those samples
   for which the monotonic constraint holds. The proportion of the samples
   collected for which the constraint holds can be used to provide a value
   for the weight of evidence in terms of Bayes factors for monotonicity
   given the data. Using the samples, probability statements can be made
   about other quantities of interest such as the number of change points
   in the data and posterior distributions on the location of the change
   points can be provided. The method is illustrated throughout by a
   reanalysis of the leukaemia data studied by Schell and Singh. Copyright
   (C) 2003 John Wiley Sons, Ltd.}},
DOI = {{10.1002/sim.1306}},
ISSN = {{0277-6715}},
Unique-ID = {{ISI:000180936300009}},
}

@article{ ISI:000183295600004,
Author = {Bascompte, J},
Title = {{Extinction thresholds: insights from simple models}},
Journal = {{ANNALES ZOOLOGICI FENNICI}},
Year = {{2003}},
Volume = {{40}},
Number = {{2}},
Pages = {{99-114}},
Note = {{Conference on Extinction Thresholds, UNIV HELSINKI, DEPT ECOL SYSTEMAT,
   HELSINKI, FINLAND, SEP 02-05, 2002}},
Abstract = {{There are two types of deterministic extinction thresholds: demographic
   thresholds such as the Allee effect, and parametric thresholds such as a
   critical effective colonization rate or a minimum amount of available
   habitat for metapopulation persistence. I introduce briefly both types
   of thresholds. First, I discuss the Allee effect in the context of
   eradication strategies of alien species. Then, I consider an example of
   parametric threshold: the critical amount of suitable habitat below
   which a metapopulation goes deterministically extinct. I review how this
   spatial threshold changes in relation to the level of spatial detail and
   the complexity of the food web. Since classical metapopulation models
   assume an infinite number of patches, I proceed by considering how the
   extinction threshold is affected by environmental variability acting on
   a small number of patches. Finally, I consider recent work suggesting
   that if the network of connectivity among patches is not random but
   highly heterogeneous, the extinction threshold may disappear.}},
ISSN = {{0003-455X}},
ResearcherID-Numbers = {{Bascompte, Jordi/B-7596-2008
   CSIC, EBD Donana/C-4157-2011}},
ORCID-Numbers = {{Bascompte, Jordi/0000-0002-0108-6411
   CSIC, EBD Donana/0000-0003-4318-6602}},
Unique-ID = {{ISI:000183295600004}},
}

@article{ ISI:000179160200018,
Author = {Roy, DP and Lewis, PE and Justice, CO},
Title = {{Burned area mapping using multi-temporal moderate spatial resolution
   data - a bi-directional reflectance model-based expectation approach}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2002}},
Volume = {{83}},
Number = {{1-2}},
Pages = {{263-286}},
Month = {{NOV}},
Abstract = {{While remote sensing offers the capability for monitoring land surface
   changes, extracting the change information from satellite data requires
   effective and automated change detection techniques. The majority of
   change detection techniques rely on empirically derived thresholds to
   differentiate changes from background variations, which are often
   considered noise. Over large areas, reliable threshold definition is
   problematic due to variations in both the surface state and those
   imposed by the sensing system. A new approach to change detection,
   applicable to high-temporal frequency satellite data, that maps the
   location and approximate day of change occurrence is described. The
   algorithm may be applied to a range of change detection applications by
   using appropriate wavelengths. The approach is applied here to the
   problem of mapping burned areas using moderate spatial resolution
   satellite data. A bi-directional reflectance model is inverted against
   multi-temporal land surface reflectance observations, providing an
   expectation and uncertainty of subsequent observations through time. The
   algorithm deals with angular variations observed in multi-temporal
   satellite data and enables the use of a statistical measure to detect
   change from a previously observed state. The algorithm is applied
   independently to geolocated pixels over a long time series of
   reflectance observations. Large discrepancies between predicted and
   measured values are attributed to change. A temporal consistency
   threshold is used to differentiate between temporary changes considered
   as noise and persistent changes of interest. The algorithm is adaptive
   to the number, viewing and illumination geometry of the observations,
   and to the amount of noise in the data. The approach is demonstrated
   using 56 days of Moderate Resolution Imaging Spectroradiometer (MODIS)
   land surface reflectance data generated for Southern Africa during the
   2000 burning season. Qualitatively, the results show high correspondence
   with contemporaneous MODIS active fire detection results and reveal a
   coherent spatio-temporal progression of burning. Validation of these
   results is in progress and recommendations for future research, pending
   the availability of independent validation data sets, are made. This
   approach is now being considered for the MODIS burned area algorithm.
   (C) 2002 Elsevier Science Inc. All rights reserved.}},
DOI = {{10.1016/S0034-4257(02)00077-9}},
Article-Number = {{PII S0034-4257(02)00077-9}},
ISSN = {{0034-4257}},
ResearcherID-Numbers = {{Lewis, Philip/C-1588-2008
   }},
ORCID-Numbers = {{Lewis, Philip/0000-0002-8562-0633
   Lewis, Philip/0000-0002-9047-9179}},
Unique-ID = {{ISI:000179160200018}},
}

@article{ ISI:000177242600029,
Author = {Hanson, ML and Solomon, KR},
Title = {{New technique for estimating thresholds of toxicity in ecological risk
   assessment}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2002}},
Volume = {{36}},
Number = {{15}},
Pages = {{3257-3264}},
Month = {{AUG 1}},
Abstract = {{The use and utility of the no observed effect concentration (NOEL) in
   ecological risk assessment is a contentious issue. One concern is that
   the NOEC is not representative of a concentration at which no
   biologically significant effect is occurring. A new method has been
   developed to estimate the threshold of toxicity, or a true NOEC, for
   aquatic plants. The method involves determining the effective
   concentration (ECx) of a number of endpoints from one species. These ECx
   values are plotted on a log-probability scale. The x-intercept, or a low
   centile, of the distribution can be interpreted as the threshold of
   toxicity for that plant at that response level. This threshold is the
   concentration at which no effects should be observed for any endpoint
   above that response level. It is based on the assumptions that multiple
   effect measures from a single species will be log-normally distributed
   and that the distribution contains all possible endpoints for that
   species. The thresholds and the distributions can then be used as a
   substitute for the NOEC or ECx in risk assessment techniques, such as
   hazard quotients and probabilistic ecological risk assessment. This new
   method of estimating toxicity thresholds is more realistic than the use
   of arbitrary uncertainty factors, is more conservative than current
   probabilistic risk assessment methods, allows for simple comparison
   between species and exposure duration to a toxicant, and may be useful
   for assessing mixture toxicity. This technique was applied to field
   derived data with Lemna gibba, Myriophyfum spicatum, and M. sibiricum to
   assess potential risks from monochloroacetic acid (MCA). Using this new
   risk assessment method, we conclude that MCA does not appear to pose a
   risk to aquatic macrophytes under field conditions at current
   environmental concentrations.}},
DOI = {{10.1021/es011490d}},
ISSN = {{0013-936X}},
ResearcherID-Numbers = {{Hanson, Mark/C-4620-2008
   }},
ORCID-Numbers = {{Solomon, Keith R/0000-0002-8496-6413}},
Unique-ID = {{ISI:000177242600029}},
}

@article{ ISI:000179211200010,
Author = {Larreta-Garde, V and Berry, H},
Title = {{Modeling extracellular matrix degradation balance with
   proteinase/transglutaminase cycle}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{2002}},
Volume = {{217}},
Number = {{1}},
Pages = {{105-124}},
Month = {{JUL 7}},
Abstract = {{Extracellular matrix mass balance is implied in many physiological and
   pathological events, such as metastasis dissemination. Widely studied,
   its destructive part is mainly catalysed by extracellular proteinases.
   Conversely, the properties of the constructive part are less obvious,
   cellular neo-synthesis being usually considered as its only element. In
   this paper, we introduce the action of transglutaminase in a
   mathematical model for extracellular matrix remodeling. This
   extracellular enzyme, catalysing intermolecular protein cross-linking,
   is considered here as a reverse proteinase as far as the extracellular
   matrix physical state is concerned. The model is based on a
   proteinase/transglutaminase cycle interconverting insoluble matrix and
   soluble proteolysis fragments, with regulation of cellular proteinase
   expression by the fragments. Under ``closed{''} (batch) conditions, i.e.
   neglecting matrix influx and fragment efflux from the system, the model
   is bistable, with reversible hysteresis. Extracellular matrix proteins
   concentration abruptly switches from low to high levels when
   transglutaminase activity exceeds a threshold value. Proteinase
   concentration usually follows the reverse complementary kinetics, but
   can become apparently uncoupled from extracellular matrix concentration
   for some parameter values. When matrix production by the cells and
   fragment degradation are taken into account, the dynamics change to
   sustained oscillations because of the emergence of a stable limit cycle.
   Transitions out of and into oscillation areas are controlled by the
   model parameters. Biological interpretation indicates that these
   oscillations could represent the normal homeostatic situation, whereas
   the other exhibited dynamics can be related to pathologies such as tumor
   invasion or fibrosis. These results allow to discuss the insights that
   the model could contribute to the comprehension of these complex
   biological events. (C) 2002 Elsevier Science Ltd. All rights reserved.}},
DOI = {{10.1006/yjtbi.3010}},
ISSN = {{0022-5193}},
ResearcherID-Numbers = {{Berry, Hugues/C-2321-2011
   larreta garde, veronique/E-9356-2013}},
ORCID-Numbers = {{Berry, Hugues/0000-0003-3470-683X
   larreta garde, veronique/0000-0003-3500-1364}},
Unique-ID = {{ISI:000179211200010}},
}

@article{ ISI:000177434400010,
Author = {Rose, KE and Rees, M and Grubb, PJ},
Title = {{Evolution in the real world: Stochastic variation and the determinants
   of fitness in Carlina vulgaris}},
Journal = {{EVOLUTION}},
Year = {{2002}},
Volume = {{56}},
Number = {{7}},
Pages = {{1416-1430}},
Month = {{JUL}},
Abstract = {{Empirical studies of life histories often ignore stochastic variation,
   despite theoretical demonstrations of its potential impact on
   life-history evolution. Here we use a novel approach to explore the
   effects of stochastic variation on life-history evolution and estimate
   the selection pressures operating on the monocarpic perennial Carlina
   vulgaris, in which flowering may be delayed by up to eight years. The
   approach is novel in that we use modern theoretical techniques to
   estimate selection pressures and the fitness landscape from a fully
   parameterised individual-based model. These approaches take into account
   temporal variation in demographic rates and density dependence. Analysis
   of 16 years' data revealed significant temporal variation in growth,
   mortality, and recruitment in our study population. Flowering was
   strongly size dependent and, unusually for such a species, also age
   dependent. Individual-based models of the flowering strategy,
   parameterized using field data, consistently underestimated the size at
   flowering, when temporal variation in demographic rates was ignored. In
   contrast, models that incorporated temporal variation in growth,
   mortality, and recruitment predicted sizes at flowering not
   significantly different from those observed in the field. Temporal
   variation in mortality, which had the largest effect on the flowering
   strategy, selected for increased size at flowering. An analytical
   approximation is presented to explain this result, extending the
   ``1-year look-ahead criterion{''} presented in Rees et al. (2000). A
   fitness landscape generated by following the fate of rare mutant
   invaders with a broad range of alternative flowering strategies
   demonstrated that the observed parameters were adaptive. However, the
   fitness landscape reveals that approximately equal fitness is achieved
   by a broad range of strategies, providing a mechanism for the
   maintenance of genetic variation. To understand how the different
   parameters that defined our models determine the fitness of rare
   mutants, we numerically estimated the elasticities and sensitivities of
   mutant fitness. This demonstrated strong selection on a number of the
   parameters. Elasticities and sensitivities estimated in constant and
   random environments were significantly positively correlated, and both
   were negatively related to the standard error of the parameter. This
   last result is surprising and, we argue, reflects the genetic and
   phenotypic responses to selection.}},
ISSN = {{0014-3820}},
EISSN = {{1558-5646}},
Unique-ID = {{ISI:000177434400010}},
}

@article{ ISI:000177488800017,
Author = {Dell'Anno, A and Mei, ML and Pusceddu, A and Danovaro, R},
Title = {{Assessing the trophic state and eutrophication of coastal marine
   systems: a new approach based on the biochemical composition of sediment
   organic matter}},
Journal = {{MARINE POLLUTION BULLETIN}},
Year = {{2002}},
Volume = {{44}},
Number = {{7}},
Pages = {{611-622}},
Month = {{JUL}},
Abstract = {{We used a biochemical approach based on the analysis of the quality and
   quantity of sedimentary organic matter for identifying new descriptors
   of the trophic state and environmental quality of coastal marine
   systems. A large-scale study, including 99 stations, belonging to 33
   transects, was carried out along 250 km of the Apulian coasts
   (Mediterranean Sea) in March and September 2000. The investigated area
   covered a wide range of anthropogenic impacts (industrial ports, tourist
   harbours, areas affected by power plants and industrial wastes,
   mariculture areas). Other sites, including marine protected areas (i.e.,
   without any apparent impact), were used as ``control{''}. Water column
   and benthic parameters provided different indications and
   classifications of the trophic state of coastal marine systems. We found
   that phytopigment content of the sediments changed in response to all
   different sources of anthropogenic impact and resulted in a useful
   descriptor of the trophic state and environmental quality. Highest
   sediment chlorophyll-a concentrations, indicating conditions of
   increasing cutrophication, were found in areas impacted by the discharge
   of heated waters from a power plant. In particular, the contribution of
   the autotrophic biomass to the biopolymeric carbon pool appeared to be a
   good descriptor of the decreasing environmental quality. Independently
   from the sampling period or the pollution source such contribution was
   significantly lower in transects subjected to anthropogenic impact than
   in control areas. Differences in trophic conditions were evident both in
   terms of quantity (i.e., total organic matter content) and quality
   (i.e., biochemical composition) of sediment organic matter. In
   particular, sediment protein concentration appeared to be a good
   descriptor of the trophic state of the benthic systems at different
   spatial scales. Multivariate (MDS) analysis allowed identifying areas
   characterised by hypertrophic, eutrophic and meso-oligotrophic
   conditions and to define relative threshold levels. A classification of
   the trophic state of coastal systems based on protein and carbohydrate
   concentrations is proposed. (C) 2002 Elsevier Science Ltd. All rights
   reserved.}},
DOI = {{10.1016/S0025-326X(01)00302-2}},
Article-Number = {{PII S0025-326X(01)00302-2}},
ISSN = {{0025-326X}},
EISSN = {{1879-3363}},
ORCID-Numbers = {{Dell'Anno, Antonio/0000-0002-4324-7834}},
Unique-ID = {{ISI:000177488800017}},
}

@article{ ISI:000178937800016,
Author = {Beuselinck, L and Hairsine, PB and Govers, G and Poesen, J},
Title = {{Evaluating a single-class net deposition equation in overland flow
   conditions}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2002}},
Volume = {{38}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{{[}1] The quantitative description of sediment transport across areas of
   net deposition is an essential part of assessing the off-site effects of
   soil erosion on hillslopes. A new approach to describing sediment
   deposition across areas of net deposition, the single-class Hairsine and
   Rose {[}1992a, 1992b] sediment deposition equation, is evaluated here
   using a series of controlled laboratory experiments. Hydraulic and
   sediment transport input parameters, sensitivity analysis, calibration
   using half the experimental data, and evaluation using the remaining
   data are presented. Simulations of overland flow hydraulics are
   evaluated using measured flow velocities. This evaluation shows that the
   Manning's equation gives a good estimate of flow velocities. Calibration
   is conducted by adjusting the fraction of stream power available for
   sediment (re) entrainment (F), and the threshold stream power below
   which no reentrainment occurs (Omega(cr)), to minimize the error between
   the simulated and the measured outflow sediment concentrations. The
   calibration runs show that the single-class Hairsine et al. equation is
   very sensitive to the characteristic settling velocity adopted. The
   evaluation found that good simulations are obtained if appropriate
   threshold values are used. Incorporation of the reentrainment term
   significantly improves sediment outflow simulations above the threshold
   value.}},
DOI = {{10.1029/2001WR000248}},
Article-Number = {{1110}},
ISSN = {{0043-1397}},
ResearcherID-Numbers = {{Govers, Gerard/A-8298-2008
   Hairsine, Peter/G-6156-2011}},
ORCID-Numbers = {{Govers, Gerard/0000-0001-9884-4778
   }},
Unique-ID = {{ISI:000178937800016}},
}

@article{ ISI:000175317600001,
Author = {Fasso, A and Negri, I},
Title = {{Non-linear statistical modelling of high frequency ground ozone data}},
Journal = {{ENVIRONMETRICS}},
Year = {{2002}},
Volume = {{13}},
Number = {{3}},
Pages = {{225-241}},
Month = {{MAY}},
Abstract = {{The problem of describing hourly data of ground ozone is considered. The
   complexity of high frequency environmental data dynamics often requires
   models covering covariates, multiple frequency periodicities, long
   memory, non-linearity and heteroscedasticity. For these reasons we
   introduce a parametric model which includes seasonal fractionally
   integrated components, self-exciting threshold autoregressive
   components, covariates and autoregressive conditionally heteroscedastic
   errors with high tails. For the general model, we present estimation and
   identification techniques.
   To show the model descriptive capability and its use, we analyse a five
   year hourly ozone data set from an air traffic pollution station located
   in Bergamo, Italy. The role of meteo and precursor covariates, periodic
   components, long memory and non-linearity is assessed. Copyright (C)
   2002 John Wiley Sons, Ltd.}},
DOI = {{10.1002/env.509}},
ISSN = {{1180-4009}},
ORCID-Numbers = {{Negri, Ilia/0000-0002-8630-2362}},
Unique-ID = {{ISI:000175317600001}},
}

@article{ ISI:000175453300009,
Author = {Workman, RD and Hayes, DB and Coon, TG},
Title = {{A model of steelhead movement in relation to water temperature in two
   Lake Michigan tributaries}},
Journal = {{TRANSACTIONS OF THE AMERICAN FISHERIES SOCIETY}},
Year = {{2002}},
Volume = {{131}},
Number = {{3}},
Pages = {{463-475}},
Month = {{MAY}},
Abstract = {{We used movement data from two Lake Michigan tributaries to develop a
   new approach for analyzing upstream adult steelhead migration. Our data
   included 28 radio-tagged steelhead Oncorhynchus mykiss in the Pere
   Marquette River and a larger (5,876-10,083-steelhead), multiyear
   (1993-1999) data set of camera-recorded steelhead passages through a
   fishway on the St. Joseph River. To quantitatively predict the
   probability of upstream movement, our model used a rule for
   temperature-based movements developed from the data. Exponential,
   logistic, and power functions were evaluated as possible ways to express
   the probability of movement. Of these, the power function resulted in
   the closest fit between observed and predicted movements. The
   probability of movement increased with increasing water temperatures
   above a movement-threshold water temperature. Stream flow was
   incorporated into the temperature-based movement (TBM) model but did not
   add substantially to the model's ability to describe the migratory
   behavior of steelhead in the Pere Marquette and St. Joseph rivers. The
   TBM modeling approach is broadly applicable and transferable to other
   Great Lakes tributaries and may work well for describing the migratory
   behavior of other species having migrations that depend on water
   temperature.}},
DOI = {{10.1577/1548-8659(2002)131<0463:AMOSMI>2.0.CO;2}},
ISSN = {{0002-8487}},
Unique-ID = {{ISI:000175453300009}},
}

@article{ ISI:000174199200003,
Author = {Cowles, MK},
Title = {{Bayesian estimation of the proportion of treatment effect captured by a
   surrogate marker}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{2002}},
Volume = {{21}},
Number = {{6}},
Pages = {{811-834}},
Month = {{MAR 30}},
Abstract = {{Surrogate endpoints in clinical trials are biological markers or events
   observable earlier than the clinical endpoints (such as death) that are
   actually of primary interest. The `proportion of treatment effect
   captured' by a surrogate endpoint (PTE) is a frequentist measure
   intended to address the question of whether trials based on a surrogate
   endpoint reach the same conclusions as would have been reached using the
   true endpoint. The question of inferential interest is whether PTE for a
   given marker exceeds some threshold value, say 0.5. Calculating PTE
   requires fitting two different models to the same data. We develop a
   Markov chain Monte Carlo based method for estimating the Bayesian
   posterior distribution of PTE. The new method conditions on the truth of
   a single model. Obtaining the full posterior distribution enables direct
   statements such as `the posterior probability that PTE > 0.5 is 0.085'.
   Furthermore, credible sets do not depend on asymptotic approximations
   and can be computed using data sets for which the frequentist methods
   may be inaccurate or even impossible to apply. We illustrate with
   Bayesian proportional hazards models for clinical trial data. As a
   by-product of developing the Bayesian method, we show that the
   frequentist estimate of PTE also may be computed from quantities in a
   single model and calculate frequentist confidence intervals for PTE that
   tend to be narrower than those produced by standard methods but that
   provide equally good coverage. Copyright (C) 2002 John Wiley Sons, Ltd.}},
DOI = {{10.1002/sim.1057}},
ISSN = {{0277-6715}},
Unique-ID = {{ISI:000174199200003}},
}

@article{ ISI:000174229900010,
Author = {Saito, R and Suzuki, H and Hayashizaki, Y},
Title = {{Interaction generality, a measurement to assess the reliability of a
   protein-protein interaction}},
Journal = {{NUCLEIC ACIDS RESEARCH}},
Year = {{2002}},
Volume = {{30}},
Number = {{5}},
Pages = {{1163-1168}},
Month = {{MAR 1}},
Abstract = {{Here we introduce the `interaction generality' measure, a new method for
   computationally assessing the reliability of protein-protein
   interactions obtained in biological experiments. This measure is
   basically the number of proteins involved in a given interaction and
   also adopts the idea that interactions observed in a complicated
   interaction network are likely to be true positives. Using a group of
   yeast protein-protein interactions identified in various biological
   experiments, we show that interactions with low generalities are more
   likely to be reproducible in other independent assays. We constructed
   more reliable networks by eliminating interactions whose generalities
   were above a particular threshold. The rate of interactions with common
   cellular roles increased from 63\% in the unadjusted estimates to 79\%
   in the refined networks. As a result, the rate of cross-talk between
   proteins with different cellular roles decreased, enabling very clear
   predictions of the functions of some unknown proteins. The results
   suggest that the interaction generality measure will make interaction
   data more useful in all organisms and may yield insights into the
   biological roles of the proteins studied.}},
DOI = {{10.1093/nar/30.5.1163}},
ISSN = {{0305-1048}},
ResearcherID-Numbers = {{Suzuki, Harukazu/N-9553-2015}},
ORCID-Numbers = {{Suzuki, Harukazu/0000-0002-8087-0836}},
Unique-ID = {{ISI:000174229900010}},
}

@article{ ISI:000174216400005,
Author = {Cao, M and Kobel, PA and Morshedi, MM and Wu, MFW and Paddon, C and
   Helmann, JD},
Title = {{Defining the Bacillus subtilis sigma(W) regulon: A comparative analysis
   of promoter consensus search, run-off transcription/macroarray analysis
   (ROMA), and transcriptional profiling approaches}},
Journal = {{JOURNAL OF MOLECULAR BIOLOGY}},
Year = {{2002}},
Volume = {{316}},
Number = {{3}},
Pages = {{443-457}},
Month = {{FEB 22}},
Abstract = {{The Bacillus subtilis extracytoplasmic function (ECF) sigma factor
   sigma(W) controls a large regulon that is strongly induced by alkali
   shock. To define the physiological role of sigma(W) we have sought to
   identify the complete set of genes under sigma(W) control. Previously,
   we described a promoter consensus search procedure to identify sigma(W)
   controlled genes. Herein, we introduce a novel method to identify
   additional target promoters: run-off transcription followed by
   macroarray analysis (ROMA). We compare the resulting list of targets
   with those identified in conventional transcriptional profiling studies
   and using the consensus search approach. While transcriptional profiling
   identifies genes that are strongly dependent on sigma(W) for in vivo
   expression, some sigma(W)-dependent promoters are not detected due to
   the masking effects of other promoter elements, overlapping recognition
   with other ECF sigma factors, or both. Taken together, the consensus
   search, ROMA, and transcriptional profiling approaches establish a
   minimum of 30 promoter sites (controlling similar to60 genes) as direct
   targets for activation by sigma(W). Significantly, no single approach
   identifies more than similar to80\% of the regulon so defined. We
   therefore suggest that a combination of two or more complementary
   approaches be employed in studies seeking to achieve maximal coverage
   when defining bacterial regulons. Our results indicate that sigma(W)
   controls genes that protect the cell against agents that impair cell
   wall biosynthesis but fail to reveal any connection to operons likely to
   function in adaptation to alkaline growth conditions. This is consistent
   with the observation that a sigW mutant is unaffected in its ability to
   survive alkali shock. We conclude that in B. subtilis sudden imposition
   of alkali stress activates the sigma(W) stress response, perhaps by
   impairing the ability of the cell wall biosynthetic machinery to
   function. (C) 2002 Elsevier Science Ltd.}},
DOI = {{10.1006/jmbi.2001.5372}},
ISSN = {{0022-2836}},
EISSN = {{1089-8638}},
ResearcherID-Numbers = {{Cao, Min/A-3532-2013
   }},
ORCID-Numbers = {{Cao, Min/0000-0003-1592-8111
   Helmann, John/0000-0002-3832-3249}},
Unique-ID = {{ISI:000174216400005}},
}

@article{ ISI:000176679000003,
Author = {Zhang, J},
Title = {{Analysis of information content for biological sequences}},
Journal = {{JOURNAL OF COMPUTATIONAL BIOLOGY}},
Year = {{2002}},
Volume = {{9}},
Number = {{3}},
Pages = {{487-503}},
Abstract = {{Decomposing a biological sequence into modular domains is a basic
   prerequisite to identify functional units in biological molecules. The
   commonly used segmentation procedures usually have two steps. First,
   collect and align a set of sequences that are homologous to the target
   sequence. Then, parse this multiple alignment into several blocks and
   identify the functionally important ones by using a semi-automatic
   method, which combines manual analysis and expert knowledge. In this
   paper, we present a novel exploratory approach to parsing and analyzing
   such kinds of multiple alignments. It is based on a type of
   analysis-of-variance (ANOVA) decomposition of the sequence information
   content. Unlike the traditional change-point method, this approach takes
   into account not only the composition biases but also the overdispersion
   effects among the blocks. The new approach is tested on the families of
   ribosomal proteins and has a promising performance. It is shown that the
   new approach provides a better way for judging some important residues
   in these proteins. This allows one to find some subsets of residues,
   which are critical to these proteins.}},
DOI = {{10.1089/106652702760138583}},
ISSN = {{1066-5277}},
Unique-ID = {{ISI:000176679000003}},
}

@article{ ISI:000178305300006,
Author = {Pastushenko, VP and Kaderabek, R and Sip, M and Borken, C and
   Kienberger, F and Hinterdorfer, P},
Title = {{Reconstruction of DNA shape from AFM data}},
Journal = {{SINGLE MOLECULES}},
Year = {{2002}},
Volume = {{3}},
Number = {{2-3}},
Pages = {{111-117}},
Abstract = {{DNA plasmid molecules modified with the antitumor drug cisplatin were
   imaged using dynamic force microscopy (DFM) in liquids. We have studied
   the positions of DNA kinks defined as points of abrupt change of the
   direction of the DNA double helix axis. The jumps of the direction were
   found using a method developed earlier for the idealization of
   electrical current through single ion channels (1). Raw AFM data were
   compensated for drift by joining adjacent scan lines of the image at
   flat portions of the sample surface. The quality of the reconstituted
   image is characterized quantitatively by an order parameter that
   describes the image wobble remaining after drift elimination. A new
   method for obtaining the molecule direction (central line) from contours
   of equal height based on the determination of central positions of
   circles inscribed between the inner and the outer contours of the
   molecule has been developed, leading to effective decrease of noise and
   drift. The positions of kinks are in good agreement with results of
   immediate analysis.}},
DOI = {{10.1002/1438-5171(200206)3:2/3<111::AID-SIMO111>3.0.CO;2-B}},
ISSN = {{1438-5163}},
ResearcherID-Numbers = {{Hinterdorfer, Peter/C-4235-2013
   Sip, Miroslav/X-5940-2018}},
ORCID-Numbers = {{Sip, Miroslav/0000-0003-4993-6523}},
Unique-ID = {{ISI:000178305300006}},
}

@article{ ISI:000172692100013,
Author = {Anten, NPR and Ackerly, DD},
Title = {{A new method of growth analysis for plants that experience periodic
   losses of leaf mass}},
Journal = {{FUNCTIONAL ECOLOGY}},
Year = {{2001}},
Volume = {{15}},
Number = {{6}},
Pages = {{804-811}},
Month = {{DEC}},
Abstract = {{1. A new method (the iterative approach) is presented by which growth
   analyses can be conducted on plants that have been subjected to
   significant losses in biomass and leaf area between harvests. The method
   is particularly useful to analyse the effects of defoliation on growth
   and biomass allocation.
   2. Values for the following parameters can be estimated: absolute growth
   rate (g day(-1)), relative growth rate (RGR, g g(-1) day(-1)), net
   assimilation rate (NAR, g m(-2) day(-1)), leaf area ratio (LAR. m(2)
   g(-1)). fraction of newly assimilated mass that is allocated to leaf
   lamina production (f(lam), g g(-1)), and daily fractional change in the
   average specific leaf area of plants (rho, day(-1)). These parameters
   are determined by means of iterations. We defined a number of growth
   functions. and the values of NAR,f(lam) and the SLA of newly produced
   leaves were changed until these functions correctly predicted the
   measured total plant mass. leaf lamina mass and leaf area at the end of
   the growth period. This avoids having to assume a constant relationship
   between leaf area and biomass (as in the `classical' approach), and it
   avoids the use of polynomial functions to fit growth data (as in the
   `functional' approach) that are unsuitable for fitting data sets
   exhibiting discontinuities such as abrupt changes in biomass.
   3. The method was applied to a greenhouse experiment in which we
   analysed the effects of sustained defoliation on growth and biomass
   allocation in a tropical understorey palm, Chamaedorea elegans Mart.
   4. We showed that C elegans plants respond to defoliation with a
   considerable increase in the allocation of new assimilates to lamina
   growth (f(lam)) and that, despite the repeated loss of leaf area and
   associated reductions in LAR, they had RGR values that were similar to
   those of undamaged plants.}},
DOI = {{10.1046/j.0269-8463.2001.00582.x}},
ISSN = {{0269-8463}},
ResearcherID-Numbers = {{Ackerly, David/A-1247-2009}},
ORCID-Numbers = {{Ackerly, David/0000-0002-1847-7398}},
Unique-ID = {{ISI:000172692100013}},
}

@article{ ISI:000172115300017,
Author = {Andreani-Aksoyoglu, S and Lu, CH and Keller, J and Prevot, ASH and
   Chang, JS},
Title = {{Variability of indicator values for ozone production sensitivity: a
   model study in Switzerland and San Joaquin Valley (California)}},
Journal = {{ATMOSPHERIC ENVIRONMENT}},
Year = {{2001}},
Volume = {{35}},
Number = {{32}},
Pages = {{5593-5604}},
Month = {{NOV}},
Abstract = {{The threshold values of indicator species and ratios delineating the
   transition between NOx and VOC sensitivity of ozone formation are
   assumed to be universal by various investigators. However, our previous
   studies suggested that threshold values might vary according to the
   locations and conditions. In this study, threshold values derived from
   various model simulations at two different locations (the area of
   Switzerland by UAM Model and San Joaquin Valley of Central California by
   SAQM Model) are examined using a new approach for defining NOy and VOC
   sensitive regimes. Possible definitions for the distinction of NO, and
   VOC sensitive ozone production regimes are given. The dependence of the
   threshold values for indicators and indicator ratios such as NOy,
   O-3/NOz, HCHO/NOy, and H2O2/HNO3 on the definition of NOx and VOC
   sensitivity is discussed. Then the variations of threshold values under
   low emission conditions and in two different days are examined in both
   areas to check whether the models respond consistently to changes in
   environmental conditions. In both cases, threshold values are shifted
   similarly when emissions are reduced. Changes in the wind fields and
   aging of the photochemical oxidants seem to cause the day-to-day
   variation of the threshold values. O-3/NOz and HCHO/NOy indicators are
   predicted to be unsatisfactory to separate the NOx and VOC sensitive
   regimes. Although NOy and H2O2/HNO3 provide a good separation of the two
   regimes, threshold values are affected by changes in the environmental
   conditions studied in this work. (C) 2001 Elsevier Science Ltd. All
   rights reserved.}},
DOI = {{10.1016/S1352-2310(01)00278-3}},
ISSN = {{1352-2310}},
ResearcherID-Numbers = {{Prevot, Andre/C-6677-2008
   Aksoyoglu, Sebnem/C-7730-2009
   Keller, Johannes/C-7732-2009}},
ORCID-Numbers = {{Prevot, Andre/0000-0002-9243-8194
   Aksoyoglu, Sebnem/0000-0002-5356-5633
   }},
Unique-ID = {{ISI:000172115300017}},
}

@article{ ISI:000171820900017,
Author = {Melin, AM and Perromat, A and Deleris, G},
Title = {{Sensitivity of Deinococcus radiodurans to gamma-irradiation: A novel
   approach by Fourier transform infrared spectroscopy}},
Journal = {{ARCHIVES OF BIOCHEMISTRY AND BIOPHYSICS}},
Year = {{2001}},
Volume = {{394}},
Number = {{2}},
Pages = {{265-274}},
Month = {{OCT 15}},
Abstract = {{Deinococcus radiodurans is a red-pigmented coccus known to be
   particularly resistant to both chemical and radiative agents. Fourier
   transform infrared (FTIR) spectroscopy was used as a convenient and
   easy-to-run method to monitor damage induced in this bacterium by
   ionizing radiations. First, stationary-p ase cultures were submitted to
   increasing doses of gamma -irradiation (Cs-137 source). Beyond a
   threshold of 11 kGy, striking changes occurred in spectra of irradiated
   samples compared with unirradiated ones, especially in the 1750-900
   cm(-1) region, which is spectroscopically assigned to amide I and II
   components, nucleotide bases, the phosphodiester backbone, and the sugar
   ring. Second, bacterial cultures were postirradiation reincubated. After
   a reincubation time of 15 h, the oxidative stress was in part
   overwhelmed, and the growth of D. radiodurans again occurred, although
   some biocellular components remained altered. Consequently, FT-IR
   analysis is an accurate means to rapidly visualize biomolecular changes
   undergone by cells both after gamma -irradiation and during the repair
   mechanism. (C) 2001 Academic Press.}},
DOI = {{10.1006/abbi.2001.2533}},
ISSN = {{0003-9861}},
Unique-ID = {{ISI:000171820900017}},
}

@article{ ISI:000171271800008,
Author = {Inoue, S and Lin, SL and Lee, YC and Inoue, Y},
Title = {{An ultrasensitive chemical method for polysialic acid analysis}},
Journal = {{GLYCOBIOLOGY}},
Year = {{2001}},
Volume = {{11}},
Number = {{9}},
Pages = {{759-767}},
Month = {{SEP}},
Abstract = {{An ultrasensitive method for analysis of polysialic acid (polySia)
   chains, using fluorescence-assisted high-performance liquid
   chromotography was developed. The new method is a substantial
   improvement of our earlier method in which the reducing terminal Sia
   residues of a homologous series of oligo/polySia hydrolytically released
   during derivatization reaction were simultaneously labeled with a
   fluorogenic reagent, 1,2-diamino-4,5-methylenedioxybenzene (DMB) in
   situ. We first studied extensively the stability of oligo/polySia in the
   acid (0.02 M trifluoracetic acid) used for
   1,2-diamino-4,5-methylenedioxybenzene derivatization under various
   conditions of reaction time and temperature, analyzing the hydrolytic
   products by high-performance anion exchange chromatography with pulsed
   electrochemical detection (HPAEC-PED). Then we optimized the reaction
   conditions to minimize degradation of the parent polySia while
   maintaining high derivatization rate. Using a DNAPac PA-100 column
   rather than a MonoQ column, baseline resolution of polySia peaks up to
   DP 90 with a detection threshold of 1.4 femtomol per resolved peak was
   achieved. The new method was used to analyze the degree of
   polymerization of a polySia-containing glycopeptide fraction derived
   from embryonic chicken brain, and the results were compared with those
   obtained by HPAEC-PED.}},
DOI = {{10.1093/glycob/11.9.759}},
ISSN = {{0959-6658}},
Unique-ID = {{ISI:000171271800008}},
}

@article{ ISI:000169072200007,
Author = {Loth, SR and Henneberg, M},
Title = {{Sexually dimorphic mandibular morphology in the first few years of life}},
Journal = {{AMERICAN JOURNAL OF PHYSICAL ANTHROPOLOGY}},
Year = {{2001}},
Volume = {{115}},
Number = {{2}},
Pages = {{179-186}},
Month = {{JUN}},
Abstract = {{Sex differences in the youngest skeletons are very subtle, and any
   method that can separate males and females significantly better than
   chance will be of value. Compounding the problem is a paucity of
   immature skeletons of documented age and sex. In 1992, S.R.L. examined
   62 juvenile mandibles of white and black South Africans of known age and
   sex (from birth to 19 years) from the Dart Collection to determine if
   the sexes could be differentiated by morphologic traits. By age 6 years,
   adult chin shapes were already recognizable. Prior to that age,
   differences were observed in the shape of the inferior border of the
   symphysis and outline of the body. The male chin base extends steeply
   downward relative to the adjacent body, coming to a point or squaring
   off at the symphysis. In females, the symphysis descends gradually to a
   more rounded base, and even when pointed, the transition is not abrupt.
   On the outer border of the corpus, the sides diverge sharply to form a
   \textbackslash{}\_/ shape from a roughly horizontal anterior region in
   males, while the female contour is rounded, reflecting the smoothly
   curved transition from front to sides. These traits were manifest from
   the eruption of the central incisors until about 4 years of age. The
   features were tested on all 19 Dart Collection mandibles in that age
   range. Average accuracy for three different testers was 81\%, and males
   were consistently identified more accurately than females. This new
   method was then tested on a known sex sample of 11 individuals from 0 to
   7 years of age. These included CT scans of 9 French children and the
   remains of 2 South African black forensic cases. Sexing accuracy was
   82\% (9/11). The only two mis-sexed cases were both female and over age
   6 years. In conclusion, the results of this study indicate that it is
   possible to determine the sex of very young mandibles. The new sexually
   dimorphic morphologic configurations introduced here have demonstrated
   repeatable discrimination with the highest level of accuracy (81\%)
   reported and tested for this age group. Preliminary research indicates
   that both the male and female shapes are clearly recognizable in
   archaeologic and premodern hominids as well as chimpanzees. (C) 2001
   Wiley-Liss, Inc.}},
DOI = {{10.1002/ajpa.1067}},
ISSN = {{0002-9483}},
Unique-ID = {{ISI:000169072200007}},
}

@article{ ISI:000169369800009,
Author = {Murakami, M and Minamihisamatsu, M and Sato, K and Hayata, I},
Title = {{Structural analysis of heavy ion radiation-induced chromosome
   aberrations by atomic force microscopy}},
Journal = {{JOURNAL OF BIOCHEMICAL AND BIOPHYSICAL METHODS}},
Year = {{2001}},
Volume = {{48}},
Number = {{3}},
Pages = {{293-301}},
Month = {{MAY 28}},
Abstract = {{Heavy ion radiation (high linear energy transfer, LET, radiation)
   induces various types of chromosome aberration. In this report, we
   describe a new method employing an atomic force microscope (AFM) for
   nanometer-level structural analysis of chromosome damage induced by
   heavy ion irradiation. Metaphase mouse chromosomes with chromatid gap or
   chromatid breaks induced by heavy ion irradiation were marked under a
   light microscope. Then the detailed structure of chromosomes of
   Giemsa-stained or unstained samples was visualized by the AFM. The
   height data of chromosomes obtained by AFM provided useful information
   to distinguish chromatid gaps and breaks. A fibrous structure was
   observed on the unstained chromosome, the average width of which was
   about 45.8 nm in the image of AFM. These structures were considered to
   be 30-nm fibers on the chromosome. The structure of the break point
   regions induced by neon- or carbon-ion irradiation was imaged by AFM. In
   some cases, the fibrous structure of break points was detected by AFM
   imaging after carbon ion irradiation. These observations indicated that
   AFM is a useful tool for analysis of chromosome aberrations induced by
   heavy ion radiation. (C) 2001 Elsevier Science B.V, All rights reserved.}},
DOI = {{10.1016/S0165-022X(01)00165-8}},
ISSN = {{0165-022X}},
Unique-ID = {{ISI:000169369800009}},
}

@article{ ISI:000167432800008,
Author = {Metz, JAJ and Gyllenberg, M},
Title = {{How should we define fitness in structured metapopulation models?
   Including an application to the calculation of evolutionarily stable
   dispersal strategies}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2001}},
Volume = {{268}},
Number = {{1466}},
Pages = {{499-508}},
Month = {{MAR 7}},
Abstract = {{We define a fitness concept applicable to structured metapopulations
   consisting of infinitely many equally coupled patches. In addition, we
   introduce a more easily calculated quantity R-m that relates to fitness
   in the same manner as R-o relates to fitness in ordinary population
   dynamics: the R-m of a mutant is only defined when the resident
   population dynamics concierges to a point equilibrium and R-m is larger
   (smaller) than 1 if and only if mutant fitness is positive (negative).
   R-m corresponds to the average number of newborn dispersers resulting
   from the ton average less than one) local colony founded by a newborn
   disperser. Efficient algorithms for calculating its numerical value are
   provided. As an example of the usefulness of these concepts we calculate
   the evolutionarily stable conditional dispersal strategy for individuals
   that can account for the local population density in their dispersal
   decisions. Below a threshold density (x) over tilde, at which staying
   and leaving are equality profitable, everybody should stay and above (x)
   over tilde everybody should leave, where profitability is measured as
   the mean number of dispersers produced through lines of descent
   consisting of only non-dispersers.}},
DOI = {{10.1098/rspb.2000.1373}},
ISSN = {{0962-8452}},
ResearcherID-Numbers = {{Gyllenberg, Mats/D-1465-2013}},
ORCID-Numbers = {{Gyllenberg, Mats/0000-0002-0967-8454}},
Unique-ID = {{ISI:000167432800008}},
}

@article{ ISI:000167948200003,
Author = {Freeman, WJ and Kozma, R and Werbos, PJ},
Title = {{Biocomplexity: adaptive behavior in complex stochastic dynamical systems}},
Journal = {{BIOSYSTEMS}},
Year = {{2001}},
Volume = {{59}},
Number = {{2}},
Pages = {{109-123}},
Month = {{FEB}},
Abstract = {{Existing methods of complexity research are capable of describing
   certain specifics of bio systems over a given narrow range of parameters
   but often they cannot account for the initial emergence of complex
   biological systems, their evolution, state changes and sometimes-abrupt
   state transitions. Chaos tools have the potential of reaching to the
   essential driving mechanisms that organize matter into living
   substances. Our basic thesis is that while established chaos tools are
   useful in describing complexity in physical systems, they lack the power
   of grasping the essence of the complexity of life. This thesis
   illustrates sensory perception of vertebrates and the operation of the
   vertebrate brain. The study of complexity, at the level of biological
   systems, cannot be completed by the analytical tools, which have been
   developed for non-living systems. We propose a new approach to chaos
   research that has the potential of characterizing biological complexity.
   Our study is biologically motivated and solidly based in the biodynamics
   of higher brain function. Our biocomplexity model has the following
   features, (1) it is high-dimensional, but the dimensionality is not
   rigid, rather it changes dynamically; (2) it is not autonomous and
   continuously interacts and communicates with individual environments
   that are selected by the model from the infinitely complex world; (3) as
   a result, it is adaptive and modifies its internal organization in
   response to environmental factors by changing them to meet its own
   goals; (4) it is a distributed object that evolves both in space and
   time towards goals that is continually re-shaping in the light of
   cumulative experience stored in memory; (5) it is driven and stabilized
   by noise of internal origin through self-organizing dynamics. The
   resulting theory of stochastic dynamical systems is a mathematical field
   at the interface of dynamical system theory and stochastic differential
   equations. This paper outlines several possible avenues to analyze these
   systems. Of special interest are input-induced and noise-generated, or
   spontaneous state-transitions and related stability issues. (C) 2001
   Elsevier Science Ireland Ltd. All rights reserved.}},
DOI = {{10.1016/S0303-2647(00)00146-5}},
ISSN = {{0303-2647}},
ResearcherID-Numbers = {{Kozma, Robert/C-6365-2013
   }},
ORCID-Numbers = {{Werbos, Paul/0000-0002-1288-3387}},
Unique-ID = {{ISI:000167948200003}},
}

@article{ ISI:000166989100002,
Author = {Iwadate, M and Asakura, T and Dubovskii, PV and Yamada, H and Akasaka, K
   and Williamson, MP},
Title = {{Pressure-dependent changes in the structure of the melittin alpha-helix
   determined by NMR}},
Journal = {{JOURNAL OF BIOMOLECULAR NMR}},
Year = {{2001}},
Volume = {{19}},
Number = {{2}},
Pages = {{115-124}},
Month = {{FEB}},
Abstract = {{A novel method is described, which uses changes in NMR chemical shifts
   to characterise the structural change in a protein with pressure.
   Melittin in methanol is a small alpha -helical protein, and its chemical
   shifts change linearly and reversibly with pressure between 1 and 2000
   bar. An improved relationship between structure and HN shift has been
   calculated, and used to drive a molecular dynamics-based calculation of
   the change in structure. With pressure, the helix is compressed, with
   the H-O distance of the NH-O=C hydrogen bonds decreased by 0.021 +/-
   0.039 Angstrom, leading to an overall compression along the entire helix
   of about 0.4 Angstrom, corresponding to a static compressibility of 6
   x10(-)6 bar(-)1. The backbone dihedral angles phi and psi are altered by
   no more than +/- 3 degrees for most residues with a negative correlation
   coefficient of -0.85 between phi (i) and psi (i-1), indicating that the
   local conformation alters to maintain hydrogen bonds in good geometries.
   The method is shown to be capable of calculating structural change with
   high precision, and the results agree with structural changes determined
   using other methodologies.}},
DOI = {{10.1023/A:1008392327013}},
ISSN = {{0925-2738}},
ResearcherID-Numbers = {{Asakura, Tetsuo/B-9970-2013
   }},
ORCID-Numbers = {{Williamson, Mike/0000-0001-5572-1903}},
Unique-ID = {{ISI:000166989100002}},
}

@article{ ISI:000166642600006,
Author = {Charrassin, JB and Kato, A and Handrich, Y and Sato, K and Naito, Y and
   Ancel, A and Bost, CA and Gauthier-Clerc, M and Ropert-Coudert, Y and Le
   Maho, Y},
Title = {{Feeding behaviour of free-ranging penguins determined by oesophageal
   temperature}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{2001}},
Volume = {{268}},
Number = {{1463}},
Pages = {{151-157}},
Month = {{JAN 22}},
Abstract = {{Sea birds play a major role in marine food webs, and it is important to
   determine when and how much they feed at sea. A major advance has been
   made by using the drop in stomach temperature after ingestion of
   ectothermic prey. This method is less sensitive when birds eat small
   prey or when the stomach is full. Moreover, in diving birds,
   independently of food ingestion, there are fluctuations in the lower
   abdominal temperature during the dives. Using oesophageal temperature,
   we present here a new method for detecting the timing of prey ingestion
   in free-ranging sea birds, and, to our knowledge, report the first data
   obtained on king penguins (Aptenodytes patagonicus). In birds ashore,
   which were hand-fed 2-15 g pieces of fish, all meal ingestions were
   detected with a sensor in the upper oesophagus. Detection was poorer
   with sensors at increasing distances from the beak. At sea, slow
   temperature drops in the upper oesophagus and stomach characterized a
   diving effect per se. For the upper oesophagus only, abrupt temperature
   variations were superimposed, therefore indicating prey ingestions. We
   determined the depths at which these occurred. Combining the changes in
   oesophageal temperatures of marine predators with their diving pattern
   opens new perspectives for understanding their foraging strategy, and,
   after validation with concurrent applications of classical techniques of
   prey survey, for assessing the distribution of their prey.}},
DOI = {{10.1098/rspb.2000.1343}},
ISSN = {{0962-8452}},
Unique-ID = {{ISI:000166642600006}},
}

@article{ ISI:000166077200001,
Author = {Li, X and Pan, XM},
Title = {{New method for accurate prediction of solvent accessibility from protein
   sequence}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND GENETICS}},
Year = {{2001}},
Volume = {{42}},
Number = {{1}},
Pages = {{1-5}},
Month = {{JAN 1}},
Abstract = {{A novel method was developed for predicting the solvent accessibility.
   Based on single sequence data, this method achieved 71.5\% accuracy with
   a correlation coefficient of 0.42 in a database of 704 proteins with
   threshold of 20\% for a two-state-defining solvent accessibility.
   Prediction in a data subset of 341 monomeric proteins achieved 72.7\%
   accuracy with a correlation coefficient of 0.43. On the average,
   prediction over short chains gives better results than that over long
   chains. With a solvent accessibility threshold of 20\%, prediction over
   236 monomeric proteins with chain length < 300 amino acid residues
   achieved 75.3\% accuracy with a correlation coefficient of 0.44 by
   jackknife analysis, which is higher than that obtained by previous
   methods using multiple sequence alignments. Proteins 2001;42:1-5. (C)
   2000 Wiley-Liss, Inc.}},
DOI = {{10.1002/1097-0134(20010101)42:1<1::AID-PROT10>3.0.CO;2-N}},
ISSN = {{0887-3585}},
Unique-ID = {{ISI:000166077200001}},
}

@article{ ISI:000171863000009,
Author = {Harp, DL},
Title = {{Specific determination of inorganic monochloramine in chlorinated
   wastewaters}},
Journal = {{WATER ENVIRONMENT RESEARCH}},
Year = {{2000}},
Volume = {{72}},
Number = {{6}},
Pages = {{706-713}},
Month = {{NOV-DEC}},
Abstract = {{The two conventional methods for wastewater chloramination control,
   residual maintenance and oxidation-reduction potential control, are not
   specific for the desired disinfectant, monochloramine.
   Organochloramines, nitrite, manganese, and chromate positively interfere
   with classical residual control measurements. Oxidation-reduction
   potential control may not be able to distinguish between monochloramine
   and weaker disinfecting organochloramines typically found in chlorinated
   domestic wastewaters. A new method of control is presented that is shown
   to be specific for the inorganic monochloramine species, The new method
   is based on a modification of the classical Berthelot analysis method
   for ammonia. Improvements to the method have greatly increased its
   selectivity toward monochloramine in chlorinated wastewater matrices.
   Break-point chlorination profiles of various chlorinated wastewaters
   demonstrate that the conventional control methods could potentially
   overestimate germicidal efficiency, especially near the breakpoint area.
   The new method has demonstrated good specificity for monochloramme in
   the samples studied and has been adapted for automated chlorine control
   in wastewater.}},
DOI = {{10.2175/106143000X138328}},
ISSN = {{1061-4303}},
Unique-ID = {{ISI:000171863000009}},
}

@article{ ISI:000089534300006,
Author = {Spahn, CMT and Penczek, PA and Leith, A and Frank, J},
Title = {{A method for differentiating proteins from nucleic acids in
   intermediate-resolution density maps: cryo-electron microscopy defines
   the quaternary structure of the Escherichia coli 70S ribosome}},
Journal = {{STRUCTURE}},
Year = {{2000}},
Volume = {{8}},
Number = {{9}},
Pages = {{937-948}},
Month = {{SEP 15}},
Abstract = {{Background: This study addresses the general problem of dividing a
   density map of a nucleic-acid-protein complex obtained by cryo-electron
   microscopy (cryo-EM) or X-ray crystallography into its two components.
   When the resolution of the density map approaches similar to 3 Angstrom
   it is generally possible to interpret its shape (i.e., the envelope
   obtained for a standard choice of threshold) in terms of molecular
   structure, and assign protein and nucleic acid elements on the basis of
   their known sequences. The interpretation of low-resolution maps in
   terms of proteins and nucleic acid elements of known structure is of
   increasing importance in the study of large macromolecular complexes,
   but such analyses are difficult.
   Results: Here we show that it is possible to separate proteins from
   nucleic acids in a cryo-EM density map, even at 11.5 Angstrom
   resolution. This is achieved by analysing the (continuous-valued)
   densities using the difference in scattering density between protein and
   nucleic acids, the contiguity constraints that the image of any nucleic
   acid molecule must obey, and the knowledge of the molecular volumes of
   all proteins.
   Conclusions: The new method, when applied to an 11.5 Angstrom cryo-EM
   map of the Escherichia coli 70S ribosome, reproduces boundary
   assignments between rRNA and proteins made from higher-resolution X-ray
   maps of the ribosomal subunits with a high degree of accuracy. Plausible
   predictions for the positions of as yet unassigned proteins and RNA
   components are also possible. One of the conclusions derived from this
   separation is that 23S rRNA is solely responsible for the catalysis of
   peptide bond formation. Application of the separation method to any
   nucleoprotein complex appears feasible.}},
DOI = {{10.1016/S0969-2126(00)00185-4}},
ISSN = {{0969-2126}},
EISSN = {{1878-4186}},
Unique-ID = {{ISI:000089534300006}},
}

@article{ ISI:000089595500007,
Author = {Snyder, RE and Nisbet, RM},
Title = {{Spatial structure and fluctuations in the contact process and related
   models}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{2000}},
Volume = {{62}},
Number = {{5}},
Pages = {{959-975}},
Month = {{SEP}},
Abstract = {{The contact process is used as a simple spatial model in many
   disciplines, yet because of the buildup of spatial correlations, its
   dynamics remain difficult to capture analytically. We introduce an
   empirically based, approximate method of characterizing the spatial
   correlations with only a single adjustable parameter. This approximation
   allows us to recast the contact process in terms of a stochastic
   birth-death process, converting a spatiotemporal problem into a simpler
   temporal one. We obtain considerably more accurate predictions of
   equilibrium population than those given by pair approximations, as well
   as good predictions of population variance and first passage time
   distributions to a given (low) threshold. A similar approach is
   applicable to any model with a combination of global and
   nearest-neighbor interactions. (C) 2000 Society for Mathematical
   Biology.}},
DOI = {{10.1006/bulm.2000.0191}},
ISSN = {{0092-8240}},
ResearcherID-Numbers = {{Nisbet, Roger/B-6951-2014
   }},
ORCID-Numbers = {{Snyder, Robin/0000-0002-6111-0284}},
Unique-ID = {{ISI:000089595500007}},
}

@article{ ISI:000089026600004,
Author = {Chen, JC and Bigelow, N and Davis, BH},
Title = {{Proposed flow cytometric reference method for the determination of
   erythroid F-cell counts}},
Journal = {{CYTOMETRY}},
Year = {{2000}},
Volume = {{42}},
Number = {{4}},
Pages = {{239-246}},
Month = {{AUG 15}},
Note = {{Meeting of the Clinical-Cytometry-Society, PALM SPRINGS, CALIFORNIA, SEP
   26-29, 1999}},
Organization = {{Clin Cytometry Soc}},
Abstract = {{Background: Quantitation of adult erythrocytes (RBC) containing fetal
   hemoglobin (F cells) is of potential clinical utility in evaluating
   erythropoietic disorders, such as myelodysplasia and hemoglobinopathies,
   and in monitoring F-cell augmenting therapy. F-cell counting
   methodologies include fluorescence microscopy and flow cytometry,
   Previous flow cytometric methods have employed an isotype antibody
   control to distinguish F cells from non-F cells. We investigated the
   feasibility of using the orange autofluorescence signal (FL2) in
   glutaraldehyde-fixed RBC to substitute for fluorescein isothiocyanate
   (FITC)-labeled isotype control antibody use in F-cell quantitation.
   Methods: Our previously published method for fetal red cell detection in
   fetomaternal hemorrhage was used, employing a FITC-labeled
   anti-hemoglobin F (HbF) monoclonal antibody reagent. Blood samples with
   varying F-cell counts were quantitated for F cells using both
   immunofluorescence microscopy and flow cytometry comparing FITC-labeled
   isotype to FL1 thresholding defined by FL2 autofluorescence.
   Results: F cell percentages obtained by using an FL2 defined threshold
   for FL1 gating correlated well with expected values in diluted blood
   samples (r(2) = 0.994, slope = 1.019, intercept = 0.24), values obtained
   using an isotype control (r(2) = 0.996, slope = 1.012, intercept =
   -0.17), and microscopic immunofluorescence counts (r(2) = 0.989, slope =
   0.999, intercept = -0.72). F-cell quantitation by the isotype control
   and FL2 autofluorescence methods was also comparable in 40 blood samples
   (r(2) = 0.994, slope = 1.014, intercept = 0.03). Intra-assay,
   interobserver, and interinstrument precision with this autofluorescence
   gating method exhibited low imprecision (coefficient of variation
   <14\%).
   Conclusion: This novel method is a more objective and less laborious
   alternative for F-cell quantitation by flow cytometry compared to using
   an isotype control or microscopy, thereby providing a more robust
   methodology for clinical studies and consideration as a laboratory
   reference method for F-cell counting, Cytometry (Comm, Clin, Cytometry)
   42:239-246, 2000, (C) 2000 Wiley-Liss, Inc.}},
DOI = {{10.1002/1097-0320(20000815)42:4<239::AID-CYTO4>3.0.CO;2-Z}},
ISSN = {{0196-4763}},
Unique-ID = {{ISI:000089026600004}},
}

@article{ ISI:000087677500020,
Author = {Pauler, DK and Laird, NM},
Title = {{A mixture model for longitudinal data with application to assessment of
   noncompliance}},
Journal = {{BIOMETRICS}},
Year = {{2000}},
Volume = {{56}},
Number = {{2}},
Pages = {{464-472}},
Month = {{JUN}},
Abstract = {{In clinical trials of a self-administered drug, repeated measures of a
   laboratory marker, which is affected by study medication and collected
   in all treatment arms, can provide valuable information on population
   and individual summaries of compliance. In this paper. we introduce a
   general finite mixture of nonlinear hierarchical models that allows
   estimates of component membership probabilities and random effect
   distributions for longitudinal data arising from multiple
   subpopulations, such as from noncomplying and complying subgroups in
   clinical trials. We outline a sampling strategy for fitting these
   models, which consists of a sequence of Gibbs. Metropolis-Hastings, and
   reversible jump steps, where the latter is required for switching
   between component models of different dimensions. Our model is applied
   to identify noncomplying subjects in the placebo arm of a clinical trial
   assessing the effectiveness of zidovudine (AZT) in the treatment of
   patients with HIV, where noncompliance was defined as initiation of AZT
   during the trial without the investigators' knowledge. We fit a
   hierarchical nonlinear change point model for increases in the marker
   MCV (mean corpuscular volume of erythrocytes) for subjects who noncomply
   and a constant mean random effects model for those who comply. As part
   of our Fully Bayesian analysis, we assess the sensitivity of conclusions
   to prior and modeling assumptions and demonstrate how external
   information and covariates call be incorporated to distinguish
   subgroups.}},
DOI = {{10.1111/j.0006-341X.2000.00464.x}},
ISSN = {{0006-341X}},
ORCID-Numbers = {{Ankerst, Donna/0000-0003-4229-2462}},
Unique-ID = {{ISI:000087677500020}},
}

@article{ ISI:000087394400048,
Author = {Block, J and Petrakis, L and Dolhert, LE and Myers, DF and Hegedus, LL
   and Webster, RP and Kukacka, LE},
Title = {{A novel approach for the in-situ chemical elimination of chrysotile from
   asbestos-containing fireproofing materials}},
Journal = {{ENVIRONMENTAL SCIENCE \& TECHNOLOGY}},
Year = {{2000}},
Volume = {{34}},
Number = {{11}},
Pages = {{2293-2298}},
Month = {{JUN 1}},
Abstract = {{We report here the development of a method for the chemical digestion of
   chrysotile asbestos in asbestos-containing fireproofing to levels lower
   than the regulatory threshold. The resulting fireproofing, no longer
   defined as asbestos-containing, can remain in place with properties
   intact. In the process, chrysotile fibers are digested without
   generating excessive gaseous byproducts, and the foam-based delivery
   system essentially eliminates release of airborne fibers. New X-ray
   diffraction methods quantified chrysotile levels with far greater
   precision than standard optical microscopic methods. Full-scale field
   testing confirmed the laboratory phase of the project. Fire testing of
   the treated fireproofing showed th at the treated material functions as
   well as the original fireproofing.}},
DOI = {{10.1021/es990432d}},
ISSN = {{0013-936X}},
Unique-ID = {{ISI:000087394400048}},
}

@article{ ISI:000087550800023,
Author = {Ricci, G and Presani, G and Guaschino, S and Simeone, R and Perticarari,
   S},
Title = {{Leukocyte detection in human semen using flow cytometry}},
Journal = {{HUMAN REPRODUCTION}},
Year = {{2000}},
Volume = {{15}},
Number = {{6}},
Pages = {{1329-1337}},
Month = {{JUN}},
Abstract = {{This study set out to establish a new method, using flow cytometry, to
   evaluate leukocytes in semen. Ejaculates of 59 males, asymptomatic for
   genitourinary infections, were examined. Routine semen analyses were
   carried out as well as peroxidase and polymorphonuclear
   granulocyte-elastase detection, Leukocytes were detected combining flow
   cytometry and monoclonal antibodies (anti-CD45, anti-CD53). This
   technique reliably assessed the total number of leukocytes and
   differentiated subpopulations even at low concentrations, The peroxidase
   test and elastase determination showed good specificity, but only
   moderate sensitivity versus flow cytometry combined with monoclonal
   antibodies, No significant association was observed between semen
   parameters and leukocytospermia whether evaluated by conventional
   methods or flow cytometry except for a moderate correlation between
   spermatozoa and CD53-positive cell concentrations. A first comparison of
   data from patients grouped on the basis of leukocytospermia (>10(6)
   white blood cells, WBC/ml) or non-leukocytospermia revealed no
   significant differences in semen parameters; lowering the threshold
   value for leukocytospermia to 2 x 10(5) WBC/ml, sperm concentration was
   reduced in the group with a low number of WBC identified by monoclonal
   antibodies. Flow cytometry using monoclonal antibodies was seen to be a
   simple, reproducible method that enables leukocytes in semen to be
   accurately detected and to identify WBC subpopulations without
   preliminary purification procedures.}},
DOI = {{10.1093/humrep/15.6.1329}},
ISSN = {{0268-1161}},
ORCID-Numbers = {{SIMEONE, ROBERTO/0000-0002-9776-3818
   RICCI, GIUSEPPE/0000-0002-8031-1102}},
Unique-ID = {{ISI:000087550800023}},
}

@article{ ISI:000087019500006,
Author = {Shaw, AGP and Vennell, R},
Title = {{A front-following algorithm for AVHRR SST imagery}},
Journal = {{REMOTE SENSING OF ENVIRONMENT}},
Year = {{2000}},
Volume = {{72}},
Number = {{3}},
Pages = {{317-327}},
Month = {{JUN}},
Abstract = {{A Front-Following Algorithm provides a new approach to determining the
   position and characteristics of thermal oceanic fronts using Advanced
   Very High Resolution Radiometer (AVHRR) sea surface temperature (SST)
   imagery. This algorithm differs from standard line enhancement,
   threshold edge detection, or classical contour techniques. Instead it
   utilizes a hyperbolic tangent function in a surface-fitting technique to
   follow an oceanic front. It has the advantage of describing the
   characteristics of an oceanic thermal front (including mean SST, SST
   difference, width and gradient across the front) and extracting
   information on the position and characteristics of the front into
   parameter form. Thus, the algorithm has the added benefit of recording
   the changes in the characteristics of the thermal front as it tracks
   along the front. The algorithm was applied to AVHRR SST imagery on part
   of the Subtropical Front known as the Southland Front (SF) off the east
   coast of the South Island of New Zealand, where subantarctic surface
   waters and subtropical surface waters converge. The algorithm was tested
   on examples of the SF and also compared with a standard gradient
   operator. The results showed that the algorithm performed well when
   following the SF, with low standard errors of parameter estimates, good
   visual verification of tracking, and consistent standards of accepted
   data. The algorithm estimated gradients better than a gradient operator.
   (C) Elsevier Science Inc., 2000.}},
DOI = {{10.1016/S0034-4257(99)00108-X}},
ISSN = {{0034-4257}},
ResearcherID-Numbers = {{Vennell, Ross/A-7425-2010
   Shaw, Andrew/D-7517-2011}},
ORCID-Numbers = {{Vennell, Ross/0000-0001-6961-9977
   }},
Unique-ID = {{ISI:000087019500006}},
}

@article{ ISI:000087006900059,
Author = {Svergun, DI and Nierhaus, KH},
Title = {{A map of protein-rRNA distribution in the 70 S Escherichia coli ribosome}},
Journal = {{JOURNAL OF BIOLOGICAL CHEMISTRY}},
Year = {{2000}},
Volume = {{275}},
Number = {{19}},
Pages = {{14432-14439}},
Month = {{MAY 12}},
Abstract = {{Neutron scattering exploits the enormous scattering difference between
   protons and deuterons. A set of 42 x-ray and neutron solution scattering
   curves from hybrid Escherichia coli ribosomes was obtained, where the
   proteins and rRNA moieties in the subunits were either protonated or
   deuterated in all possible combinations. This extensive data set is
   analyzed using a novel method. The volume defined by the cryoelectron
   microscopic model of Frank and co-workers (Frank, J., Zhu, J., Penczek,
   P., Li, Y. H., Srivastava, S., Verschoor, A., Radermacher, NI,,
   Grassucci, R., Lata, R. K., and Agrawal, R, K. (1995) Nature 376,
   441-444) is divided into 7890 densely packed spheres of radius 0.5 nm.
   Simulated annealing is employed to assign each sphere to solvent,
   protein, or rRNA moieties to simultaneously fit all scattering curves.
   Twelve independent reconstructions starting from random approximations
   yielded reproducible results. The resulting model at a resolution of 3
   nm represents the volumes occupied by rRNA and protein moieties at 95\%
   probability threshold and displays 15 and 20 protein subvolumes in the
   30 S and 50 S, respectively, connected by rRNA. 17 proteins with known
   atomic structure can be tentatively positioned into the protein
   subvolumes within the ribosome in agreement with the results from other
   methods. The protein-rRNA map enlarges the basis for the models of the
   rRNA folding and can further help to localize proteins in
   high-resolution crystallographic density maps.}},
DOI = {{10.1074/jbc.275.19.14432}},
ISSN = {{0021-9258}},
ORCID-Numbers = {{Svergun, Dmitri/0000-0003-0830-5696}},
Unique-ID = {{ISI:000087006900059}},
}

@article{ ISI:000085951400016,
Author = {Bi, XN and Haque, TS and Zhou, J and Skillman, AG and Lin, B and Lee, CE
   and Kuntz, ID and Ellman, JA and Lynch, G},
Title = {{Novel cathepsin D inhibitors block the formation of hyperphosphorylated
   tau fragments in hippocampus}},
Journal = {{JOURNAL OF NEUROCHEMISTRY}},
Year = {{2000}},
Volume = {{74}},
Number = {{4}},
Pages = {{1469-1477}},
Month = {{APR}},
Abstract = {{Lysosomal disturbances may be a contributing factor to Alzheimer's
   disease. We used novel compounds to test if suppression of the lysosomal
   protease cathepsin D blocks production of known precursors to
   neurofibrillary tangles. Partial lysosomal dysfunction was induced in
   cultured hippocampal slices with a selective inhibitor of cathepsins B
   and L. This led within 48 h to hyperphosphorylated tau protein fragments
   recognized by antibodies against human tangles. Potent nonpeptidic
   cathepsin D inhibitors developed using combinatorial chemistry and
   structure-based design blocked production of the fragments in a
   dose-dependent fashion. Threshold was in the submicromolar range, with
   higher concentrations producing complete suppression. The effects were
   selective and not accompanied by pathophysiology. Comparable results
   were obtained with three structurally distinct inhibitors. These results
   support the hypothesis that cathepsin D links lysosomal dysfunction to
   the etiology of Alzheimer's disease and suggest a new approach to
   treating the disease.}},
DOI = {{10.1046/j.1471-4159.2000.0741469.x}},
ISSN = {{0022-3042}},
ResearcherID-Numbers = {{Ellman, Jonathan/C-7732-2013
   Bi, Xiaoning/C-3156-2011
   }},
ORCID-Numbers = {{Lin, Binshan/0000-0002-8481-302X}},
Unique-ID = {{ISI:000085951400016}},
}

@article{ ISI:000085475500008,
Author = {Boutonnat, J and Barbier, M and Muirhead, K and Mousseau, M and
   Grunwald, D and Ronot, X and Seigneurin, D},
Title = {{Response of chemosensitive and chemoresistant leukemic cell lines to
   drug therapy: Simultaneous assessment of proliferation, apoptosis, and
   necrosis}},
Journal = {{CYTOMETRY}},
Year = {{2000}},
Volume = {{42}},
Number = {{1}},
Pages = {{50-60}},
Month = {{FEB 15}},
Abstract = {{Background: The balance between cell proliferation and drug-induced cell
   death by apoptosis or necrosis plays a major role in determining
   response to chemotherapy. Commonly-used DNA analysis methods cannot
   study both parameters simultaneously. A new approach described here
   combines a green fluorescent membrane-intercalating dye (PKH67) with
   Hoechst 33342 or annexin V and propidium iodide, to allow simultaneous
   assessment of cell division, cell cycle status, apoptosis, and necrosis,
   respectively. Methods: To test this approach, we used cultured K562
   leukemic cell lines which are drug-sensitive (K562S) or drug-resistant
   (K562R) by virtue of whether they lack or exhibit expression,
   respectively, of the gp-170 (PGP) glycoprotein pump involved in
   multidrug resistance. Results: We found that: 1) PKH67 fluorescence
   intensity decreases proportionately to number of cell divisions, 2)
   labeling with PKH67 does not alter either cell cycle distribution, as
   assessed by vital DNA staining with Hoechst 33342, or cell growth, and
   3) using a simple threshold analysis method suitable for real-time
   sorting decisions, subpopulations of proliferating cells present at
   initial levels of greater than or equal to 10\% can readily be detected
   after two cell division times, based on decreased PKH67 intensity.
   Finally, we demonstrated that after treatment of an admixture of K562S
   and K562R with vincristine, triple-labeling with PKH67, annexin V, and
   propidium iodide can be used to identify and sort those cells which
   remain not only viable (nonnecrotic, nonapoptotic) but actively dividing
   (decreased PKH67 intensity) in the presence of drug. Conclusions:
   Although the studies described here were carried out in a model system
   using cells having known drug resistance phenotypes, we expect that the
   methods described will he useful in ex vivo studies of clinical leukemic
   specimens designed to identify the role played by specific
   chemoresistance proteins and mechanisms in therapeutic outcomes for
   individual patients. (C) 2000 Wiley-Liss, Inc.}},
DOI = {{10.1002/(SICI)1097-0320(20000215)42:1<50::AID-CYTO8>3.3.CO;2-3}},
ISSN = {{0196-4763}},
ResearcherID-Numbers = {{Mousseau, Mireille/M-5942-2014
   RONOT, Xavier/B-8412-2014}},
Unique-ID = {{ISI:000085475500008}},
}

@article{ ISI:000084682200013,
Author = {Gubbins, S and Gilligan, CA},
Title = {{Invasion thresholds for fungicide resistance: deterministic and
   stochastic analyses}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{1999}},
Volume = {{266}},
Number = {{1437}},
Pages = {{2539-2549}},
Month = {{DEC 22}},
Abstract = {{Fungicide resistance is an important practical problem, but one that is
   poorly understood at the population level. Here we introduce a simple
   nonlinear model for fungicide resistance in botanical epidemics which
   includes the dynamics of the chemical control agent and the host
   population, while also allowing for demographic stochasticity in the
   host-parasite dynamics. This provides a mathematical framework for
   analysing the risk of fungicide resistance developing by including the
   parameters for the amount applied, longevity and application frequency
   of the fungicide. The model demonstrates the existence of thresholds for
   the invasion of the resistant strain in the parasite population which
   depend on two quantities: the relative fitness of the resistant strain
   and the effectiveness of control. This threshold marks a change from
   definite elimination of the resistant strain below the threshold to a
   finite probability of invasion which increases above the threshold. The
   fungicide decay rate, the amount of fungicide applied and the period
   between applications affect the effectiveness of control and,
   consequently, they influence whether or not resistance develops and the
   time taken to achieve a critical frequency of resistance. All three
   parameters are amenable to control by the grower or by coordinating the
   activity of a population of growers. Providing crude estimates of the
   effectiveness of control and relative fitness are available, the results
   can be used to predict the consequences of changing these parameters for
   the risk of invasion and the proportion of sites at which this might be
   expected to occur. Although motivated for fungicide resistance, the
   model has broader application to herbicide, antibiotic and antiviral
   resistance. The modelling approach and results are discussed in the
   contest of resistance to chemical control in general.}},
DOI = {{10.1098/rspb.1999.0957}},
ISSN = {{0962-8452}},
EISSN = {{1471-2954}},
ORCID-Numbers = {{Gubbins, Simon/0000-0003-0538-4173}},
Unique-ID = {{ISI:000084682200013}},
}

@article{ ISI:000084382500005,
Author = {Pope, B and Brown, R and Gibson, J and Joshua, D},
Title = {{The bone marrow plasma cell labeling index by flow cytometry}},
Journal = {{CYTOMETRY}},
Year = {{1999}},
Volume = {{38}},
Number = {{6}},
Pages = {{286-292}},
Month = {{DEC 15}},
Abstract = {{The bone marrow plasma cell labeling index is the most important
   prognostic indicator for patients with multiple myeloma. Traditionally,
   this test has been performed as a two color immunofluorescent microscope
   technique which is time consuming and requires a degree of subjectivity
   in its interpretation. We have assessed various adaptations of this
   method to flow cytometry. A bromodeoxyuridine method has been compared
   with a propidium iodide DNA method to detect cells in S phase and
   CD38-FITC has been compared with CD38-FITC + CD138-FITC and CD38-biotin
   + streptavidin FITC to identify plasma cells. The mean channel
   fluorescent intensity of the plasma cell peaks for each of these markers
   was 12.7, 17.4 and 35.3 respectively demonstrating the superiority of
   CD38-biotin + streptavidin FITC. Analysis after propidium iodide
   staining provided a good correlation with the slide technique (r = 0.71;
   P < 0.0001) but the bromodeoxyuridine method did not correlate with the
   slide method (r = 0.09; P = NS). The labeling index values obtained from
   either of the flow methods were greater than the microscopic method.
   Thus a labeling index of >4\% will replace the traditional >1\%
   threshold for identifying patients with a significantly increased
   labeling index. The advantages of the new method are that it takes less
   time to perform, is more objective and provides additional data on
   ploidy and cell cycle status. Cytometry (Comm. Clin. Cytometry)
   38:286-292, 1999. (C) 1999 Wiley-Liss, Inc.}},
DOI = {{10.1002/(SICI)1097-0320(19991215)38:6<286::AID-CYTO5>3.0.CO;2-7}},
ISSN = {{0196-4763}},
Unique-ID = {{ISI:000084382500005}},
}

@article{ ISI:000084295000008,
Author = {Yoda, K and Sato, K and Niizuma, Y and Kurita, M and Bost, CA and Le
   Maho, Y and Naito, Y},
Title = {{Precise monitoring of porpoising behaviour of Adelie penguins determined
   using acceleration data loggers}},
Journal = {{JOURNAL OF EXPERIMENTAL BIOLOGY}},
Year = {{1999}},
Volume = {{202}},
Number = {{22}},
Pages = {{3121-3126}},
Month = {{NOV}},
Abstract = {{A new method using acceleration data loggers enabled us to measure the
   porpoising behaviour of Adelie penguins (Pygoscelis adeliae), defined as
   a continuous rapid swimming with rhythmic serial leaps. Previous
   hydrodynamic models suggested that leaping would be energetically
   cheaper when an animal swims continuously at depths of less than three
   maximum body diameters below the water surface. In the present study,
   free-ranging Adelie penguins leapt at a mean speed of 2.8 m s(-1) above
   the predicted threshold speed (0.18-1.88 m s(-1)). Wild penguins reduced
   drag by swimming deeper (0.91 m) and did not swim continuously within
   the high-drag layer while submerged. This indicates that previous
   calculations may be incomplete. Moreover, leaps represented an average
   of only 3.8\% of the total distance travelled during the porpoising
   cycle, which would make energy savings marginal. Among the six penguins
   used in our study, two did not porpoise and three porpoised for less
   than 7 min, also indicating that this behaviour was not important during
   travel to and from foraging sites, as has been previously suggested,
   Birds mainly porpoised at the start and end of a trip. One explanation
   of porpoising might be an escape behaviour from predators.}},
ISSN = {{0022-0949}},
Unique-ID = {{ISI:000084295000008}},
}

@article{ ISI:000082821400032,
Author = {Gerber, LR and DeMaster, DP},
Title = {{A quantitative approach to endangered species act classification of
   long-lived vertebrates: Application to the North Pacific humpback whale}},
Journal = {{CONSERVATION BIOLOGY}},
Year = {{1999}},
Volume = {{13}},
Number = {{5}},
Pages = {{1203-1214}},
Month = {{OCT}},
Abstract = {{The US. Endangered Species Act (ESA) mandates that recovery plans
   include specific criteria to determine when a species should be removed
   from the List of Endangered and Threatened Wildlife. To meet this
   mandate, we developed a new approach to determining classification
   criteria for long-lived vertebrates The key idea is that endangerment
   depends on two critical aspects of a population: population size and
   trends in population size due to intrinsic variability in population
   growth rates. The way to combine these features is to identify a
   population size and range of population growth rates (where lambda
   denotes the annual multiplicative rate of change of a population) above
   which there is a negligible probability of extinction. To do so, (1)
   information on the current population size and its variance is
   specified; (2) available information on vital rates or changes in
   abundance over time is used to generate a probability distribution for
   the population's lambda; (3) the lower fifth percentile value for lambda
   (denoted as lambda((0.05)))is obtained from the frequency distribution
   of lambda s and (4) if lambda((0.05)) is <1.0, a backwards population
   trajectory starting at 500 individuals for a period of 10 years is
   performed and the resulting population size is designated as the
   threshold for listing a species as endangered, or if lambda((0.05)) is
   greater than or equal to 1.0, the threshold for endangerment is set at
   500 animals. A similar approach can be used to determine the threshold
   for listing a species as threatened under the ESA. We applied this
   approach to North Pacific humpback whales (Megaptera novaeangliae) and
   used Monte Carlo simulations to produce a frequency distribution of
   lambda s for the whales under three different scenarios. Using
   lambda((0.05)) it was determined that the best estimates of current
   abundance for the central population of North Pacific humpback whales
   were larger than the estimated threshold for endangered status but less
   than the estimated threshold for threatened status if accepted by the
   responsible management agency this analysis would be consistent with a
   recommendation to downlist the central stock of humpback whales to a
   status of threatened, whereas the status of eastern and western stocks
   would remain endangered.}},
DOI = {{10.1046/j.1523-1739.1999.98277.x}},
ISSN = {{0888-8892}},
EISSN = {{1523-1739}},
Unique-ID = {{ISI:000082821400032}},
}

@article{ ISI:000082682500006,
Author = {Hsieh, HP and Wu, YT and Chen, ST and Wang, KT},
Title = {{Direct solid-phase synthesis of octreotide conjugates: Precursors for
   use as tumor-targeted radiopharmaceuticals}},
Journal = {{BIOORGANIC \& MEDICINAL CHEMISTRY}},
Year = {{1999}},
Volume = {{7}},
Number = {{9}},
Pages = {{1797-1803}},
Month = {{SEP}},
Abstract = {{Somatostatin analogues, such as octreotide, are useful for the
   visualization and treatment of tumors. Unfortunately, these compounds
   were produced synthetically using complex and inefficient procedures.
   Here, we describe a novel approach for the synthesis of octreotide and
   its analogues using p-carboxybenzaldehyde to anchor Fmoc-threoninol to
   solid phase resins. The reaction of the two hydroxyl groups of
   Fmoc-threoninol with p-carboxybenzaldehyde was catalyzed with
   p-toluenesulphonic acid in chloroform using a Dean-Stark apparatus to
   form Fmoc-threoninol p-carboxybenzacetal in 91\% yield. The
   Fmoc-threoninol p-carboxybenzacetal acted as an Fmoc-amino acid
   derivative and the carboxyl group of Fmoc-threoninol p-carboxybenzacetal
   was coupled to an amine-resin via a DCC coupling reaction. The synthesis
   of protected octreotide and its conjugates were carried out in their
   entirety using a conventional Fmoc protocol and an autosynthesizer. The
   acetal was stable during the stepwise elongation of each Fmoc-amino acid
   as shown by the averaged coupling yield (>95\%). Octreotide (74 to 78\%
   yield) and five conjugated derivatives were synthesized with high yields
   using this procedure, including three radiotherapy octreotides (62 to
   75\% yield) and two cellular markers (72 to 76\% yield). This novel
   approach provides a strategy for the rapid and efficient large-scale
   synthesis of octreotide and its analogues for radiopharmaceutical and
   tagged conjugates. (C) 1999 Elsevier Science Ltd. All rights reserved.}},
DOI = {{10.1016/S0968-0896(99)00125-X}},
ISSN = {{0968-0896}},
ResearcherID-Numbers = {{Hsieh, Hsing-Pang/B-2649-2010
   Hsieh, Hsing-Peng/A-9303-2008}},
Unique-ID = {{ISI:000082682500006}},
}

@article{ ISI:000083100300003,
Author = {Briden, PE and Holt, PD and Simmons, JA},
Title = {{The track structures of ionizing particles and their application to
   radiation biophysics - I. A new analytical method for investigating two
   biophysical models}},
Journal = {{RADIATION AND ENVIRONMENTAL BIOPHYSICS}},
Year = {{1999}},
Volume = {{38}},
Number = {{3}},
Pages = {{175-184}},
Month = {{SEP}},
Abstract = {{A new approach to the interpretation of the effects of radiation on
   cells is described, in which sample particle tracks are constructed
   using a Monte Carlo computer program and the exposure of cellular
   targets to these tracks is simulated using a second program known as
   BIOPHYS. Data on the shapes and DNA contents of the cell nuclei are
   obtained from the literature. It is assumed that the sensitive material
   is DNA, and that the target is divided into cubes of approximately 2 nm
   (the diameter of the DNA helix) per side; the numbers of these cubes
   containing different numbers of ionizations are derived. Two different
   methods of analysing the output of BIOPHYS are described. In the first,
   it is assumed that lethality is caused by the occurrence of a number of
   ionizations equal to or greater than a certain threshold in one cube; in
   the second method, it is assumed that only two ionizations are required,
   in different parts of the cube, but that only some fraction of the cube
   is sensitive. These models have been applied to the interpretation of
   the variation of radiosensitivity with a linear energy transfer (LET) of
   spores of Bacillus subtilis exposed wet and dry, and good fits to the
   published experimental data were obtained using both models. Fits to
   experimental data for a range of other cell lines will be presented in a
   second paper.}},
DOI = {{10.1007/s004110050153}},
ISSN = {{0301-634X}},
Unique-ID = {{ISI:000083100300003}},
}

@article{ ISI:000081325500004,
Author = {Albert, JM},
Title = {{A threshold causal model for clinical trials with departures from
   intended treatment}},
Journal = {{STATISTICS IN MEDICINE}},
Year = {{1999}},
Volume = {{18}},
Number = {{13}},
Pages = {{1615-1626}},
Month = {{JUL 15}},
Abstract = {{Randomized clinical trials often are planned to study a specific
   intervention. However, the collection of data on treatment actually
   received often reveals variable levels of treatment exposure (or `dose')
   across subjects, due to non-compliance or other reasons, This paper
   presents a new method, using such `dose' data as well as control group
   responses, to assess a causal dose-response relationship. The specific
   model utilizes a threshold function and incorporates a random effect
   term to allow for heterogeneous treatment responses among subjects.
   Further modelling of the random effects allows for reduction of error
   variance and control for potential confounders. The threshold dose is
   estimated using a residual variance criterion based on a transformed
   model. Estimates of standard errors and confidence intervals are
   obtained using a bootstrap procedure. The method is applied to data from
   an AIDS clinical trial. A simulation study demonstrates the adequacy of
   the threshold estimates for particular sample sizes and error variances.
   The limitations of this essentially exploratory method, as well as some
   possible extensions, are discussed. Published in 1999 by John Wiley \&
   Sons, Ltd. This article is a US Government Work and is in the public
   domain in the United States.}},
DOI = {{10.1002/(SICI)1097-0258(19990715)18:13<1615::AID-SIM161>3.0.CO;2-6}},
ISSN = {{0277-6715}},
Unique-ID = {{ISI:000081325500004}},
}

@article{ ISI:000081775200004,
Author = {Thyregod, P and Carstensen, J and Madsen, H and Arnbjerg-Nielsen, K},
Title = {{Integer valued autoregressive models for tipping bucket rainfall
   measurements}},
Journal = {{ENVIRONMETRICS}},
Year = {{1999}},
Volume = {{10}},
Number = {{4}},
Pages = {{395-411}},
Month = {{JUL-AUG}},
Abstract = {{A new method for modelling the dynamics of rain sampled by a tipping
   bucket rain gauge is proposed. The considered models belong to the class
   of integer valued autoregressive processes. The models take the
   autocorrelation and discrete nature of the data into account. A first
   order, a second order and a threshold model are presented together with
   methods to estimate the parameters of each model. The models are
   demonstrated to provide a good description of data from actual rain
   events requiring only two to four parameters. (C) 1999 John Wiley \&
   Sons, Ltd.}},
DOI = {{10.1002/(SICI)1099-095X(199907/08)10:4<395::AID-ENV364>3.0.CO;2-M}},
ISSN = {{1180-4009}},
ResearcherID-Numbers = {{Carstensen, Jacob/I-7460-2013
   Arnbjerg-Nielsen, Karsten/J-7792-2012
   }},
ORCID-Numbers = {{Carstensen, Jacob/0000-0003-0016-6118
   Arnbjerg-Nielsen, Karsten/0000-0002-6221-9505
   Madsen, Henrik/0000-0003-0690-3713}},
Unique-ID = {{ISI:000081775200004}},
}

@article{ ISI:000081323100004,
Author = {Pantazatos, DP and MacDonald, RC},
Title = {{Directly observed membrane fusion between oppositely charged
   phospholipid bilayers}},
Journal = {{JOURNAL OF MEMBRANE BIOLOGY}},
Year = {{1999}},
Volume = {{170}},
Number = {{1}},
Pages = {{27-38}},
Month = {{JUL 1}},
Abstract = {{A novel method was developed for the direct examination of pairwise
   encounters between positively and negatively charged phospholipid
   bilayer vesicles. Giant bilayer vesicles (unilamellar, 4-20 mu m in
   diameter) prepared from 1,2-dioleoyl-sn-glycero-3-ethylphosphocholine, a
   new cationic phospholipid derivative, were electrophoretically
   maneuvered into contact with individual anionic phospholipid vesicles.
   Fluorescence video microscopy revealed that such vesicles commonly
   underwent fusion within milliseconds (1 video field) after contact,
   without leakage. Fusion occurred at constant volume and, since flaccid
   vesicles were rare, the excess membrane was not available after fusion.
   Hemifusion (the outer monolayers of each vesicle fused while the inner
   monolayers remained intact) was inferred from membrane-bound dye
   transfer and a change in the contact area. Hemifusion was observed as a
   final stable state and as an intermediate to fusion of vesicles composed
   of charged phospholipids plus zwitterionic phospholipids. Hemifusion
   occurred in one of three ways following adhesion: either delayed with an
   abrupt increase in area of contact, immediately with a gradual increase
   in area of contact, or with retraction during which adherent vesicles
   dissociated from a flat contact to a point contact.
   Phosphatidylethanolamine strongly promoted immediate hemifusion; the
   resultant hemifused state was stable and seldom underwent complete
   fusion. Although sometimes single contacts between vesicles led to
   rupture of both, in other cases, a single vesicle underwent multiple
   fusion events. Direct observation has unequivocally demonstrated the
   fusion of two, isolated bilayer-bounded bodies to yield a stable,
   non-leaky product, as occurs in cells, in the absence of proteins.}},
DOI = {{10.1007/s002329900535}},
ISSN = {{0022-2631}},
ORCID-Numbers = {{Pantazatos, Dionysios/0000-0002-8562-1521}},
Unique-ID = {{ISI:000081323100004}},
}

@article{ ISI:000081223100014,
Author = {Taveau, JC and Boisset, N and Vinogradov, SN and Lamy, JN},
Title = {{Three-dimensional reconstruction of Lumbricus terrestris hemoglobin at
   22 angstrom resolution: Intramolecular localization of the globin and
   linker chains}},
Journal = {{JOURNAL OF MOLECULAR BIOLOGY}},
Year = {{1999}},
Volume = {{289}},
Number = {{5}},
Pages = {{1343-1359}},
Month = {{JUN 25}},
Abstract = {{A 3D reconstruction of the hemoglobin (Hb) of the earthworm Lumbricus
   terrestris was carried out by the 3D projection alignment method from
   electron microscopy images of a frozen-hydrated specimen at 22 Angstrom
   resolution. The results were analyzed by a new approach taking into
   account the evolution of the 210 densities forming the 3D volume as a
   function of the threshold of surface representation. The whole oligomer
   with D-6 point-group symmetry is comprised of 12 hollow globular
   substructures (HGS) with local 3-fold symmetry tethered to a complex
   network of linking subunits (linker complex). The 12 globin subunits of
   each HGS are distributed around local 3-fold axis in four layers of
   three subunits. The first layer, the most external, contains monomeric
   globin chains 2A, 3A, and 5A. The three trimers corresponding to the
   nine remaining subunits have one subunit in each of the second (2B, 3B,
   5B), third (1A, 4A, 6A), and fourth (1B, 4B, 6B) layer. The distances
   between the centers of the globin chains forming the trimers are in the
   ranges 20-32 Angstrom and 45-52 Angstrom. The linker complex is made up
   of two types of linking units. The first type forms three loops
   connecting globin chains of the second, third and fourth layers. The
   average molecular mass (Mm) of these subunits was 25 kDa. The second
   type forms the central structure, termed hexagonal toroid, and its 12
   connections to the HGS. This structure corresponds to a hexamer of a
   single linking unit with a Mm (31.2 kDa), size and a shape different
   from those of the HGS loops. A careful study of 3D volume architecture
   shows that each toroid linking unit is bound to the three loops of a HGS
   pair located in the upper and lower hexagonal layers, respectively. As
   shown in a model of architecture, hexagonal bilayered (HBL) Hbs can be
   built very simply from 144 globin chains and 42 linker chains belonging
   to two different types. We also propose a simple assembly sequence for
   the construction of HBL Hbs based on the architecture model. (C) 1999
   Academic Press.}},
DOI = {{10.1006/jmbi.1999.2824}},
ISSN = {{0022-2836}},
Unique-ID = {{ISI:000081223100014}},
}

@article{ ISI:000082595200008,
Author = {Meecham, WC},
Title = {{Scaleless algebraic energy spectra for the incompressible Navier-Stokes
   equation; relation to other nonlinear problems}},
Journal = {{JOURNAL OF MARINE SYSTEMS}},
Year = {{1999}},
Volume = {{21}},
Number = {{1-4}},
Pages = {{113-130}},
Month = {{JUN}},
Note = {{29th International Liege Colloquium on Ocean Hydrodynamics, LIEGE,
   BELGIUM, MAY 05-09, 1997}},
Abstract = {{We use the Wiener expansion, a.k.a, the Wiener-Hermite (W-H) expansion,
   which is based on weighted integrals of the white noise process (also
   called Feynman path integrals) to represent turbulence, near to
   Gaussian. Fully developed turbulence-fluctuation Reynolds (Re) numbers
   greater than 10(4) are treated. The energy transfer function T(k, q)
   (transfer from wavenumber q to k) is shown, and for the equilibrium 5/3
   law energy spectrum, it is seen that the dependence on k(0), the energy
   range wavenumber, drops out so that dimensionally the energy spectrum
   must be algebraic, 5/3 in the inertial subrange (ISR) at equilibrium,
   for driven turbulence. An interesting consequence of the k, independence
   is the loss of the last evidence of the large scale drive of the
   turbulence. The form of the driving force has no influence. The same
   conclusion follows for decaying turbulence, with the slight modification
   that rate of energy loss (viscous decay) is a (slow) function of time.
   The integral over the transfer (the net energy gain/loss) converges
   independently of k,, the viscous cutoff so that the turbulence is
   scaleless (see Barenblatt and Zel'dovich {[}Barenblatt, G.I.,
   Zel'dovich, Ya.B., 1972. Self-similar solutions as intermediate
   asymptotics. Ann. Rev. Fluid Mech. 4, 285-312.] for a full discussion of
   the influence of this property). The scaleless character is a necessary
   ingredient of fractals. Fractals of course have the property that they
   preserve their qualitative character at any magnification, and this
   characteristic would be upset by the presence of a scale external to the
   process. The transfer from wavenumbers of order k dominates that from
   wave numbers of order k(0) because of the following. First, the volume
   of phase space available for transfer, of order the large wavenumber k,
   is enormous compared with the small region in the vicinity of the energy
   spectrum peak, k(0). Second, the pressure term in conjunction with
   incompressibility yields geometric coefficients (in the transfer) which
   are proportional to the square of the (small) wavenumber, q, in the
   region k,. The effect of moderate compressibility (which will be
   discussed) is to destroy this second small behavior of the transfer in
   the energy range at a threshold Mach number; then a different ISR
   spectrum can develop, as is known to be the case. This opens the way for
   processes with other phenomena, e.g., buoyancy effects, to yield other
   spectra (than the 5/3). We introduce a compressible flow model by
   relieving incompressibility and calculate the resulting transfer. There
   is a critical (fluctuation) Reynolds number, Re'(c) = 2.5(Ma')(-8). If
   Re' < Re-c', there is only the compressible spectrum, -6/3. If Re' is
   larger, then there is a critical wavenumber k(c) = 2k(0)(Ma')(-6),
   forming a knee. For k < k(c), the spectrum is -5/3, and for k > k(c), it
   will be -6/3. There are geophysical (and of course astrophysical)
   processes where this condition is fulfilled. The discussion here gives,
   to the author's knowledge, the first explanation of the 5/3 spectrum
   which goes beyond dimensional analysis (with additional ad-hoc
   assumptions concerning appropriate parameters, specifically an assumed
   independence of k(0)). (C) 1999 Elsevier Science B.V. All rights
   reserved.}},
DOI = {{10.1016/S0924-7963(99)00009-3}},
ISSN = {{0924-7963}},
EISSN = {{1879-1573}},
Unique-ID = {{ISI:000082595200008}},
}

@article{ ISI:000079796700010,
Author = {Sveiczer, A and Novak, B and Mitchison, JM},
Title = {{Mitotic control in the absence of cdc25 mitotic inducer in fission yeast}},
Journal = {{JOURNAL OF CELL SCIENCE}},
Year = {{1999}},
Volume = {{112}},
Number = {{7}},
Pages = {{1085-1092}},
Month = {{APR}},
Abstract = {{Fission yeast cells tolerate the total absence of the cdc25 mitotic
   inducer in two cases, either in cdc2-3w or in wee1 genetic backgrounds,
   In the cdc2-3w cdc25 Delta double mutant, the rate-limiting step leading
   to mitosis is reaching a critical size. However, the size control of
   this mutant operates in late G(2), which is different from wild-type
   (WT) cells. This fact suggests that in WT the rate-limiting molecular
   process during the G(2) timer is the Tyr15 dephosphorylation of cdc2,
   for which the cdc25 phosphatase (together with its back-up, pyp3) is
   dependent. In the wee1-50 cdc25 Delta mutant, the population splits into
   different clusters, all lacking mitotic size control. This strain
   maintains size homeostasis by a novel method, which is random movement
   of the cells from one cluster to another in the successive generations.
   These cells should normally have a `minimal cycle', a `timer' with short
   G(1) and G(2) phases, However, very often the cells abort mitosis,
   possibly at an early event and return back to early Gz, thus lengthening
   their cycles. The inability of these cells to start anaphase might be
   caused by the absence of the main mitotic regulators (wee1 and cdc25)
   and the improper regulation of their back-up copies (mik1 and pyp3,
   respectively).}},
ISSN = {{0021-9533}},
ResearcherID-Numbers = {{Sveiczer, Akos/J-6321-2012
   Novak, Bela/D-5365-2009}},
ORCID-Numbers = {{Novak, Bela/0000-0002-6961-1366}},
Unique-ID = {{ISI:000079796700010}},
}

@article{ ISI:000078990600002,
Author = {Liu, LP and Deber, CM},
Title = {{Combining hydrophobicity and helicity: A novel approach to membrane
   protein structure prediction}},
Journal = {{BIOORGANIC \& MEDICINAL CHEMISTRY}},
Year = {{1999}},
Volume = {{7}},
Number = {{1}},
Pages = {{1-7}},
Month = {{JAN}},
Abstract = {{In spite of the overwhelming numbers and critical biological functions
   of membrane proteins, only a few have been characterized by
   high-resolution structural techniques. From the structures that are
   known, it is seen that their transmembrane (TM) segments tend to fold
   most often into alpha-helices. To evaluate systematically the features
   of these TM segments, we have taken two approaches: (1) using the
   experimentally-measured residence behavior of specifically designed
   hydrophobic peptides in RP-HPLC, a scale was derived based directly on
   the properties of individual amino acids incorporated into
   membrane-interactive helices; and (2) the relative alpha-helical
   propensity of each of the 20 amino acids was measured in the organic
   non-polar environment of n-butanol. By combining the resulting
   hydrophobicity and helical propensity data, in conjunction with
   consideration of the `threshold hydrophobicity' required for spontaneous
   membrane integration of protein segments, an approach was developed for
   prediction of TM segments wherein each must fulfill the dual
   requirements of hydrophobicity and helicity. Evaluated against the
   available high-resolution structural data on membrane proteins, the
   present combining method is shown to provide accurate predictions for
   the locations of TM helices. In contrast, no segment in soluble proteins
   was predicted as a `TM helix'. (C) 1999 Elsevier Science Ltd. All rights
   reserved.}},
DOI = {{10.1016/S0968-0896(98)00233-8}},
ISSN = {{0968-0896}},
EISSN = {{1464-3391}},
Unique-ID = {{ISI:000078990600002}},
}

@article{ ISI:000081186400001,
Author = {Zheng, CX and Elston, RC},
Title = {{Multipoint linkage disequilibrium mapping with particular reference to
   the African-American population}},
Journal = {{GENETIC EPIDEMIOLOGY}},
Year = {{1999}},
Volume = {{17}},
Number = {{2}},
Pages = {{79-101}},
Abstract = {{A new approach to scanning the genome is presented to detect linkage
   disequilibrium caused specifically by population admixture. In contrast
   to current linkage genome scanning methods to find causal genes for
   complex diseases, this new method should be powerful to find genes for
   multilocus traits, particularly those genes that lead to the highest
   population attributable risk. Such a scan using the African-American
   population is generally feasible for mapping common diseases. A
   conservative threshold is also provided for such association mapping.
   (C) 1999 Wiley-Liss, Inc.}},
DOI = {{10.1002/(SICI)1098-2272(1999)17:2<79::AID-GEPI1>3.0.CO;2-N}},
ISSN = {{0741-0395}},
Unique-ID = {{ISI:000081186400001}},
}

@article{ ISI:000077947500021,
Author = {Post, H and Schulz, R and Vahlhaus, C and Husing, J and Hirche, H and
   Gallagher, KP and Heusch, G},
Title = {{Impact of resting and ischemic blood flow on infarct probability in
   ischemic preconditioning - a new approach to infarct size-blood flow
   data by logistic regression}},
Journal = {{JOURNAL OF MOLECULAR AND CELLULAR CARDIOLOGY}},
Year = {{1998}},
Volume = {{30}},
Number = {{12}},
Pages = {{2719-2728}},
Month = {{DEC}},
Abstract = {{The linear regression analysis of infarct size (IS) nu ischemic
   myocardial blood flow (MBF) does not account for the heterogeneity of
   MBF and infarcted tissue; moreover, it cannot assess a blood now
   threshold for infarction (MBFT) accurately, as with ischemic
   preconditioning (IP) the close relationship between ischemic MBF and IS
   otherwise observed is lost. Finally, the impact of resting blood flow on
   myocardial infarction cannot be considered in such analysis. Therefore,
   in a retrospective data analysis of 32 enflurane-anaesthetized swine
   undergoing 90 min severe ischemia and 120 min reperfusion without (CON,
   n=12) or with IP induced by either 3 (IP3, n=8) or 10 min ischemia
   (IP10, n=12) and 15 min reperfusion, a MBFT was assessed by logistic
   regression (LR) in individual tissue pieces. MBFT was arbitrarily
   defined as that ischemic MBE (microspheres) at which infarct probability
   was 0.2, derived from the ratio of infarcted (n=141, TTC) to all tissue
   samples (n=684). The duration of the preconditioning ischemia and MBF
   both at rest and during the sustained ischemia were significant
   predictors of infarct probability. Ischemic MBFT at an infarct
   probability of 0.2, was 0.089 +/- 0.023 ml/min/g in CON. MBFT was
   decreased to 0.051 +/- 0.03 ml/min/g with IP3 (P<0.05 v CON) and further
   to 0.004 +/- 0.03 7 ml/min/g with IP10 (P<0.05 nu CON, IP3).
   Corresponding to the leftward shift of MBFT, the relationships between
   infarct probability and MBF were shifted in parallel by IP with no
   change in their slopes. (C) 1998 Academic Press.}},
DOI = {{10.1006/jmcc.1998.0836}},
ISSN = {{0022-2828}},
EISSN = {{1095-8584}},
ORCID-Numbers = {{Schulz, Rainer/0000-0003-3017-0476}},
Unique-ID = {{ISI:000077947500021}},
}

@article{ ISI:000078153100004,
Author = {Breidbach, O and Holthausen, K and Scheidt, B and Frenzel, J},
Title = {{Analysis of EEG data room in sudden infant death risk patients}},
Journal = {{THEORY IN BIOSCIENCES}},
Year = {{1998}},
Volume = {{117}},
Number = {{4}},
Pages = {{377-392}},
Month = {{DEC}},
Abstract = {{A novel method for EEG feature analysis applying neural networks and
   evolutionary algorithms is proposed. Polysomnographic data of neonate
   infants are analyzed in order to detect characteristic EEG signatures
   that correlate with a certain apnea risk group.
   The method described works without preselected categories, showing the
   possibility of a purely EEG-based detection of sudden infant death risk
   patients. Our method employed the registration of relative distance
   functions and trend analysis of variable EEG features. It is thus shown
   that an adaptive hyperplane within the multidimensional EEG data room
   successfully converges towards a unique feature vector that represents
   the relative EEG characteristics within the patient pool.}},
ISSN = {{1431-7613}},
Unique-ID = {{ISI:000078153100004}},
}

@article{ ISI:000076108000008,
Author = {Morales-Nin, B and Lombarte, A and Japon, B},
Title = {{Approaches to otolith age determination: image signal treatment and age
   attribution}},
Journal = {{SCIENTIA MARINA}},
Year = {{1998}},
Volume = {{62}},
Number = {{3}},
Pages = {{247-256}},
Month = {{SEP}},
Abstract = {{Population studies of fish depend upon correct age estimates. However
   ageing from otoliths is often a subjective activity, based on
   experience. To produce more objective and reproducible results, image
   analysis systems for semi-automatic otolith reading are being developed.
   We propose determinig the daily growth and annual structures in each
   otolith by means of Fourier analysis of the luminous signal, after
   filtering the high frequencies by means of a moving threshold. A new
   approach using spatio-frequential wavelet analysis is discussed for
   otolith age determination. The different readings of each otolith are
   standardised and weighted to obtain a combined age determination and
   vector of increment widths.}},
ISSN = {{0214-8358}},
EISSN = {{1886-8134}},
ResearcherID-Numbers = {{Lombarte, Antoni/D-3142-2013
   }},
ORCID-Numbers = {{Lombarte, Antoni/0000-0001-5215-4587
   Morales-Nin, Beatriz/0000-0002-7264-0918}},
Unique-ID = {{ISI:000076108000008}},
}

@article{ ISI:000075269600018,
Author = {Efthymiadis, A and Dottorini, T and Jans, DA},
Title = {{A novel system to quantitate nuclear-cytoplasmic flux in vivo: Kinetics
   of signal-dependent nuclear protein export}},
Journal = {{ARCHIVES OF BIOCHEMISTRY AND BIOPHYSICS}},
Year = {{1998}},
Volume = {{355}},
Number = {{2}},
Pages = {{254-261}},
Month = {{JUL 15}},
Abstract = {{Compared to signal-mediated nuclear protein import, there is a paucity
   of kinetic information with respect to signal-mediated nuclear protein
   export. In this study we use the novel approach of simultaneous
   nuclear/cytoplasmic microinjection of beta-galactosidase fusion proteins
   to examine nuclear import and export conferred by the leucine-rich
   nuclear export signals (NESs) of HIV-1 Rev and the cAMP-dependent
   protein kinase inhibitor PKI, comparing results to those for either a
   fusion protein containing a conventional nuclear localization sequence
   (NLS) or beta-galactosidase itself. We also analyze nuclear transport of
   the proteins in vitro. Both the Rev and PKI NESs confer nuclear export,
   in contrast to the NLS or mutated inactive NESs; steady state was
   achieved within 40-45 min although not all NES-containing protein had
   been exported from the nucleus at this time point. Interestingly, the
   Rev and PKI NES fusion proteins, in stark contrast to beta-galactosidase
   itself, exhibited nuclear entry in vivo and nuclear accumulation to
   levels about twofold those in the cytoplasm in vitro. We conclude that
   NESs, rather than exclusively conferring nuclear export, may be able to
   mediate shuttling between the nuclear and cytoplasmic compartments. (C)
   1998 Academic Press.}},
DOI = {{10.1006/abbi.1998.0719}},
ISSN = {{0003-9861}},
EISSN = {{1096-0384}},
Unique-ID = {{ISI:000075269600018}},
}

@article{ ISI:000074461300007,
Author = {Bonabeau, E and Theraulaz, G and Deneubourg, JL},
Title = {{Fixed response thresholds and the regulation of division of labor in
   insect societies}},
Journal = {{BULLETIN OF MATHEMATICAL BIOLOGY}},
Year = {{1998}},
Volume = {{60}},
Number = {{4}},
Pages = {{753-807}},
Month = {{JUL}},
Abstract = {{We introduce a simple mathematical model of regulation of division of
   labor in insect societies based on fixed-response thresholds.
   Individuals with different thresholds respond differently to
   task-associated stimuli. Low-threshold individuals become involved at a
   lower level of stimulus than high-threshold individuals. We show that
   this simple model can account for experimental observations of Wilson
   (1984), extend the model to more complicated situations, explore its
   properties, and study under what conditions it can account for temporal
   polyethism. (C) 1998 Society for Mathematical Biology.}},
DOI = {{10.1006/bulm.1998.0041}},
ISSN = {{0092-8240}},
EISSN = {{1522-9602}},
Unique-ID = {{ISI:000074461300007}},
}

@article{ ISI:000073193200015,
Author = {Horness, BH and Lomax, DP and Johnson, LL and Myers, MS and Pierce, SM
   and Collier, TK},
Title = {{Sediment quality thresholds: Estimates from hockey stick regression of
   liver lesion prevalence in English sole (Pleuronectes vetulus)}},
Journal = {{ENVIRONMENTAL TOXICOLOGY AND CHEMISTRY}},
Year = {{1998}},
Volume = {{17}},
Number = {{5}},
Pages = {{872-882}},
Month = {{MAY}},
Abstract = {{Comprehensive, integrative assessments of coastal sediment quality are
   best effected by using large, diverse data sets that include measures of
   biological dysfunction observed in association with chronic exposure to
   sediment contaminants. Under the auspices of the National Oceanic and
   Atmospheric Administration's National Status and Trends Program, the
   National Benthic Surveillance Project accumulated a database of synoptic
   sediment contaminant concentrations and indices of biological effects
   that were measured in indigenous animals collected during field surveys
   conducted from 1984 to 1994. This compilation of data provided the
   opportunity to develop a new approach for determining sediment quality
   criteria to add to the current repertoire of environmental assessment
   tools. Using a two-segment hockey stick regression, statistically
   significant chemical thresholds of biological effects were estimated for
   hepatic lesion prevalences in English sole (Pleuronectes vetulus,
   formerly Parophrys vetulus) in relation to sediment concentrations of
   polycyclic aromatic hydrocarbons. These threshold estimates are notably
   lower than many of those reported for other techniques. Application of
   this relatively simple dose-response model to subacute, chronic effects
   that are involved in hepatocarcinogenesis and associated with sediment
   toxicant content (1) reflects the link between toxicopathic disease
   progression and conditions observed in benthic fish exposed to
   contaminants and (2) provides endpoints for assessing sediment quality
   contaminant concentrations that are not necessarily acutely fatal but
   may have long-term health implications for populations that are
   chronically exposed.}},
DOI = {{10.1897/1551-5028(1998)017<0872:SQTEFH>2.3.CO;2}},
ISSN = {{0730-7268}},
EISSN = {{1552-8618}},
Unique-ID = {{ISI:000073193200015}},
}

@article{ ISI:000073407600024,
Author = {Zafirakou-Koulouris, A and Vogel, RM and Craig, SM and Habermeier, J},
Title = {{L moment diagrams for censored observations}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{1998}},
Volume = {{34}},
Number = {{5}},
Pages = {{1241-1249}},
Month = {{MAY}},
Abstract = {{Observed data sets containing values above or below the analytical
   threshold of measuring equipment are referred to as censored. Such data
   are frequently encountered in quality and quantity monitoring
   applications of water, soil, and air samples. Most of the previous
   literature on the statistical analysis of censored data relates to the
   problems of moment, parameter, and quantile estimation methods. Such
   estimation methods usually assume an underlying probability
   distribution. Few goodness-of-fit methods exist for censored data. We
   introduce L moment diagrams for the evaluation of the goodness of fit of
   alternative distributional hypotheses for left-censored data.
   Experiments with artificial censored data sets document the conditions
   under which L moment diagrams should be useful. Our approach, like
   Hosking's {[}1995] approach for right censoring, derived L moment
   diagrams for left-censored observations from partial
   probability-weighted moments.}},
DOI = {{10.1029/97WR03712}},
ISSN = {{0043-1397}},
ResearcherID-Numbers = {{Vogel, Richard/A-8513-2008}},
ORCID-Numbers = {{Vogel, Richard/0000-0001-9759-0024}},
Unique-ID = {{ISI:000073407600024}},
}

@article{ ISI:000072358400002,
Author = {Gudmundsson, A and Liden, G},
Title = {{Determination of cyclone model variability using a time-of-flight
   instrument}},
Journal = {{AEROSOL SCIENCE AND TECHNOLOGY}},
Year = {{1998}},
Volume = {{28}},
Number = {{3}},
Pages = {{197-214}},
Month = {{MAR}},
Abstract = {{A test method for the measurement of the internal penetration of
   personal cyclones using a modified TSI Aerodynamic Particle Sizer (APS)
   is presented. A new method to generate a polydisperse polystyrene
   aerosol has been developed. The effects of phantom particle generation
   in the recorded size distributions determined mere reduced mainly by
   using the Large Particle Processor (LPP) data and extending the
   particle-size range of the LPP by reducing the threshold voltage of the
   LPP. The performance of five commercially available versions of the
   Higgins-Dewell-type cyclone has been tested using this method. The
   cyclones have virtually identical internal geometric dimensions but are
   machined from different materials and have different internal surfaces.
   It was found that the measured penetration curve for each model has a
   unique combination of cutoff diameter and slope. It was also shown that
   accurate determinations of cyclone penetration require particle loading
   to be considered. (C) 1998 American Association for Aerosol Research.}},
DOI = {{10.1080/02786829808965521}},
ISSN = {{0278-6826}},
Unique-ID = {{ISI:000072358400002}},
}

@article{ ISI:000073086800013,
Author = {Geddes, LA},
Title = {{Success-failure diagram for defibrillation}},
Journal = {{MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING}},
Year = {{1998}},
Volume = {{36}},
Number = {{2}},
Pages = {{229-230}},
Month = {{MAR}},
Abstract = {{A new method for presenting defibrillation threshold data (current,
   voltage or energy) is presented in the form of a success-failure
   diagram, the y-axis of which is the shock strength employed. A vertical
   line divides the successful shocks from the unsuccessful shocks for each
   shock strength. The diagram shows clearly how many shocks have been
   given at each strength and how the success for defibrillation increases
   with increasing shock strength.}},
DOI = {{10.1007/BF02510748}},
ISSN = {{0140-0118}},
Unique-ID = {{ISI:000073086800013}},
}

@article{ ISI:000072130500009,
Author = {Grove, GN and Brudvig, GW},
Title = {{Calcium binding studies of photosystem II using a calcium-selective
   electrode}},
Journal = {{BIOCHEMISTRY}},
Year = {{1998}},
Volume = {{37}},
Number = {{6}},
Pages = {{1532-1539}},
Month = {{FEB 10}},
Abstract = {{The identification of Ca2+ as a cofactor in photosynthetic O-2 evolution
   has encouraged research into the role of Ca2+ in photosystem II (PSII).
   Previous methods used to identify the number of binding sites and their
   affinities were nor able to measure Ca2+ binding at thermodynamic
   equilibrium. We introduce the use of a Ca2+-selective electrode to study
   equilibrium binding of Ca2+ to PSII. The number and affinities of
   binding sites were determined via Scatchard analysis on a series of PSII
   membrane preparations progressively depleted of the extrinsic
   polypeptides and Mn. Untreated PSII membranes bound approximately 4 Ca2+
   per PSII with high affinity (K = 1.8 mu M) and a larger number of Ca2+
   with lower affinity, The high-affinity sites are assigned to divalent
   cation-binding sites on the light-harvesting complex II that are
   involved in membrane stacking, and the lower-affinity sites are
   attributed to nonspecific surface-binding sites. These sites were also
   observed in ail of the extrinsic polypeptide- and Mn-depleted
   preparations. Depletion of the extrinsic polypeptides and/or Mn exposed
   additional very high-affinity Ca2+-binding sites which were nor in
   equilibrium with free Ca2+ in untreated PSII, owing to the diffusion
   barrier created by the extrinsic polypeptides. Ca2+-depleted PSII
   membranes lacking the 23 and 17 kDa extrinsic proteins bound an
   additional 2.5 Ca2+ per PSII with K = 0.15 mu M. This number of very
   high-affinity Ca2+-binding sites agrees with the previous work of
   Cheniae and co-workers {[}Kalosaka, K., et al, (1990) in Current
   Research in Photosynthesis (Baltscheffsky, M., Ed.) pp 721-724, Kluwer,
   Dordrecht, The Netherlands] whose procedure for Ca2+ depletion was used.
   Further depletion of the 33 kDa extrinsic protein yielded a sample that
   bound only 0.7 very high-affinity Ca2+ per PSII with K = 0.19 mu M. The
   loss of 2 very high-affinity Ca2+-binding sites upon depletion of the 33
   kDa extrinsic protein could be due to a structural change of the
   O-2-evolving complex which lost 2-3 of the 4 Mn ions in this sample,
   Finally, PSII membranes depleted of Mn and the 33, 23, and 17 kDa
   extrinsic proteins bound approximately 4 very high-affinity Ca2+ per
   PSII with K = 0.08 mu M. These sites are assigned to Ca2+ binding to the
   vacant Mn sites.}},
DOI = {{10.1021/bi971356z}},
ISSN = {{0006-2960}},
Unique-ID = {{ISI:000072130500009}},
}

@article{ ISI:000072623100011,
Author = {Sarvazyan, N},
Title = {{A new approach to assess viability of adult cardiomyocytes:
   Computer-assisted image analysis}},
Journal = {{JOURNAL OF MOLECULAR AND CELLULAR CARDIOLOGY}},
Year = {{1998}},
Volume = {{30}},
Number = {{2}},
Pages = {{297-301}},
Month = {{FEB}},
Abstract = {{Objective: to develop a computerized procedure to obtain the percentage
   of rod-shaped cells in preparations of isolated adult cardiomyocytes.
   Methods: (1) isolated adult rat myocytes were either pretreated with a
   fixative (0.2\% glutaraldehyde, 0.25\% Triton X-100 and 0.1\% trypan
   blue) or monitored via phase-contrast optics without fixation; (2) an
   image field with several hundred cells was captured by a
   microscope-mounted video camera, which was connected to a frame grabber,
   and saved as a TIFF file: (3) the image was processed using Mocha
   software. Analysis consisted of setting the image threshold, an object
   count, automatic measurements of objects and their classification.
   Results: though the software could measure many geometrical
   characteristics, a combination of two parameters, object size and shape
   factor, was found to be the most efficient. With these parameters, four
   classes of sample objects were constructed including rod-shaped
   myocytes, round damaged cells, overlapping cell clusters, and small
   debris. Test objects were classified automatically based on which sample
   object their parameters matched the most, Excellent correlation was
   obtained between manual counts and computer analysis of cell viability
   (r = 0.98). Conclusion: a more efficient method, based on
   computer-assisted classification of objects, has been developed to
   assess the viability of isolated adult cardiomyocytes. (C) 1998 Academic
   Press Limited.}},
DOI = {{10.1006/jmcc.1997.0624}},
ISSN = {{0022-2828}},
EISSN = {{1095-8584}},
Unique-ID = {{ISI:000072623100011}},
}

@article{ ISI:000071132800002,
Author = {Yamato, T and Kakitani, T},
Title = {{Molecular dynamics simulation of the excited-state dynamics of
   bacteriorhodopsin}},
Journal = {{PHOTOCHEMISTRY AND PHOTOBIOLOGY}},
Year = {{1997}},
Volume = {{66}},
Number = {{6}},
Pages = {{735-740}},
Month = {{DEC}},
Abstract = {{The excited-state dynamics of bacteriorhodopsin was studied by molecular
   dynamics simulation. For the purpose of suppressing large displacement
   of amino acid residues on the surface of bacteriorhodopsin, positional
   restraints were imposed on these residues. A new method was developed to
   investigate the movement of amino acid residues upon photoexcitation and
   their role on the ultrafast photoisomerization of the chromophore. The
   structural change of bacteriorhodopsin was then traced up to 200 fs,
   i.e. until the formation of the intermediate I. We found that when all
   the conjugated bonds of the chromophore were allowed to twist freely in
   the excited state, many bonds including the C13=C14 bond twist in large
   scale within 100 fs. When only the C13=C14 bond and the single bonds
   were allowed to twist freely, the twisting took place at most 20 degrees
   within 200 fs. From these results, it is claimed that a special
   potential surface is provided for the C13=C14 bond twisting by the
   protein environment in the course of the isomerization reaction, giving
   rise to the specific, ultrafast photoisomerization of bacteriorhodopsin.
   As a trace of such a mechanism, we observed that several functionally
   important residues incuding Asp85, Asp212 and Tyr185 responded quickly
   to the photoexcitation of the chromophore.}},
DOI = {{10.1111/j.1751-1097.1997.tb03217.x}},
ISSN = {{0031-8655}},
ORCID-Numbers = {{YAMATO, Takahisa/0000-0002-0685-861X}},
Unique-ID = {{ISI:000071132800002}},
}

@article{ ISI:A1997XY63200016,
Author = {Moussa, R},
Title = {{Is the drainage network a fractal Sierpinski space?}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{1997}},
Volume = {{33}},
Number = {{10}},
Pages = {{2399-2408}},
Month = {{OCT}},
Abstract = {{The most common approach to extract the channel network from digital
   elevation models is to specify a threshold area, S, which is the minimum
   area required to drain to a point within which a channel forms. This
   threshold area is usually specified arbitrarily, although it is
   recognized that different threshold areas will result in substantially
   different channel networks for the same basin. This paper studies the
   effects of S, which is also the scale of observation, on scaling and
   morphometric proper;ties such as the number of sources and the areas of
   source basins and lateral basins. The Garden basin, located in southern
   France, was extensively studied. The results indicate that morphometric
   properties vary considerably with S and that the spatial distribution of
   source basins can be considered as a fractal Sierpinski space. A simple
   model based on the procedure similar to the Sierpinski carpet
   construction is proposed to explain empirical relations between the
   number of source areas and the threshold, S. The model presents a new
   approach to estimate basin fractal properties and defines new indices to
   characterize the spatial distribution of first-order basins. This
   methodology is useful for hydrologists and geomorphologists dealing with
   river networks and spatial patterns of various basin properties such as
   vegetation, soil, soil moisture, and human activities.}},
DOI = {{10.1029/97WR01899}},
ISSN = {{0043-1397}},
Unique-ID = {{ISI:A1997XY63200016}},
}

@article{ ISI:A1997XQ53300003,
Author = {Soldati, L and Spaventa, R and Vezzoli, G and Zerbi, S and Adamo, D and
   Caumo, A and Rivera, R and Bianchi, G},
Title = {{Characterization of voltage-dependent calcium influx in human
   erythrocytes by fura-2}},
Journal = {{BIOCHEMICAL AND BIOPHYSICAL RESEARCH COMMUNICATIONS}},
Year = {{1997}},
Volume = {{236}},
Number = {{3}},
Pages = {{549-554}},
Month = {{JUL 30}},
Abstract = {{Thus far, the methods used to determine erythrocyte Ca2+ influx have not
   allowed the assessment of the kinetics of ion uptake. To overcome this
   drawback, we studied a new method, using the fluorescent Ca2+-chelator
   fura-2, which directly quantifies intracellular Ca2+ changes in human
   erythrocytes. This method has the advantage over previous techniques
   that it monitors continuously cellular Ca2+ levels. The Ca2+ influx is
   modulated by cellular membrane potential in the presence of a
   transmembrane Ca2+ concentration gradient and exhibits a first slow
   increase of the intracellular Ca2+ concentration, followed, after the
   reachment of a threshold value of 125 +/- 13 nM Ca2+, by a faster
   increase until a plateau is reached. The influx rate is inhibited by
   dihydropyridines in the micromolar range. These findings support the
   hypothesis that erythrocyte Ca2+ influx is mediated by a carrier similar
   to the slow Ca2+ channels and is dependent on membrane depolarization.
   (C) 1997 Academic Press.}},
DOI = {{10.1006/bbrc.1997.7002}},
ISSN = {{0006-291X}},
EISSN = {{1090-2104}},
ResearcherID-Numbers = {{Caumo, Andrea/N-1062-2016
   }},
ORCID-Numbers = {{Caumo, Andrea/0000-0002-2507-0780
   Vezzoli, Giuseppe/0000-0003-4481-5693}},
Unique-ID = {{ISI:A1997XQ53300003}},
}

@article{ ISI:A1997XP52100011,
Author = {Kimber, I and Basketter, DA},
Title = {{Contact sensitization: A new approach to risk assessment}},
Journal = {{HUMAN AND ECOLOGICAL RISK ASSESSMENT}},
Year = {{1997}},
Volume = {{3}},
Number = {{3}},
Pages = {{385-395}},
Month = {{JUL}},
Abstract = {{The murine local lymph node assay is a novel predictive test for the
   identification of skin sensitizing chemicals. The method measures
   sensitization potential of a chemical in mice as a function of
   proliferative activity induced in lymph nodes draining the site of
   topical exposure to that test chemical. Here we describe the use of the
   local lymph node assay for evaluation of the relative potency of skin
   sensitizing chemicals via derivation of the concentration required to
   produce a threshold positive reaction. Subsequently, the development of
   risk assessments based on comparisons with index contact allergens is
   outlined.}},
DOI = {{10.1080/10807039709383695}},
ISSN = {{1080-7039}},
Unique-ID = {{ISI:A1997XP52100011}},
}

@article{ ISI:A1997WR69500005,
Author = {Barra, PFA},
Title = {{The action potentials and currents on the I-V plane in the molluscan
   neuron}},
Journal = {{COMPARATIVE BIOCHEMISTRY AND PHYSIOLOGY A-PHYSIOLOGY}},
Year = {{1997}},
Volume = {{116}},
Number = {{4}},
Pages = {{313-321}},
Month = {{APR}},
Abstract = {{The action potentials and the corresponding transmembrane currents,
   directly recorded in the F1 neuron of Helix aspersa by the Self-clamp
   Technique, were plotted on the I-V plane to represent the real
   electrical cycle of the cell membrane during activity. The membrane
   electrical cycle, experimentally obtained, agreed in several aspects
   with a similar cycle obtained from calculated data on the giant axon of
   Loligo, but not for the sign, with the consequence of a different
   localization, as far as voltage and time are concerned, of the negative
   impedance period. The negative impedance proved to be -614 +/- 181 Omega
   cm(2) and corresponded to the late phase of the repolarization after the
   action potential peak. A constant positive impedance was found of 522
   +/- 131 Omega cm(2) during the ascending tract of the action potential.
   These two results are in contrast with previous analyses. The
   simultaneous availability of the conjugate voltage and current directly
   measured signals led to the immediate representation of the membrane
   total conductance in its real time course during activity, in agreement:
   with the Hodgkin and Huxley predictive model. The peak conductance was
   1.9 +/- 0.7 mmho/cm(2) in this preparation. The electrical work spent to
   sustain a single active event proved to be 70 +/- 19 nJ/cm(2). A
   vectorial representation of the membrane electrical activity is proposed
   to describe analytically the characteristic behaviour of excitable
   cells, as well as a new method that utilizes the only action potential
   to measure the threshold potential in spontaneously discharging cells.
   The proposed new experimental protocol, based on the use of the
   Self-clamp Technique, proved to be faster, easier, more productive when
   compared with the conventional methods; it could be used advantageously
   in the electrophysiological studies on excitable cells both to define
   the basic conditions of the investigated preparation and to directly
   evaluate the effects of subsequent pharmacological stimulations.
   Copyright (C) 1997 Elsevier Science Inc.}},
DOI = {{10.1016/S0300-9629(96)00208-3}},
ISSN = {{0300-9629}},
Unique-ID = {{ISI:A1997WR69500005}},
}

@article{ ISI:A1997WX98400001,
Author = {Rowat, PF and Selverston, AI},
Title = {{Oscillatory mechanisms in pairs of neurons connected with fast
   inhibitory synapses}},
Journal = {{JOURNAL OF COMPUTATIONAL NEUROSCIENCE}},
Year = {{1997}},
Volume = {{4}},
Number = {{2}},
Pages = {{103-127}},
Month = {{APR}},
Abstract = {{We study dynamical mechanisms underlying oscillatory behavior in
   reciprocal inhibitory pairs of neurons, using a two-dimensional cell
   model. We introduce one-and-two dimensional phase portraits to
   illustrate the behaviors, thus reducing the study of dynamical
   mechanisms to planar geometrical properties. We examined whether other
   mechanisms besides the escape and release mechanisms (Wang and Rinzel,
   1992) might be needed for some cases of reciprocal inhibition, and show
   that, within the confines of a simple two-dimensional cell model, escape
   and release are sufficient for all cases. We divided the behaviors of a
   single cell into six different types and examined the joint behaviors
   arising from every combination of pairs of cells with behaviors drawn
   from these six types. For the case of two quiescent cells or two cells
   each having plateau potentials, bifurcation diagrams demonstrate the
   relations between synaptic threshold and synaptic strength necessary for
   oscillations by escape, oscillations by release, or network-generated
   plateau potentials. Thus we clarify the relationship between plateau
   potentials and oscillations in a cell. Using the two dimensional cell
   model we examine 1:N beating between cells and find that our simple
   model displays many of the essential dynamical properties displayed by
   more sophisticated models, some of which relate to thalamocortical
   spindling.}},
DOI = {{10.1023/A:1008869411135}},
ISSN = {{0929-5313}},
Unique-ID = {{ISI:A1997WX98400001}},
}

@article{ ISI:A1997WV50700001,
Author = {Vedyushkin, MA},
Title = {{Vegetation response to global warming: The role of hysteresis effect}},
Journal = {{WATER AIR AND SOIL POLLUTION}},
Year = {{1997}},
Volume = {{95}},
Number = {{1-4}},
Pages = {{1-12}},
Month = {{APR}},
Abstract = {{A new approach is proposed for the evaluation of vegetation equilibrium
   response to global warming. The approach considers the dependence of the
   position of biome boundaries as significantly multi-valued function of
   climatic conditions; the reason for the multiplicity may be partly due
   to capacity of vegetation to change its environment. This result in
   hysteresis manifestations (threshold and irreversibility effects) in
   response to climate change. Matthews' global vegetation data set and
   IIASA climatic data base were used to reconstruct the domains of
   different biomes in a space of climatic factors (biotemperature and
   average precipitation). Based on the overlap of these domains, the maps
   of biomes' potential extent are calculated for present climate and for
   two scenarios of global warming (GISS and GFDL). These results imply a
   significant role for hysteresis phenomena in the global vegetation
   pattern. Maps of vegetation changes under two climate scenarios
   calculated with the help of a new algorithm to account for hysteresis
   indicate much less change than equivalent maps obtained by other
   equilibrium approaches under the two climate change scenarios. Changes
   are predicted for 20\% of terrestrial area. A relatively small increase
   of forest and decrease of nonforest vegetation area predicted by both
   scenarios.}},
DOI = {{10.1023/A:1019280700745}},
ISSN = {{0049-6979}},
Unique-ID = {{ISI:A1997WV50700001}},
}

@article{ ISI:A1997WP14300018,
Author = {Melo, F and Feytmans, E},
Title = {{Novel knowledge-based mean force potential at atomic level}},
Journal = {{JOURNAL OF MOLECULAR BIOLOGY}},
Year = {{1997}},
Volume = {{267}},
Number = {{1}},
Pages = {{207-222}},
Month = {{MAR 21}},
Abstract = {{We present a new approach at the atomic level for the development of
   knowledge-based mean force potentials (MFPs) that can be used in fold
   recognition, ab initio structure prediction, comparative modelling and
   molecular recognition. Our method is based on atom-type definitions,
   raising the total frequency of the pairwise distributions and leading to
   very accurate and specific distance-dependent energy functions.
   Forty different heavy atom types were defined depending on their bond
   connectivity, chemical nature and location level (side-chain or
   backbone). Using this approach it has been possible to obtain average
   frequencies of pairwise contacts about 15 times higher than the ones
   obtained using the classic way of one heavy atom definition for each
   amino acid (i.e. alpha-carbon, beta-carbon, virtual centroid or virtual
   beta-carbon co-ordinates).
   In this paper we use this approach to develop a MFP that can be used in
   fold recognition and we compare it with a classic MFP at the amino acid
   level compiled from the alpha-carbon distances between the different
   amino acid pairs. Both potentials involve all the pairwise contacts
   extracted from a non-redundant folds database of 180 protein chains with
   a sequence identity threshold of 25\%.
   The pairwise energy functions of the MFP at the atomic level have a deep
   and very well defined minimum for each pairwise interaction, in contrast
   to the same curves obtained from the MFP developed at the amino acid
   level, which generally have multiple minima with similar depth.
   Our results also show that this MFP is able to produce very similar
   energy profiles for couples of proteins that share a very low sequence
   identity but are closely related at the structural level. When these
   profiles are plotted considering the structure-structure alignment, they
   are mostly superimposed, showing a correlation with the
   structure-structure similarity. Ln the same test, the MFP at the amino
   acid level fails to produce similar profiles.
   We suggest that using this MFP at the atomic level in the last stages of
   fold recognition or threading, when some candidates are available, can
   improve the sequence-structure alignments and, therefore, the final
   models. We also discuss the possibility of using this approach in the
   development of new MFPs to be used in nb initio structure prediction,
   comparative modelling and molecular recognition procedures. (C) 1997
   Academic Press Limited.}},
DOI = {{10.1006/jmbi.1996.0868}},
ISSN = {{0022-2836}},
ResearcherID-Numbers = {{Melo, Francisco/M-3923-2014
   Melo, Francisco/I-5139-2012
   }},
ORCID-Numbers = {{Melo, Francisco/0000-0002-0424-5991}},
Unique-ID = {{ISI:A1997WP14300018}},
}

@article{ ISI:A1997WL64300014,
Author = {Narlikar, GJ and Khosla, M and Usman, N and Herschlag, D},
Title = {{Quantitating tertiary binding energies of 2' OH groups on the P1 duplex
   of the Tetrahymena ribozyme: Intrinsic binding energy in an RNA enzyme}},
Journal = {{BIOCHEMISTRY}},
Year = {{1997}},
Volume = {{36}},
Number = {{9}},
Pages = {{2465-2477}},
Month = {{MAR 4}},
Abstract = {{Binding of the Tetrahymena ribozyme's oligonucleotide substrate (S)
   involves P1 duplex formation with the ribozyme's internal guide sequence
   (IGS) to give an open complex, followed by docking of the P1 duplex into
   the catalytic core via tertiary interactions to give a closed complex.
   The overall binding energies provided by 2' OH groups on S and IGS have
   been measured previously. To obtain the energetic contribution of each
   of these 2' OH groups in the docking step, we have separately measured
   their contribution to the stability of a model P1 duplex using
   `'substrate inhibition''. This new approach allows measurement of duplex
   stabilities under conditions identical to those used for ribozyme
   binding measurements. The tertiary binding energies from the individual
   2' OH groups include a small destabilizing contribution of 0.7 kcal/mol
   and stabilizing contributions of up to -2.9 kcal/mol. The energetic
   contributions of specific 2' OH groups are discussed in the context of
   considerable previous work that has characterized the tertiary
   interactions of the P1 duplex. A `'threshold'' model for the open and
   closed complexes is presented that provides a framework to interpret the
   energetic effects of functional group substitutions on the P1 duplex.
   The sum of the tertiary stabilization provided by the conserved G . U
   wobble at the cleavage site and the individual 2' OH groups on the P1
   duplex is significantly greater than the observed tertiary stabilization
   of S (11.0 vs 2.2 kcal/mol). It is suggested that there is an energetic
   cost for docking the P1 duplex into the active site that is paid for by
   the `'intrinsic binding energy'' of groups on the P1 duplex. Substrates
   that lack sufficient tertiary binding energy to overcome this energetic
   barrier exhibit reduced reactivities. Thus, the ribozyme appears to use
   the intrinsic binding energy of groups on the pi duplex for catalysis.
   This intrinsic binding energy may be used to position reactants within
   the active site and to induce electrostatic destabilization of the
   substrate, relative to its interactions in solution.}},
DOI = {{10.1021/bi9610820}},
ISSN = {{0006-2960}},
Unique-ID = {{ISI:A1997WL64300014}},
}

@article{ ISI:A1997XV97800008,
Author = {Fielding, AH and Bell, JF},
Title = {{A review of methods for the assessment of prediction errors in
   conservation presence/absence models}},
Journal = {{ENVIRONMENTAL CONSERVATION}},
Year = {{1997}},
Volume = {{24}},
Number = {{1}},
Pages = {{38-49}},
Month = {{MAR}},
Abstract = {{Predicting the distribution of endangered species from habitat data is
   frequently perceived to be a useful technique. Models that predict the
   presence or absence of a species are normally judged by the number of
   prediction errors. These may be of two types: false positives and false
   negatives. Many of the prediction errors can be traced to ecological
   processes such as unsaturated habitat and species interactions.
   Consequently, if prediction errors are not placed in an ecological
   context the results of the model may be misleading. The simplest, and
   most widely used, measure of prediction accuracy is the number of
   correctly classified cases. There are other measures of prediction
   success that may be more appropriate. Strategies for assessing the
   causes and costs of these errors are discussed. A range of techniques
   for measuring error in presence/absence models, including some that are
   seldom used by ecologists (e.g. ROC plots and cost matrices), are
   described. A new approach to estimating prediction error, which is based
   on the spatial characteristics of the errors, is proposed. Thirteen
   recommendations are made to enable the objective selection of an error
   assessment technique for ecological presence/absence models.}},
DOI = {{10.1017/S0376892997000088}},
ISSN = {{0376-8929}},
Unique-ID = {{ISI:A1997XV97800008}},
}

@article{ ISI:A1997WJ33800014,
Author = {Patterson, HW},
Title = {{Setting standards for radiation protection: The process appraised}},
Journal = {{HEALTH PHYSICS}},
Year = {{1997}},
Volume = {{72}},
Number = {{3}},
Pages = {{450-457}},
Month = {{MAR}},
Abstract = {{Present radiation protection standards are based to a large extent on
   data that have been forced to conform with the linear non-threshold
   model. A review of the literature shea that there are examples of both
   data and theory that disagree with such a model. Established standard
   setting bodies seem not to have recognized this disagreement; indeed, as
   will be shown, there are many studies that they have neither cited,
   discussed, nor refuted. Additionally, examples of data adaptation and
   circular reasoning are to be found in the standard-setting process.
   Consequently, a new approach to the process is desirable. Numerous
   citations and quotations are given.}},
DOI = {{10.1097/00004032-199703000-00013}},
ISSN = {{0017-9078}},
EISSN = {{1538-5159}},
Unique-ID = {{ISI:A1997WJ33800014}},
}

@article{ ISI:A1997XX63800008,
Author = {Sullivan, PJ},
Title = {{Modelling the hazards posed by sudden release of a quantity of
   contaminants}},
Journal = {{ENVIRONMENTAL MODELLING \& SOFTWARE}},
Year = {{1997}},
Volume = {{12}},
Number = {{1}},
Pages = {{59-65}},
Abstract = {{The consequence of an accidental release of toxic, flammable or noxious
   material into an environmental flow needs to be assessed in terms of the
   territory that is subjected to the contaminant at unacceptable
   concentration levels. Environmental flows are turbulent and generally
   unsteady and inhomogeneous such that the contaminant concentration
   values in question are non-stationary, inhomogeneous random variables.
   Thus the practically important, basic, problem of describing the
   evolution of the concentration field in a contaminant cloud presents
   some serious theoretical and experimental challenges. A new approach is
   developed to describe the diffusion of a contaminant cloud in terms of
   its location, size and state. The state of the cloud will characterize
   the dilution of the contaminant concentration values within the cloud. A
   new measure-the `expected mass fraction' function-is introduced to
   describe the state of cloud dilution. The advantages of this approach
   are discussed in terms of the experimental difficulties associated with
   taking averages. The theoretical advantages that follow from this new
   approach are illustrated in terms of recent, simple, models of the
   evolution of the moments of the one-point probability density function
   of concentration. (C) 1997 Elsevier Science Ltd.}},
DOI = {{10.1016/S1364-8152(96)00027-8}},
ISSN = {{1364-8152}},
EISSN = {{1873-6726}},
Unique-ID = {{ISI:A1997XX63800008}},
}

@article{ ISI:A1997XE59000002,
Author = {Neumann, K and Moegelin, A and Temminghoff, N and Radlanski, RJ and
   Langford, A and Unger, M and Langer, R and Bier, J},
Title = {{3D-Computed tomography: A new method for the evaluation of fetal cranial
   morphology}},
Journal = {{JOURNAL OF CRANIOFACIAL GENETICS AND DEVELOPMENTAL BIOLOGY}},
Year = {{1997}},
Volume = {{17}},
Number = {{1}},
Pages = {{9-22}},
Month = {{JAN-MAR}},
Abstract = {{This study is the first presentation of three-dimensional computed
   tomography (3D-CT) for the in vitro evaluation of the prenatal human
   cranium. The study was based on CT examinations from 26 aborted normal
   fetuses between 10 and 25 weeks gestational age, Incremental coronal and
   transverse CT slices of 1 mm thickness and a threshold segmentation
   algorithm were used to generate 3D-CT reconstructions (surface-shaded
   display, SSD) of the cranial bones similar to their anatomical
   appearance. The threshold of the segmentation algorithm was selected
   after comparison of the 3D-CT images generated with varying thresholds
   and graphically reconstructed histological serial sections of particular
   sutures in five specimens. The variation of the segmentation threshold
   resulted in alterations of the bone sizes and suture widths. However,
   3D-CT images allowed sensitive identification of the cranial
   ossification centers and accurate evaluation of the bone topography,
   Cutting and rotating procedures made it possible to evaluate all imaged
   bones in arbitrary views without disturbing superpositions, thus making
   isolated examinations of particular macroscopic sections of the
   specimens unnecessary. In conclusion, 3D-CT of the fetal cranium
   promises to be of considerable help in the evaluation of prenatal
   cranial development.}},
ISSN = {{0270-4145}},
Unique-ID = {{ISI:A1997XE59000002}},
}

@article{ ISI:A1996VV65300010,
Author = {Papageorgiou, S and Almirantis, Y},
Title = {{Gradient model describes the spatial-temporal expression pattern of Hoxa
   genes in the developing vertebrate limb}},
Journal = {{DEVELOPMENTAL DYNAMICS}},
Year = {{1996}},
Volume = {{207}},
Number = {{4}},
Pages = {{461-469}},
Month = {{DEC}},
Abstract = {{Pattern formation of the developing vertebrate limb is mainly controlled
   by the zone of polarizing activity (ZPA) and the apical ectodermal ridge
   (AER) which may act as sources of diffusing morphogens. These sources
   are tightly interconnected and maintained by positive feedback and,
   together with the established role of Wnt7a on the dorsal side of the
   bud, they constitute a cartesian reference frame for the processes of
   patterning and growth of the limb bud, As an input to our model we have
   used the local extent and temporal activity of the AER source as it is
   reflected by Fgf-4 expression in the ridge. We have assumed that this
   source produces a morphogen which diffuses in the three-dimensional limb
   field and degradates by first-order kinetics. When in a cell the
   morphogen concentration exceeds a particular threshold value, a gene is
   switched on. To every threshold corresponds a specific gene. In the
   following we introduce an order of increasing concentration thresholds
   corresponding to the sequence of Hoxa-10, 11, and 13 genes (threshold
   collinearity). With this simple rule of correspondence we can reproduce
   both spatial and temporal collinearities of Hoxa gene expression. This
   outcome may be the first direct observable effect of a putative
   morphogen in the developing limb, The expression patterns are
   essentially transient, and they are followed by sequential refinements
   which lead to the final limb structures, Furthermore, the continuous
   flow of the morphogen through the progress zone guarantees the coherent
   course of patterning and limb growth. Several experiments are proposed
   for additional tests of the validity of the model and the eventual
   reversibility of Hoxa gene expression. (C) 1996 Wiley-Liss, Inc.}},
ISSN = {{1058-8388}},
Unique-ID = {{ISI:A1996VV65300010}},
}

@article{ ISI:A1996VV23700004,
Author = {IbarraMolero, B and SanchezRuiz, JM},
Title = {{A model-independent, nonlinear extrapolation procedure for the
   characterization of protein folding energetics from solvent-denaturation
   data}},
Journal = {{BIOCHEMISTRY}},
Year = {{1996}},
Volume = {{35}},
Number = {{47}},
Pages = {{14689-14702}},
Month = {{NOV 26}},
Abstract = {{We have characterized the guanidine-induced denaturation of hen egg
   white lysozyme within the 30-75 degrees C temperature range on the basis
   of equilibrium fluorescence measurements, unfolding assays, kinetic
   fluorescence measurements, and differential scanning calorimetry.
   Analysis of the guanidine denaturation profiles according to the linear
   extrapolation method yields values for the denaturation Gibbs energy
   which are about 15 kJ/mol lower than those derived from differential
   scanning calorimetry. Our results strongly suggest that this discrepancy
   is not due to deviations from the two-state denaturation mechanism. We
   propose a new method for the determination of denaturation Gibbs
   energies from solvent-denaturation data (the constant-Delta G
   extrapolation procedure). It employs several solvent-denaturation
   profiles (obtained at different temperatures) to generate the protein
   stability curve at zero denaturant concentration within the -8 to 8
   kJ/mol Delta G range. The method is model-independent and provides a
   practical, nonlinear alternative to the commonly employed linear
   extrapolation procedure. The application of the constant-Delta G method
   to our data suggests that the guanidine-concentration dependence of the
   denaturation Gibbs energy is approximately linear over an extended
   concentration range but, also, that strong deviations from linearity may
   occur at low guanidine concentrations. We tentatively attribute these
   deviations to the abrupt change of the contribution to protein stability
   that arises from pairwise charge-charge electrostatic interactions. This
   contribution may be positive, negative, or close to zero, depending on
   the pH value and the charge distribution on the native protein surface
   {[}Yang, A.-S., \& Honig, B. (1993) J. Mol. Biol. 231, 459-474], which
   may help to explain why disparate effects have been found when studying
   protein denaturation at low guanidine concentrations. Kinetic nz values
   for lysozyme denaturation depend on temperature, in a manner which
   appears consistent with Hammond behavior.}},
DOI = {{10.1021/bi961836a}},
ISSN = {{0006-2960}},
ResearcherID-Numbers = {{Sanchez-Ruiz, Jose/H-2849-2015}},
ORCID-Numbers = {{Sanchez-Ruiz, Jose/0000-0002-9056-3928}},
Unique-ID = {{ISI:A1996VV23700004}},
}

@article{ ISI:A1996VE42900012,
Author = {Christakos, G and Hristopulos, DT},
Title = {{Characterization of atmospheric pollution by means of stochastic
   indicator parameters}},
Journal = {{ATMOSPHERIC ENVIRONMENT}},
Year = {{1996}},
Volume = {{30}},
Number = {{22}},
Pages = {{3811-3823}},
Month = {{NOV}},
Abstract = {{Deriving indicator parameters for determining regions at risk and
   reducing uncertainties is an important part of atmospheric pollution
   characterization. In this paper we introduce regional indicator
   parameters that characterize contamination levels by means of stochastic
   analysis. The expressions obtained are general, and can be used for
   different types and distributions of pollutant concentrations, We
   calculate indicator parameters that represent excess contamination above
   a threshold level specified by the environmental standards, and we study
   certain scaling properties of the level-crossing contours. Regional
   indicator parameters are expressed in terms of the pollutant probability
   distribution, and are calculated explicitly for specific cases. Certain
   properties of the indicator parameters related to scale effects are also
   discussed. An application involving a sulfate deposition data set is
   discussed, which provides valuable insight regarding stochastic
   indicator parameters and their importance in environmental policy and
   decision making. Copyright (C) 1996 Elsevier Science Ltd.}},
DOI = {{10.1016/1352-2310(96)00083-0}},
ISSN = {{1352-2310}},
ORCID-Numbers = {{HRISTOPULOS, DIONISSIOS/0000-0002-5189-5612
   Christakos, George/0000-0002-1865-5764}},
Unique-ID = {{ISI:A1996VE42900012}},
}

@article{ ISI:A1996UY27300012,
Author = {Tan, RKZ and Sprous, D and Harvey, SC},
Title = {{Molecular dynamics simulations of small DNA plasmids: Effects of
   sequence and supercoiling on intramolecular motions}},
Journal = {{BIOPOLYMERS}},
Year = {{1996}},
Volume = {{39}},
Number = {{2}},
Pages = {{259-278}},
Month = {{AUG}},
Abstract = {{Small (600 base pair) DNA plasmids were modeled with a simplified
   representation (3DNA) and the intramolecular motions were studied using
   molecular mechanics and molecular dynamics techniques. The model is
   detailed enough to incorporate sequence effects. At the same time, it is
   simple enough to allow long molecular dynamics simulations. The
   simulations revealed that large-scale slithering occurs in a homogeneous
   sequence. In a heterogeneous sequence, containing numerous small
   intrinsic curves, the centers of the curves are preferentially
   positioned at the tips of loops. With more curves than loop tips (two in
   unbranched supercoiled DNA), the heterogeneous sequence plasmid slithers
   short distances to reposition other curves into the loop tips. However,
   the DNA is immobilized most of the time, with the loop tips positioned
   over a few favored curve centers. Branching or looping also appears in
   the heterogeneous sequence as a new method of repositioning the loop
   tips. Instead of a smooth progression of increasing writhing with
   increasing linking difference, theoretical studies have predicted that
   there is a threshold between unwrithed and writhed DNA at a linking
   difference between one and two. This has previously been observed in
   simulations of static structures and is demonstrated here for dynamic
   homogeneous closed DNA. Such an abrupt transition is not found in the
   heterogeneous sequence in both the static and dynamic cases. (C) 1996
   John Wiley \& Sons, Inc.}},
DOI = {{10.1002/(SICI)1097-0282(199608)39:2<259::AID-BIP12>3.0.CO;2-9}},
ISSN = {{0006-3525}},
Unique-ID = {{ISI:A1996UY27300012}},
}

@article{ ISI:A1996UZ02700006,
Author = {Bascompte, J and Sole, RV},
Title = {{Habitat fragmentation and extinction thresholds in spatially explicit
   models}},
Journal = {{JOURNAL OF ANIMAL ECOLOGY}},
Year = {{1996}},
Volume = {{65}},
Number = {{4}},
Pages = {{465-473}},
Month = {{JUL}},
Abstract = {{1. The incidence of habitat destruction on the survivorship of a single
   metapopulation is studied by means of a spatially explicit model.
   2. As the proportion of destroyed sites increases, the structural
   properties of the resulting landscape change in a non-linear way,
   showing the existence of critical thresholds and phase transitions.
   3. Such critical thresholds are identified by means of an order
   parameter, which discriminates a quantitative process, i.e. habitat
   loss, from a qualitative one, i.e. habitat fragmentation, This
   difference is only well understood using a spatially explicit framework.
   4. We introduce on such a fragmented landscape the dynamics of a
   metapopulation balanced by local colonization and extinction by means of
   the cellular automaton formalism.
   5. The existence of extinction thresholds when a given fraction of
   habitat is destroyed is reported. These thresholds are determined both
   by the critical behaviour of the landscape structural properties, and by
   the demographic properties of the metapopulation.
   6. Some differences between these results and those derived from the
   study of spatially implicit models are described and explained. In
   particular, the percentage of patch occupancy is lower for a given value
   of habitat destruction in the spatially explicit formulation. Extinction
   threshold also take place for a lower destruction value. Some
   implications for the management of natural landscapes are discussed.}},
DOI = {{10.2307/5781}},
ISSN = {{0021-8790}},
ResearcherID-Numbers = {{Bascompte, Jordi/B-7596-2008
   Sole, Ricard/I-3379-2015}},
ORCID-Numbers = {{Bascompte, Jordi/0000-0002-0108-6411
   }},
Unique-ID = {{ISI:A1996UZ02700006}},
}

@article{ ISI:A1996UR81500013,
Author = {Baird, SJS and Cohen, JT and Graham, JD and Shlyakhter, AI and Evans, JS},
Title = {{Noncancer risk assessment: A probabilistic alternative to current
   practice}},
Journal = {{HUMAN AND ECOLOGICAL RISK ASSESSMENT}},
Year = {{1996}},
Volume = {{2}},
Number = {{1}},
Pages = {{79-102}},
Month = {{MAR}},
Abstract = {{Based on imperfect data and theory, agencies such as the United States
   Environmental Protection Agency (USEPA) currently derive `'reference
   doses'' (RfDs) to guide risk managers charged with ensuring that human
   exposures to chemicals are below population thresholds. The RfD for a
   chemical is typically reported as a single number, even though it is
   widely acknowledged that there are significant uncertainties inherent in
   the derivation of this number.
   In this article, the authors propose a probabilistic alternative to the
   EPA's method that expresses the human population threshold as a
   probability distribution of values (rather than a single RfD value),
   taking into account the major sources of scientific uncertainty in such
   estimates. The approach is illustrated using much of the same data that
   USEPA uses to justify their current RfD procedure.
   Like the EPA's approach, our approach recognizes the four key
   extrapolations that are necessary to define the human population
   threshold based on animal data: animal to human, human heterogeneity,
   LOAEL to NOAEL, and subchronic to chronic. Rather than using available
   data to define point estimates of `'uncertainty factors'' for these
   extrapolations, the proposed approach uses available data to define a
   probability distribution of adjustment factors. These initial
   characterizations of uncertainty can then be refined when more robust or
   specific data become available for a particular chemical or class of
   chemicals.
   Quantitative characterization of uncertainty in noncancer risk
   assessment will be useful to risk managers who face complex trade-offs
   between control costs and protection of public health. The new approach
   can help decision-makers understand how much extra control cost must be
   expended to achieve a specified increase in confidence that the human
   population threshold is not being exceeded.}},
DOI = {{10.1080/10807039.1996.10387463}},
ISSN = {{1080-7039}},
EISSN = {{1549-7860}},
Unique-ID = {{ISI:A1996UR81500013}},
}

@article{ ISI:A1996TQ04900004,
Author = {Thomas, RS and Bigelow, PL and Keefe, TJ and Yang, RSH},
Title = {{Variability in biological exposure indices using physiologically based
   pharmacokinetic modeling and Monte Carlo simulation}},
Journal = {{AMERICAN INDUSTRIAL HYGIENE ASSOCIATION JOURNAL}},
Year = {{1996}},
Volume = {{57}},
Number = {{1}},
Pages = {{23-32}},
Month = {{JAN}},
Abstract = {{By using physiologically based pharmacokinetic (PBPK) modeling coupled
   with Monte Carte simulation,the interindividual variability in the
   concentrations of chemicals in a worker's exhaled breath and urine were
   estimated and compared with existing biological exposure indices (BEIs).
   The PBPK model simulated an exposure regimen similar to a typical
   workday, while exposure concentrations were set to equal the ambient
   threshold limit values (TLV(R)s) of six industrial solvents (benzene,
   chloroform, carbon tetrachloride, methylene chloride, methyl chloroform,
   and trichloroethylene). Based on model predictions incorporating
   interindividual variability, the percentage of population protected was
   derived using TLVs as the basis for worker protection. Results showed
   that current BEIs may not protect the majority or all of the workers in
   an occupational setting. For instance, current end-expired air indices
   for benzene and methyl chloroform protect 95\% and less than 10\% of the
   worker population, respectively. Urinary metabolite concentrations for
   benzene, methyl chloroform, and trichloroethylene were also estimated.
   The current BEI recommendation for phenol metabolite concentration at
   the end-of-shift sampling interval was estimated to protect 68\% of the
   worker population, while trichloroacetic acid (TCAA) and
   trichloroethanol (TCOH) concentrations for methyl chloroform exposure
   were estimated to protect 54\% and 97\%, respectively. The recommended
   concentration of TCAA in urine as a determinant of trichloroethylene
   exposure protects an estimated 84\% of the workers. Although many of the
   existing BEIs considered appear to protect a majority of the worker
   population, an inconsistent proportion of the population is protected.
   The information presented in this study may provide a new approach for
   administrative decisions establishing BEIs and allow uniform application
   of biological monitoring among different chemicals.}},
DOI = {{10.1080/15428119691015188}},
ISSN = {{0002-8894}},
ORCID-Numbers = {{Thomas, Russell/0000-0002-2340-0301}},
Unique-ID = {{ISI:A1996TQ04900004}},
}

@article{ ISI:A1995RR11500006,
Author = {SUWA, M and HIROKAWA, T and MITAKU, S},
Title = {{A CONTINUUM THEORY FOR THE PREDICTION OF LATERAL AND ROTATIONAL
   POSITIONING OF ALPHA-HELICES IN MEMBRANE-PROTEINS - BACTERIORHODOPSIN}},
Journal = {{PROTEINS-STRUCTURE FUNCTION AND GENETICS}},
Year = {{1995}},
Volume = {{22}},
Number = {{4}},
Pages = {{363-377}},
Month = {{AUG}},
Abstract = {{We have developed a new method for the prediction of the lateral and the
   rotational positioning of transmembrane helices, based upon the present
   status of knowledge about the dominant interaction of the tertiary
   structure formation, The basic assumption about the interaction is that
   the interhelix binding is due to the polar interactions and that very
   short extramembrane loop segments restrict the relative position of the
   helices, Another assumption is made for the simplification of the
   prediction that a helix may be regarded as a continuum rod having polar
   interaction fields around it, The polar interaction field is calculated
   by a probe helix method, using a copolymer of serine and alanine as
   probe helices, The lateral position of helices is determined by the
   strength of the interhelix binding estimated from the polar interaction
   field together with the length of linking loop segments, The rotational
   positioning is determined by the polar interaction field, assuming the
   optimum lateral configuration. The structural change due to the binding
   of a prosthetic group is calculated, fixing the rotational freedom of a
   helix that is connected to the prosthetic group, Applying this method to
   bacteriorhodopsin, the optimum lateral and rotational positioning of
   transmembrane helices that are very similar to the experimental
   configuration was obtained, This method was implemented by a software
   system, which was developed for this work, and automatic calculation
   became possible for membrane proteins comprised of several transmembrane
   helices. (C) 1995 Wiley-Liss, Inc.}},
DOI = {{10.1002/prot.340220407}},
ISSN = {{0887-3585}},
ResearcherID-Numbers = {{Suwa, Makiko/M-5056-2018}},
ORCID-Numbers = {{Suwa, Makiko/0000-0001-8812-1023}},
Unique-ID = {{ISI:A1995RR11500006}},
}

@article{ ISI:A1995RD51300005,
Author = {ALTMANN, M},
Title = {{SUSCEPTIBLE-INFECTED-REMOVED EPIDEMIC MODELS WITH DYNAMIC PARTNERSHIPS}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{1995}},
Volume = {{33}},
Number = {{6}},
Pages = {{661-675}},
Month = {{JUN}},
Abstract = {{The author extends the classical, stochastic,
   Susceptible-Infected-Removed (SIR) epidemic model to allow for disease
   transmission through a dynamic network of partnerships. A new method of
   analysis allows for a fairly complete understanding of the dynamics of
   the system for small and large time. The key insight is to analyze the
   model by tracking the configurations of all possible dyads, rather than
   individuals. For large populations, the initial dynamics are
   approximated by a branching process whose threshold for growth
   determines the epidemic threshold, R(0), and whose growth rate, Lambda,
   determines the rate at which the number of cases increases. The fraction
   of the population that is ever infected, Omega, is shown to bear the
   same relationship to R(0) as in models without partnerships. Explicit
   formulas for these three fundamental quantities are obtained for the
   simplest version of the model, in which the population is treated as
   homogeneous, and all transitions are Markov. The formulas allow a
   modeler to determine the error introduced by the usual assumption of
   instantaneous contacts for any particular set of biological and
   sociological parameters. The model and the formulas are then generalized
   to allow for non-Markov partnership dynamics, non-uniform contact rates
   within partnerships, and variable infectivity. The model and the method
   of analysis could also be further generalized to allow for demographic
   effects, recurrent susceptibility and heterogeneous populations, using
   the same strategies that have been developed for models without
   partnerships.}},
DOI = {{10.1007/BF00298647}},
ISSN = {{0303-6812}},
Unique-ID = {{ISI:A1995RD51300005}},
}

@article{ ISI:A1995RP37200001,
Author = {ROWAN, DJ and CORNETT, RJ and KING, K and RISTO, B},
Title = {{SEDIMENT FOCUSING AND PB-210 DATING - A NEW APPROACH}},
Journal = {{JOURNAL OF PALEOLIMNOLOGY}},
Year = {{1995}},
Volume = {{13}},
Number = {{2}},
Pages = {{107-118}},
Month = {{MAR}},
Abstract = {{In this paper we test the utility of the mud deposition boundary depth
   (mud DBD) theory (Rowan ct al. 1992) as a means of maximizing sampling
   efficiency in paleolimnological investigations, particularly those that
   apply to Pb-210 dating. The mud DBD is defined by the relationship
   between near bottom wave velocity and particle threshold velocity, with
   wave and particle threshold theory simplified to terms of exposure and
   depth. Mud DBD theory can be used to define the depositional zone in
   lakes, and within the depositional zone defined by the mud DBD: 1) there
   is a high probability of obtaining a representative core, 2) variation
   in mass sediment accumulation rate (MSAR) is not correlated with water
   depth, and 3) variation in MSAR is considerably reduced from the whole
   lake average. This suggests that mud DBD theory can account for the
   effects of sediment focusing, and that the mud DBD defined depositional
   zone is the zone to which fine-grained sediments are focused. Finally,
   we have shown that to optimize sampling effort, 5 to 10 cores within the
   depositional zone are necessary for a reasonably precise estimate of the
   mean mass sediment accumulation rate. In addition, the use of mud DBD
   theory prior to sampling can dramatically reduce the cost associated
   with analyzing large numbers of cores for Pb-210.}},
DOI = {{10.1007/BF00678101}},
ISSN = {{0921-2728}},
Unique-ID = {{ISI:A1995RP37200001}},
}

@article{ ISI:A1995QL76100002,
Author = {RAND, DA and WILSON, HB},
Title = {{USING SPATIO-TEMPORAL CHAOS AND INTERMEDIATE-SCALE DETERMINISM TO
   QUANTIFY SPATIALLY EXTENDED ECOSYSTEMS}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{1995}},
Volume = {{259}},
Number = {{1355}},
Pages = {{111-117}},
Month = {{FEB 22}},
Abstract = {{We discuss the question of how to quantify and analyse dynamics and
   patterns in spatially extended ecologies and introduce several new tools
   and ideas which use space-time dynamical structure. To illustrate our
   ideas, we introduce an artificial ecology model of a
   resource-predator-prey community which is interesting in its own right
   both ecologically and mathematically. This is a generalized
   probabilistic cellular automata. The model is stochastic and spatially
   non-homogeneous, We show hers to identify a spatial scale intermediate
   between the noise dominated microscale and the infinite size limit at
   which non-trivial determinism is maximized. This is the scale at which
   to measure the system's dynamics. At this stale the population dynamics
   are essentially deterministic, low-dimensional and chaotic. This allows
   us to characterize the complex spatial patterns by a low-dimensional
   vector. This mapping from spatial patterns to low dimensional vectors
   provides effective and faithful data compression and is a powerful
   technique for synthesizing ecological information. It facilitates new
   analytical techniques. As an application we consider how to distinguish
   structural change within an ecosystem from natural dynamics. Such change
   is detected by using our parameterization to construct recurrence plots.
   Other applications such as the reconstruction of the dynamics of
   invisible species are discussed elsewhere.}},
DOI = {{10.1098/rspb.1995.0017}},
ISSN = {{0962-8452}},
ResearcherID-Numbers = {{Rand, David/J-9085-2012}},
ORCID-Numbers = {{Rand, David/0000-0002-2217-3274}},
Unique-ID = {{ISI:A1995QL76100002}},
}

@article{ ISI:A1994QB14000018,
Author = {BARBUR, JL and HARLOW, AJ and PLANT, GT},
Title = {{INSIGHTS INTO THE DIFFERENT EXPLOITS OF COLOR IN THE VISUAL-CORTEX}},
Journal = {{PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{1994}},
Volume = {{258}},
Number = {{1353}},
Pages = {{327-334}},
Month = {{DEC 22}},
Abstract = {{A new method that allows controlled masking of luminance contrast has
   been developed to study the use of chromatic signals in human vision.
   The method also makes it possible to examine the different uses of
   chromatic signals (e.g. the generation of perceived colour, or the
   construction and representation of object structure and form). By using
   this technique, we studied the threshold detection of chromatic signals
   in normal trichromats. The results show that chromatic signals are
   virtually unaffected by ongoing, randomly varying, luminance contrast
   changes. These findings suggest that chromatic signals are either
   processed independently or can be separated completely from any
   confounding luminance contrast components in the stimulus. Thresholds
   for detection of colour changes only, and for extraction of stimulus
   structure from chromatic signals in normal trichromats, in subjects with
   single cone receptor deficiency (i.e. dichromats) and in three subjects
   with abnormal colour vision caused by bilateral damage to ventromedial,
   extra-striate visual cortex (i.e. subjects with cerebral achromatopsia)
   have also been measured. No significant difference in thresholds for the
   two conditions was observed either in normal trichromats or in
   dichromats. Subjects with cerebral achromatopsia, however, reveal
   markedly different thresholds. The results suggest that chromatic
   signals are processed independently to generate perceived object colour
   or to construct spatially structured objects, and that these functions
   involve different neural substrates. The results help to explain, at
   least in part, why cerebral achromatopsia is a het and why there can be
   significant differences in the effective use of chromatic signals in
   subjects described as cerebral achromatopsics.}},
DOI = {{10.1098/rspb.1994.0181}},
ISSN = {{0962-8452}},
Unique-ID = {{ISI:A1994QB14000018}},
}

@article{ ISI:A1994NZ88700004,
Author = {BRUELHEIDE, H and FLINTROP, T},
Title = {{ARRANGING PHYTOSOCIOLOGICAL TABLES BY SPECIES-RELEVE GROUPS}},
Journal = {{JOURNAL OF VEGETATION SCIENCE}},
Year = {{1994}},
Volume = {{5}},
Number = {{3}},
Pages = {{311-316}},
Month = {{JUN}},
Abstract = {{A new method of arranging phytosociological tables by species-releve
   groups (blocks) on the basis of the density of rows and columns
   (relative species frequency and relative species number) is presented.
   Each block fulfils a given minimum criterion regarding its density. With
   a density threshold of e.g. 50 \%, blocks are formed with only those
   species that are present in at least 50 \% of the block's releves, while
   at the same time the block's releves comprise at least 50 \% of the
   block's species. The procedure starts with the complete table and
   iteratively masks species and releves with the smallest densities. This
   is performed step by step with species and releves and continues until
   the given density threshold for all species and releves is exceeded.
   Without any further parameters other than the minimum density,
   species-releve groups of gradually decreasing size are formed. Data
   processing is performed with a computer program (ESPRESSO). The main
   application of the method is found in an effective pre-sorting of releve
   data. The blocks formed can be arranged in the final table as desired.}},
DOI = {{10.2307/3235854}},
ISSN = {{1100-9233}},
ResearcherID-Numbers = {{Bruelheide, Helge/G-3907-2013}},
ORCID-Numbers = {{Bruelheide, Helge/0000-0003-3135-0356}},
Unique-ID = {{ISI:A1994NZ88700004}},
}

@article{ ISI:A1994NH26500016,
Author = {NAMGUNG, YY and YANG, MCK},
Title = {{OUTLIER REDUCTION BY AN OPTION-3 MEASUREMENT SCHEME}},
Journal = {{BIOMETRICS}},
Year = {{1994}},
Volume = {{50}},
Number = {{1}},
Pages = {{173-182}},
Month = {{MAR}},
Abstract = {{Detecting changes in longitudinal data is important in medical research.
   However, the existence of measurement outliers can cause an unexpected
   increase in the false alarm rate in claiming changes. To reduce the
   outliers, a new method has been developed. In this scheme, two measures
   are initially taken and, if they are closer than a specified threshold,
   the average of the two is considered to be the estimate of the true
   mean; otherwise a third measurement is taken, and the mean of the
   closest pair is considered to be the estimate. It is shown that this
   method has considerable sample size advantage over naive repeated
   measurements. Moreover, this scheme is robust for outlier error
   distribution. Evidence on outlier removal in dental attachment probing
   is used as an example.}},
DOI = {{10.2307/2533207}},
ISSN = {{0006-341X}},
Unique-ID = {{ISI:A1994NH26500016}},
}

@article{ ISI:A1994MV15600004,
Author = {BARBUR, JL and HARLOW, AJ and WEISKRANTZ, L},
Title = {{SPATIAL AND TEMPORAL RESPONSE PROPERTIES OF RESIDUAL VISION IN CASE OF
   HEMIANOPIA}},
Journal = {{PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES}},
Year = {{1994}},
Volume = {{343}},
Number = {{1304}},
Pages = {{157-166}},
Month = {{JAN 29}},
Abstract = {{Residual vision in subjects with damage of the primary visual cortex
   (striate cortex) has been demonstrated in many previous studies and is
   taken to reflect the properties of known subcortical and extrastriate
   visual pathways. In this report we describe psychophysical experiments
   carried out on a subject clinically blind in half of his visual field
   (i.e. homonymous hemianopia) caused by striate cortex damage. They
   reveal the existence of two distinct channels mediating such vision. One
   channel responds to spatial structure and the other to light flux
   changes. The spatially tuned channel has a peak response at about 1.2
   cycles per degree and shows rapid loss of sensitivity at both high and
   low spatial frequencies. This channel does not respond to diffuse
   illumination. The light flux channel, however, responds only to sudden
   increments in light flux levels on the retina and shows extensive
   spatial summation. Both channels require transient inputs, with a peak
   sensitivity at about 10 cycles per second and show virtually complete
   attenuation at temporal frequencies below 2 cycles per second. The
   spatiotemporal characteristics of these two channels account for much of
   the reported limits of visual performance attributed to subcortical or
   extrastriate pathways in some patients, and especially for their
   relatively good sensitivity for the detection of abrupt, transient
   stimuli or fast-moving targets. A new method is also applied to the
   measurement of the amount of light scatter in the eye. The measurements
   show that light scatter into the sighted hemifield could not account for
   the results obtained with the stimuli used to characterize the residual
   vision of this subject.}},
DOI = {{10.1098/rstb.1994.0018}},
ISSN = {{0962-8436}},
EISSN = {{1471-2970}},
Unique-ID = {{ISI:A1994MV15600004}},
}

@article{ ISI:A1994NQ63400009,
Author = {LAURENCELLE, L and QUIRION, A and NADEAU, S},
Title = {{LACTATE THRESHOLD DETERMINATION - A MONTE-CARLO COMPARISON OF 2
   INTERPOLATION METHODS}},
Journal = {{ARCHIVES INTERNATIONALES DE PHYSIOLOGIE DE BIOCHIMIE ET DE BIOPHYSIQUE}},
Year = {{1994}},
Volume = {{102}},
Number = {{1}},
Pages = {{43-49}},
Month = {{JAN-FEB}},
Abstract = {{A heuristic model of the relation between blood lactate (L) and VO2
   during exercise was used to assess the comparative merits of two methods
   of reference threshold determination : the habitual linear interpolation
   method (LT), and a new method using inverse parabolic (second degree)
   interpolation (PT); the new method capitalizes on the demonstrated
   curvilinear relation between blood lactate and O2 uptake. Both types of
   interpolated thresholds were computed, and their error evaluated against
   the `'true'' 4-mmol lactate threshold. A combined parametric and Monte
   Carlo investigation showed that parabolic thresholds are generally
   superior, being less biassed and more precise than their linear
   counterpart.}},
DOI = {{10.3109/13813459408996104}},
ISSN = {{0003-9799}},
Unique-ID = {{ISI:A1994NQ63400009}},
}

@article{ ISI:A1994MT79600004,
Author = {MACKEN, CA and LEVIN, SA and WALDSTATTER, R},
Title = {{THE DYNAMICS OF BACTERIA-PLASMID SYSTEMS}},
Journal = {{JOURNAL OF MATHEMATICAL BIOLOGY}},
Year = {{1994}},
Volume = {{32}},
Number = {{2}},
Pages = {{123-145}},
Month = {{JAN}},
Abstract = {{We introduce a general model for the dynamics of a single plasmid type
   and a single bacterial cell type, following Stewart and Levin (1977) in
   subdividing the population into plasmid-bearing and plasmid-free cells.
   For the particular case of mortality being a linear function of
   population sizes, we demonstrate the existence of multiple stable states
   and threshold values, thus illustrating that, in some cases, the
   establishment of a population of plasmids may depend on introduction of
   plasmids at sufficiently high levels. We also analyze in detail, for a
   general monotonically increasing mortality function, the
   `'epidemiological'' case in which plasmids confer a net cost on their
   hosts, and demonstrate that it is possible for such plasmids to become
   established. Stewart and Levin previously demonstrated this effect in a
   more restricted model.}},
DOI = {{10.1007/BF00163028}},
ISSN = {{0303-6812}},
Unique-ID = {{ISI:A1994MT79600004}},
}

@article{ ISI:A1993MF48000001,
Author = {KUNG, HT and YING, LG and LIU, YC},
Title = {{FUZZY CLUSTERING ANALYSIS IN ENVIRONMENTAL IMPACT ASSESSMENT - A
   COMPLEMENT TOOL TO ENVIRONMENTAL-QUALITY INDEX}},
Journal = {{ENVIRONMENTAL MONITORING AND ASSESSMENT}},
Year = {{1993}},
Volume = {{28}},
Number = {{1}},
Pages = {{1-14}},
Month = {{OCT}},
Abstract = {{In spite of rapid progress achieved in the methodological research
   underlying environmental impact assessment (EIA), the problem of
   weighting various parameters has not yet been solved. This paper
   presents a new approach, fuzzy clustering analysis, which is illustrated
   with an EIA case study on Baoshan-Wusong District in Shanghai, China.
   Fuzzy clustering analysis may be used whenever a composite
   classification of environmental quality/impact incorporates multiple
   parameters. In such cases the technique may be used as a complement or
   an alternative to comprehensive assessment. In fuzzy clustering
   analysis, the classification is determined by a fuzzy relation. After a
   fuzzy similarity matrix has been established and the fuzzy relation
   stabilized, a dynamic clustering chart can be developed. Given a
   suitable threshold, the appropriate classification can be accomplished.
   The methodology is relatively simple and the results can be interpreted
   to provide valuable information to support decision making and improve
   management of the environment.}},
DOI = {{10.1007/BF00547208}},
ISSN = {{0167-6369}},
Unique-ID = {{ISI:A1993MF48000001}},
}

@article{ ISI:A1993LY16100016,
Author = {GOODMAN, EM and GREENEBAUM, B and MARRON, MT},
Title = {{ALTERED PROTEIN-SYNTHESIS IN A CELL-FREE SYSTEM EXPOSED TO A SINUSOIDAL
   MAGNETIC-FIELD}},
Journal = {{BIOCHIMICA ET BIOPHYSICA ACTA}},
Year = {{1993}},
Volume = {{1202}},
Number = {{1}},
Pages = {{107-112}},
Month = {{SEP 3}},
Abstract = {{This report describes a new approach for examining weak extremely low
   frequency (ELF) electric and magnetic field interactions with living
   systems that exploits a cell-free transcription/translation system
   derived from Escherichia coli. Using two-dimensional polyacrylamide gel
   electrophoresis we previously had determined that the level of the alpha
   subunit of RNA polymerase in intact E. coli was elevated by exposure to
   weak ELF magnetic fields. In this paper, plasmids containing the alpha,
   or both the beta,beta' subunits of the RNA polymerase from E. coli were
   placed into a cell-free expression system. When this
   transcription/translation system was exposed to a 72-Hz sinusoidal
   magnetic field in the range 0.07 to 1.1 mT (rms) for periods of 5 min to
   1 h, expression was enhanced. Weaker fields must be applied longer to
   produce an effect. For 10 min of field exposure, the threshold for an
   effect is 0.1 mT. These experiments demonstrate that an intact membrane
   is not an absolute requirement for transducing magnetic bio-effects.}},
DOI = {{10.1016/0167-4838(93)90070-8}},
ISSN = {{0006-3002}},
Unique-ID = {{ISI:A1993LY16100016}},
}

@article{ ISI:A1993LH71400003,
Author = {JUELL, JE and FUREVIK, DM and BJORDAL, A},
Title = {{DEMAND FEEDING IN SALMON FARMING BY HYDROACOUSTIC FOOD DETECTION}},
Journal = {{AQUACULTURAL ENGINEERING}},
Year = {{1993}},
Volume = {{12}},
Number = {{3}},
Pages = {{155-167}},
Abstract = {{A new method for demand feeding of salmon in sea cages where automatic
   feeders are controlled by a `food detector' is described Hydroacoustic
   detection of food pellets at 2.5 m depth was used as an indicator of
   reduced appetite. Feeding was terminated when echo energy from food
   pellets sinking through a 360-degrees acoustic beam exceeded a preset
   threshold. In an 83-day full-scale test, the food intake and growth of
   salmon whose feeding was controlled by this method (detector group) were
   compared with those of fish fed in accordance with growth rate estimates
   (control group). The specific growth rates (\% wet weight/day) were 1.01
   and 0.71 in the detector and control groups respectively. This
   difference in growth was mainly explained by a considerable higher food
   intake in the detector group. The results indicate that demand feeding
   by hydroacoustic food detection automatically adjusts food ration to
   fish appetite, so that food waste is avoided and the growth potential of
   the fish is utilized}},
DOI = {{10.1016/0144-8609(93)90008-Y}},
ISSN = {{0144-8609}},
Unique-ID = {{ISI:A1993LH71400003}},
}

@article{ ISI:A1993LN64800112,
Author = {HARPER, MJ and NELSON, ME},
Title = {{EXPERIMENTAL-VERIFICATION OF A SUPERHEATED LIQUID DROPLET (BUBBLE)
   NEUTRON DETECTOR THEORETICAL-MODEL}},
Journal = {{RADIATION PROTECTION DOSIMETRY}},
Year = {{1993}},
Volume = {{47}},
Number = {{1-4}},
Pages = {{535-542}},
Note = {{10TH INTERNATIONAL CONF ON SOLID STATE DOSIMETRY, WASHINGTON, DC, JUL
   13-17, 1992}},
Organization = {{NIH; US DOE; INT SENSOR TECHNOL; LANDAUER; NUCL TECHNOL PUBL; SCI APPL
   INT; SIEMENS GAMMASON; SOLON TECHNOL; VICTOREEN}},
Abstract = {{Superheated liquid droplet ('bubble') neutron detectors utilise
   thousands of microscopic droplets of freon-based compounds suspended in
   a viscous matrix material. Neutrons can interact with the atoms of the
   superheated liquid droplets, resulting in the formation of energetic
   charged recoil ions. These ions transfer their energy to the liquid in
   the droplets, sometimes resulting in the droplets vaporising and
   producing visible bubbles. The basis of bubble detector operation is
   identical to that of bubble chambers, which have been well characterised
   by researchers such as Wilson, Glaser, Seitz, and others since the
   1950s. Each of the microscopic superheated liquid droplets behaves like
   an independent bubble chamber. This paper presents a theoretical model
   that considers the three principal aspects of detector operation:
   nuclear reactions, charged particle energy deposition, and thermodynamic
   bubble formation. All possible nuclear reactions were examined and those
   which could reasonably result in recoil ions sufficiently energetic to
   vaporise a droplet were analysed in detail. Feasible interactions having
   adequate cross sections include clastic and inelastic scattering,
   n-proton, and n-alpha reactions. Ziegler's transport of ions in matter
   (TRIM) code was used to calculate the ions' stopping powers in various
   compounds based on the ionic energies predicted by standard scattering
   distributions. If the ions deposit enough energy in a small enough
   volume then the bubble of vapour that forms will grow to `critical'
   size, and spontaneously vaporise the rest of the entire droplet without
   further energy input. Various theories as to the vaporisation of
   droplets by ionising radiation were studied and a novel method of
   predicting the critical (minimum) energy was developed. This method can
   be used to calculate the minimum required stopping power for the ion,
   from which the threshold neutron energy is obtainable. Experimental
   verification of the model was accomplished by measuring the response of
   two different types of bubble detectors to monoenergetic thermal
   neutrons, as well as to neutrons from the polyenergetic spectra of bare
   and moderated californium spontaneous fission sources. The model's
   predicted response compared favourably with the experimental data.}},
ISSN = {{0144-8420}},
Unique-ID = {{ISI:A1993LN64800112}},
}

@article{ ISI:A1992JL74500021,
Author = {KUNIKATA, T and YAMANO, H and NAGAMURA, T and NITTA, Y},
Title = {{STUDY ON THE INTERACTION BETWEEN SOYBEAN BETA-AMYLASE AND SUBSTRATE BY
   THE STOPPED-FLOW METHOD}},
Journal = {{JOURNAL OF BIOCHEMISTRY}},
Year = {{1992}},
Volume = {{112}},
Number = {{3}},
Pages = {{421-425}},
Month = {{SEP}},
Abstract = {{The hydrolysis of substrates (maltoheptaose, maltopentaose, and
   maltotetraose) catalyzed by soybean beta-amylase {[}EC 3.2.1.2] at pH
   5.4 and 25-degrees-C was followed by monitoring small changes in the
   quenching of fluorescence due to tryptophan residues by the stopped-flow
   method. By analysis of whole time course, the dissociation constants,
   K(d)s, of enzyme-substrate and enzyme-product complexes were reasonably
   evaluated; and the difference in fluorescence intensities per mol
   between the enzyme-complex (ES or EP) and the free enzyme, DELTA-F, was
   determined. The molecular activity, k0, was also determined by a new
   method of half time analysis. The K(d)s and k0 values are in good
   agreement with our kinetic data reported previously. The DELTA-Fs of
   substrates were of smaller magnitude than those of products (G2 and G3),
   which means that the higher the binding affinity of the ligand is, the
   smaller the DELTA-F value is. This indicates that at least two
   tryptophan residues must be located in the active site if the enzyme is
   rigid, or that if there is only one, the active site must undergo a
   structural change caused by the binding of ligand.}},
DOI = {{10.1093/oxfordjournals.jbchem.a123915}},
ISSN = {{0021-924X}},
Unique-ID = {{ISI:A1992JL74500021}},
}

@article{ ISI:A1992JQ98900005,
Author = {NIBEDITA, R and KUMAR, RA and MAJUMDAR, A and HOSUR, RV},
Title = {{SIMULATION OF NOESY SPECTRA OF DNA SEGMENTS - A NEW SCALING PROCEDURE
   FOR ITERATIVE COMPARISON OF CALCULATED AND EXPERIMENTAL NOE INTENSITIES}},
Journal = {{JOURNAL OF BIOMOLECULAR NMR}},
Year = {{1992}},
Volume = {{2}},
Number = {{5}},
Pages = {{467-476}},
Month = {{SEP}},
Abstract = {{A new algorithm for simulation of two-dimensional NOESY spectra of DNA
   segments has been developed. For any given structure, NOE intensities
   are calculated using the relaxation matrix approach and a new realistic
   procedure is suggested for 1:1 comparison of calculated and experimental
   intensities. The procedure involves a novel method for scaling of
   calculated NOE intensities to represent volumes of digitised cross peaks
   in NOESY spectra. A data base of fine structures of all the relevant
   cross peaks with Lorentzian line shapes and in-phase components, is
   generated in a digitised manner by two-dimensional Fourier
   transformation of simulated time domain data, assuming a total intensity
   of 1.0 for each of the cross peaks. With this procedure, it is shown
   that the integrated volumes of these digitised cross peaks above any
   given threshold scale exactly as the total intensity of the respective
   peaks. This procedure eliminates the repetitive generation of digitised
   cross peaks by two-dimensional Fourier transformation during the
   iterative process of structure alteration and NOE intensity calculation
   and thus enhances the speed of DNA structure optimization. Illustrative
   fits of experimental and calculated spectra obtained using the new
   procedure are shown.}},
DOI = {{10.1007/BF02192809}},
ISSN = {{0925-2738}},
EISSN = {{1573-5001}},
Unique-ID = {{ISI:A1992JQ98900005}},
}

@article{ ISI:A1992HR30600003,
Author = {SAFRAN, P},
Title = {{THEORETICAL-ANALYSIS OF THE WEIGHT-LENGTH RELATIONSHIP IN FISH JUVENILES}},
Journal = {{MARINE BIOLOGY}},
Year = {{1992}},
Volume = {{112}},
Number = {{4}},
Pages = {{545-551}},
Month = {{APR}},
Abstract = {{The weight-length relationship in fish juveniles was investigated
   theoretically, to assess the significance of the allometric factor and
   the validity of the condition factor; these biological factors often
   remain undetermined, because most fishery studies have been conducted
   for commercial-sized and/or adult populations. The exponent b
   (allometric factor) seemed to be the main parameter, performing a key
   role in the equation W = aL(b), where W = weight, a is a constant and L
   = length. The parameters a (condition factor) and K (ponderal index; K =
   10(3) W/L3) were judged to be less important in comparative studies,
   since these parameters were closely correlated with b. It is recommended
   that the assumed theoretical value of b = 3 not be used in applied
   ichthyological surveys, since this value was rarely obtained in the
   studies, and since a much wider range is usually seen. These analyses
   led to a new working hypothesis - not yet verified - which opens a new
   approach to understanding the biological significance of the allometric
   factor. This approach involves the fractal theory (where b may be
   considered as a fractal dimension equivalent) linked to the theory of
   saltatory ontogeny {[}where b is a threshold characteristic in the
   (early) life history of fishes].}},
DOI = {{10.1007/BF00346171}},
ISSN = {{0025-3162}},
ORCID-Numbers = {{Safran, Patrick/0000-0002-5440-4719}},
Unique-ID = {{ISI:A1992HR30600003}},
}

@article{ ISI:A1991GT48300060,
Author = {THEILER, R and NIEDERMAN, RA},
Title = {{LOCALIZATION OF CHROMATOPHORE PROTEINS OF RHODOBACTER-SPHAEROIDES .1.
   RAPID CA2+-INDUCED FUSION OF CHROMATOPHORES WITH PHOSPHATIDYLGLYCEROL
   LIPOSOMES FOR PROTEINASE DELIVERY TO THE LUMINAL MEMBRANE-SURFACE}},
Journal = {{JOURNAL OF BIOLOGICAL CHEMISTRY}},
Year = {{1991}},
Volume = {{266}},
Number = {{34}},
Pages = {{23157-23162}},
Month = {{DEC 5}},
Abstract = {{A protease delivery system was developed for the exclusive and
   controlled digestion of proteins exposed at the morphological inside
   (periplasmic surface) of Rhodobacter sphaeroides chromatophores. In this
   procedure, proteinase K is encapsulated within large unilamellar
   liposomes which are fused to the chromatophores in the presence of Ca2+
   ions. The liposomes were prepared by a detergent dialysis procedure from
   native phosphatidylglycerol and found to undergo rapid bilayer fusion
   with purified chromatophore preparations above a threshold concentration
   of 12.5 mM CaCl2. The fusion process was complete within 10 min at 35 mM
   Ca2+ with about 80\% of the pigment located in the fusion products.
   Electron micrographs of freeze-fracture replicas confirmed the
   intermixing of the lipid bilayers and the unilamellar structure of the
   fused membrane vesicles. The procedure did not affect the labile B800
   chromophore of the B800-850 antenna complex, but reduced slightly the
   absorption due to the B875 core antenna. Emission from both
   light-harvesting complexes was increased in the fused membranes,
   suggesting a partial dissociation of photosynthetic units in the
   expanded bilayer. The results, together with those presented in the
   following paper (Theiler, R., and Niederman, R. A. (1991) J. Biol. Chem.
   266, 23163-23168), demonstrate that this new method fulfills the
   stringent requirements for a successful delivery of macromolecules to
   the chromatophore interior.}},
ISSN = {{0021-9258}},
Unique-ID = {{ISI:A1991GT48300060}},
}

@article{ ISI:A1991FY24700003,
Author = {TCHURAEV, RN},
Title = {{A NEW METHOD FOR THE ANALYSIS OF THE DYNAMICS OF THE MOLECULAR
   GENETIC-CONTROL SYSTEMS .1. DESCRIPTION OF THE METHOD OF GENERALIZED
   THRESHOLD MODELS}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{1991}},
Volume = {{151}},
Number = {{1}},
Pages = {{71-87}},
Month = {{JUL 7}},
DOI = {{10.1016/S0022-5193(05)80144-4}},
ISSN = {{0022-5193}},
Unique-ID = {{ISI:A1991FY24700003}},
}

@article{ ISI:A1991FY24700004,
Author = {PROKUDINA, EI and VALEEV, RY and TCHURAEV, RN},
Title = {{A NEW METHOD FOR THE ANALYSIS OF THE DYNAMICS OF THE MOLECULAR
   GENETIC-CONTROL SYSTEMS .2. APPLICATION OF THE METHOD OF GENERALIZED
   THRESHOLD MODELS IN THE INVESTIGATION OF CONCRETE GENETIC SYSTEMS}},
Journal = {{JOURNAL OF THEORETICAL BIOLOGY}},
Year = {{1991}},
Volume = {{151}},
Number = {{1}},
Pages = {{89-110}},
Month = {{JUL 7}},
DOI = {{10.1016/S0022-5193(05)80145-6}},
ISSN = {{0022-5193}},
Unique-ID = {{ISI:A1991FY24700004}},
}

@article{ ISI:A1991GG24800011,
Author = {JUNG, JM and KLEIN, J and VOLTZ, R},
Title = {{PHOTOIONIZATION EXCITATION-SPECTRA IN ORGANIC LIQUIDS}},
Journal = {{JOURNAL DE CHIMIE PHYSIQUE ET DE PHYSICO-CHIMIE BIOLOGIQUE}},
Year = {{1991}},
Volume = {{88}},
Number = {{6}},
Pages = {{779-790}},
Month = {{JUN}},
Abstract = {{A new approach for studying the excitation spectra of photoionization in
   liquid hydrocarbons is presented.   It is based upon the measurements of
   delayed recombination luminescence in the liquids containing small
   concentrations of fluorescent solutes.  The results obtained with liquid
   cyclopentane and 2,2,4 trimethylpentane excited by synchrotron radiation
   are reported and compared with the excitation spectra of
   photoconductivity.  In the analysis of the experimental spectra, special
   attention is given to the behavior near the threshold; the
   interpretation is based on simple assumptions concerning the physics of
   energy relaxation of the photoemitted electrons in the organic medium.}},
DOI = {{10.1051/jcp/1991880779}},
ISSN = {{0021-7689}},
Unique-ID = {{ISI:A1991GG24800011}},
}

@article{ ISI:A1991FA34800023,
Author = {MENCKE, AP and CAFFREY, M},
Title = {{KINETICS AND MECHANISM OF THE PRESSURE-INDUCED LAMELLAR ORDER-DISORDER
   TRANSITION IN PHOSPHATIDYLETHANOLAMINE - A TIME-RESOLVED
   X-RAY-DIFFRACTION STUDY}},
Journal = {{BIOCHEMISTRY}},
Year = {{1991}},
Volume = {{30}},
Number = {{9}},
Pages = {{2453-2463}},
Month = {{MAR 5}},
Abstract = {{By using synchrotron radiation, a movie was made of the X-ray scattering
   pattern from a biological liquid crystal undergoing a phase transition
   induced by a pressure jump. The system studied includes the fully
   hydrated phospholipid dihexadecylphosphatidylethanolamine in the
   lamellar gel (L-beta') phase at a temperature of 68-degrees-C and a
   pressure of 9.7 MPa (1400 psig). Following the rapid release of pressure
   to atmospheric the L-beta' phase transforms slowly into the lamellar
   liquid crystal (L-alpha) phase. The pressure perturbation is applied
   with the intention of producing a sudden phase disequilibrium followed
   by monitoring the system as it relaxes to its new equilibrium condition.
   Remarkably, the proportion of sample in the L-alpha phase grows linearly
   with time, taking 37 s to totally consume the L-beta' phase. The time
   dependencies of radius, peak intensity, and width of the powder
   diffraction ring of the low-angle (001) lamellar reflections were
   obtained from the movie by image processing. The concept of an
   ``effective pressure{''} is introduced to account for the temperature
   variations that accompany the phase transition and to establish that the
   observed large transit time is indeed intrinsic to the sample and not
   due to heat exchange with the environment. The reverse transformation,
   L-alpha to L-beta', induced by a sudden jump from atmospheric pressure
   to 9.7 MPa, is complete in less than 13 s. These measurements represent
   a new approach for studying the kinetics of lipid phase transitions and
   for gaining insights into the mechanism of the lamellar order/disorder
   transition.}},
DOI = {{10.1021/bi00223a023}},
ISSN = {{0006-2960}},
Unique-ID = {{ISI:A1991FA34800023}},
}

@article{ ISI:A1991FD18100003,
Author = {HOCEVAR, A and SEGULAILIC, A},
Title = {{MODELING RELATION OF 1ST SHOOT EMERGENCE IN VARIOUS TREE SPECIES TO
   METEOROLOGICAL PARAMETERS IN SLOVENIA}},
Journal = {{INTERNATIONAL JOURNAL OF BIOMETEOROLOGY}},
Year = {{1991}},
Volume = {{34}},
Number = {{4}},
Pages = {{221-225}},
Abstract = {{The relation of first shoot emergence in various tree species (birch,
   beech, oak and spruce) to meteorological parameters was studied on the
   basis of phenological and meteorological observations at five locations
   in Slovenia during 1967-1986.  A physical model was developed using tree
   branch temperature obtained by the energy balance equation.  The gained
   linear dependence of first shoot emergence on effective tree branch
   temperature was compared with the results obtained by multiple
   regression analysis among first shoot emergence, effective temperature,
   global radiation, wind velocity and precipitation.  A new method was
   developed to define the proper biological temperature threshold which
   was used for effective temperature calculations.  Results of the
   physical model and of the multiple regression analysis are statistically
   significant and give similar correlations between first shoot emergence
   and meteorological parameters.}},
DOI = {{10.1007/BF01041832}},
ISSN = {{0020-7128}},
Unique-ID = {{ISI:A1991FD18100003}},
}
